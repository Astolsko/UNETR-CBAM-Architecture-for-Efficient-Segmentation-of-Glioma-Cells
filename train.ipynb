{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Number of GPUs: 1\n",
      "Current GPU: NVIDIA RTX A6000\n",
      "GPU 0: NVIDIA RTX A6000\n",
      "Memory Allocated: 0.00 GB\n",
      "Memory Cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Memory Allocated: {torch.cuda.memory_allocated(i) / 1024 ** 3:.2f} GB\")\n",
    "        print(f\"Memory Cached: {torch.cuda.memory_reserved(i) / 1024 ** 3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    Resized,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    "    ToTensord,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/user/Desktop/Abul Hasan/Dataset/BraTS2020/BraTS2020_TrainingData/Combined\"\n",
    "output_dir = \"/home/user/Desktop/Abul Hasan/OUTPUT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNETR + CBAM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import cfg\n",
    "from blocks.Conv3DBlock import Conv3DBlock\n",
    "from blocks.Deconv3DBlock import Deconv3DBlock\n",
    "from blocks.CBAM import CBAM\n",
    "from blocks.Transformer import Transformer\n",
    "from blocks.SingleDeconv3DBlock import SingleDeconv3DBlock\n",
    "from blocks.SingleConv3DBlock import SingleConv3DBlock\n",
    "\n",
    "\n",
    "class UNETR(nn.Module):\n",
    "    def __init__(self, img_shape=cfg.unetr.img_shape, input_dim=cfg.unetr.input_dim, output_dim=cfg.unetr.output_dim, embed_dim=cfg.unetr.embed_dim, patch_size=cfg.unetr.patch_size, num_heads=cfg.unetr.num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.img_shape = img_shape\n",
    "        self.patch_size = patch_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = cfg.unetr.num_layers\n",
    "        self.ext_layers = cfg.unetr.extract_layers\n",
    "\n",
    "        self.patch_dim = [int(x / patch_size) for x in img_shape]\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.transformer = Transformer(\n",
    "            input_dim=input_dim,\n",
    "            embed_dim=embed_dim,\n",
    "            cube_size=img_shape,\n",
    "            patch_size=patch_size,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=dropout,\n",
    "            extract_layers=self.ext_layers\n",
    "        )\n",
    "\n",
    "        # U-Net Decoder\n",
    "        self.decoder0 = nn.Sequential(\n",
    "            Conv3DBlock(input_dim, 32, 3),\n",
    "            Conv3DBlock(32, 64, 3)\n",
    "        )\n",
    "\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            Deconv3DBlock(embed_dim, 512),\n",
    "            Deconv3DBlock(512, 256),\n",
    "            Deconv3DBlock(256, 128)\n",
    "        )\n",
    "\n",
    "        self.decoder6 = nn.Sequential(\n",
    "            Deconv3DBlock(embed_dim, 512),\n",
    "            Deconv3DBlock(512, 256),\n",
    "        )\n",
    "\n",
    "        self.decoder9 = Deconv3DBlock(embed_dim, 512)\n",
    "\n",
    "        self.decoder12_upsampler = SingleDeconv3DBlock(embed_dim, 512)\n",
    "\n",
    "        self.decoder9_upsampler = nn.Sequential(\n",
    "            Conv3DBlock(1024, 512),\n",
    "            Conv3DBlock(512, 512),\n",
    "            Conv3DBlock(512, 512),\n",
    "            CBAM(512),  # Adding CBAM\n",
    "            SingleDeconv3DBlock(512, 256)\n",
    "        )\n",
    "\n",
    "        self.decoder6_upsampler = nn.Sequential(\n",
    "            Conv3DBlock(512, 256),\n",
    "            Conv3DBlock(256, 256),\n",
    "            CBAM(256),  # Adding CBAM\n",
    "            SingleDeconv3DBlock(256, 128)\n",
    "        )\n",
    "\n",
    "        self.decoder3_upsampler = nn.Sequential(\n",
    "            Conv3DBlock(256, 128),\n",
    "            Conv3DBlock(128, 128),\n",
    "            CBAM(128),  # Adding CBAM\n",
    "            SingleDeconv3DBlock(128, 64)\n",
    "        )\n",
    "\n",
    "        self.decoder0_header = nn.Sequential(\n",
    "            Conv3DBlock(128, 64),\n",
    "            Conv3DBlock(64, 64),\n",
    "            SingleConv3DBlock(64, output_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.transformer(x)\n",
    "        z0, z3, z6, z9, z12 = x, *z\n",
    "        z3 = z3.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "        z6 = z6.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "        z9 = z9.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "        z12 = z12.transpose(-1, -2).view(-1, self.embed_dim, *self.patch_dim)\n",
    "\n",
    "        z12 = self.decoder12_upsampler(z12)\n",
    "        z9 = self.decoder9(z9)\n",
    "        z9 = self.decoder9_upsampler(torch.cat([z9, z12], dim=1))\n",
    "        z6 = self.decoder6(z6)\n",
    "        z6 = self.decoder6_upsampler(torch.cat([z6, z9], dim=1))\n",
    "        z3 = self.decoder3(z3)\n",
    "        z3 = self.decoder3_upsampler(torch.cat([z3, z6], dim=1))\n",
    "        z0 = self.decoder0(z0)\n",
    "        output = self.decoder0_header(torch.cat([z0, z3], dim=1))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import ConvertToMultiChannelBasedOnBratsClassesd\n",
    "transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=cfg.unetr.img_shape, mode=\"nearest\"),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import BratsDataset\n",
    "train_ds = BratsDataset(\n",
    "    root_dir=root_dir,\n",
    "    section=\"training\",\n",
    "    transform=transform,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4)\n",
    "val_ds = BratsDataset(\n",
    "    root_dir=root_dir,\n",
    "    section=\"validation\",\n",
    "    transform=transform,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([4, 96, 96, 96])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label shape: torch.Size([3, 96, 96, 96])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABaIAAAHcCAYAAAAk+t1cAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABEnElEQVR4nO3deXgV9d034C8BsrCFTYKUVaWixa2CgPgoraivpRVbqtJikdqnVgUV8a2Ka92K9rFK3beKS0XU1g19KlpA1BZFcaVV3LDSKqhVCGpZJPP+wcs0CYmQcIYk5L6vK9cV5syZ88tE85n5nDm/aZIkSRIAAAAAAJCRvLoeAAAAAAAAWzdFNAAAAAAAmVJEAwAAAACQKUU0AAAAAACZUkQDAAAAAJApRTQAAAAAAJlSRAMAAAAAkClFNAAAAAAAmVJEAwAAAACQKUU0kFO33HJLNGnSJJo0aRK/+MUvarWNMWPGpNt4/PHH6934ANh0jz/+ePp3d8yYMTnf/vpt9+zZM+fbbszeeeeddN8OGTKkVtvIMnNzMT6ArdEvfvGL9O/jLbfcUtfD2SSyvP7o2bNn+vuojazzeXPHR91TRFNvlP+DsrGvXJeT9c2aNWvi5ptvjgMPPDA6deoUBQUF0b179xg6dGhcf/31sWLFiroeIgBbsZtuuinN3GOPPbbCY5MnT04fGzhwYIXH/vSnP6WPffvb396SQ86p8ifxG/vKolwHYOu2sZxp27ZtXQ+x0Sn/xvnGvhTmUHvN6noAQEX//Oc/Y/jw4TF//vwKyxcvXhyLFy+OmTNnRklJSRx66KF1M0AAtnqDBg1Kv587d26Fx8r/+4UXXohVq1ZFQUHBBo9VLqkBAKry5JNPRkREYWFhHY8EyJoimnrj97//faxcuTL992GHHRZLliyJiIgrrrgi9thjj/SxXXbZZYuPLxfKyspi9erV1Qbs6tWr45BDDonnn38+IiLatm0bp5xySgwcODBWrVoVc+fOjd/+9rc5H9dnn30WLVu2zPl2AWiYdtppp2jTpk2UlpbGggULYsWKFdG6deuIiHj66afT9VavXh0vvPBCWjpvLUX00UcfHUOHDk3/ffPNN8eUKVMiIuLggw+OM844I32spKRki48vV+Q/QN2rnCsREc2aNa6qZp999qnrIcQee+yRFuIR695sP/HEEyMionPnznHPPfekjzXkwlz2U9dMzUG90a9fv9hnn33Sr/VXV0WsK57XL//iiy+ibdu2VX4ctqqPylSen/Dqq6+Onj17RsuWLeNb3/pWLF68OFauXBknnXRSdOzYMVq3bh1HHHFEfPzxxxuMcdasWTFs2LDo2LFj5OfnR7du3WLMmDHxxhtvVFiv/Eetbr755rjwwgujR48e0bx58won8JXdcsstaQndtGnTmD17dpx11lkxdOjQGDZsWFx44YXx+uuvR79+/dLnJEkSN9xwQwwcODBat24dhYWF0adPnzjjjDNi+fLlFbY/ZMiQdFzPP/98HH300dGxY8do1apVus6nn34av/jFL6Jv375RVFQUbdq0iSFDhsQf//jH6n95m+D++++PQw45JHr16hWtW7eO/Pz86NGjR/z4xz+Od955p9rnffHFF3HeeedFt27doqioKPbdd990H5W3aNGi+OlPfxo9evSIgoKC6NSpUxxxxBHx6quvbta4ARqjvLy8GDBgQESsexN13rx5ERHx/vvvx7vvvhsRETvvvHNE/KeYTpIknnnmmfT5e+21V5Xbnj17dgwcODAKCwuje/fuccUVV2ywTmlpaZx55pmx0047RVFRUbRu3ToGDBgQ119/fSRJskk/w5o1a+Kyyy6LPffcM1q2bBktW7aMAQMGxO9+97uNPrd79+4Vjkm6d++ePtapU6cKjx1wwAFVzlVY3f0Oyh+rvPzyy7HvvvtGixYtok+fPvH73/8+Ita9Of+1r30tCgoKYrfddotZs2ZtMMYlS5bEiSeeGNtvv30UFBRE27ZtY8iQIRVOlCM2nKvxiSeeiEGDBkVRUVGMHTt2k/ZleQsWLIhRo0bFzjvvHO3bt4/mzZtHp06dYtiwYfHEE0986XPvvvvu2GWXXaKwsDB23nnnmDp16gbrZHUcAlBfVc6VffbZp8KbuZXvtTBjxozo37//l+Zoeddcc0307t272kypyXla+Wx79NFH45xzzomuXbtGYWFhDB48OF566aUNXv/VV1+NMWPGpOdp22yzTXzzm9+MmTNnputsynn87373u+jbt28UFBTEV7/61bj77rs3eK05c+ak+2b77bePq666apPvV1BcXFzhd1D+4reCgoIKj1111VVVZnx1r1X+PHz+/Plx5JFHRuvWraNz587xi1/8IpIkiZdffjm+8Y1vRFFRUbW/19WrV8cll1wSu+++e7Rs2TJatGgRu+22W1x88cWxevXqCuuWn/r03XffjREjRkRxcXH07du32n1Qnc8++yyOO+646NevX5SUlER+fn4UFxfHoEGDNnqh3Isvvhjf+MY3okWLFtGlS5c4++yz44svvqiwTpIkMWXKlBg8eHC0adMmioqKYrfddovf/OY3UVZWVuPxUs8lUE/16NEjiYgkIpLZs2eny2fPnp0uP+qooyo8Z/3yHj16pMumTJmSLt9+++3T79d/7brrrsmhhx66wfJRo0ZV2PbVV1+dNGnSZIP1IiJp3bp1Mm/evHTdc889N31su+22q7Bu+Z+lsm9+85vpemPGjNnoPiorK0tGjhxZ5ZgiIunTp0/y8ccfp+vvt99+1Y4rSZJk2bJlyS677FLt9q6++uqNjqn8/j733HPT5T/72c+q3W5JSUmydOnSdN2jjjqqwu+n8vpt2rRJFi5cmK4/f/78pG3btlVuu1WrVskzzzyz0fEBUNE555yT/r284IILkiRJkj/84Q9JRCS9e/dOfv7znycRkRx++OFJkiTJa6+9lq7/ta99Ld1O+dzeYYcdkmbNmm3wt/qxxx5L1//444+TPn36VJsZI0eOrDDOqrJ/9erVyf7771/tNk499dQa7YvyuV752KP88Up55bOsfPavX9a2bdukQ4cOFcbVpEmT5KyzzqryOKN8nr/99ttJ586dq/35TjvttHTdRYsWpcu7dOmSFBYWVvuzlFf+efvtt1+6/M4776z2dfPy8pJZs2al65bP3OqOL6ZOnZquX5PjkOrGB9AQfFmuVFY+R3v06JHk5eV9aY6W3/ZOO+200Uyp7Xla5fPJiEh69uyZrFmzJl3/kUceSYqKiqrcdvlzsaqyvHyGVPVaeXl5yWuvvZauP3fu3KSgoGCD9Xbbbbdanf9V3u/lVZfx1Z1rlj8Pr6qTOOGEE6o8ny3/e125cmWy7777Vvu72nfffZNVq1al65c/Pim//yr/LJVVdVzz/vvvV/u6EZGcd9556brl87lHjx5JmzZtNlj/Zz/7WYXXHD16dLXbPuKIIzY6PhoWV0TTqLz11ltx6qmnxgMPPBBf+cpXIiLi5ZdfjoceeiguvfTSmDp1ahQVFUVExLRp09IrihcvXhwnn3xyJEkSeXl5cdZZZ8XDDz8chx12WERErFixIsaMGVPlVVpvv/12jBo1Kh5++OG47bbb0tetSvl3kP/rv/5roz/P3XffHdOmTYuIiHbt2sUNN9wQ9913X+y6664REfHaa69t8DGv9d59990499xzY8aMGXH55ZdHRMSZZ54Zr7zySkREfOtb30rH3Llz54iIOPnkk2Px4sUbHVdVDjzwwLj++utj+vTp8fjjj8cjjzwSp5xySkRELF26NG666aYqn/fmm2/Gb37zm7j//vvTK8FLS0tj4sSJERGRJEkcddRRsWzZsoiIOOWUU+LRRx+NSy65JJo2bRqffvpp/PjHP97kK+gAWKf81Vjrp9xYf/XzwIEDY++9966wbFOm5XjzzTdj2LBhMX369Bg5cmS6/Prrr0+/P+OMM+K1116LiHWfiLr33nvjpptuinbt2kXEuny+6667vnTsv/nNb9IrrQYOHBj33Xdf/P73v48dd9wxIiJ+9atfpVdv15Vly5ZF796948EHH0z3RZIkceGFF8bw4cPjoYceSj+qvGLFigpXDx9//PHp9GVDhgyJBx98MC677LL0o8KXXHJJlT/fe++9F127do3f/e538b//+7+1ut/EjjvuGL/+9a/j/vvvj1mzZsXMmTPj2muvjYKCgigrK4tJkyZV+bxXXnklTjrppHj44YfjyCOPTJdPmDAh1qxZExHZHocA1Fe33nrrJt8I9+9//3t85zvf+dIcLe/VV1+N0047LR588MHYbbfdImLDTKntedrixYvjkksuiXvvvTe6desWEes+hTNjxoyIiPj8889j9OjR8e9//zsi1p3f3nXXXfHggw/GhAkTajQ9xNtvvx0/+clP4qGHHor9998/ItZ9Yqv82CZMmBCrVq2KiIhvfOMbMX369DjvvPPSXKkvVqxYEXfeeWf88pe/TJddeeWV0blz57jvvvviuOOOS5eX/71Onjw5/eRRt27dYurUqXHnnXemn9p64okn0vP6ypYuXRqXXXZZPProo9X2A1+mRYsWcf7558fdd98djz76aMyePTumTZsWvXv3joiI//mf/9ngiuyIdf+9Dhw4MKZPnx4XXHBBNG3aNP25Xn755YhY9ymw2267LSLWHWPceeedMX369PRY8q677trocR8NTJ3W4PAlsrgieu+9906Xjx07Nl3+ox/9KF0+bNiwdPmLL76YJEmSXHbZZemyESNGpOuuXr26whVJL7zwQpIkFd+FHjx48Cb/zOWvEvvjH/+40fUPOeSQdP0rr7wyXf7KK6+ky9u1a5eUlZUlSVLxndgzzjijwrbWrl2btGvXLomIJD8/P/nTn/6UPPnkk8mTTz6ZHH/88enzLr300i8dU3XvAv/rX/9KJkyYkOy4445Vviv+3e9+N123/DvMZ555Zrr89ddfT5cXFhYmq1evTl544YV02e67756O+cknn0wGDRqUPvbcc8996fgAqOjjjz9OPwnUvn37pKysLPmv//qvJCKSa665JlmyZEn69/S9995LjjnmmPTfN954Y7qd8rndqVOnZOXKlUmSJBWev/vuuydJUjGLIiJ55ZVX0u1ceeWV6fLhw4eny6vK/vJXP919991pLpx//vnp8nHjxm3yvsjiiuiISF5//fUkSZLk2WefTZe1aNEiKS0tTZIkSe655550+fjx45MkWZen638vBQUFyUcffZRu+5RTTknXP+mkk5IkqXhlUuWrx75MdVccf/HFF8nkyZOT/v37J61bt97g02Lt2rVL1y2fueWPh7744ouke/fu6WNPPPFEjY9DXBENNGTlc6Wqr/JZU5Mcrbzt8nk5bdq0DTIlSWp/nrY+Z5IkSS6++OJ0+eTJk5MkSZL77rsvXdarV6903FXZ2Hn8brvtli5/+umn0+WHHnpokiRJsnTp0nRZ5Wws/wni+nBF9A033JAub9WqVbp85syZSZIkyYcffljl77X8J4WnT5+eLp8+fXqV+6n88Un519yY6o5rpk+fnhxwwAFJx44dk6ZNm27w38lLL72UJEnFfG7RokWybNmydBujRo1KHzv//POTJEmS4cOHp8uuuOKKNPtvvPHGdPm3v/3tjY6PhqNxzYBPo1d+vsr27dun35efc7ljx47p9+uvsn399dfTZevnzIyIaN68eeyxxx7pvIWvv/567L777hVe89vf/vYmj6+4uDj+9a9/RcS6q5Y2prpx9e3bN1q0aBGff/55fPLJJ/Hhhx9Gp06dKjz3O9/5ToV/f/TRR/HJJ59ExLq5p8rfpKm82sy5vHbt2hg6dGi88MIL1a6zfl9XVv7n6t27d7Rr1y4++eSTWLlyZbz33nsV9sGLL75Y7ZXkr776auy55541HjtAY9WuXbv46le/GgsXLoyPP/44/vrXv8b8+fMjYt1VxiUlJdGrV69YtGhRPP300xXugVDdFdEDBw5M7wHRoUOHdPn6DPjwww/TLGrRokWFeQzLZ3j5v/1VKf/44YcfXuU6dX0PgbZt26ZXEpU/Jtlxxx3TG0NWdUzyxhtvpJ/y2X777Svsx43to969e6dXhdfWhAkTvnQ+0k3J86ZNm8aee+6Zzjf+9ttvx4477pjZcQhAfVbVzQqruxHuxnK0sv322y/9vqr1N+c8bWPbLp9DQ4cOrXAPqJra2Gu9/fbb6bLK2Tho0KD0U8T1QfmsbteuXXz66acR8Z9Ooqrsj6j+3H9Tjo8qn/vX1L333hsjRoz40nWq+u+kT58+UVxcnP57r732ijvuuCMi/vM7Kz/m9TeHrEz2b11MzUGDU/5mQGvXrk2//+ijjzb63PJ/BPPy/vOff5s2bapcf/2J3qaOpyrVHURUZf3HpSIi/vznP2/y82qjJuMq77PPPqvxc/785z+nBzfbbrtt3HrrrfHEE0/EnXfema6zqTch2Nj+rk5txg3Q2A0aNCj9/rrrrovPP/88WrRokU4Btf7xRx99NBYsWBAR6zJ1/Y0MK1s/vUZERLNm/7keoqq8rfz3vrZ//6uTq1yo7XFJfT8mqcrq1avjhhtuiIh1v7+LL744Zs+eHU8++WR64rwp49yUsVZHngNbm6puVrj+jcrKapKjm7L+5pyn1XQsm2Nzjh+ysKU6iVxkf8Tm5/9VV12Vfj9mzJh49NFH48knn4wDDjggXb4p5/OynwhFNA1Q+T/c6+dHjIh45JFHMnvNr371q+n38+bNS79fs2ZNhXePy6+3Xk3+2B5xxBHp97fddls6b1J5K1asiH/84x9fOq4FCxbE559/HhHrQnubbbbZ6Lg6duyYBnyrVq1ixYoVkSRJha+1a9fGlClTNvnnWe+f//xn+v0Pf/jDGD169CbNgV3553rzzTfj448/joiIwsLC6NKlS4V9sN9++20w5iRJ4rPPPouf/exnNR43QGNXvoi+5ZZbIiKif//+6Rx/6x+//fbb0xOQ/v37Vzixqoltttkm2rZtGxHrTjr++te/po+Vn/O4qrwtr/zjb7/9dpXZsH4O6c1V1XHJihUrMntDeYcddkgz/K233ko/SRWx8X20uSfn//rXv2LlypURse7N89NOOy2GDBkS2223XZrP1Smf52vXro3nnnsu/fd2222X6XEIAFXbnPO0jSmfQ3/605+qnEM4V7bffvv0+7feeiv9hE1ExXtY5Ep96iQ25fhoc/O//H8nV155ZRxwwAGx9957V1helYULF0ZpaWmVY91uu+0iouKYZ8+eXeUx21tvvbVZ46d+MTUHDU6vXr0iLy8vysrKYtasWXHGGWdE69at4+KLL87sNb///e/HaaedFmvWrIl77703zj333Bg4cGDceuut8f7770dExM4771zhiubaGDNmTFx33XXxwgsvxBdffBFDhgyJ//t//2/stddesWrVqpg7d2789re/jWuvvTa6du0aP/zhD+PBBx+MiIhzzjknCgoKomPHjnHeeeel2zziiCM2KXjy8vLiBz/4QVxzzTXx6aefxoEHHhgnnnhidOzYMf7xj3/EggUL4t57742bb745hgwZUqOfq0ePHun3f/jDH2KfffaJTz75JE4//fSNPvfyyy+PkpKS6N69e1x00UXp8oMPPjiaN28eu+22W/Tt2zcWLFgQc+bMidGjR8dhhx0WzZs3j3feeSfmzZsX9913X4WDEQA2TfkpNtZfjVJ+2foiuvyVKtVNy7Ep8vLyYuTIkXHddddFRMSoUaPi3HPPjU8++STOPffcdL0f/OAHX7qdUaNGpTcA/va3vx2nnnpqdO3aNd5///147bXX4oEHHohTTjml2ptB1cQOO+yQvtbo0aNjxIgRcfvtt1f7UebN1aFDhzjooIPikUceiVWrVsXhhx8eJ598crz11ltxzTXXpOttbB/VRklJSRQWFsbKlSvjlVdeiRtuuCFKSkriggsu2OiVUE899VRMmDAhDjjggJg2bVo6LUdJSUkMHDgw0+MQgPrsgw8+iKeeemqD5f3799+s6Sw2xeacp23MgQceGJ06dYoPPvggFi1aFAceeGCMGzcuCgsL46mnnooOHTrEz3/+881+nYh1b2Tvvffe8Ze//CVWrlwZI0eOjBNPPDGef/75uPvuu3PyGuXtsMMO6fdnnXVWLFu2LP7yl7/k7E3uqvzwhz9ML1QbO3ZsrFixIpo0aVLhd5VF9kes++9k/RQa55xzThx00EFx++23x9/+9rcvfd5nn30WRxxxRIwbNy5eeumlClOkDB8+PCLWHbM98MADERHxox/9KM4888zo3bt3fPjhh/HGG2/Eww8/HAcffHCF40AaNkU0DU5xcXEcccQRceedd1a4O/tOO+1U4d22XOrWrVtMnjw5xo0bF2VlZXH++edXeLx169Zxyy23bPY7jfn5+TF9+vQ45JBD4vnnn49PPvkkzjzzzGrXP/zww+O+++6Lu+66Kz7++OP46U9/WuHxPn36VLgb78ZcdNFF8eSTT8Yrr7wSc+fOzdm7xwMGDIhdd901Xn755XjnnXfiu9/9bkREDB48OD744IMvfW6XLl3ihBNOqLCsVatW6c/VpEmTuPXWW2P//fePZcuWxe233x633357TsYN0Nj17ds3WrduHStWrEiXlS+ad9ttt/SeBFU9XhsXXXRRPP744/Haa6/FSy+9FN/73vcqPD5y5Mhq531e76STTooZM2bEzJkz429/+1tOCufqHHPMMfGHP/whIiJmzZoVs2bNimbNmsUOO+wQb775ZiavefXVV8fgwYNjyZIl6WuWd9ppp1WYPzJX8vLy4ic/+UlcffXVsXr16vTTRr17907LhurssMMOcfnll8fll19eYfmll14azZs3j4jsjkMA6rM//vGP6T2Hylu0aFH07Nkz09fenPO0jWnRokXccsst8d3vfjdWrVoVc+bMiTlz5qSP57pY/PWvfx377bdfrF69Oh599NF49NFHIyLSny+XfvCDH8TEiRPj008/jXfeeSfGjRsXEes6iazmMx4/fnw8/PDD8eSTT8bf//73DUrnfffdN04++eRMXvuYY46Jxx57LCIizfLCwsLYc8890/uHVOUrX/lKzJkzZ4Mrxf/7v/87nebtsMMOi9GjR8dtt90W//jHP+K4447bYDv/5//8nxz+NNQ1U3PQIF155ZVx2GGHRcuWLaO4uDhGjx4dTzzxRKavefzxx8djjz0WBx98cLRv3z6aNWsWXbp0idGjR8f8+fOjf//+OXmdr3zlK/H000/HTTfdFEOHDo2OHTtG8+bNo0uXLrHffvvF1VdfHfvvv39ErCthp06dGtddd13stdde0bJlyygoKIivfvWrcfrpp8fTTz9dYT6tjWnbtm3MnTs3Lrjggthtt92iqKgoWrRoEb17947vf//7ceedd9aqYGjatGk8/PDDMXz48CguLo5tttkmTjrppLjppps2+tyrrroqTjvttNh2222joKAg9tlnn5g9e3b06dMnXefrX/96vPjii3HsscfGdtttF/n5+dG2bdvo27dvHHvssZm+Mw2wNcvLy6twE5yIikVzs2bNKtzwt/LjtdG+fft4+umnY+LEibHjjjtGQUFBtGzZMvr37x/XXnttTJ06daNv/Obn58cjjzwSV1xxRey1117RunXrKCwsjF69esWwYcPit7/9bXqyvbkOPPDAmDx5cnTt2jUKCgpir732ihkzZsTgwYNzsv2qbLfddvH888/HuHHjolevXtG8efNo06ZN7LvvvnHXXXdl+imxSy+9NMaPHx/bbrtttGrVKg455JCYOXNmFBUVfenzRo0aFVOmTIk+ffpEfn5+7LjjjnH77bfHkUcema6T1XEIAFXbnPO0TXHwwQfH/Pnz40c/+lF07do1mjdvHh06dIghQ4bkbAqQ9QYOHBgzZsyIfv36RX5+fvTs2TMmT54cRx99dLpOixYtcvJaHTp0iPvvvz923XXXyM/Pj+233z6uvvrqOPXUU3Oy/aoUFBTEY489FhdffHHsuuuuUVRUFIWFhbHLLrvEpEmT4tFHH438/PxMXvv73/9+XH/99dG7d+8oLCyM/v37xyOPPFLhptJV2WGHHWLWrFkxePDgKCwsjM6dO8cZZ5wR1157bYX1br311rjttttiv/32i+Li4sjPz4/u3bvH/vvvH1dccUUcf/zxmfxc1I0mSRYzyQMAAADAFpAkSZVvVI8cOTLuuuuuiIi49957c/ZGNFA7puYAAAAAoMH6+9//Hscdd1wce+yxscsuu8TKlSvjnnvuSeeIbt++fQwdOrSORwm4IhoAAACABuudd96JXr16VflYfn5+3HXXXXHooYdu2UEBGzBHNAAAAAANVvv27eO///u/o0+fPtGqVavIz8+PHj16xOjRo+PZZ59VQkM94YpoAAAAAAAy5YpoAAAAAAAylVkRffXVV0fPnj2jsLAwBgwYEPPmzcvqpQCAWpLXAFD/yWsAtgaZTM1x1113xejRo+O6666LAQMGxOTJk+Oee+6JhQsXRqdOnb70uWVlZfHee+9F69ato0mTJrkeGgBEkiSxYsWK6NKlS+TlNd4PB21OXkfIbACyJa/XkdcA1Gc1yetMiugBAwZE//7946qrroqIdcHXrVu3OOGEE+L000//0uf+4x//iG7duuV6SACwgcWLF0fXrl3rehh1ZnPyOkJmA7BlyGt5DUD9tyl53SzXL7p69eqYP39+TJw4MV2Wl5cXQ4cOjblz526w/qpVq2LVqlXpv9f34vvEt6JZNM/18AAgvog18VT8b7Ru3bquh1JnaprXETIbgC1LXstrAOq/muR1zovojz76KNauXRslJSUVlpeUlMRrr722wfqTJk2K8847r4qBNY9mTYQkABn4/58FaswfT61pXkfIbAC2MHktrwGo/2qQ13U+0dbEiRNj+fLl6dfixYvrekgAQBVkNgDUf/IagPoq51dEd+zYMZo2bRpLly6tsHzp0qXRuXPnDdYvKCiIgoKCXA8DAPgSNc3rCJkNAFuavAZga5LzK6Lz8/Njzz33jJkzZ6bLysrKYubMmTFo0KBcvxwAUAvyGgDqP3kNwNYk51dER0RMmDAhjjrqqOjXr1/stddeMXny5Pjss8/ixz/+cRYvBwDUgrwGgPpPXgOwtcikiD7iiCPiww8/jHPOOSeWLFkSu+++ezzyyCMb3GABAKg78hoA6j95DcDWokmSJEldD6K80tLSKC4ujiEx3B19AcjEF8maeDweiOXLl0ebNm3qejgNlswGIEvyOjfkNQBZqkle53yOaAAAAAAAKE8RDQAAAABAphTRAAAAAABkShENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKYU0QAAAAAAZEoRDQAAAABAphTRAAAAAABkShENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKYU0QAAAAAAZEoRDQAAAABAphTRAAAAAABkShENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKYU0QAAAAAAZEoRDQAAAABAphTRAAAAAABkShENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKYU0QAAAAAAZEoRDQAAAABAphTRAAAAAABkShENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKYU0QAAAAAAZEoRDQAAAABAphTRAAAAAABkShENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKYU0QAAAAAAZEoRDQAAAABAphTRAAAAAABkShENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKYU0QAAAAAAZEoRDQAAAABAphTRAAAAAABkShENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKYU0QAAAAAAZEoRDQAAAABAphTRAAAAAABkShENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKYU0QAAAAAAZEoRDQAAAABAphTRAAAAAABkShENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKYU0QAAAAAAZEoRDQAAAABAphTRAAAAAABkShENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKYU0QAAAAAAZEoRDQAAAABAphTRAAAAAABkShENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKYU0QAAAAAAZEoRDQAAAABApmpURE+aNCn69+8frVu3jk6dOsWhhx4aCxcurLDOypUrY+zYsdGhQ4do1apVjBgxIpYuXZrTQQMA1ZPXANAwyGwAGpMaFdFz5syJsWPHxtNPPx2PPfZYrFmzJg488MD47LPP0nVOPvnkmD59etxzzz0xZ86ceO+99+J73/tezgcOAFRNXgNAwyCzAWhMmiRJktT2yR9++GF06tQp5syZE/vuu28sX748ttlmm5g6dWp8//vfj4iI1157LXbaaaeYO3duDBw4cKPbLC0tjeLi4hgSw6NZk+a1HRoAVOuLZE08Hg/E8uXLo02bNnU9nMxlkdcRMhuAbDW2vI5wjg1Aw1OTvN6sOaKXL18eERHt27ePiIj58+fHmjVrYujQoek6ffr0ie7du8fcuXM356UAgFqS1wDQMMhsALZmzWr7xLKyshg/fnwMHjw4+vbtGxERS5Ysifz8/Gjbtm2FdUtKSmLJkiVVbmfVqlWxatWq9N+lpaW1HRIAUEmu8jpCZgNAlpxjA7C1q/UV0WPHjo0FCxbEtGnTNmsAkyZNiuLi4vSrW7dum7U9AOA/cpXXETIbALLkHBuArV2tiuhx48bFQw89FLNnz46uXbumyzt37hyrV6+OZcuWVVh/6dKl0blz5yq3NXHixFi+fHn6tXjx4toMCQCoJJd5HSGzASArzrEBaAxqVEQnSRLjxo2L++67L2bNmhW9evWq8Piee+4ZzZs3j5kzZ6bLFi5cGO+++24MGjSoym0WFBREmzZtKnwBALWXRV5HyGwAyDXn2AA0JjWaI3rs2LExderUeOCBB6J169bpnFTFxcVRVFQUxcXF8ZOf/CQmTJgQ7du3jzZt2sQJJ5wQgwYN2qS7+QIAm09eA0DDILMBaExqVERfe+21ERExZMiQCsunTJkSY8aMiYiIyy+/PPLy8mLEiBGxatWqOOigg+Kaa67JyWABgI2T1wDQMMhsABqTJkmSJHU9iPJKS0ujuLg4hsTwaNakeV0PB4Ct0BfJmng8Hojly5f7uOpmkNkAZEle54a8BiBLNcnrWt2sEAAAAAAANpUiGgAAAACATCmiAQAAAADIVI1uVggAADUx470Xa/ycg7rsnvNxAAAAdcsV0QAAAAAAZEoRDQAAAABAphTRAAAAAABkShENAAAAAECm3KwQAICI2PDGgrW5aWBtbk6Yi224wSEAANRvrogGAAAAACBTimgAAAAAADKliAYAAAAAIFPmiAYAaIQ2ZR7mjc0ZnYv5oHMlF/NbAwAA2XFFNAAAAAAAmVJEAwAAAACQKUU0AAAAAACZMkc0AMBWKIv5m+vTnNAAAEDD4opoAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFNuVggAwFan8o0VD+qye52MAwAAWMcV0QAAAAAAZEoRDQAAAABAphTRAAAAAABkyhzRAABbocpzIleeMxkAAGBLckU0AAAAAACZUkQDAAAAAJApRTQAAAAAAJkyRzQAQAOzKfM9V54jGgAAoC65IhoAAAAAgEwpogEAAAAAyJQiGgAAAACATJkjGgBgK7Qp80g3JubVBgCAuuWKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTblYIANDAbMpN9dysEAAAqE9cEQ0AAAAAQKYU0QAAAAAAZEoRDQAAAABApswRTYO0KfNebsr8mQCwpW2pDNvYNswhDQCsV/m4wPk0kAVXRAMAAAAAkClFNAAAAAAAmVJEAwAAAACQKXNEs8VtqTkpzXEFQEOVRVbKQQDYOm3suKGqY4CNPcd9mYAsuCIaAAAAAIBMKaIBAAAAAMiUIhoAAAAAgEwpogEAAAAAyJSbFTZiubiZ35a68WAuuHkhAHWhvmRlfRkHAGyt6us5dlbHAM6xgZpyRTQAAAAAAJlSRAMAAAAAkClFNAAAAAAAmTJHdCOysXmhzB0JAJtHlgJA4+EcG6BmXBENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKbMEU2jVXm+roO67F4n4wAAAICGzjk2sDGuiAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU+aI3kpVnpsJAIAvZ25LAKpTV+fYm5JFGxtbVdvQGQB1wRXRAAAAAABkShENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKbcrBD+v6pu1uAmRQDURC5uKET94eaFAGyO2uRG5ezZlPPULZVPlV9nY8c0zrGBylwRDQAAAABAphTRAAAAAABkShENAAAAAECmzBENAABVMI8lAJujvtxrYFPmaq4vYwW2bq6IBgAAAAAgU4poAAAAAAAytVlF9MUXXxxNmjSJ8ePHp8tWrlwZY8eOjQ4dOkSrVq1ixIgRsXTp0s0dJwBQS/IaABoGmQ3A1qzWc0Q/++yzcf3118euu+5aYfnJJ58cDz/8cNxzzz1RXFwc48aNi+9973vx5z//ebMHS/WqmvMJAOQ1bDrzYQJ1SWbXL1vqHLs2r5PF2GQgsCXU6oroTz/9NEaNGhU33nhjtGvXLl2+fPny+O1vfxuXXXZZfPOb34w999wzpkyZEn/5y1/i6aefztmgAYCNk9cA0DDIbAAag1oV0WPHjo1hw4bF0KFDKyyfP39+rFmzpsLyPn36RPfu3WPu3LlVbmvVqlVRWlpa4QsA2Hy5zOsImQ0AWXGODUBjUOOpOaZNmxbPP/98PPvssxs8tmTJksjPz4+2bdtWWF5SUhJLliypcnuTJk2K8847r6bDAAC+RK7zOkJmA0AWnGMD0FjU6IroxYsXx0knnRR33HFHFBYW5mQAEydOjOXLl6dfixcvzsl2AaCxyiKvI2Q2AOSac2wAGpMaXRE9f/78+OCDD+LrX/96umzt2rXxxBNPxFVXXRUzZsyI1atXx7Jlyyq8Y7t06dLo3LlzldssKCiIgoKC2o0eANhAFnkdIbOrUvlmQW70A0BNOMduXGpzk8FNObbY2HZzsY3acFwEVFajInr//fePV155pcKyH//4x9GnT5847bTTolu3btG8efOYOXNmjBgxIiIiFi5cGO+++24MGjQod6MGAKolrwGgYZDZADQmNSqiW7duHX379q2wrGXLltGhQ4d0+U9+8pOYMGFCtG/fPtq0aRMnnHBCDBo0KAYOHJi7UQMA1ZLXANAwyGwAGpMa36xwYy6//PLIy8uLESNGxKpVq+Kggw6Ka665JtcvAwBsBnkNAA2DzAZga9EkSZKkrgdRXmlpaRQXF8eQGB7NmjSv6+E0GFnM59TYVDV/lbk/Yev0RbImHo8HYvny5dGmTZu6Hk6D1RgzW942HJUze1N+d3Ie6hd5nRuNMa9zYUtlfm2yp6Ecj8hVaBxqktd5W2hMAAAAAAA0UopoAAAAAAAypYgGAAAAACBTOb9ZITQUmzJflTmtAGDrINMBqI+2pvsSNeSxA1uGK6IBAAAAAMiUIhoAAAAAgEwpogEAAAAAyJQ5orcSWczFVHmuqobOfFUA0HhsTXNuArDlbancqJxXDek8XLYCNeWKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTblZItTblxgNb6kYKboIAAP9RnzK6pqoae30dKwBkrXIuOscGtmauiAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU+aIZrNkMZ+VuaoAqK/qah7Hyiq/bl1lZ0PK7Kp+Vw1p/AA0Ds6xga2ZK6IBAAAAAMiUIhoAAAAAgEwpogEAAAAAyJQ5oskpc08B0JjU1ZzRWypv6+p1arMf68v83QCQS86xga2JK6IBAAAAAMiUIhoAAAAAgEwpogEAAAAAyJQ5ogEAGritbT5k82ECAMDWxxXRAAAAAABkShENAAAAAECmFNEAAAAAAGRKEQ0AAAAAQKbcrBAAIEequsle5RsJZnEjPjf3q52a3uTRfgYAgNpzRTQAAAAAAJlSRAMAAAAAkClFNAAAAAAAmTJHNABAhswrvGVsbL5nvwcAAKhbrogGAAAAACBTimgAAAAAADKliAYAAAAAIFPmiAYAoMEzBzQAANRvrogGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAyVeMi+p///GcceeSR0aFDhygqKopddtklnnvuufTxJEninHPOiW233TaKiopi6NCh8cYbb+R00ADAl5PXANAwyGwAGosaFdGffPJJDB48OJo3bx5//OMf429/+1v8+te/jnbt2qXr/OpXv4orrrgirrvuunjmmWeiZcuWcdBBB8XKlStzPngAYEPyGgAaBpkNQGPSrCYrX3LJJdGtW7eYMmVKuqxXr17p90mSxOTJk+Oss86K4cOHR0TEbbfdFiUlJXH//ffHyJEjczRsAKA68hoAGgaZDUBjUqMroh988MHo169fHHbYYdGpU6fYY4894sYbb0wfX7RoUSxZsiSGDh2aLisuLo4BAwbE3LlzczdqAKBa8hoAGgaZDUBjUqMi+u23345rr702evfuHTNmzIjjjjsuTjzxxLj11lsjImLJkiUREVFSUlLheSUlJeljla1atSpKS0srfAEAtZdFXkfIbADINefYADQmNZqao6ysLPr16xe//OUvIyJijz32iAULFsR1110XRx11VK0GMGnSpDjvvPNq9VwAYENZ5HWEzAaAXHOODUBjUqMrorfddtvYeeedKyzbaaed4t13342IiM6dO0dExNKlSyuss3Tp0vSxyiZOnBjLly9PvxYvXlyTIQEAlWSR1xEyGwByzTk2AI1JjYrowYMHx8KFCysse/3116NHjx4Rse6mCp07d46ZM2emj5eWlsYzzzwTgwYNqnKbBQUF0aZNmwpfAEDtZZHXETIbAHLNOTYAjUmNpuY4+eSTY++9945f/vKXcfjhh8e8efPihhtuiBtuuCEiIpo0aRLjx4+PCy+8MHr37h29evWKs88+O7p06RKHHnpoFuMHACqR1wDQMMhsABqTGhXR/fv3j/vuuy8mTpwY559/fvTq1SsmT54co0aNStc59dRT47PPPotjjjkmli1bFvvss0888sgjUVhYmPPBAwAbktcA0DDIbAAakyZJkiR1PYjySktLo7i4OIbE8GjWpHldDweArdAXyZp4PB6I5cuX+7jqZpDZAGRJXueGvAYgSzXJ6xrNEQ0AAAAAADWliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMqWIBgAAAAAgU4poAAAAAAAypYgGAAAAACBTimgAAAAAADKliAYAAAAAIFOKaAAAAAAAMlWjInrt2rVx9tlnR69evaKoqCi23377uOCCCyJJknSdJEninHPOiW233TaKiopi6NCh8cYbb+R84ABA1eQ1ADQMMhuAxqRGRfQll1wS1157bVx11VXx6quvxiWXXBK/+tWv4sorr0zX+dWvfhVXXHFFXHfddfHMM89Ey5Yt46CDDoqVK1fmfPAAwIbkNQA0DDIbgMakWU1W/stf/hLDhw+PYcOGRUREz549484774x58+ZFxLp3aidPnhxnnXVWDB8+PCIibrvttigpKYn7778/Ro4cmePhAwCVyWsAaBhkNgCNSY2uiN57771j5syZ8frrr0dExEsvvRRPPfVUHHzwwRERsWjRoliyZEkMHTo0fU5xcXEMGDAg5s6dW+U2V61aFaWlpRW+AIDayyKvI2Q2AOSac2wAGpMaXRF9+umnR2lpafTp0yeaNm0aa9eujYsuuihGjRoVERFLliyJiIiSkpIKzyspKUkfq2zSpElx3nnn1WbsAEAVssjrCJkNALnmHBuAxqRGV0Tffffdcccdd8TUqVPj+eefj1tvvTUuvfTSuPXWW2s9gIkTJ8by5cvTr8WLF9d6WwBANnkdIbMBINecYwPQmNToiuif//zncfrpp6fzUO2yyy7x97//PSZNmhRHHXVUdO7cOSIili5dGttuu236vKVLl8buu+9e5TYLCgqioKCglsMHACrLIq8jZDYA5JpzbAAakxpdEf35559HXl7FpzRt2jTKysoiIqJXr17RuXPnmDlzZvp4aWlpPPPMMzFo0KAcDBcA2Bh5DQANg8wGoDGp0RXR3/nOd+Kiiy6K7t27x9e+9rV44YUX4rLLLoujjz46IiKaNGkS48ePjwsvvDB69+4dvXr1irPPPju6dOkShx56aBbjBwAqkdcA0DDIbAAakxoV0VdeeWWcffbZcfzxx8cHH3wQXbp0iZ/97GdxzjnnpOuceuqp8dlnn8UxxxwTy5Yti3322SceeeSRKCwszPngAYANyWsAaBhkNgCNSZMkSZK6HkR5paWlUVxcHENieDRr0ryuhwPAVuiLZE08Hg/E8uXLo02bNnU9nAZLZgOQJXmdG/IagCzVJK9rNEc0AAAAAADUlCIaAAAAAIBMKaIBAAAAAMiUIhoAAAAAgEwpogEAAAAAyJQiGgAAAACATCmiAQAAAADIlCIaAAAAAIBMKaIBAAAAAMiUIhoAAAAAgEwpogEAAAAAyJQiGgAAAACATCmiAQAAAADIlCIaAAAAAIBMKaIBAAAAAMiUIhoAAAAAgEwpogEAAAAAyJQiGgAAAACATCmiAQAAAADIlCIaAAAAAIBMKaIBAAAAAMiUIhoAAAAAgEwpogEAAAAAyJQiGgAAAACATCmiAQAAAADIlCIaAAAAAIBMKaIBAAAAAMiUIhoAAAAAgEwpogEAAAAAyJQiGgAAAACATCmiAQAAAADIlCIaAAAAAIBMKaIBAAAAAMiUIhoAAAAAgEwpogEAAAAAyJQiGgAAAACATCmiAQAAAADIlCIaAAAAAIBMKaIBAAAAAMiUIhoAAAAAgEwpogEAAAAAyJQiGgAAAACATCmiAQAAAADIlCIaAAAAAIBMKaIBAAAAAMiUIhoAAAAAgEwpogEAAAAAyJQiGgAAAACATCmiAQAAAADIlCIaAAAAAIBMKaIBAAAAAMiUIhoAAAAAgEwpogEAAAAAyJQiGgAAAACATCmiAQAAAADIlCIaAAAAAIBMKaIBAAAAAMiUIhoAAAAAgEw1q+sBVJYkSUREfBFrIpI6HgwAW6UvYk1E/CdzqB2ZDUCW5HVuyGsAslSTvK53RfSKFSsiIuKp+N86HgkAW7sVK1ZEcXFxXQ+jwZLZAGwJ8nrzyGsAtoRNyesmST17e7msrCzee++9SJIkunfvHosXL442bdrU9bC2GqWlpdGtWzf7NYfs02zYr7lnn/5HkiSxYsWK6NKlS+TlmaWqtmR2dvz/mg37Nffs02zYr+vI69yQ19ny/2vu2afZsF9zzz5dpyZ5Xe+uiM7Ly4uuXbtGaWlpRES0adOmUf8ys2K/5p59mg37Nffs03VcWbX5ZHb27NNs2K+5Z59mw36V17kgr7cM+zX37NNs2K+5Z59uel57WxkAAAAAgEwpogEAAAAAyFS9LaILCgri3HPPjYKCgroeylbFfs09+zQb9mvu2adkxX9buWefZsN+zT37NBv2K1nw31U27Nfcs0+zYb/mnn1ac/XuZoUAAAAAAGxd6u0V0QAAAAAAbB0U0QAAAAAAZEoRDQAAAABAphTRAAAAAABkqt4W0VdffXX07NkzCgsLY8CAATFv3ry6HlKDMWnSpOjfv3+0bt06OnXqFIceemgsXLiwwjorV66MsWPHRocOHaJVq1YxYsSIWLp0aR2NuOG5+OKLo0mTJjF+/Ph0mX1aO//85z/jyCOPjA4dOkRRUVHssssu8dxzz6WPJ0kS55xzTmy77bZRVFQUQ4cOjTfeeKMOR1z/rV27Ns4+++zo1atXFBUVxfbbbx8XXHBBlL83rf1KrsjrzSOzsyezc0Ne5568ZkuT2bUnr7Mnr3NHZueWvM6xpB6aNm1akp+fn9x8883JX//61+SnP/1p0rZt22Tp0qV1PbQG4aCDDkqmTJmSLFiwIHnxxReTb33rW0n37t2TTz/9NF3n2GOPTbp165bMnDkzee6555KBAwcme++9dx2OuuGYN29e0rNnz2TXXXdNTjrppHS5fVpzH3/8cdKjR49kzJgxyTPPPJO8/fbbyYwZM5I333wzXefiiy9OiouLk/vvvz956aWXkkMOOSTp1atX8u9//7sOR16/XXTRRUmHDh2Shx56KFm0aFFyzz33JK1atUp+85vfpOvYr+SCvN58MjtbMjs35HU25DVbkszePPI6W/I6d2R27snr3KqXRfRee+2VjB07Nv332rVrky5duiSTJk2qw1E1XB988EESEcmcOXOSJEmSZcuWJc2bN0/uueeedJ1XX301iYhk7ty5dTXMBmHFihVJ7969k8ceeyzZb7/90pC0T2vntNNOS/bZZ59qHy8rK0s6d+6c/M///E+6bNmyZUlBQUFy5513bokhNkjDhg1Ljj766ArLvve97yWjRo1KksR+JXfkde7J7NyR2bkjr7Mhr9mSZHZuyevckde5JbNzT17nVr2bmmP16tUxf/78GDp0aLosLy8vhg4dGnPnzq3DkTVcy5cvj4iI9u3bR0TE/PnzY82aNRX2cZ8+faJ79+728UaMHTs2hg0bVmHfRdintfXggw9Gv3794rDDDotOnTrFHnvsETfeeGP6+KJFi2LJkiUV9mtxcXEMGDDAfv0Se++9d8ycOTNef/31iIh46aWX4qmnnoqDDz44IuxXckNeZ0Nm547Mzh15nQ15zZYis3NPXueOvM4tmZ178jq3mtX1ACr76KOPYu3atVFSUlJheUlJSbz22mt1NKqGq6ysLMaPHx+DBw+Ovn37RkTEkiVLIj8/P9q2bVth3ZKSkliyZEkdjLJhmDZtWjz//PPx7LPPbvCYfVo7b7/9dlx77bUxYcKEOOOMM+LZZ5+NE088MfLz8+Ooo45K911Vfw/s1+qdfvrpUVpaGn369ImmTZvG2rVr46KLLopRo0ZFRNiv5IS8zj2ZnTsyO7fkdTbkNVuKzM4teZ078jr3ZHbuyevcqndFNLk1duzYWLBgQTz11FN1PZQGbfHixXHSSSfFY489FoWFhXU9nK1GWVlZ9OvXL375y19GRMQee+wRCxYsiOuuuy6OOuqoOh5dw3X33XfHHXfcEVOnTo2vfe1r8eKLL8b48eOjS5cu9ivUYzI7N2R27snrbMhraJjkdW7I62zI7NyT17lV76bm6NixYzRt2nSDO6EuXbo0OnfuXEejapjGjRsXDz30UMyePTu6du2aLu/cuXOsXr06li1bVmF9+7h68+fPjw8++CC+/vWvR7NmzaJZs2YxZ86cuOKKK6JZs2ZRUlJin9bCtttuGzvvvHOFZTvttFO8++67ERHpvvP3oGZ+/vOfx+mnnx4jR46MXXbZJX70ox/FySefHJMmTYoI+5XckNe5JbNzR2bnnrzOhrxmS5HZuSOvc0deZ0Nm5568zq16V0Tn5+fHnnvuGTNnzkyXlZWVxcyZM2PQoEF1OLKGI0mSGDduXNx3330xa9as6NWrV4XH99xzz2jevHmFfbxw4cJ499137eNq7L///vHKK6/Eiy++mH7169cvRo0alX5vn9bc4MGDY+HChRWWvf7669GjR4+IiOjVq1d07ty5wn4tLS2NZ555xn79Ep9//nnk5VX88960adMoKyuLCPuV3JDXuSGzc09m5568zoa8ZkuR2ZtPXueevM6GzM49eZ1jdXyzxCpNmzYtKSgoSG655Zbkb3/7W3LMMcckbdu2TZYsWVLXQ2sQjjvuuKS4uDh5/PHHk/fffz/9+vzzz9N1jj322KR79+7JrFmzkueeey4ZNGhQMmjQoDocdcNT/o6+SWKf1sa8efOSZs2aJRdddFHyxhtvJHfccUfSokWL5He/+126zsUXX5y0bds2eeCBB5KXX345GT58eNKrV6/k3//+dx2OvH476qijkq985SvJQw89lCxatCi59957k44dOyannnpquo79Si7I680ns7cMmb155HU25DVbkszePPJ6y5DXm09m5568zq16WUQnSZJceeWVSffu3ZP8/Pxkr732Sp5++um6HlKDERFVfk2ZMiVd59///ndy/PHHJ+3atUtatGiRfPe7303ef//9uht0A1Q5JO3T2pk+fXrSt2/fpKCgIOnTp09yww03VHi8rKwsOfvss5OSkpKkoKAg2X///ZOFCxfW0WgbhtLS0uSkk05KunfvnhQWFibbbbddcuaZZyarVq1K17FfyRV5vXlk9pYhszefvM49ec2WJrNrT15vGfI6N2R2bsnr3GqSJEmypa/CBgAAAACg8ah3c0QDAAAAALB1UUQDAAAAAJApRTQAAAAAAJlSRAMAAAAAkClFNAAAAAAAmVJEAwAAAACQKUU0AAAAAACZUkQDAAAAAJApRTQAAAAAAJlSRAMAAAAAkClFNAAAAAAAmVJEAwAAAACQqf8HsLloVXkK8EEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_channels = ['FLAIR', 'T1w', 'T1gd', 'T2w']\n",
    "label_channels = ['Tumor Core', 'Whole Tumor', 'Enhancing Tumor']\n",
    "\n",
    "# pick one image from BratsDataset to visualize and check the 4 channels\n",
    "val_data_example = val_ds[27]\n",
    "print(f\"image shape: {val_data_example['image'].shape}\")\n",
    "plt.figure(\"image\", (24, 6))\n",
    "for i in range(len(image_channels)):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.title(f\"{image_channels[i]} image\", weight='bold')\n",
    "    plt.imshow(val_data_example[\"image\"][i, :, :, val_data_example['image'].shape[-1] // 2], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# also visualize the 3 channels label corresponding to this image\n",
    "print(f\"label shape: {val_data_example['label'].shape}\")\n",
    "plt.figure(\"label\", (18, 6))\n",
    "for i in range(len(label_channels)):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.title(f\"{label_channels[i]} label\", weight='bold')\n",
    "    plt.imshow(val_data_example[\"label\"][i, :, :, val_data_example['image'].shape[-1] // 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.plot import plot_data_distribution \n",
    "val_frac = val_ds.val_frac\n",
    "test_frac = val_ds.test_frac\n",
    "\n",
    "num_train = len(train_ds)\n",
    "num_val = len(val_ds)\n",
    "num_test = int(test_frac * num_val / val_frac)\n",
    "\n",
    "plot_data_distribution(num_train, num_val, num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model,Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from monai.losses import DiceCELoss\n",
    "val_interval = 1\n",
    "VAL_AMP = True\n",
    "device = torch.device(\"cpu:0\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "print(device)\n",
    "\n",
    "model = UNETR(\n",
    "    img_shape=cfg.unetr.img_shape,\n",
    "    input_dim=cfg.unetr.input_dim,\n",
    "    output_dim=cfg.unetr.output_dim,\n",
    "    embed_dim=cfg.unetr.embed_dim,\n",
    "    patch_size=cfg.unetr.patch_size,\n",
    "    num_heads=cfg.unetr.num_heads,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "\n",
    "loss_function = DiceCELoss(to_onehot_y=False, sigmoid=True, include_background=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epoch)\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold_values=True, logit_thresh=0.5)])\n",
    "\n",
    "# define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=cfg.unetr.img_shape,\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "# use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/281, train_loss: 1.3268, step time: 0.2495\n",
      "193/281, train_loss: 1.3294, step time: 0.2474\n",
      "194/281, train_loss: 1.3201, step time: 0.2467\n",
      "195/281, train_loss: 1.3547, step time: 0.2456\n",
      "196/281, train_loss: 1.3324, step time: 0.2436\n",
      "197/281, train_loss: 1.3173, step time: 0.2442\n",
      "198/281, train_loss: 1.2996, step time: 0.2442\n",
      "199/281, train_loss: 1.3269, step time: 0.2487\n",
      "200/281, train_loss: 1.3230, step time: 0.2532\n",
      "201/281, train_loss: 1.3237, step time: 0.2630\n",
      "202/281, train_loss: 1.3074, step time: 0.2505\n",
      "203/281, train_loss: 1.3135, step time: 0.2531\n",
      "204/281, train_loss: 1.3078, step time: 0.2571\n",
      "205/281, train_loss: 1.2963, step time: 0.2539\n",
      "206/281, train_loss: 1.3027, step time: 0.2495\n",
      "207/281, train_loss: 1.3095, step time: 0.2489\n",
      "208/281, train_loss: 1.3234, step time: 0.2499\n",
      "209/281, train_loss: 1.2922, step time: 0.2463\n",
      "210/281, train_loss: 1.3055, step time: 0.2486\n",
      "211/281, train_loss: 1.3181, step time: 0.2477\n",
      "212/281, train_loss: 1.3007, step time: 0.2420\n",
      "213/281, train_loss: 1.2922, step time: 0.2435\n",
      "214/281, train_loss: 1.2984, step time: 0.2450\n",
      "215/281, train_loss: 1.3909, step time: 0.2466\n",
      "216/281, train_loss: 1.3356, step time: 0.2415\n",
      "217/281, train_loss: 1.2951, step time: 0.2487\n",
      "218/281, train_loss: 1.2915, step time: 0.2496\n",
      "219/281, train_loss: 1.2896, step time: 0.2519\n",
      "220/281, train_loss: 1.3296, step time: 0.2497\n",
      "221/281, train_loss: 1.2884, step time: 0.2424\n",
      "222/281, train_loss: 1.3242, step time: 0.2463\n",
      "223/281, train_loss: 1.2815, step time: 0.2429\n",
      "224/281, train_loss: 1.3168, step time: 0.2474\n",
      "225/281, train_loss: 1.3123, step time: 0.2386\n",
      "226/281, train_loss: 1.2804, step time: 0.2365\n",
      "227/281, train_loss: 1.2978, step time: 0.2392\n",
      "228/281, train_loss: 1.3190, step time: 0.2395\n",
      "229/281, train_loss: 1.2811, step time: 0.2400\n",
      "230/281, train_loss: 1.2897, step time: 0.2448\n",
      "231/281, train_loss: 1.2689, step time: 0.2433\n",
      "232/281, train_loss: 1.2957, step time: 0.2435\n",
      "233/281, train_loss: 1.2954, step time: 0.2446\n",
      "234/281, train_loss: 1.3139, step time: 0.2433\n",
      "235/281, train_loss: 1.2619, step time: 0.2541\n",
      "236/281, train_loss: 1.3035, step time: 0.2378\n",
      "237/281, train_loss: 1.2722, step time: 0.2345\n",
      "238/281, train_loss: 1.2785, step time: 0.2514\n",
      "239/281, train_loss: 1.2736, step time: 0.2469\n",
      "240/281, train_loss: 1.2748, step time: 0.2412\n",
      "241/281, train_loss: 1.2702, step time: 0.2397\n",
      "242/281, train_loss: 1.2689, step time: 0.2558\n",
      "243/281, train_loss: 1.2814, step time: 0.2635\n",
      "244/281, train_loss: 1.2726, step time: 0.2496\n",
      "245/281, train_loss: 1.2977, step time: 0.2470\n",
      "246/281, train_loss: 1.2644, step time: 0.2568\n",
      "247/281, train_loss: 1.2659, step time: 0.2584\n",
      "248/281, train_loss: 1.2783, step time: 0.2530\n",
      "249/281, train_loss: 1.2689, step time: 0.2568\n",
      "250/281, train_loss: 1.2658, step time: 0.2535\n",
      "251/281, train_loss: 1.2446, step time: 0.2454\n",
      "252/281, train_loss: 1.2987, step time: 0.2394\n",
      "253/281, train_loss: 1.2752, step time: 0.2460\n",
      "254/281, train_loss: 1.2370, step time: 0.2462\n",
      "255/281, train_loss: 1.2397, step time: 0.2480\n",
      "256/281, train_loss: 1.2621, step time: 0.2543\n",
      "257/281, train_loss: 1.2575, step time: 0.2599\n",
      "258/281, train_loss: 1.2643, step time: 0.2434\n",
      "259/281, train_loss: 1.2442, step time: 0.2450\n",
      "260/281, train_loss: 1.2367, step time: 0.2468\n",
      "261/281, train_loss: 1.2662, step time: 0.2484\n",
      "262/281, train_loss: 1.2471, step time: 0.2437\n",
      "263/281, train_loss: 1.2633, step time: 0.2586\n",
      "264/281, train_loss: 1.2442, step time: 0.2577\n",
      "265/281, train_loss: 1.2421, step time: 0.2590\n",
      "266/281, train_loss: 1.2386, step time: 0.2570\n",
      "267/281, train_loss: 1.2375, step time: 0.2534\n",
      "268/281, train_loss: 1.2491, step time: 0.2473\n",
      "269/281, train_loss: 1.2472, step time: 0.2427\n",
      "270/281, train_loss: 1.2520, step time: 0.2538\n",
      "271/281, train_loss: 1.2747, step time: 0.2473\n",
      "272/281, train_loss: 1.2297, step time: 0.2573\n",
      "273/281, train_loss: 1.2305, step time: 0.2479\n",
      "274/281, train_loss: 1.2454, step time: 0.2377\n",
      "275/281, train_loss: 1.2807, step time: 0.2383\n",
      "276/281, train_loss: 1.2242, step time: 0.2407\n",
      "277/281, train_loss: 1.2485, step time: 0.2386\n",
      "278/281, train_loss: 1.2445, step time: 0.2452\n",
      "279/281, train_loss: 1.2428, step time: 0.2511\n",
      "280/281, train_loss: 1.2390, step time: 0.2661\n",
      "281/281, train_loss: 1.2372, step time: 0.2489\n",
      "282/281, train_loss: 1.2138, step time: 2.3698\n",
      "epoch 1 average loss: 1.4737\n",
      "saved new best metric model\n",
      "current epoch: 1 current mean dice: 0.3289 tc: 0.0087 wt: 0.7048 et: 0.2740\n",
      "best mean dice: 0.3289 at epoch: 1\n",
      "time consuming of epoch 1 is: 967.8906\n",
      "----------\n",
      "epoch 2/200\n",
      "1/281, train_loss: 1.2309, step time: 0.2877\n",
      "2/281, train_loss: 1.2247, step time: 0.2571\n",
      "3/281, train_loss: 1.2303, step time: 0.2607\n",
      "4/281, train_loss: 1.2196, step time: 0.2592\n",
      "5/281, train_loss: 1.2269, step time: 0.2591\n",
      "6/281, train_loss: 1.2168, step time: 0.2641\n",
      "7/281, train_loss: 1.2080, step time: 0.2639\n",
      "8/281, train_loss: 1.2262, step time: 0.2617\n",
      "9/281, train_loss: 1.2405, step time: 0.2554\n",
      "10/281, train_loss: 1.2306, step time: 0.2569\n",
      "11/281, train_loss: 1.2434, step time: 0.2548\n",
      "12/281, train_loss: 1.2108, step time: 0.2571\n",
      "13/281, train_loss: 1.2389, step time: 0.2575\n",
      "14/281, train_loss: 1.2397, step time: 0.2656\n",
      "15/281, train_loss: 1.2356, step time: 0.2583\n",
      "16/281, train_loss: 1.2232, step time: 0.2683\n",
      "17/281, train_loss: 1.2006, step time: 0.2672\n",
      "18/281, train_loss: 1.2057, step time: 0.2592\n",
      "19/281, train_loss: 1.2015, step time: 0.2649\n",
      "20/281, train_loss: 1.2208, step time: 0.2579\n",
      "21/281, train_loss: 1.1988, step time: 0.2563\n",
      "22/281, train_loss: 1.2274, step time: 0.2608\n",
      "23/281, train_loss: 1.2198, step time: 0.2647\n",
      "24/281, train_loss: 1.2217, step time: 0.2665\n",
      "25/281, train_loss: 1.2161, step time: 0.2630\n",
      "26/281, train_loss: 1.2140, step time: 0.2654\n",
      "27/281, train_loss: 1.2223, step time: 0.2653\n",
      "28/281, train_loss: 1.1970, step time: 0.2665\n",
      "29/281, train_loss: 1.2092, step time: 0.2645\n",
      "30/281, train_loss: 1.1979, step time: 0.2568\n",
      "31/281, train_loss: 1.2390, step time: 0.2555\n",
      "32/281, train_loss: 1.2097, step time: 0.2607\n",
      "33/281, train_loss: 1.1866, step time: 0.2568\n",
      "34/281, train_loss: 1.2088, step time: 0.2640\n",
      "35/281, train_loss: 1.1881, step time: 0.2595\n",
      "36/281, train_loss: 1.1941, step time: 0.2642\n",
      "37/281, train_loss: 1.2019, step time: 0.2574\n",
      "38/281, train_loss: 1.1995, step time: 0.2641\n",
      "39/281, train_loss: 1.2023, step time: 0.2665\n",
      "40/281, train_loss: 1.2032, step time: 0.2679\n",
      "41/281, train_loss: 1.1963, step time: 0.2617\n",
      "42/281, train_loss: 1.1797, step time: 0.2670\n",
      "43/281, train_loss: 1.2092, step time: 0.2654\n",
      "44/281, train_loss: 1.1839, step time: 0.2655\n",
      "45/281, train_loss: 1.1771, step time: 0.2620\n",
      "46/281, train_loss: 1.2042, step time: 0.2617\n",
      "47/281, train_loss: 1.1748, step time: 0.2648\n",
      "48/281, train_loss: 1.1876, step time: 0.2645\n",
      "49/281, train_loss: 1.1788, step time: 0.2683\n",
      "50/281, train_loss: 1.1739, step time: 0.2621\n",
      "51/281, train_loss: 1.2031, step time: 0.2614\n",
      "52/281, train_loss: 1.1767, step time: 0.2537\n",
      "53/281, train_loss: 1.1819, step time: 0.2552\n",
      "54/281, train_loss: 1.2161, step time: 0.2689\n",
      "55/281, train_loss: 1.1846, step time: 0.2644\n",
      "56/281, train_loss: 1.1726, step time: 0.2672\n",
      "57/281, train_loss: 1.1797, step time: 0.2634\n",
      "58/281, train_loss: 1.1602, step time: 0.2595\n",
      "59/281, train_loss: 1.2073, step time: 0.2577\n",
      "60/281, train_loss: 1.1813, step time: 0.2586\n",
      "61/281, train_loss: 1.1696, step time: 0.2617\n",
      "62/281, train_loss: 1.2054, step time: 0.2644\n",
      "63/281, train_loss: 1.1638, step time: 0.2629\n",
      "64/281, train_loss: 1.1732, step time: 0.2675\n",
      "65/281, train_loss: 1.1789, step time: 0.2632\n",
      "66/281, train_loss: 1.1885, step time: 0.2632\n",
      "67/281, train_loss: 1.1641, step time: 0.2578\n",
      "68/281, train_loss: 1.1653, step time: 0.2630\n",
      "69/281, train_loss: 1.1846, step time: 0.2644\n",
      "70/281, train_loss: 1.1549, step time: 0.2617\n",
      "71/281, train_loss: 1.1464, step time: 0.2659\n",
      "72/281, train_loss: 1.1989, step time: 0.2628\n",
      "73/281, train_loss: 1.1595, step time: 0.2575\n",
      "74/281, train_loss: 1.1710, step time: 0.2600\n",
      "75/281, train_loss: 1.1594, step time: 0.2622\n",
      "76/281, train_loss: 1.1591, step time: 0.2604\n",
      "77/281, train_loss: 1.1818, step time: 0.2582\n",
      "78/281, train_loss: 1.1662, step time: 0.2600\n",
      "79/281, train_loss: 1.1627, step time: 0.2584\n",
      "80/281, train_loss: 1.1869, step time: 0.2579\n",
      "81/281, train_loss: 1.1713, step time: 0.2583\n",
      "82/281, train_loss: 1.1679, step time: 0.2536\n",
      "83/281, train_loss: 1.1686, step time: 0.2628\n",
      "84/281, train_loss: 1.1842, step time: 0.2662\n",
      "85/281, train_loss: 1.1758, step time: 0.2653\n",
      "86/281, train_loss: 1.1665, step time: 0.2628\n",
      "87/281, train_loss: 1.1343, step time: 0.2663\n",
      "88/281, train_loss: 1.1839, step time: 0.2600\n",
      "89/281, train_loss: 1.1819, step time: 0.2637\n",
      "90/281, train_loss: 1.1579, step time: 0.2594\n",
      "91/281, train_loss: 1.1589, step time: 0.2600\n",
      "92/281, train_loss: 1.1676, step time: 0.2596\n",
      "93/281, train_loss: 1.1647, step time: 0.2663\n",
      "94/281, train_loss: 1.1738, step time: 0.2652\n",
      "95/281, train_loss: 1.1618, step time: 0.2594\n",
      "96/281, train_loss: 1.1596, step time: 0.2586\n",
      "97/281, train_loss: 1.1435, step time: 0.2604\n",
      "98/281, train_loss: 1.1386, step time: 0.2640\n",
      "99/281, train_loss: 1.1349, step time: 0.2638\n",
      "100/281, train_loss: 1.1590, step time: 0.2585\n",
      "101/281, train_loss: 1.1636, step time: 0.2578\n",
      "102/281, train_loss: 1.1953, step time: 0.2628\n",
      "103/281, train_loss: 1.1423, step time: 0.2622\n",
      "104/281, train_loss: 1.1793, step time: 0.2608\n",
      "105/281, train_loss: 1.1638, step time: 0.2612\n",
      "106/281, train_loss: 1.1408, step time: 0.2621\n",
      "107/281, train_loss: 1.1407, step time: 0.2581\n",
      "108/281, train_loss: 1.1620, step time: 0.2543\n",
      "109/281, train_loss: 1.1651, step time: 0.2577\n",
      "110/281, train_loss: 1.1298, step time: 0.2646\n",
      "111/281, train_loss: 1.1373, step time: 0.2633\n",
      "112/281, train_loss: 1.1768, step time: 0.2668\n",
      "113/281, train_loss: 1.1311, step time: 0.2621\n",
      "114/281, train_loss: 1.1599, step time: 0.2678\n",
      "115/281, train_loss: 1.1604, step time: 0.2685\n",
      "116/281, train_loss: 1.1344, step time: 0.2710\n",
      "117/281, train_loss: 1.1545, step time: 0.2674\n",
      "118/281, train_loss: 1.1413, step time: 0.2715\n",
      "119/281, train_loss: 1.1260, step time: 0.2644\n",
      "120/281, train_loss: 1.1264, step time: 0.2633\n",
      "121/281, train_loss: 1.1315, step time: 0.2687\n",
      "122/281, train_loss: 1.1653, step time: 0.2683\n",
      "123/281, train_loss: 1.1438, step time: 0.2600\n",
      "124/281, train_loss: 1.1554, step time: 0.2595\n",
      "125/281, train_loss: 1.1210, step time: 0.2595\n",
      "126/281, train_loss: 1.1340, step time: 0.2611\n",
      "127/281, train_loss: 1.1362, step time: 0.2591\n",
      "128/281, train_loss: 1.1371, step time: 0.2678\n",
      "129/281, train_loss: 1.1346, step time: 0.2645\n",
      "130/281, train_loss: 1.1208, step time: 0.2620\n",
      "131/281, train_loss: 1.1029, step time: 0.2640\n",
      "132/281, train_loss: 1.1536, step time: 0.2641\n",
      "133/281, train_loss: 1.1386, step time: 0.2623\n",
      "134/281, train_loss: 1.1319, step time: 0.2641\n",
      "135/281, train_loss: 1.1434, step time: 0.2698\n",
      "136/281, train_loss: 1.1233, step time: 0.2662\n",
      "137/281, train_loss: 1.1314, step time: 0.2728\n",
      "138/281, train_loss: 1.1231, step time: 0.2675\n",
      "139/281, train_loss: 1.1465, step time: 0.2683\n",
      "140/281, train_loss: 1.1142, step time: 0.2598\n",
      "141/281, train_loss: 1.1316, step time: 0.2665\n",
      "142/281, train_loss: 1.1426, step time: 0.2611\n",
      "143/281, train_loss: 1.1318, step time: 0.2577\n",
      "144/281, train_loss: 1.1154, step time: 0.2548\n",
      "145/281, train_loss: 1.1286, step time: 0.2514\n",
      "146/281, train_loss: 1.1095, step time: 0.2529\n",
      "147/281, train_loss: 1.1192, step time: 0.2545\n",
      "148/281, train_loss: 1.1303, step time: 0.2586\n",
      "149/281, train_loss: 1.1232, step time: 0.2595\n",
      "150/281, train_loss: 1.1041, step time: 0.2624\n",
      "151/281, train_loss: 1.1503, step time: 0.2536\n",
      "152/281, train_loss: 1.1361, step time: 0.2539\n",
      "153/281, train_loss: 1.1336, step time: 0.2583\n",
      "154/281, train_loss: 1.1004, step time: 0.2592\n",
      "155/281, train_loss: 1.1247, step time: 0.2631\n",
      "156/281, train_loss: 1.1202, step time: 0.2580\n",
      "157/281, train_loss: 1.1219, step time: 0.2632\n",
      "158/281, train_loss: 1.1392, step time: 0.2666\n",
      "159/281, train_loss: 1.1342, step time: 0.2561\n",
      "160/281, train_loss: 1.1234, step time: 0.2591\n",
      "161/281, train_loss: 1.1430, step time: 0.2572\n",
      "162/281, train_loss: 1.1044, step time: 0.2563\n",
      "163/281, train_loss: 1.1425, step time: 0.2500\n",
      "164/281, train_loss: 1.1205, step time: 0.2772\n",
      "165/281, train_loss: 1.1345, step time: 0.2549\n",
      "166/281, train_loss: 1.1054, step time: 0.2567\n",
      "167/281, train_loss: 1.1438, step time: 0.2599\n",
      "168/281, train_loss: 1.1138, step time: 0.2631\n",
      "169/281, train_loss: 1.1299, step time: 0.2598\n",
      "170/281, train_loss: 1.1391, step time: 0.2561\n",
      "171/281, train_loss: 1.0972, step time: 0.2552\n",
      "172/281, train_loss: 1.1229, step time: 0.2594\n",
      "173/281, train_loss: 1.1056, step time: 0.2659\n",
      "174/281, train_loss: 1.0999, step time: 0.2589\n",
      "175/281, train_loss: 1.1123, step time: 0.2571\n",
      "176/281, train_loss: 1.1383, step time: 0.2566\n",
      "177/281, train_loss: 1.1092, step time: 0.2500\n",
      "178/281, train_loss: 1.1144, step time: 0.2524\n",
      "179/281, train_loss: 1.0996, step time: 0.2507\n",
      "180/281, train_loss: 1.0841, step time: 0.2583\n",
      "181/281, train_loss: 1.1174, step time: 0.2615\n",
      "182/281, train_loss: 1.0988, step time: 0.2653\n",
      "183/281, train_loss: 1.0823, step time: 0.2683\n",
      "184/281, train_loss: 1.1317, step time: 0.2513\n",
      "185/281, train_loss: 1.1366, step time: 0.2515\n",
      "186/281, train_loss: 1.1186, step time: 0.2533\n",
      "187/281, train_loss: 1.0942, step time: 0.2469\n",
      "188/281, train_loss: 1.0981, step time: 0.2554\n",
      "189/281, train_loss: 1.0808, step time: 0.2539\n",
      "190/281, train_loss: 1.0779, step time: 0.2591\n",
      "191/281, train_loss: 1.0852, step time: 0.2607\n",
      "192/281, train_loss: 1.0833, step time: 0.2590\n",
      "193/281, train_loss: 1.1152, step time: 0.2586\n",
      "194/281, train_loss: 1.0827, step time: 0.2610\n",
      "195/281, train_loss: 1.0971, step time: 0.2620\n",
      "196/281, train_loss: 1.0761, step time: 0.2527\n",
      "197/281, train_loss: 1.1007, step time: 0.2521\n",
      "198/281, train_loss: 1.0982, step time: 0.2524\n",
      "199/281, train_loss: 1.1299, step time: 0.2535\n",
      "200/281, train_loss: 1.0976, step time: 0.2523\n",
      "201/281, train_loss: 1.0833, step time: 0.2501\n",
      "202/281, train_loss: 1.1036, step time: 0.2554\n",
      "203/281, train_loss: 1.1131, step time: 0.2583\n",
      "204/281, train_loss: 1.0814, step time: 0.2506\n",
      "205/281, train_loss: 1.1173, step time: 0.2522\n",
      "206/281, train_loss: 1.0981, step time: 0.2535\n",
      "207/281, train_loss: 1.1161, step time: 0.2517\n",
      "208/281, train_loss: 1.1099, step time: 0.2521\n",
      "209/281, train_loss: 1.0938, step time: 0.2598\n",
      "210/281, train_loss: 1.1146, step time: 0.2609\n",
      "211/281, train_loss: 1.1158, step time: 0.2498\n",
      "212/281, train_loss: 1.1106, step time: 0.2496\n",
      "213/281, train_loss: 1.0923, step time: 0.2488\n",
      "214/281, train_loss: 1.0760, step time: 0.2512\n",
      "215/281, train_loss: 1.0896, step time: 0.2547\n",
      "216/281, train_loss: 1.0749, step time: 0.2576\n",
      "217/281, train_loss: 1.0979, step time: 0.2584\n",
      "218/281, train_loss: 1.0825, step time: 0.2546\n",
      "219/281, train_loss: 1.1016, step time: 0.2570\n",
      "220/281, train_loss: 1.1054, step time: 0.2741\n",
      "221/281, train_loss: 1.0728, step time: 0.2577\n",
      "222/281, train_loss: 1.0980, step time: 0.2760\n",
      "223/281, train_loss: 1.0748, step time: 0.2612\n",
      "224/281, train_loss: 1.1233, step time: 0.2563\n",
      "225/281, train_loss: 1.0811, step time: 0.2513\n",
      "226/281, train_loss: 1.0873, step time: 0.2886\n",
      "227/281, train_loss: 1.0680, step time: 0.2561\n",
      "228/281, train_loss: 1.0976, step time: 0.2566\n",
      "229/281, train_loss: 1.0933, step time: 0.2577\n",
      "230/281, train_loss: 1.0838, step time: 0.2494\n",
      "231/281, train_loss: 1.1029, step time: 0.2528\n",
      "232/281, train_loss: 1.0842, step time: 0.2458\n",
      "233/281, train_loss: 1.0558, step time: 0.2479\n",
      "234/281, train_loss: 1.0917, step time: 0.2487\n",
      "235/281, train_loss: 1.0574, step time: 0.2626\n",
      "236/281, train_loss: 1.0744, step time: 0.2521\n",
      "237/281, train_loss: 1.0870, step time: 0.2527\n",
      "238/281, train_loss: 1.0960, step time: 0.2561\n",
      "239/281, train_loss: 1.0782, step time: 0.2566\n",
      "240/281, train_loss: 1.0988, step time: 0.2548\n",
      "241/281, train_loss: 1.0919, step time: 0.2502\n",
      "242/281, train_loss: 1.0886, step time: 0.2480\n",
      "243/281, train_loss: 1.0807, step time: 0.2505\n",
      "244/281, train_loss: 1.0946, step time: 0.2512\n",
      "245/281, train_loss: 1.0810, step time: 0.2531\n",
      "246/281, train_loss: 1.0781, step time: 0.2501\n",
      "247/281, train_loss: 1.0753, step time: 0.2503\n",
      "248/281, train_loss: 1.0845, step time: 0.2536\n",
      "249/281, train_loss: 1.0966, step time: 0.2500\n",
      "250/281, train_loss: 1.0778, step time: 0.2548\n",
      "251/281, train_loss: 1.0734, step time: 0.2522\n",
      "252/281, train_loss: 1.0498, step time: 0.2494\n",
      "253/281, train_loss: 1.0861, step time: 0.2511\n",
      "254/281, train_loss: 1.0911, step time: 0.2473\n",
      "255/281, train_loss: 1.0723, step time: 0.2459\n",
      "256/281, train_loss: 1.0919, step time: 0.2626\n",
      "257/281, train_loss: 1.0621, step time: 0.2517\n",
      "258/281, train_loss: 1.0906, step time: 0.2494\n",
      "259/281, train_loss: 1.1022, step time: 0.2443\n",
      "260/281, train_loss: 1.0574, step time: 0.2534\n",
      "261/281, train_loss: 1.0731, step time: 0.2506\n",
      "262/281, train_loss: 1.0530, step time: 0.2455\n",
      "263/281, train_loss: 1.0828, step time: 0.2438\n",
      "264/281, train_loss: 1.0695, step time: 0.2573\n",
      "265/281, train_loss: 1.1041, step time: 0.2525\n",
      "266/281, train_loss: 1.0750, step time: 0.2533\n",
      "267/281, train_loss: 1.0799, step time: 0.2469\n",
      "268/281, train_loss: 1.0865, step time: 0.2508\n",
      "269/281, train_loss: 1.0467, step time: 0.2498\n",
      "270/281, train_loss: 1.0715, step time: 0.2476\n",
      "271/281, train_loss: 1.0635, step time: 0.2476\n",
      "272/281, train_loss: 1.0889, step time: 0.2479\n",
      "273/281, train_loss: 1.0902, step time: 0.2476\n",
      "274/281, train_loss: 1.0843, step time: 0.2484\n",
      "275/281, train_loss: 1.0682, step time: 0.2516\n",
      "276/281, train_loss: 1.0633, step time: 0.2526\n",
      "277/281, train_loss: 1.0686, step time: 0.2509\n",
      "278/281, train_loss: 1.0941, step time: 0.2495\n",
      "279/281, train_loss: 1.0654, step time: 0.2513\n",
      "280/281, train_loss: 1.0542, step time: 0.2497\n",
      "281/281, train_loss: 1.0893, step time: 0.2465\n",
      "282/281, train_loss: 1.0456, step time: 0.1471\n",
      "epoch 2 average loss: 1.1368\n",
      "current epoch: 2 current mean dice: 0.3289 tc: 0.0089 wt: 0.6874 et: 0.2921\n",
      "best mean dice: 0.3289 at epoch: 1\n",
      "time consuming of epoch 2 is: 427.5223\n",
      "----------\n",
      "epoch 3/200\n",
      "1/281, train_loss: 1.0678, step time: 0.2539\n",
      "2/281, train_loss: 1.0844, step time: 0.2566\n",
      "3/281, train_loss: 1.0763, step time: 0.2604\n",
      "4/281, train_loss: 1.0878, step time: 0.2503\n",
      "5/281, train_loss: 1.0918, step time: 0.2596\n",
      "6/281, train_loss: 1.0661, step time: 0.2578\n",
      "7/281, train_loss: 1.0843, step time: 0.2589\n",
      "8/281, train_loss: 1.0757, step time: 0.2601\n",
      "9/281, train_loss: 1.0550, step time: 0.2545\n",
      "10/281, train_loss: 1.0717, step time: 0.2599\n",
      "11/281, train_loss: 1.0706, step time: 0.2597\n",
      "12/281, train_loss: 1.0427, step time: 0.2606\n",
      "13/281, train_loss: 1.0917, step time: 0.2593\n",
      "14/281, train_loss: 1.0800, step time: 0.2615\n",
      "15/281, train_loss: 1.0338, step time: 0.2652\n",
      "16/281, train_loss: 1.1017, step time: 0.2557\n",
      "17/281, train_loss: 1.0627, step time: 0.2581\n",
      "18/281, train_loss: 1.0824, step time: 0.2599\n",
      "19/281, train_loss: 1.0305, step time: 0.2544\n",
      "20/281, train_loss: 1.0773, step time: 0.2604\n",
      "21/281, train_loss: 1.0813, step time: 0.2591\n",
      "22/281, train_loss: 1.0751, step time: 0.2582\n",
      "23/281, train_loss: 1.0751, step time: 0.2577\n",
      "24/281, train_loss: 1.0627, step time: 0.2601\n",
      "25/281, train_loss: 1.0716, step time: 0.2609\n",
      "26/281, train_loss: 1.0509, step time: 0.2578\n",
      "27/281, train_loss: 1.0437, step time: 0.2591\n",
      "28/281, train_loss: 1.0321, step time: 0.2573\n",
      "29/281, train_loss: 1.0616, step time: 0.2673\n",
      "30/281, train_loss: 1.0775, step time: 0.2654\n",
      "31/281, train_loss: 1.0814, step time: 0.2638\n",
      "32/281, train_loss: 1.0595, step time: 0.2656\n",
      "33/281, train_loss: 1.0737, step time: 0.2641\n",
      "34/281, train_loss: 1.0836, step time: 0.2641\n",
      "35/281, train_loss: 1.0775, step time: 0.2656\n",
      "36/281, train_loss: 1.0561, step time: 0.2622\n",
      "37/281, train_loss: 1.0735, step time: 0.2577\n",
      "38/281, train_loss: 1.0717, step time: 0.2594\n",
      "39/281, train_loss: 1.0592, step time: 0.2623\n",
      "40/281, train_loss: 1.0263, step time: 0.2653\n",
      "41/281, train_loss: 1.0560, step time: 0.2615\n",
      "42/281, train_loss: 1.0792, step time: 0.2659\n",
      "43/281, train_loss: 1.0590, step time: 0.2581\n",
      "44/281, train_loss: 1.0702, step time: 0.2622\n",
      "45/281, train_loss: 1.0704, step time: 0.2634\n",
      "46/281, train_loss: 1.0627, step time: 0.2583\n",
      "47/281, train_loss: 1.0458, step time: 0.2556\n",
      "48/281, train_loss: 1.0578, step time: 0.2581\n",
      "49/281, train_loss: 1.0337, step time: 0.2610\n",
      "50/281, train_loss: 1.0471, step time: 0.2590\n",
      "51/281, train_loss: 1.0543, step time: 0.2523\n",
      "52/281, train_loss: 1.0383, step time: 0.2576\n",
      "53/281, train_loss: 1.0557, step time: 0.2591\n",
      "54/281, train_loss: 1.0444, step time: 0.2652\n",
      "55/281, train_loss: 1.0212, step time: 0.2604\n",
      "56/281, train_loss: 1.0611, step time: 0.2556\n",
      "57/281, train_loss: 1.0597, step time: 0.2543\n",
      "58/281, train_loss: 1.0387, step time: 0.2603\n",
      "59/281, train_loss: 1.0440, step time: 0.2560\n",
      "60/281, train_loss: 1.0495, step time: 0.2630\n",
      "61/281, train_loss: 1.0122, step time: 0.2623\n",
      "62/281, train_loss: 1.0698, step time: 0.2671\n",
      "63/281, train_loss: 1.0501, step time: 0.2606\n",
      "64/281, train_loss: 1.0675, step time: 0.2596\n",
      "65/281, train_loss: 1.0767, step time: 0.2621\n",
      "66/281, train_loss: 1.0362, step time: 0.2607\n",
      "67/281, train_loss: 1.0494, step time: 0.2630\n",
      "68/281, train_loss: 1.0492, step time: 0.2677\n",
      "69/281, train_loss: 1.0425, step time: 0.2670\n",
      "70/281, train_loss: 1.0469, step time: 0.2661\n",
      "71/281, train_loss: 1.0431, step time: 0.2576\n",
      "72/281, train_loss: 1.0383, step time: 0.2600\n",
      "73/281, train_loss: 1.0379, step time: 0.2636\n",
      "74/281, train_loss: 1.0379, step time: 0.2642\n",
      "75/281, train_loss: 1.0485, step time: 0.2675\n",
      "76/281, train_loss: 1.0673, step time: 0.2676\n",
      "77/281, train_loss: 1.0259, step time: 0.2577\n",
      "78/281, train_loss: 1.0533, step time: 0.2575\n",
      "79/281, train_loss: 1.0354, step time: 0.2600\n",
      "80/281, train_loss: 1.0698, step time: 0.2582\n",
      "81/281, train_loss: 1.0123, step time: 0.2627\n",
      "82/281, train_loss: 1.0499, step time: 0.2595\n",
      "83/281, train_loss: 1.0508, step time: 0.2586\n",
      "84/281, train_loss: 1.0461, step time: 0.2594\n",
      "85/281, train_loss: 1.0448, step time: 0.2604\n",
      "86/281, train_loss: 1.0368, step time: 0.2592\n",
      "87/281, train_loss: 1.0642, step time: 0.2604\n",
      "88/281, train_loss: 1.0184, step time: 0.2692\n",
      "89/281, train_loss: 1.0226, step time: 0.2645\n",
      "90/281, train_loss: 1.0512, step time: 0.2574\n",
      "91/281, train_loss: 1.0140, step time: 0.2687\n",
      "92/281, train_loss: 1.0502, step time: 0.2634\n",
      "93/281, train_loss: 1.0729, step time: 0.2595\n",
      "94/281, train_loss: 1.0341, step time: 0.2524\n",
      "95/281, train_loss: 1.0208, step time: 0.2558\n",
      "96/281, train_loss: 1.0771, step time: 0.2597\n",
      "97/281, train_loss: 1.0433, step time: 0.2657\n",
      "98/281, train_loss: 1.0547, step time: 0.2657\n",
      "99/281, train_loss: 0.9869, step time: 0.2655\n",
      "100/281, train_loss: 1.0166, step time: 0.2590\n",
      "101/281, train_loss: 1.0411, step time: 0.2550\n",
      "102/281, train_loss: 1.0178, step time: 0.2574\n",
      "103/281, train_loss: 1.0496, step time: 0.2578\n",
      "104/281, train_loss: 1.0407, step time: 0.2627\n",
      "105/281, train_loss: 1.0173, step time: 0.2625\n",
      "106/281, train_loss: 1.0434, step time: 0.2680\n",
      "107/281, train_loss: 1.0599, step time: 0.2674\n",
      "108/281, train_loss: 1.0562, step time: 0.2613\n",
      "109/281, train_loss: 1.0630, step time: 0.2668\n",
      "110/281, train_loss: 1.0451, step time: 0.2632\n",
      "111/281, train_loss: 1.0359, step time: 0.2627\n",
      "112/281, train_loss: 1.0482, step time: 0.2646\n",
      "113/281, train_loss: 1.0609, step time: 0.2594\n",
      "114/281, train_loss: 1.0180, step time: 0.2560\n",
      "115/281, train_loss: 0.9834, step time: 0.2632\n",
      "116/281, train_loss: 1.0491, step time: 0.2579\n",
      "117/281, train_loss: 1.0319, step time: 0.2545\n",
      "118/281, train_loss: 1.0154, step time: 0.2543\n",
      "119/281, train_loss: 1.0274, step time: 0.2595\n",
      "120/281, train_loss: 1.0606, step time: 0.2590\n",
      "121/281, train_loss: 1.0204, step time: 0.2521\n",
      "122/281, train_loss: 1.0458, step time: 0.2562\n",
      "123/281, train_loss: 1.0351, step time: 0.2566\n",
      "124/281, train_loss: 1.0073, step time: 0.2596\n",
      "125/281, train_loss: 1.0271, step time: 0.2602\n",
      "126/281, train_loss: 1.0409, step time: 0.2513\n",
      "127/281, train_loss: 1.0428, step time: 0.2561\n",
      "128/281, train_loss: 1.0672, step time: 0.2567\n",
      "129/281, train_loss: 1.0513, step time: 0.2609\n",
      "130/281, train_loss: 1.0430, step time: 0.2596\n",
      "131/281, train_loss: 0.9945, step time: 0.2603\n",
      "132/281, train_loss: 1.0294, step time: 0.2635\n",
      "133/281, train_loss: 1.0110, step time: 0.2636\n",
      "134/281, train_loss: 1.0405, step time: 0.2631\n",
      "135/281, train_loss: 1.0134, step time: 0.2648\n",
      "136/281, train_loss: 1.0424, step time: 0.2553\n",
      "137/281, train_loss: 1.0059, step time: 0.2624\n",
      "138/281, train_loss: 1.0514, step time: 0.2552\n",
      "139/281, train_loss: 1.0359, step time: 0.2621\n",
      "140/281, train_loss: 1.0299, step time: 0.2625\n",
      "141/281, train_loss: 1.0125, step time: 0.2743\n",
      "142/281, train_loss: 0.9997, step time: 0.2585\n",
      "143/281, train_loss: 0.9891, step time: 0.2671\n",
      "144/281, train_loss: 1.0068, step time: 0.2655\n",
      "145/281, train_loss: 1.0446, step time: 0.2560\n",
      "146/281, train_loss: 0.9964, step time: 0.2559\n",
      "147/281, train_loss: 1.0213, step time: 0.2574\n",
      "148/281, train_loss: 1.0162, step time: 0.2569\n",
      "149/281, train_loss: 1.0182, step time: 0.2596\n",
      "150/281, train_loss: 1.0189, step time: 0.2560\n",
      "151/281, train_loss: 1.0286, step time: 0.2561\n",
      "152/281, train_loss: 1.0123, step time: 0.2618\n",
      "153/281, train_loss: 1.0035, step time: 0.2549\n",
      "154/281, train_loss: 1.0165, step time: 0.2506\n",
      "155/281, train_loss: 1.0251, step time: 0.2472\n",
      "156/281, train_loss: 1.0257, step time: 0.2547\n",
      "157/281, train_loss: 1.0074, step time: 0.2738\n",
      "158/281, train_loss: 1.0045, step time: 0.2656\n",
      "159/281, train_loss: 1.0357, step time: 0.2600\n",
      "160/281, train_loss: 1.0082, step time: 0.2578\n",
      "161/281, train_loss: 1.0163, step time: 0.2492\n",
      "162/281, train_loss: 1.0051, step time: 0.2559\n",
      "163/281, train_loss: 1.0062, step time: 0.2524\n",
      "164/281, train_loss: 1.0256, step time: 0.2618\n",
      "165/281, train_loss: 0.9967, step time: 0.2535\n",
      "166/281, train_loss: 1.0518, step time: 0.2481\n",
      "167/281, train_loss: 1.0202, step time: 0.2610\n",
      "168/281, train_loss: 1.0198, step time: 0.2635\n",
      "169/281, train_loss: 1.0219, step time: 0.2537\n",
      "170/281, train_loss: 1.0180, step time: 0.2508\n",
      "171/281, train_loss: 1.0056, step time: 0.2513\n",
      "172/281, train_loss: 1.0101, step time: 0.2556\n",
      "173/281, train_loss: 0.9960, step time: 0.2522\n",
      "174/281, train_loss: 1.0028, step time: 0.2538\n",
      "175/281, train_loss: 1.0199, step time: 0.2600\n",
      "176/281, train_loss: 1.0223, step time: 0.2535\n",
      "177/281, train_loss: 1.0134, step time: 0.2552\n",
      "178/281, train_loss: 0.9938, step time: 0.2533\n",
      "179/281, train_loss: 1.0085, step time: 0.2505\n",
      "180/281, train_loss: 0.9725, step time: 0.2481\n",
      "181/281, train_loss: 1.0147, step time: 0.2547\n",
      "182/281, train_loss: 0.9959, step time: 0.2740\n",
      "183/281, train_loss: 1.0237, step time: 0.2442\n",
      "184/281, train_loss: 0.9937, step time: 0.2506\n",
      "185/281, train_loss: 1.0073, step time: 0.2775\n",
      "186/281, train_loss: 1.0042, step time: 0.2537\n",
      "187/281, train_loss: 1.0071, step time: 0.2601\n",
      "188/281, train_loss: 1.0133, step time: 0.2590\n",
      "189/281, train_loss: 1.0058, step time: 0.2671\n",
      "190/281, train_loss: 1.0449, step time: 0.2605\n",
      "191/281, train_loss: 1.0111, step time: 0.2498\n",
      "192/281, train_loss: 0.9924, step time: 0.2519\n",
      "193/281, train_loss: 1.0147, step time: 0.2569\n",
      "194/281, train_loss: 1.0315, step time: 0.2539\n",
      "195/281, train_loss: 0.9970, step time: 0.2558\n",
      "196/281, train_loss: 1.0101, step time: 0.2771\n",
      "197/281, train_loss: 1.0143, step time: 0.2773\n",
      "198/281, train_loss: 1.0157, step time: 0.2545\n",
      "199/281, train_loss: 1.0193, step time: 0.2570\n",
      "200/281, train_loss: 0.9931, step time: 0.2528\n",
      "201/281, train_loss: 1.0214, step time: 0.2695\n",
      "202/281, train_loss: 1.0209, step time: 0.2524\n",
      "203/281, train_loss: 1.0283, step time: 0.2510\n",
      "204/281, train_loss: 1.0039, step time: 0.2518\n",
      "205/281, train_loss: 0.9726, step time: 0.2594\n",
      "206/281, train_loss: 0.9896, step time: 0.2587\n",
      "207/281, train_loss: 0.9934, step time: 0.2582\n",
      "208/281, train_loss: 0.9937, step time: 0.2562\n",
      "209/281, train_loss: 1.0192, step time: 0.2564\n",
      "210/281, train_loss: 0.9794, step time: 0.2478\n",
      "211/281, train_loss: 1.0182, step time: 0.2594\n",
      "212/281, train_loss: 0.9909, step time: 0.2596\n",
      "213/281, train_loss: 1.0348, step time: 0.2727\n",
      "214/281, train_loss: 0.9814, step time: 0.3002\n",
      "215/281, train_loss: 1.0121, step time: 0.2538\n",
      "216/281, train_loss: 1.0113, step time: 0.2506\n",
      "217/281, train_loss: 0.9419, step time: 0.2569\n",
      "218/281, train_loss: 1.0187, step time: 0.2548\n",
      "219/281, train_loss: 1.0204, step time: 0.2604\n",
      "220/281, train_loss: 1.0149, step time: 0.2542\n",
      "221/281, train_loss: 1.0145, step time: 0.2534\n",
      "222/281, train_loss: 1.0255, step time: 0.2573\n",
      "223/281, train_loss: 0.9632, step time: 0.2553\n",
      "224/281, train_loss: 1.0191, step time: 0.2506\n",
      "225/281, train_loss: 1.0086, step time: 0.2506\n",
      "226/281, train_loss: 1.0044, step time: 0.2545\n",
      "227/281, train_loss: 1.0112, step time: 0.2591\n",
      "228/281, train_loss: 1.0181, step time: 0.2598\n",
      "229/281, train_loss: 0.9897, step time: 0.2597\n",
      "230/281, train_loss: 0.9985, step time: 0.2496\n",
      "231/281, train_loss: 1.0025, step time: 0.2563\n",
      "232/281, train_loss: 0.9949, step time: 0.2530\n",
      "233/281, train_loss: 1.0240, step time: 0.2536\n",
      "234/281, train_loss: 0.9842, step time: 0.2619\n",
      "235/281, train_loss: 0.9863, step time: 0.2561\n",
      "236/281, train_loss: 0.9835, step time: 0.2627\n",
      "237/281, train_loss: 0.9672, step time: 0.2739\n",
      "238/281, train_loss: 0.9906, step time: 0.2567\n",
      "239/281, train_loss: 1.0314, step time: 0.2495\n",
      "240/281, train_loss: 0.9715, step time: 0.2586\n",
      "241/281, train_loss: 1.0045, step time: 0.2560\n",
      "242/281, train_loss: 1.0131, step time: 0.2527\n",
      "243/281, train_loss: 0.9824, step time: 0.2510\n",
      "244/281, train_loss: 0.9840, step time: 0.2565\n",
      "245/281, train_loss: 0.9888, step time: 0.2580\n",
      "246/281, train_loss: 1.0136, step time: 0.2607\n",
      "247/281, train_loss: 0.9948, step time: 0.2478\n",
      "248/281, train_loss: 1.0140, step time: 0.2551\n",
      "249/281, train_loss: 0.9987, step time: 0.2588\n",
      "250/281, train_loss: 1.0235, step time: 0.2570\n",
      "251/281, train_loss: 1.0205, step time: 0.2571\n",
      "252/281, train_loss: 0.9937, step time: 0.2590\n",
      "253/281, train_loss: 0.9784, step time: 0.2625\n",
      "254/281, train_loss: 0.9888, step time: 0.2573\n",
      "255/281, train_loss: 0.9817, step time: 0.2492\n",
      "256/281, train_loss: 0.9763, step time: 0.2558\n",
      "257/281, train_loss: 1.0497, step time: 0.2500\n",
      "258/281, train_loss: 1.0030, step time: 0.2522\n",
      "259/281, train_loss: 0.9977, step time: 0.2543\n",
      "260/281, train_loss: 1.0205, step time: 0.2503\n",
      "261/281, train_loss: 1.0020, step time: 0.2541\n",
      "262/281, train_loss: 0.9627, step time: 0.2601\n",
      "263/281, train_loss: 0.9724, step time: 0.2474\n",
      "264/281, train_loss: 0.9759, step time: 0.2621\n",
      "265/281, train_loss: 1.0187, step time: 0.2626\n",
      "266/281, train_loss: 1.0160, step time: 0.2572\n",
      "267/281, train_loss: 0.9645, step time: 0.2640\n",
      "268/281, train_loss: 0.9733, step time: 0.2556\n",
      "269/281, train_loss: 0.9904, step time: 0.2557\n",
      "270/281, train_loss: 0.9791, step time: 0.2622\n",
      "271/281, train_loss: 1.0007, step time: 0.2584\n",
      "272/281, train_loss: 0.9790, step time: 0.2615\n",
      "273/281, train_loss: 0.9978, step time: 0.2576\n",
      "274/281, train_loss: 0.9696, step time: 0.2564\n",
      "275/281, train_loss: 0.9944, step time: 0.2543\n",
      "276/281, train_loss: 0.9611, step time: 0.2455\n",
      "277/281, train_loss: 0.9901, step time: 0.2479\n",
      "278/281, train_loss: 0.9725, step time: 0.2508\n",
      "279/281, train_loss: 0.9575, step time: 0.2563\n",
      "280/281, train_loss: 0.9678, step time: 0.2505\n",
      "281/281, train_loss: 0.9715, step time: 0.2457\n",
      "282/281, train_loss: 1.0205, step time: 0.1464\n",
      "epoch 3 average loss: 1.0261\n",
      "saved new best metric model\n",
      "current epoch: 3 current mean dice: 0.3810 tc: 0.0090 wt: 0.7703 et: 0.3655\n",
      "best mean dice: 0.3810 at epoch: 3\n",
      "time consuming of epoch 3 is: 177.2553\n",
      "----------\n",
      "epoch 4/200\n",
      "1/281, train_loss: 0.9449, step time: 0.2660\n",
      "2/281, train_loss: 0.9831, step time: 0.2582\n",
      "3/281, train_loss: 0.9859, step time: 0.2659\n",
      "4/281, train_loss: 1.0130, step time: 0.2529\n",
      "5/281, train_loss: 1.0121, step time: 0.2538\n",
      "6/281, train_loss: 1.0325, step time: 0.2602\n",
      "7/281, train_loss: 0.9995, step time: 0.2538\n",
      "8/281, train_loss: 0.9992, step time: 0.2593\n",
      "9/281, train_loss: 1.0089, step time: 0.2551\n",
      "10/281, train_loss: 0.9922, step time: 0.2596\n",
      "11/281, train_loss: 0.9661, step time: 0.2712\n",
      "12/281, train_loss: 0.9782, step time: 0.2623\n",
      "13/281, train_loss: 0.9625, step time: 0.2636\n",
      "14/281, train_loss: 1.0073, step time: 0.2657\n",
      "15/281, train_loss: 0.9927, step time: 0.2660\n",
      "16/281, train_loss: 0.9855, step time: 0.2659\n",
      "17/281, train_loss: 0.9597, step time: 0.2609\n",
      "18/281, train_loss: 0.9884, step time: 0.2643\n",
      "19/281, train_loss: 0.9428, step time: 0.2611\n",
      "20/281, train_loss: 0.9646, step time: 0.2649\n",
      "21/281, train_loss: 0.9820, step time: 0.2661\n",
      "22/281, train_loss: 0.9859, step time: 0.2652\n",
      "23/281, train_loss: 1.0103, step time: 0.2595\n",
      "24/281, train_loss: 0.9806, step time: 0.2672\n",
      "25/281, train_loss: 0.9653, step time: 0.2628\n",
      "26/281, train_loss: 0.9560, step time: 0.2599\n",
      "27/281, train_loss: 0.9834, step time: 0.2555\n",
      "28/281, train_loss: 0.9587, step time: 0.2590\n",
      "29/281, train_loss: 1.0185, step time: 0.2670\n",
      "30/281, train_loss: 0.9567, step time: 0.2588\n",
      "31/281, train_loss: 1.0185, step time: 0.2707\n",
      "32/281, train_loss: 0.9548, step time: 0.2700\n",
      "33/281, train_loss: 0.9924, step time: 0.2605\n",
      "34/281, train_loss: 0.9715, step time: 0.2588\n",
      "35/281, train_loss: 0.9949, step time: 0.2647\n",
      "36/281, train_loss: 1.0081, step time: 0.2610\n",
      "37/281, train_loss: 0.9723, step time: 0.2592\n",
      "38/281, train_loss: 0.9501, step time: 0.2643\n",
      "39/281, train_loss: 0.9544, step time: 0.2622\n",
      "40/281, train_loss: 0.9526, step time: 0.2643\n",
      "41/281, train_loss: 0.9979, step time: 0.2640\n",
      "42/281, train_loss: 0.9782, step time: 0.2676\n",
      "43/281, train_loss: 0.9855, step time: 0.2611\n",
      "44/281, train_loss: 0.9486, step time: 0.2588\n",
      "45/281, train_loss: 0.9632, step time: 0.2570\n",
      "46/281, train_loss: 0.9272, step time: 0.2594\n",
      "47/281, train_loss: 0.9906, step time: 0.2686\n",
      "48/281, train_loss: 0.9924, step time: 0.2625\n",
      "49/281, train_loss: 0.9284, step time: 0.2674\n",
      "50/281, train_loss: 0.9837, step time: 0.2627\n",
      "51/281, train_loss: 0.9439, step time: 0.2595\n",
      "52/281, train_loss: 0.9887, step time: 0.2667\n",
      "53/281, train_loss: 0.9664, step time: 0.2644\n",
      "54/281, train_loss: 0.9863, step time: 0.2621\n",
      "55/281, train_loss: 0.9830, step time: 0.2587\n",
      "56/281, train_loss: 1.0156, step time: 0.2587\n",
      "57/281, train_loss: 0.9954, step time: 0.2592\n",
      "58/281, train_loss: 1.0082, step time: 0.2638\n",
      "59/281, train_loss: 1.0030, step time: 0.2584\n",
      "60/281, train_loss: 0.9492, step time: 0.2584\n",
      "61/281, train_loss: 0.9509, step time: 0.2696\n",
      "62/281, train_loss: 0.9859, step time: 0.2579\n",
      "63/281, train_loss: 0.9656, step time: 0.2588\n",
      "64/281, train_loss: 0.9906, step time: 0.2621\n",
      "65/281, train_loss: 0.9758, step time: 0.2665\n",
      "66/281, train_loss: 0.9553, step time: 0.2554\n",
      "67/281, train_loss: 0.9569, step time: 0.2637\n",
      "68/281, train_loss: 0.9742, step time: 0.2567\n",
      "69/281, train_loss: 0.9726, step time: 0.2701\n",
      "70/281, train_loss: 0.9359, step time: 0.2576\n",
      "71/281, train_loss: 0.9318, step time: 0.2582\n",
      "72/281, train_loss: 0.9953, step time: 0.2605\n",
      "73/281, train_loss: 0.9789, step time: 0.2585\n",
      "74/281, train_loss: 0.9821, step time: 0.2556\n",
      "75/281, train_loss: 0.9169, step time: 0.2570\n",
      "76/281, train_loss: 0.9175, step time: 0.2698\n",
      "77/281, train_loss: 0.9812, step time: 0.2613\n",
      "78/281, train_loss: 0.9778, step time: 0.2619\n",
      "79/281, train_loss: 0.9622, step time: 0.2679\n",
      "80/281, train_loss: 0.9250, step time: 0.2630\n",
      "81/281, train_loss: 0.9510, step time: 0.2688\n",
      "82/281, train_loss: 0.9734, step time: 0.2569\n",
      "83/281, train_loss: 0.9298, step time: 0.2592\n",
      "84/281, train_loss: 1.0095, step time: 0.2573\n",
      "85/281, train_loss: 0.9706, step time: 0.2564\n",
      "86/281, train_loss: 0.9560, step time: 0.2580\n",
      "87/281, train_loss: 0.9426, step time: 0.2542\n",
      "88/281, train_loss: 1.0011, step time: 0.2635\n",
      "89/281, train_loss: 0.9863, step time: 0.2597\n",
      "90/281, train_loss: 0.9902, step time: 0.2585\n",
      "91/281, train_loss: 0.9565, step time: 0.2573\n",
      "92/281, train_loss: 0.9649, step time: 0.2575\n",
      "93/281, train_loss: 0.9456, step time: 0.2567\n",
      "94/281, train_loss: 0.9435, step time: 0.2576\n",
      "95/281, train_loss: 0.9414, step time: 0.2550\n",
      "96/281, train_loss: 0.9409, step time: 0.2583\n",
      "97/281, train_loss: 0.9435, step time: 0.2658\n",
      "98/281, train_loss: 0.9612, step time: 0.2638\n",
      "99/281, train_loss: 0.9629, step time: 0.2622\n",
      "100/281, train_loss: 0.9846, step time: 0.2597\n",
      "101/281, train_loss: 0.9584, step time: 0.2675\n",
      "102/281, train_loss: 0.9243, step time: 0.2654\n",
      "103/281, train_loss: 0.9747, step time: 0.2626\n",
      "104/281, train_loss: 0.9529, step time: 0.2629\n",
      "105/281, train_loss: 0.9442, step time: 0.2607\n",
      "106/281, train_loss: 0.9158, step time: 0.2584\n",
      "107/281, train_loss: 0.9677, step time: 0.2505\n",
      "108/281, train_loss: 0.9646, step time: 0.2521\n",
      "109/281, train_loss: 1.0025, step time: 0.2511\n",
      "110/281, train_loss: 0.9481, step time: 0.2537\n",
      "111/281, train_loss: 0.9601, step time: 0.2515\n",
      "112/281, train_loss: 0.9750, step time: 0.2511\n",
      "113/281, train_loss: 1.0177, step time: 0.2527\n",
      "114/281, train_loss: 0.9519, step time: 0.2523\n",
      "115/281, train_loss: 0.9377, step time: 0.2524\n",
      "116/281, train_loss: 0.9703, step time: 0.2463\n",
      "117/281, train_loss: 0.9717, step time: 0.2495\n",
      "118/281, train_loss: 0.9505, step time: 0.2498\n",
      "119/281, train_loss: 0.9939, step time: 0.2554\n",
      "120/281, train_loss: 0.9910, step time: 0.2592\n",
      "121/281, train_loss: 0.9915, step time: 0.2512\n",
      "122/281, train_loss: 0.9154, step time: 0.2619\n",
      "123/281, train_loss: 0.9346, step time: 0.2556\n",
      "124/281, train_loss: 0.9338, step time: 0.2624\n",
      "125/281, train_loss: 1.0117, step time: 0.2558\n",
      "126/281, train_loss: 0.9261, step time: 0.2596\n",
      "127/281, train_loss: 0.9116, step time: 0.2571\n",
      "128/281, train_loss: 0.9564, step time: 0.2559\n",
      "129/281, train_loss: 0.9298, step time: 0.2630\n",
      "130/281, train_loss: 0.9495, step time: 0.2597\n",
      "131/281, train_loss: 0.9690, step time: 0.2590\n",
      "132/281, train_loss: 0.9950, step time: 0.2552\n",
      "133/281, train_loss: 0.9387, step time: 0.2628\n",
      "134/281, train_loss: 0.9693, step time: 0.2617\n",
      "135/281, train_loss: 0.9139, step time: 0.2810\n",
      "136/281, train_loss: 0.9704, step time: 0.2538\n",
      "137/281, train_loss: 0.9595, step time: 0.2611\n",
      "138/281, train_loss: 0.9666, step time: 0.2663\n",
      "139/281, train_loss: 0.9427, step time: 0.2519\n",
      "140/281, train_loss: 0.9162, step time: 0.2576\n",
      "141/281, train_loss: 0.9803, step time: 0.2554\n",
      "142/281, train_loss: 0.9664, step time: 0.2598\n",
      "143/281, train_loss: 1.0094, step time: 0.2613\n",
      "144/281, train_loss: 0.9846, step time: 0.2643\n",
      "145/281, train_loss: 0.9613, step time: 0.2629\n",
      "146/281, train_loss: 0.9089, step time: 0.2609\n",
      "147/281, train_loss: 0.9313, step time: 0.2633\n",
      "148/281, train_loss: 0.8876, step time: 0.2632\n",
      "149/281, train_loss: 0.9643, step time: 0.2560\n",
      "150/281, train_loss: 1.0051, step time: 0.2604\n",
      "151/281, train_loss: 0.9360, step time: 0.2607\n",
      "152/281, train_loss: 0.9806, step time: 0.2685\n",
      "153/281, train_loss: 0.8879, step time: 0.2594\n",
      "154/281, train_loss: 0.9226, step time: 0.2598\n",
      "155/281, train_loss: 0.9174, step time: 0.2616\n",
      "156/281, train_loss: 0.9399, step time: 0.2635\n",
      "157/281, train_loss: 0.9912, step time: 0.2533\n",
      "158/281, train_loss: 0.9958, step time: 0.2562\n",
      "159/281, train_loss: 0.9138, step time: 0.2526\n",
      "160/281, train_loss: 0.9188, step time: 0.2530\n",
      "161/281, train_loss: 0.9627, step time: 0.2639\n",
      "162/281, train_loss: 0.9488, step time: 0.2698\n",
      "163/281, train_loss: 0.9320, step time: 0.2622\n",
      "164/281, train_loss: 0.9239, step time: 0.2605\n",
      "165/281, train_loss: 0.9714, step time: 0.2616\n",
      "166/281, train_loss: 0.9672, step time: 0.2570\n",
      "167/281, train_loss: 0.9434, step time: 0.2529\n",
      "168/281, train_loss: 0.9132, step time: 0.2563\n",
      "169/281, train_loss: 0.9924, step time: 0.2536\n",
      "170/281, train_loss: 0.9436, step time: 0.2509\n",
      "171/281, train_loss: 1.0169, step time: 0.2530\n",
      "172/281, train_loss: 0.9904, step time: 0.2530\n",
      "173/281, train_loss: 0.9786, step time: 0.2618\n",
      "174/281, train_loss: 0.9238, step time: 0.2535\n",
      "175/281, train_loss: 0.9931, step time: 0.2502\n",
      "176/281, train_loss: 0.9348, step time: 0.2548\n",
      "177/281, train_loss: 0.9789, step time: 0.2521\n",
      "178/281, train_loss: 0.9423, step time: 0.2662\n",
      "179/281, train_loss: 0.9367, step time: 0.2638\n",
      "180/281, train_loss: 0.9484, step time: 0.2577\n",
      "181/281, train_loss: 0.9392, step time: 0.2629\n",
      "182/281, train_loss: 0.9787, step time: 0.2564\n",
      "183/281, train_loss: 0.9352, step time: 0.2576\n",
      "184/281, train_loss: 0.9626, step time: 0.2586\n",
      "185/281, train_loss: 0.9490, step time: 0.2541\n",
      "186/281, train_loss: 0.9704, step time: 0.2559\n",
      "187/281, train_loss: 0.9571, step time: 0.2666\n",
      "188/281, train_loss: 0.9456, step time: 0.2591\n",
      "189/281, train_loss: 0.9557, step time: 0.2594\n",
      "190/281, train_loss: 0.9527, step time: 0.2602\n",
      "191/281, train_loss: 0.8963, step time: 0.2532\n",
      "192/281, train_loss: 0.9049, step time: 0.2685\n",
      "193/281, train_loss: 0.9101, step time: 0.2553\n",
      "194/281, train_loss: 0.9445, step time: 0.2585\n",
      "195/281, train_loss: 0.9457, step time: 0.2590\n",
      "196/281, train_loss: 0.9344, step time: 0.2674\n",
      "197/281, train_loss: 0.9245, step time: 0.2762\n",
      "198/281, train_loss: 0.9941, step time: 0.2629\n",
      "199/281, train_loss: 0.9547, step time: 0.2571\n",
      "200/281, train_loss: 0.9681, step time: 0.2590\n",
      "201/281, train_loss: 0.9427, step time: 0.2569\n",
      "202/281, train_loss: 0.9339, step time: 0.2578\n",
      "203/281, train_loss: 0.9250, step time: 0.2589\n",
      "204/281, train_loss: 0.9320, step time: 0.2589\n",
      "205/281, train_loss: 0.9674, step time: 0.2624\n",
      "206/281, train_loss: 0.8895, step time: 0.2807\n",
      "207/281, train_loss: 0.9454, step time: 0.2596\n",
      "208/281, train_loss: 0.9680, step time: 0.2748\n",
      "209/281, train_loss: 0.9406, step time: 0.2677\n",
      "210/281, train_loss: 0.9525, step time: 0.2589\n",
      "211/281, train_loss: 0.9320, step time: 0.2619\n",
      "212/281, train_loss: 0.9176, step time: 0.2580\n",
      "213/281, train_loss: 0.9016, step time: 0.2525\n",
      "214/281, train_loss: 0.9730, step time: 0.2520\n",
      "215/281, train_loss: 0.9235, step time: 0.2469\n",
      "216/281, train_loss: 0.9240, step time: 0.2527\n",
      "217/281, train_loss: 0.9243, step time: 0.2859\n",
      "218/281, train_loss: 0.9174, step time: 0.2575\n",
      "219/281, train_loss: 0.8998, step time: 0.2588\n",
      "220/281, train_loss: 0.9564, step time: 0.2591\n",
      "221/281, train_loss: 0.9294, step time: 0.2527\n",
      "222/281, train_loss: 0.9794, step time: 0.2548\n",
      "223/281, train_loss: 0.9304, step time: 0.2583\n",
      "224/281, train_loss: 0.9335, step time: 0.2529\n",
      "225/281, train_loss: 0.9101, step time: 0.2537\n",
      "226/281, train_loss: 0.9181, step time: 0.2596\n",
      "227/281, train_loss: 0.9171, step time: 0.2579\n",
      "228/281, train_loss: 0.9177, step time: 0.2598\n",
      "229/281, train_loss: 0.8718, step time: 0.2559\n",
      "230/281, train_loss: 0.9243, step time: 0.2563\n",
      "231/281, train_loss: 0.8728, step time: 0.2609\n",
      "232/281, train_loss: 0.9067, step time: 0.2595\n",
      "233/281, train_loss: 0.9173, step time: 0.2604\n",
      "234/281, train_loss: 0.8995, step time: 0.2671\n",
      "235/281, train_loss: 0.8987, step time: 0.2529\n",
      "236/281, train_loss: 0.9163, step time: 0.2583\n",
      "237/281, train_loss: 0.9219, step time: 0.2585\n",
      "238/281, train_loss: 0.9468, step time: 0.2712\n",
      "239/281, train_loss: 0.8795, step time: 0.2536\n",
      "240/281, train_loss: 0.9003, step time: 0.2575\n",
      "241/281, train_loss: 0.8967, step time: 0.2587\n",
      "242/281, train_loss: 0.9265, step time: 0.2558\n",
      "243/281, train_loss: 0.9335, step time: 0.2529\n",
      "244/281, train_loss: 0.9120, step time: 0.2487\n",
      "245/281, train_loss: 0.9263, step time: 0.2537\n",
      "246/281, train_loss: 0.9141, step time: 0.2602\n",
      "247/281, train_loss: 0.8920, step time: 0.2559\n",
      "248/281, train_loss: 0.9038, step time: 0.2594\n",
      "249/281, train_loss: 0.9107, step time: 0.2567\n",
      "250/281, train_loss: 0.9288, step time: 0.2560\n",
      "251/281, train_loss: 0.9703, step time: 0.2590\n",
      "252/281, train_loss: 0.9190, step time: 0.2574\n",
      "253/281, train_loss: 0.8877, step time: 0.2586\n",
      "254/281, train_loss: 0.9518, step time: 0.2561\n",
      "255/281, train_loss: 0.8924, step time: 0.2537\n",
      "256/281, train_loss: 0.9694, step time: 0.2595\n",
      "257/281, train_loss: 0.9281, step time: 0.2584\n",
      "258/281, train_loss: 0.9784, step time: 0.2603\n",
      "259/281, train_loss: 0.9090, step time: 0.2516\n",
      "260/281, train_loss: 0.8763, step time: 0.2513\n",
      "261/281, train_loss: 0.8985, step time: 0.2580\n",
      "262/281, train_loss: 0.9556, step time: 0.2567\n",
      "263/281, train_loss: 0.9581, step time: 0.2574\n",
      "264/281, train_loss: 0.9204, step time: 0.2553\n",
      "265/281, train_loss: 0.9396, step time: 0.2510\n",
      "266/281, train_loss: 0.9071, step time: 0.2498\n",
      "267/281, train_loss: 0.9127, step time: 0.2531\n",
      "268/281, train_loss: 0.9018, step time: 0.2512\n",
      "269/281, train_loss: 0.9763, step time: 0.2561\n",
      "270/281, train_loss: 0.9158, step time: 0.2549\n",
      "271/281, train_loss: 0.9206, step time: 0.2611\n",
      "272/281, train_loss: 0.9316, step time: 0.2566\n",
      "273/281, train_loss: 1.0011, step time: 0.2568\n",
      "274/281, train_loss: 0.9406, step time: 0.2523\n",
      "275/281, train_loss: 0.9049, step time: 0.2552\n",
      "276/281, train_loss: 0.9499, step time: 0.2543\n",
      "277/281, train_loss: 0.9033, step time: 0.2504\n",
      "278/281, train_loss: 0.9100, step time: 0.2502\n",
      "279/281, train_loss: 0.9459, step time: 0.2510\n",
      "280/281, train_loss: 0.8884, step time: 0.2496\n",
      "281/281, train_loss: 0.9459, step time: 0.2539\n",
      "282/281, train_loss: 0.8536, step time: 0.1513\n",
      "epoch 4 average loss: 0.9518\n",
      "current epoch: 4 current mean dice: 0.3664 tc: 0.0090 wt: 0.7606 et: 0.3304\n",
      "best mean dice: 0.3810 at epoch: 3\n",
      "time consuming of epoch 4 is: 180.6347\n",
      "----------\n",
      "epoch 5/200\n",
      "1/281, train_loss: 0.9073, step time: 0.2571\n",
      "2/281, train_loss: 0.9523, step time: 0.2617\n",
      "3/281, train_loss: 0.9594, step time: 0.2622\n",
      "4/281, train_loss: 0.8912, step time: 0.2672\n",
      "5/281, train_loss: 0.9419, step time: 0.2624\n",
      "6/281, train_loss: 0.9140, step time: 0.2613\n",
      "7/281, train_loss: 0.9014, step time: 0.2571\n",
      "8/281, train_loss: 0.8935, step time: 0.2559\n",
      "9/281, train_loss: 0.9184, step time: 0.2583\n",
      "10/281, train_loss: 0.9487, step time: 0.2564\n",
      "11/281, train_loss: 0.9205, step time: 0.2528\n",
      "12/281, train_loss: 0.8816, step time: 0.2530\n",
      "13/281, train_loss: 0.8732, step time: 0.2601\n",
      "14/281, train_loss: 0.9133, step time: 0.2619\n",
      "15/281, train_loss: 0.9401, step time: 0.2605\n",
      "16/281, train_loss: 0.9492, step time: 0.2613\n",
      "17/281, train_loss: 0.9768, step time: 0.2570\n",
      "18/281, train_loss: 0.8640, step time: 0.2583\n",
      "19/281, train_loss: 0.9303, step time: 0.2589\n",
      "20/281, train_loss: 0.8971, step time: 0.2574\n",
      "21/281, train_loss: 0.9563, step time: 0.2593\n",
      "22/281, train_loss: 0.9435, step time: 0.2589\n",
      "23/281, train_loss: 1.0046, step time: 0.2598\n",
      "24/281, train_loss: 0.8785, step time: 0.2582\n",
      "25/281, train_loss: 0.9394, step time: 0.2613\n",
      "26/281, train_loss: 0.8935, step time: 0.2594\n",
      "27/281, train_loss: 0.9110, step time: 0.2701\n",
      "28/281, train_loss: 0.9898, step time: 0.2566\n",
      "29/281, train_loss: 0.9515, step time: 0.2634\n",
      "30/281, train_loss: 0.8803, step time: 0.2610\n",
      "31/281, train_loss: 0.8757, step time: 0.2607\n",
      "32/281, train_loss: 0.9035, step time: 0.2644\n",
      "33/281, train_loss: 0.9057, step time: 0.2688\n",
      "34/281, train_loss: 0.8878, step time: 0.2595\n",
      "35/281, train_loss: 0.8430, step time: 0.2629\n",
      "36/281, train_loss: 0.9165, step time: 0.2627\n",
      "37/281, train_loss: 0.9124, step time: 0.2705\n",
      "38/281, train_loss: 0.8802, step time: 0.2584\n",
      "39/281, train_loss: 0.9479, step time: 0.2582\n",
      "40/281, train_loss: 0.9235, step time: 0.2565\n",
      "41/281, train_loss: 0.8670, step time: 0.2533\n",
      "42/281, train_loss: 0.8943, step time: 0.2471\n",
      "43/281, train_loss: 0.9267, step time: 0.2480\n",
      "44/281, train_loss: 0.9167, step time: 0.2532\n",
      "45/281, train_loss: 0.8718, step time: 0.2617\n",
      "46/281, train_loss: 0.8748, step time: 0.2581\n",
      "47/281, train_loss: 0.8628, step time: 0.2598\n",
      "48/281, train_loss: 0.8611, step time: 0.2616\n",
      "49/281, train_loss: 0.8749, step time: 0.2687\n",
      "50/281, train_loss: 0.9520, step time: 0.2632\n",
      "51/281, train_loss: 0.8879, step time: 0.2712\n",
      "52/281, train_loss: 0.9402, step time: 0.2584\n",
      "53/281, train_loss: 0.9120, step time: 0.2637\n",
      "54/281, train_loss: 0.8864, step time: 0.2542\n",
      "55/281, train_loss: 0.9201, step time: 0.2547\n",
      "56/281, train_loss: 0.8868, step time: 0.2566\n",
      "57/281, train_loss: 0.9589, step time: 0.2601\n",
      "58/281, train_loss: 0.9172, step time: 0.2592\n",
      "59/281, train_loss: 0.8872, step time: 0.2604\n",
      "60/281, train_loss: 0.8876, step time: 0.2662\n",
      "61/281, train_loss: 0.8633, step time: 0.2693\n",
      "62/281, train_loss: 0.9428, step time: 0.2709\n",
      "63/281, train_loss: 0.8951, step time: 0.2611\n",
      "64/281, train_loss: 0.8575, step time: 0.2672\n",
      "65/281, train_loss: 0.8966, step time: 0.2711\n",
      "66/281, train_loss: 0.9476, step time: 0.2659\n",
      "67/281, train_loss: 0.8800, step time: 0.2694\n",
      "68/281, train_loss: 0.9083, step time: 0.2684\n",
      "69/281, train_loss: 0.8717, step time: 0.2720\n",
      "70/281, train_loss: 0.9458, step time: 0.2704\n",
      "71/281, train_loss: 0.9139, step time: 0.2688\n",
      "72/281, train_loss: 0.8411, step time: 0.2657\n",
      "73/281, train_loss: 0.9122, step time: 0.2697\n",
      "74/281, train_loss: 0.8484, step time: 0.2616\n",
      "75/281, train_loss: 0.8891, step time: 0.2593\n",
      "76/281, train_loss: 0.8884, step time: 0.2643\n",
      "77/281, train_loss: 0.8704, step time: 0.2649\n",
      "78/281, train_loss: 0.8972, step time: 0.2637\n",
      "79/281, train_loss: 0.9168, step time: 0.2616\n",
      "80/281, train_loss: 0.8719, step time: 0.2632\n",
      "81/281, train_loss: 0.8419, step time: 0.2672\n",
      "82/281, train_loss: 0.9087, step time: 0.2642\n",
      "83/281, train_loss: 0.9359, step time: 0.2623\n",
      "84/281, train_loss: 0.9334, step time: 0.2626\n",
      "85/281, train_loss: 0.9365, step time: 0.2637\n",
      "86/281, train_loss: 0.8953, step time: 0.2611\n",
      "87/281, train_loss: 0.9162, step time: 0.2567\n",
      "88/281, train_loss: 0.9184, step time: 0.2609\n",
      "89/281, train_loss: 0.8856, step time: 0.2566\n",
      "90/281, train_loss: 0.8904, step time: 0.2541\n",
      "91/281, train_loss: 0.8150, step time: 0.2519\n",
      "92/281, train_loss: 0.8618, step time: 0.2474\n",
      "93/281, train_loss: 0.8862, step time: 0.2599\n",
      "94/281, train_loss: 0.9085, step time: 0.2598\n",
      "95/281, train_loss: 0.8803, step time: 0.2705\n",
      "96/281, train_loss: 0.8658, step time: 0.2589\n",
      "97/281, train_loss: 0.8686, step time: 0.2666\n",
      "98/281, train_loss: 0.8544, step time: 0.2668\n",
      "99/281, train_loss: 0.8817, step time: 0.2645\n",
      "100/281, train_loss: 0.8838, step time: 0.2645\n",
      "101/281, train_loss: 0.8314, step time: 0.2593\n",
      "102/281, train_loss: 0.9394, step time: 0.2642\n",
      "103/281, train_loss: 0.9483, step time: 0.2689\n",
      "104/281, train_loss: 0.9178, step time: 0.2638\n",
      "105/281, train_loss: 0.8966, step time: 0.2911\n",
      "106/281, train_loss: 0.8775, step time: 0.2827\n",
      "107/281, train_loss: 0.9421, step time: 0.2694\n",
      "108/281, train_loss: 0.9070, step time: 0.2595\n",
      "109/281, train_loss: 0.8387, step time: 0.2658\n",
      "110/281, train_loss: 0.8661, step time: 0.2765\n",
      "111/281, train_loss: 0.8903, step time: 0.2702\n",
      "112/281, train_loss: 0.9270, step time: 0.2561\n",
      "113/281, train_loss: 0.8207, step time: 0.2728\n",
      "114/281, train_loss: 0.9425, step time: 0.2910\n",
      "115/281, train_loss: 0.9215, step time: 0.2661\n",
      "116/281, train_loss: 0.8760, step time: 0.2581\n",
      "117/281, train_loss: 0.9711, step time: 0.2703\n",
      "118/281, train_loss: 0.9740, step time: 0.2691\n",
      "119/281, train_loss: 0.8681, step time: 0.2611\n",
      "120/281, train_loss: 0.9487, step time: 0.2619\n",
      "121/281, train_loss: 0.8793, step time: 0.2616\n",
      "122/281, train_loss: 0.9098, step time: 0.2617\n",
      "123/281, train_loss: 0.8501, step time: 0.2614\n",
      "124/281, train_loss: 0.8458, step time: 0.2609\n",
      "125/281, train_loss: 0.8761, step time: 0.2662\n",
      "126/281, train_loss: 0.8924, step time: 0.2607\n",
      "127/281, train_loss: 0.8471, step time: 0.2705\n",
      "128/281, train_loss: 0.8759, step time: 0.2669\n",
      "129/281, train_loss: 0.8501, step time: 0.2596\n",
      "130/281, train_loss: 0.9057, step time: 0.2633\n",
      "131/281, train_loss: 0.8883, step time: 0.2645\n",
      "132/281, train_loss: 0.8912, step time: 0.2609\n",
      "133/281, train_loss: 0.8174, step time: 0.2672\n",
      "134/281, train_loss: 0.8896, step time: 0.2592\n",
      "135/281, train_loss: 0.8450, step time: 0.2577\n",
      "136/281, train_loss: 0.8728, step time: 0.2576\n",
      "137/281, train_loss: 0.8914, step time: 0.2591\n",
      "138/281, train_loss: 0.8589, step time: 0.2623\n",
      "139/281, train_loss: 0.8754, step time: 0.2588\n",
      "140/281, train_loss: 0.8888, step time: 0.2588\n",
      "141/281, train_loss: 0.8682, step time: 0.2503\n",
      "142/281, train_loss: 0.8556, step time: 0.2563\n",
      "143/281, train_loss: 0.8331, step time: 0.2643\n",
      "144/281, train_loss: 0.8535, step time: 0.2627\n",
      "145/281, train_loss: 0.8764, step time: 0.2673\n",
      "146/281, train_loss: 0.8766, step time: 0.2615\n",
      "147/281, train_loss: 0.9402, step time: 0.2600\n",
      "148/281, train_loss: 0.9501, step time: 0.2592\n",
      "149/281, train_loss: 0.9488, step time: 0.2590\n",
      "150/281, train_loss: 0.8087, step time: 0.2592\n",
      "151/281, train_loss: 0.8920, step time: 0.2560\n",
      "152/281, train_loss: 0.8585, step time: 0.2609\n",
      "153/281, train_loss: 0.9045, step time: 0.2731\n",
      "154/281, train_loss: 0.8863, step time: 0.2706\n",
      "155/281, train_loss: 0.9215, step time: 0.2699\n",
      "156/281, train_loss: 0.8537, step time: 0.2615\n",
      "157/281, train_loss: 0.8429, step time: 0.2643\n",
      "158/281, train_loss: 0.8044, step time: 0.2615\n",
      "159/281, train_loss: 0.7989, step time: 0.2634\n",
      "160/281, train_loss: 0.8335, step time: 0.2623\n",
      "161/281, train_loss: 0.9422, step time: 0.2683\n",
      "162/281, train_loss: 0.9105, step time: 0.2747\n",
      "163/281, train_loss: 0.9091, step time: 0.2613\n",
      "164/281, train_loss: 0.8745, step time: 0.2647\n",
      "165/281, train_loss: 0.8310, step time: 0.2608\n",
      "166/281, train_loss: 0.8172, step time: 0.2641\n",
      "167/281, train_loss: 0.9035, step time: 0.2596\n",
      "168/281, train_loss: 0.7971, step time: 0.2635\n",
      "169/281, train_loss: 0.8802, step time: 0.2520\n",
      "170/281, train_loss: 0.8517, step time: 0.2569\n",
      "171/281, train_loss: 0.8499, step time: 0.2637\n",
      "172/281, train_loss: 0.9478, step time: 0.2665\n",
      "173/281, train_loss: 0.8482, step time: 0.2691\n",
      "174/281, train_loss: 0.9403, step time: 0.2585\n",
      "175/281, train_loss: 0.9145, step time: 0.2648\n",
      "176/281, train_loss: 0.8294, step time: 0.2843\n",
      "177/281, train_loss: 0.8296, step time: 0.2600\n",
      "178/281, train_loss: 0.8496, step time: 0.2586\n",
      "179/281, train_loss: 0.9001, step time: 0.2653\n",
      "180/281, train_loss: 0.8501, step time: 0.2598\n",
      "181/281, train_loss: 0.8733, step time: 0.2671\n",
      "182/281, train_loss: 0.7759, step time: 0.2575\n",
      "183/281, train_loss: 0.8660, step time: 0.2601\n",
      "184/281, train_loss: 0.8937, step time: 0.2559\n",
      "185/281, train_loss: 0.8545, step time: 0.2536\n",
      "186/281, train_loss: 0.8798, step time: 0.2620\n",
      "187/281, train_loss: 0.8252, step time: 0.2590\n",
      "188/281, train_loss: 0.8131, step time: 0.2595\n",
      "189/281, train_loss: 0.8647, step time: 0.2623\n",
      "190/281, train_loss: 0.8939, step time: 0.2575\n",
      "191/281, train_loss: 0.7855, step time: 0.2640\n",
      "192/281, train_loss: 0.9150, step time: 0.2727\n",
      "193/281, train_loss: 0.8183, step time: 0.2723\n",
      "194/281, train_loss: 0.8629, step time: 0.2647\n",
      "195/281, train_loss: 0.8340, step time: 0.2664\n",
      "196/281, train_loss: 0.9063, step time: 0.2594\n",
      "197/281, train_loss: 0.8881, step time: 0.2579\n",
      "198/281, train_loss: 0.9002, step time: 0.2537\n",
      "199/281, train_loss: 0.8408, step time: 0.2615\n",
      "200/281, train_loss: 0.9368, step time: 0.2654\n",
      "201/281, train_loss: 0.8368, step time: 0.2675\n",
      "202/281, train_loss: 0.9083, step time: 0.2648\n",
      "203/281, train_loss: 0.9037, step time: 0.2604\n",
      "204/281, train_loss: 0.8607, step time: 0.2694\n",
      "205/281, train_loss: 0.8482, step time: 0.2570\n",
      "206/281, train_loss: 0.8976, step time: 0.2567\n",
      "207/281, train_loss: 0.8695, step time: 0.2572\n",
      "208/281, train_loss: 0.8912, step time: 0.2570\n",
      "209/281, train_loss: 0.8610, step time: 0.2580\n",
      "210/281, train_loss: 0.7926, step time: 0.2675\n",
      "211/281, train_loss: 0.8595, step time: 0.2632\n",
      "212/281, train_loss: 0.8399, step time: 0.2608\n",
      "213/281, train_loss: 0.8134, step time: 0.2627\n",
      "214/281, train_loss: 0.8842, step time: 0.2716\n",
      "215/281, train_loss: 0.8917, step time: 0.2628\n",
      "216/281, train_loss: 0.8922, step time: 0.2599\n",
      "217/281, train_loss: 0.9077, step time: 0.2606\n",
      "218/281, train_loss: 0.8704, step time: 0.2605\n",
      "219/281, train_loss: 0.9278, step time: 0.2574\n",
      "220/281, train_loss: 0.8983, step time: 0.2791\n",
      "221/281, train_loss: 0.8384, step time: 0.2729\n",
      "222/281, train_loss: 0.7906, step time: 0.2637\n",
      "223/281, train_loss: 0.8388, step time: 0.2604\n",
      "224/281, train_loss: 0.8893, step time: 0.2645\n",
      "225/281, train_loss: 0.8147, step time: 0.2670\n",
      "226/281, train_loss: 0.8595, step time: 0.2686\n",
      "227/281, train_loss: 0.8079, step time: 0.2616\n",
      "228/281, train_loss: 0.8420, step time: 0.2601\n",
      "229/281, train_loss: 0.8577, step time: 0.2611\n",
      "230/281, train_loss: 0.8692, step time: 0.2709\n",
      "231/281, train_loss: 0.8193, step time: 0.2597\n",
      "232/281, train_loss: 0.8316, step time: 0.2602\n",
      "233/281, train_loss: 0.8315, step time: 0.2594\n",
      "234/281, train_loss: 0.7966, step time: 0.2579\n",
      "235/281, train_loss: 0.9341, step time: 0.2599\n",
      "236/281, train_loss: 0.8396, step time: 0.2603\n",
      "237/281, train_loss: 0.9256, step time: 0.2588\n",
      "238/281, train_loss: 0.8633, step time: 0.2593\n",
      "239/281, train_loss: 0.8252, step time: 0.2591\n",
      "240/281, train_loss: 0.8495, step time: 0.2691\n",
      "241/281, train_loss: 0.8901, step time: 0.2613\n",
      "242/281, train_loss: 0.8054, step time: 0.2631\n",
      "243/281, train_loss: 0.8498, step time: 0.2576\n",
      "244/281, train_loss: 0.8301, step time: 0.2664\n",
      "245/281, train_loss: 0.8842, step time: 0.2610\n",
      "246/281, train_loss: 0.9505, step time: 0.2685\n",
      "247/281, train_loss: 0.8843, step time: 0.2554\n",
      "248/281, train_loss: 0.7905, step time: 0.2593\n",
      "249/281, train_loss: 0.7857, step time: 0.2590\n",
      "250/281, train_loss: 0.9046, step time: 0.2535\n",
      "251/281, train_loss: 0.7813, step time: 0.2549\n",
      "252/281, train_loss: 0.8327, step time: 0.2587\n",
      "253/281, train_loss: 0.8379, step time: 0.2571\n",
      "254/281, train_loss: 0.7902, step time: 0.2554\n",
      "255/281, train_loss: 0.8690, step time: 0.2582\n",
      "256/281, train_loss: 0.7918, step time: 0.2713\n",
      "257/281, train_loss: 0.9164, step time: 0.2607\n",
      "258/281, train_loss: 0.8810, step time: 0.2661\n",
      "259/281, train_loss: 0.7928, step time: 0.2662\n",
      "260/281, train_loss: 0.8746, step time: 0.2550\n",
      "261/281, train_loss: 0.8738, step time: 0.2537\n",
      "262/281, train_loss: 0.7951, step time: 0.2536\n",
      "263/281, train_loss: 0.8376, step time: 0.2574\n",
      "264/281, train_loss: 0.8709, step time: 0.2564\n",
      "265/281, train_loss: 0.9001, step time: 0.2567\n",
      "266/281, train_loss: 0.8040, step time: 0.2589\n",
      "267/281, train_loss: 0.8154, step time: 0.2576\n",
      "268/281, train_loss: 0.8705, step time: 0.2620\n",
      "269/281, train_loss: 0.8704, step time: 0.2593\n",
      "270/281, train_loss: 0.8323, step time: 0.2546\n",
      "271/281, train_loss: 0.7759, step time: 0.2533\n",
      "272/281, train_loss: 0.9176, step time: 0.2610\n",
      "273/281, train_loss: 0.8757, step time: 0.2583\n",
      "274/281, train_loss: 0.8520, step time: 0.2837\n",
      "275/281, train_loss: 0.8463, step time: 0.2534\n",
      "276/281, train_loss: 0.8427, step time: 0.2651\n",
      "277/281, train_loss: 0.8925, step time: 0.2635\n",
      "278/281, train_loss: 0.7704, step time: 0.2584\n",
      "279/281, train_loss: 0.9065, step time: 0.2589\n",
      "280/281, train_loss: 0.8910, step time: 0.2574\n",
      "281/281, train_loss: 0.7815, step time: 0.2576\n",
      "282/281, train_loss: 0.8350, step time: 0.1534\n",
      "epoch 5 average loss: 0.8794\n",
      "saved new best metric model\n",
      "current epoch: 5 current mean dice: 0.4307 tc: 0.0090 wt: 0.8558 et: 0.4273\n",
      "best mean dice: 0.4307 at epoch: 5\n",
      "time consuming of epoch 5 is: 187.7805\n",
      "----------\n",
      "epoch 6/200\n",
      "1/281, train_loss: 0.8547, step time: 0.2638\n",
      "2/281, train_loss: 0.8348, step time: 0.2584\n",
      "3/281, train_loss: 0.8475, step time: 0.2570\n",
      "4/281, train_loss: 0.8976, step time: 0.2591\n",
      "5/281, train_loss: 0.7976, step time: 0.2644\n",
      "6/281, train_loss: 0.7836, step time: 0.2608\n",
      "7/281, train_loss: 0.7811, step time: 0.2670\n",
      "8/281, train_loss: 0.7934, step time: 0.2607\n",
      "9/281, train_loss: 0.8468, step time: 0.2701\n",
      "10/281, train_loss: 0.8336, step time: 0.2691\n",
      "11/281, train_loss: 0.8287, step time: 0.2645\n",
      "12/281, train_loss: 0.8717, step time: 0.2588\n",
      "13/281, train_loss: 0.8468, step time: 0.2641\n",
      "14/281, train_loss: 0.9464, step time: 0.2619\n",
      "15/281, train_loss: 0.8083, step time: 0.2635\n",
      "16/281, train_loss: 0.8782, step time: 0.2616\n",
      "17/281, train_loss: 0.8517, step time: 0.2572\n",
      "18/281, train_loss: 0.7847, step time: 0.2608\n",
      "19/281, train_loss: 0.8237, step time: 0.2572\n",
      "20/281, train_loss: 0.8766, step time: 0.2598\n",
      "21/281, train_loss: 0.9150, step time: 0.2674\n",
      "22/281, train_loss: 0.8382, step time: 0.2655\n",
      "23/281, train_loss: 0.9333, step time: 0.2616\n",
      "24/281, train_loss: 0.7946, step time: 0.2798\n",
      "25/281, train_loss: 0.8737, step time: 0.2620\n",
      "26/281, train_loss: 0.7991, step time: 0.2673\n",
      "27/281, train_loss: 0.8344, step time: 0.2618\n",
      "28/281, train_loss: 0.7621, step time: 0.2564\n",
      "29/281, train_loss: 0.8198, step time: 0.2635\n",
      "30/281, train_loss: 0.8105, step time: 0.2626\n",
      "31/281, train_loss: 0.8393, step time: 0.2653\n",
      "32/281, train_loss: 0.7633, step time: 0.2649\n",
      "33/281, train_loss: 0.8503, step time: 0.2674\n",
      "34/281, train_loss: 0.7882, step time: 0.2718\n",
      "35/281, train_loss: 0.8465, step time: 0.2707\n",
      "36/281, train_loss: 0.8410, step time: 0.2593\n",
      "37/281, train_loss: 0.8796, step time: 0.2591\n",
      "38/281, train_loss: 0.7820, step time: 0.2583\n",
      "39/281, train_loss: 0.7903, step time: 0.2604\n",
      "40/281, train_loss: 0.9066, step time: 0.2588\n",
      "41/281, train_loss: 0.8584, step time: 0.2612\n",
      "42/281, train_loss: 0.8531, step time: 0.2671\n",
      "43/281, train_loss: 0.7623, step time: 0.2644\n",
      "44/281, train_loss: 0.8990, step time: 0.2577\n",
      "45/281, train_loss: 0.7667, step time: 0.2663\n",
      "46/281, train_loss: 0.8719, step time: 0.2589\n",
      "47/281, train_loss: 0.8190, step time: 0.2604\n",
      "48/281, train_loss: 0.7823, step time: 0.2574\n",
      "49/281, train_loss: 0.8258, step time: 0.2546\n",
      "50/281, train_loss: 0.8273, step time: 0.2534\n",
      "51/281, train_loss: 0.7665, step time: 0.2571\n",
      "52/281, train_loss: 0.8782, step time: 0.2591\n",
      "53/281, train_loss: 0.8243, step time: 0.2534\n",
      "54/281, train_loss: 0.7672, step time: 0.2495\n",
      "55/281, train_loss: 0.8175, step time: 0.2609\n",
      "56/281, train_loss: 0.7830, step time: 0.2657\n",
      "57/281, train_loss: 0.7692, step time: 0.2678\n",
      "58/281, train_loss: 0.7646, step time: 0.2665\n",
      "59/281, train_loss: 0.8148, step time: 0.2645\n",
      "60/281, train_loss: 0.7557, step time: 0.2640\n",
      "61/281, train_loss: 0.7541, step time: 0.2708\n",
      "62/281, train_loss: 0.8160, step time: 0.2656\n",
      "63/281, train_loss: 0.8814, step time: 0.2616\n",
      "64/281, train_loss: 0.8722, step time: 0.2586\n",
      "65/281, train_loss: 0.7456, step time: 0.2645\n",
      "66/281, train_loss: 0.8437, step time: 0.2724\n",
      "67/281, train_loss: 0.7666, step time: 0.2643\n",
      "68/281, train_loss: 0.8455, step time: 0.2650\n",
      "69/281, train_loss: 0.8153, step time: 0.2595\n",
      "70/281, train_loss: 0.8103, step time: 0.2644\n",
      "71/281, train_loss: 0.7416, step time: 0.2670\n",
      "72/281, train_loss: 0.8485, step time: 0.2656\n",
      "73/281, train_loss: 0.8879, step time: 0.2627\n",
      "74/281, train_loss: 0.8073, step time: 0.2678\n",
      "75/281, train_loss: 0.7961, step time: 0.2644\n",
      "76/281, train_loss: 0.7676, step time: 0.2619\n",
      "77/281, train_loss: 0.8154, step time: 0.2659\n",
      "78/281, train_loss: 0.8321, step time: 0.2624\n",
      "79/281, train_loss: 0.7730, step time: 0.2604\n",
      "80/281, train_loss: 0.7543, step time: 0.2602\n",
      "81/281, train_loss: 0.7656, step time: 0.2618\n",
      "82/281, train_loss: 0.9109, step time: 0.2627\n",
      "83/281, train_loss: 0.8283, step time: 0.2638\n",
      "84/281, train_loss: 0.9144, step time: 0.2633\n",
      "85/281, train_loss: 0.7490, step time: 0.2688\n",
      "86/281, train_loss: 0.8367, step time: 0.2643\n",
      "87/281, train_loss: 0.8042, step time: 0.2558\n",
      "88/281, train_loss: 0.8452, step time: 0.2566\n",
      "89/281, train_loss: 0.8244, step time: 0.2544\n",
      "90/281, train_loss: 0.8397, step time: 0.2579\n",
      "91/281, train_loss: 0.9435, step time: 0.2658\n",
      "92/281, train_loss: 0.8661, step time: 0.2663\n",
      "93/281, train_loss: 0.8113, step time: 0.2606\n",
      "94/281, train_loss: 0.8047, step time: 0.2626\n",
      "95/281, train_loss: 0.8898, step time: 0.2605\n",
      "96/281, train_loss: 0.8542, step time: 0.2640\n",
      "97/281, train_loss: 0.7491, step time: 0.2635\n",
      "98/281, train_loss: 0.8202, step time: 0.2607\n",
      "99/281, train_loss: 0.7565, step time: 0.2567\n",
      "100/281, train_loss: 0.8642, step time: 0.2620\n",
      "101/281, train_loss: 0.8043, step time: 0.2658\n",
      "102/281, train_loss: 0.7696, step time: 0.2624\n",
      "103/281, train_loss: 0.7871, step time: 0.2575\n",
      "104/281, train_loss: 0.7813, step time: 0.2613\n",
      "105/281, train_loss: 0.7531, step time: 0.2666\n",
      "106/281, train_loss: 0.9022, step time: 0.2572\n",
      "107/281, train_loss: 0.7784, step time: 0.2588\n",
      "108/281, train_loss: 0.7982, step time: 0.2598\n",
      "109/281, train_loss: 0.8698, step time: 0.2597\n",
      "110/281, train_loss: 0.7765, step time: 0.2665\n",
      "111/281, train_loss: 0.8078, step time: 0.2657\n",
      "112/281, train_loss: 0.7556, step time: 0.2529\n",
      "113/281, train_loss: 0.7701, step time: 0.2578\n",
      "114/281, train_loss: 0.8043, step time: 0.2845\n",
      "115/281, train_loss: 0.9156, step time: 0.2611\n",
      "116/281, train_loss: 0.8026, step time: 0.2618\n",
      "117/281, train_loss: 0.7613, step time: 0.2549\n",
      "118/281, train_loss: 0.7749, step time: 0.2611\n",
      "119/281, train_loss: 0.7689, step time: 0.2600\n",
      "120/281, train_loss: 0.8186, step time: 0.2630\n",
      "121/281, train_loss: 0.7928, step time: 0.2620\n",
      "122/281, train_loss: 0.7529, step time: 0.2571\n",
      "123/281, train_loss: 0.7723, step time: 0.2582\n",
      "124/281, train_loss: 0.8120, step time: 0.2519\n",
      "125/281, train_loss: 0.7899, step time: 0.2535\n",
      "126/281, train_loss: 0.7227, step time: 0.2630\n",
      "127/281, train_loss: 0.7449, step time: 0.2775\n",
      "128/281, train_loss: 0.9068, step time: 0.2613\n",
      "129/281, train_loss: 0.8249, step time: 0.2608\n",
      "130/281, train_loss: 0.8405, step time: 0.2612\n",
      "131/281, train_loss: 0.6991, step time: 0.2635\n",
      "132/281, train_loss: 0.7533, step time: 0.2777\n",
      "133/281, train_loss: 0.8605, step time: 0.2611\n",
      "134/281, train_loss: 0.7707, step time: 0.2567\n",
      "135/281, train_loss: 0.8204, step time: 0.2601\n",
      "136/281, train_loss: 0.8240, step time: 0.2597\n",
      "137/281, train_loss: 0.8354, step time: 0.2614\n",
      "138/281, train_loss: 0.7485, step time: 0.2602\n",
      "139/281, train_loss: 0.8708, step time: 0.2584\n",
      "140/281, train_loss: 0.8089, step time: 0.2563\n",
      "141/281, train_loss: 0.8167, step time: 0.2595\n",
      "142/281, train_loss: 0.7343, step time: 0.2585\n",
      "143/281, train_loss: 0.7760, step time: 0.2624\n",
      "144/281, train_loss: 0.7713, step time: 0.2645\n",
      "145/281, train_loss: 0.9489, step time: 0.2655\n",
      "146/281, train_loss: 0.8572, step time: 0.2639\n",
      "147/281, train_loss: 0.7494, step time: 0.2585\n",
      "148/281, train_loss: 0.8089, step time: 0.2623\n",
      "149/281, train_loss: 0.7829, step time: 0.2863\n",
      "150/281, train_loss: 0.7262, step time: 0.2586\n",
      "151/281, train_loss: 0.8033, step time: 0.2551\n",
      "152/281, train_loss: 0.8201, step time: 0.2574\n",
      "153/281, train_loss: 0.8008, step time: 0.2589\n",
      "154/281, train_loss: 0.7470, step time: 0.2605\n",
      "155/281, train_loss: 0.8710, step time: 0.2569\n",
      "156/281, train_loss: 0.8584, step time: 0.2586\n",
      "157/281, train_loss: 0.7918, step time: 0.2621\n",
      "158/281, train_loss: 0.8824, step time: 0.2586\n",
      "159/281, train_loss: 0.7565, step time: 0.2587\n",
      "160/281, train_loss: 0.7494, step time: 0.2594\n",
      "161/281, train_loss: 0.7147, step time: 0.2641\n",
      "162/281, train_loss: 0.8048, step time: 0.2625\n",
      "163/281, train_loss: 0.8190, step time: 0.2614\n",
      "164/281, train_loss: 0.8801, step time: 0.2669\n",
      "165/281, train_loss: 0.8474, step time: 0.2702\n",
      "166/281, train_loss: 0.7814, step time: 0.2670\n",
      "167/281, train_loss: 0.8114, step time: 0.2665\n",
      "168/281, train_loss: 0.7633, step time: 0.2624\n",
      "169/281, train_loss: 0.8427, step time: 0.2680\n",
      "170/281, train_loss: 0.8613, step time: 0.2570\n",
      "171/281, train_loss: 0.7703, step time: 0.2575\n",
      "172/281, train_loss: 0.7871, step time: 0.2593\n",
      "173/281, train_loss: 0.7962, step time: 0.2608\n",
      "174/281, train_loss: 0.7527, step time: 0.2562\n",
      "175/281, train_loss: 0.8752, step time: 0.2596\n",
      "176/281, train_loss: 0.8203, step time: 0.2569\n",
      "177/281, train_loss: 0.8174, step time: 0.2574\n",
      "178/281, train_loss: 0.8366, step time: 0.2583\n",
      "179/281, train_loss: 0.7691, step time: 0.2644\n",
      "180/281, train_loss: 0.7716, step time: 0.2555\n",
      "181/281, train_loss: 0.7706, step time: 0.2548\n",
      "182/281, train_loss: 0.7392, step time: 0.2502\n",
      "183/281, train_loss: 0.8855, step time: 0.2603\n",
      "184/281, train_loss: 0.8079, step time: 0.2509\n",
      "185/281, train_loss: 0.7815, step time: 0.2540\n",
      "186/281, train_loss: 0.7735, step time: 0.2575\n",
      "187/281, train_loss: 0.8049, step time: 0.2585\n",
      "188/281, train_loss: 0.7708, step time: 0.2577\n",
      "189/281, train_loss: 0.8470, step time: 0.2590\n",
      "190/281, train_loss: 0.8334, step time: 0.2578\n",
      "191/281, train_loss: 0.7386, step time: 0.2688\n",
      "192/281, train_loss: 0.7493, step time: 0.2535\n",
      "193/281, train_loss: 0.8166, step time: 0.2586\n",
      "194/281, train_loss: 0.7300, step time: 0.2572\n",
      "195/281, train_loss: 0.7887, step time: 0.2559\n",
      "196/281, train_loss: 0.7756, step time: 0.2553\n",
      "197/281, train_loss: 0.7067, step time: 0.2524\n",
      "198/281, train_loss: 0.7330, step time: 0.2540\n",
      "199/281, train_loss: 0.8664, step time: 0.2541\n",
      "200/281, train_loss: 0.7780, step time: 0.2554\n",
      "201/281, train_loss: 0.7582, step time: 0.2540\n",
      "202/281, train_loss: 0.8010, step time: 0.2553\n",
      "203/281, train_loss: 0.7506, step time: 0.2622\n",
      "204/281, train_loss: 0.8349, step time: 0.2585\n",
      "205/281, train_loss: 0.8957, step time: 0.2590\n",
      "206/281, train_loss: 0.8141, step time: 0.2593\n",
      "207/281, train_loss: 0.7675, step time: 0.2524\n",
      "208/281, train_loss: 0.7566, step time: 0.2546\n",
      "209/281, train_loss: 0.7315, step time: 0.2534\n",
      "210/281, train_loss: 0.8608, step time: 0.2546\n",
      "211/281, train_loss: 0.7777, step time: 0.2566\n",
      "212/281, train_loss: 0.7482, step time: 0.2539\n",
      "213/281, train_loss: 0.8466, step time: 0.2623\n",
      "214/281, train_loss: 0.8711, step time: 0.2577\n",
      "215/281, train_loss: 0.8025, step time: 0.2568\n",
      "216/281, train_loss: 0.7100, step time: 0.2613\n",
      "217/281, train_loss: 0.7437, step time: 0.2675\n",
      "218/281, train_loss: 0.7320, step time: 0.2564\n",
      "219/281, train_loss: 0.7420, step time: 0.2591\n",
      "220/281, train_loss: 0.7976, step time: 0.2536\n",
      "221/281, train_loss: 0.8213, step time: 0.2580\n",
      "222/281, train_loss: 0.7260, step time: 0.2583\n",
      "223/281, train_loss: 0.7517, step time: 0.2642\n",
      "224/281, train_loss: 0.8151, step time: 0.2586\n",
      "225/281, train_loss: 0.7340, step time: 0.2558\n",
      "226/281, train_loss: 0.7639, step time: 0.2579\n",
      "227/281, train_loss: 0.7856, step time: 0.2577\n",
      "228/281, train_loss: 0.6988, step time: 0.2526\n",
      "229/281, train_loss: 0.8930, step time: 0.2613\n",
      "230/281, train_loss: 0.8328, step time: 0.2568\n",
      "231/281, train_loss: 0.7674, step time: 0.2501\n",
      "232/281, train_loss: 0.7369, step time: 0.2616\n",
      "233/281, train_loss: 0.7582, step time: 0.2645\n",
      "234/281, train_loss: 0.8332, step time: 0.2551\n",
      "235/281, train_loss: 0.7777, step time: 0.2595\n",
      "236/281, train_loss: 0.8743, step time: 0.2624\n",
      "237/281, train_loss: 0.7941, step time: 0.2642\n",
      "238/281, train_loss: 0.8107, step time: 0.2584\n",
      "239/281, train_loss: 0.7028, step time: 0.2568\n",
      "240/281, train_loss: 0.8564, step time: 0.2562\n",
      "241/281, train_loss: 0.7989, step time: 0.2580\n",
      "242/281, train_loss: 0.7824, step time: 0.2633\n",
      "243/281, train_loss: 0.7430, step time: 0.2600\n",
      "244/281, train_loss: 0.7545, step time: 0.2534\n",
      "245/281, train_loss: 0.7841, step time: 0.2586\n",
      "246/281, train_loss: 0.7893, step time: 0.2625\n",
      "247/281, train_loss: 0.8666, step time: 0.2588\n",
      "248/281, train_loss: 0.7669, step time: 0.2577\n",
      "249/281, train_loss: 0.7833, step time: 0.2561\n",
      "250/281, train_loss: 0.8262, step time: 0.2579\n",
      "251/281, train_loss: 0.8074, step time: 0.2559\n",
      "252/281, train_loss: 0.7774, step time: 0.2581\n",
      "253/281, train_loss: 0.8540, step time: 0.2615\n",
      "254/281, train_loss: 0.7536, step time: 0.2591\n",
      "255/281, train_loss: 0.8294, step time: 0.2587\n",
      "256/281, train_loss: 0.7741, step time: 0.2577\n",
      "257/281, train_loss: 0.7674, step time: 0.2556\n",
      "258/281, train_loss: 0.7568, step time: 0.2655\n",
      "259/281, train_loss: 0.7827, step time: 0.2607\n",
      "260/281, train_loss: 0.7992, step time: 0.2597\n",
      "261/281, train_loss: 0.7895, step time: 0.2667\n",
      "262/281, train_loss: 0.7359, step time: 0.2616\n",
      "263/281, train_loss: 0.7152, step time: 0.2571\n",
      "264/281, train_loss: 0.7670, step time: 0.2679\n",
      "265/281, train_loss: 0.7477, step time: 0.2588\n",
      "266/281, train_loss: 0.7074, step time: 0.2581\n",
      "267/281, train_loss: 0.7379, step time: 0.2680\n",
      "268/281, train_loss: 0.7061, step time: 0.2668\n",
      "269/281, train_loss: 0.8093, step time: 0.2628\n",
      "270/281, train_loss: 0.7381, step time: 0.2677\n",
      "271/281, train_loss: 0.8690, step time: 0.2677\n",
      "272/281, train_loss: 0.7671, step time: 0.2812\n",
      "273/281, train_loss: 0.6956, step time: 0.2663\n",
      "274/281, train_loss: 0.8463, step time: 0.2618\n",
      "275/281, train_loss: 0.7965, step time: 0.2646\n",
      "276/281, train_loss: 0.7040, step time: 0.2626\n",
      "277/281, train_loss: 0.7478, step time: 0.2632\n",
      "278/281, train_loss: 0.7198, step time: 0.2576\n",
      "279/281, train_loss: 0.7407, step time: 0.2616\n",
      "280/281, train_loss: 0.7986, step time: 0.2554\n",
      "281/281, train_loss: 0.7506, step time: 0.2577\n",
      "282/281, train_loss: 0.7275, step time: 0.1544\n",
      "epoch 6 average loss: 0.8022\n",
      "saved new best metric model\n",
      "current epoch: 6 current mean dice: 0.4349 tc: 0.0090 wt: 0.8509 et: 0.4492\n",
      "best mean dice: 0.4349 at epoch: 6\n",
      "time consuming of epoch 6 is: 258.7688\n",
      "----------\n",
      "epoch 7/200\n",
      "1/281, train_loss: 0.7418, step time: 0.2653\n",
      "2/281, train_loss: 0.7779, step time: 0.2551\n",
      "3/281, train_loss: 0.8407, step time: 0.2510\n",
      "4/281, train_loss: 0.8147, step time: 0.2582\n",
      "5/281, train_loss: 0.7467, step time: 0.2494\n",
      "6/281, train_loss: 0.7936, step time: 0.2553\n",
      "7/281, train_loss: 0.7861, step time: 0.2541\n",
      "8/281, train_loss: 0.7212, step time: 0.2559\n",
      "9/281, train_loss: 0.6947, step time: 0.2517\n",
      "10/281, train_loss: 0.7736, step time: 0.2549\n",
      "11/281, train_loss: 0.7364, step time: 0.2962\n",
      "12/281, train_loss: 0.8171, step time: 0.2562\n",
      "13/281, train_loss: 0.7627, step time: 0.2627\n",
      "14/281, train_loss: 0.7649, step time: 0.2561\n",
      "15/281, train_loss: 0.7604, step time: 0.2644\n",
      "16/281, train_loss: 0.7055, step time: 0.2529\n",
      "17/281, train_loss: 0.8477, step time: 0.2507\n",
      "18/281, train_loss: 0.8082, step time: 0.2523\n",
      "19/281, train_loss: 0.7702, step time: 0.2500\n",
      "20/281, train_loss: 0.7906, step time: 0.2570\n",
      "21/281, train_loss: 0.7869, step time: 0.2620\n",
      "22/281, train_loss: 0.7940, step time: 0.2528\n",
      "23/281, train_loss: 0.9042, step time: 0.2576\n",
      "24/281, train_loss: 0.7728, step time: 0.2538\n",
      "25/281, train_loss: 0.7795, step time: 0.2514\n",
      "26/281, train_loss: 0.8164, step time: 0.2569\n",
      "27/281, train_loss: 0.7916, step time: 0.2619\n",
      "28/281, train_loss: 0.7083, step time: 0.2610\n",
      "29/281, train_loss: 0.7816, step time: 0.2572\n",
      "30/281, train_loss: 0.7049, step time: 0.2613\n",
      "31/281, train_loss: 0.7526, step time: 0.2577\n",
      "32/281, train_loss: 0.8222, step time: 0.2505\n",
      "33/281, train_loss: 0.7643, step time: 0.2574\n",
      "34/281, train_loss: 0.7043, step time: 0.2543\n",
      "35/281, train_loss: 0.8340, step time: 0.2579\n",
      "36/281, train_loss: 0.7218, step time: 0.2586\n",
      "37/281, train_loss: 0.7053, step time: 0.2550\n",
      "38/281, train_loss: 0.7477, step time: 0.2539\n",
      "39/281, train_loss: 0.8542, step time: 0.2522\n",
      "40/281, train_loss: 0.7515, step time: 0.2565\n",
      "41/281, train_loss: 0.7674, step time: 0.2576\n",
      "42/281, train_loss: 0.7672, step time: 0.2596\n",
      "43/281, train_loss: 0.8641, step time: 0.2600\n",
      "44/281, train_loss: 0.7965, step time: 0.2526\n",
      "45/281, train_loss: 0.7442, step time: 0.2576\n",
      "46/281, train_loss: 0.6788, step time: 0.2553\n",
      "47/281, train_loss: 0.7094, step time: 0.2543\n",
      "48/281, train_loss: 0.7283, step time: 0.2551\n",
      "49/281, train_loss: 0.8323, step time: 0.2543\n",
      "50/281, train_loss: 0.6918, step time: 0.2510\n",
      "51/281, train_loss: 0.8013, step time: 0.2529\n",
      "52/281, train_loss: 0.7392, step time: 0.2572\n",
      "53/281, train_loss: 0.7238, step time: 0.2524\n",
      "54/281, train_loss: 0.7364, step time: 0.2585\n",
      "55/281, train_loss: 0.7639, step time: 0.2998\n",
      "56/281, train_loss: 0.7904, step time: 0.2586\n",
      "57/281, train_loss: 0.7264, step time: 0.2520\n",
      "58/281, train_loss: 0.8010, step time: 0.2548\n",
      "59/281, train_loss: 0.7244, step time: 0.2604\n",
      "60/281, train_loss: 0.7993, step time: 0.2571\n",
      "61/281, train_loss: 0.7167, step time: 0.2583\n",
      "62/281, train_loss: 0.7310, step time: 0.2518\n",
      "63/281, train_loss: 0.7877, step time: 0.2528\n",
      "64/281, train_loss: 0.7334, step time: 0.2525\n",
      "65/281, train_loss: 0.7539, step time: 0.2572\n",
      "66/281, train_loss: 0.6978, step time: 0.2601\n",
      "67/281, train_loss: 0.7303, step time: 0.2635\n",
      "68/281, train_loss: 0.7624, step time: 0.2663\n",
      "69/281, train_loss: 0.7365, step time: 0.2563\n",
      "70/281, train_loss: 0.7991, step time: 0.2525\n",
      "71/281, train_loss: 0.7935, step time: 0.2549\n",
      "72/281, train_loss: 0.7006, step time: 0.2596\n",
      "73/281, train_loss: 0.7410, step time: 0.2588\n",
      "74/281, train_loss: 0.7335, step time: 0.2672\n",
      "75/281, train_loss: 0.7604, step time: 0.2551\n",
      "76/281, train_loss: 0.7976, step time: 0.2586\n",
      "77/281, train_loss: 0.7484, step time: 0.2597\n",
      "78/281, train_loss: 0.7150, step time: 0.2643\n",
      "79/281, train_loss: 0.7412, step time: 0.2589\n",
      "80/281, train_loss: 0.7295, step time: 0.2721\n",
      "81/281, train_loss: 0.7480, step time: 0.2578\n",
      "82/281, train_loss: 0.7072, step time: 0.2619\n",
      "83/281, train_loss: 0.7072, step time: 0.2578\n",
      "84/281, train_loss: 0.7180, step time: 0.2563\n",
      "85/281, train_loss: 0.7623, step time: 0.2600\n",
      "86/281, train_loss: 0.7469, step time: 0.2729\n",
      "87/281, train_loss: 0.7323, step time: 0.2642\n",
      "88/281, train_loss: 0.7720, step time: 0.2575\n",
      "89/281, train_loss: 0.7372, step time: 0.2597\n",
      "90/281, train_loss: 0.7243, step time: 0.2601\n",
      "91/281, train_loss: 0.8152, step time: 0.2627\n",
      "92/281, train_loss: 0.7518, step time: 0.2602\n",
      "93/281, train_loss: 0.7220, step time: 0.2619\n",
      "94/281, train_loss: 0.7251, step time: 0.2635\n",
      "95/281, train_loss: 0.7482, step time: 0.2586\n",
      "96/281, train_loss: 0.7529, step time: 0.2574\n",
      "97/281, train_loss: 0.7722, step time: 0.2516\n",
      "98/281, train_loss: 0.7816, step time: 0.2567\n",
      "99/281, train_loss: 0.7016, step time: 0.2545\n",
      "100/281, train_loss: 0.7452, step time: 0.2513\n",
      "101/281, train_loss: 0.7185, step time: 0.2605\n",
      "102/281, train_loss: 0.8492, step time: 0.2523\n",
      "103/281, train_loss: 0.6618, step time: 0.2554\n",
      "104/281, train_loss: 0.6923, step time: 0.2596\n",
      "105/281, train_loss: 0.7400, step time: 0.2659\n",
      "106/281, train_loss: 0.7120, step time: 0.2562\n",
      "107/281, train_loss: 0.8116, step time: 0.2602\n",
      "108/281, train_loss: 0.8079, step time: 0.2519\n",
      "109/281, train_loss: 0.7080, step time: 0.2553\n",
      "110/281, train_loss: 0.6731, step time: 0.2628\n",
      "111/281, train_loss: 0.7299, step time: 0.2595\n",
      "112/281, train_loss: 0.6554, step time: 0.2609\n",
      "113/281, train_loss: 0.6889, step time: 0.2594\n",
      "114/281, train_loss: 0.7278, step time: 0.2544\n",
      "115/281, train_loss: 0.7428, step time: 0.2588\n",
      "116/281, train_loss: 0.7026, step time: 0.2592\n",
      "117/281, train_loss: 0.8104, step time: 0.2532\n",
      "118/281, train_loss: 0.7327, step time: 0.2561\n",
      "119/281, train_loss: 0.7044, step time: 0.2588\n",
      "120/281, train_loss: 0.7012, step time: 0.2533\n",
      "121/281, train_loss: 0.7061, step time: 0.2541\n",
      "122/281, train_loss: 0.7149, step time: 0.2571\n",
      "123/281, train_loss: 0.6611, step time: 0.2566\n",
      "124/281, train_loss: 0.8276, step time: 0.2577\n",
      "125/281, train_loss: 0.6917, step time: 0.2594\n",
      "126/281, train_loss: 0.7527, step time: 0.2555\n",
      "127/281, train_loss: 0.7792, step time: 0.2538\n",
      "128/281, train_loss: 0.6962, step time: 0.2511\n",
      "129/281, train_loss: 0.7351, step time: 0.2561\n",
      "130/281, train_loss: 0.7400, step time: 0.2548\n",
      "131/281, train_loss: 0.8086, step time: 0.2544\n",
      "132/281, train_loss: 0.7072, step time: 0.2584\n",
      "133/281, train_loss: 0.7207, step time: 0.2603\n",
      "134/281, train_loss: 0.7192, step time: 0.2543\n",
      "135/281, train_loss: 0.6728, step time: 0.2556\n",
      "136/281, train_loss: 0.6979, step time: 0.2491\n",
      "137/281, train_loss: 0.6549, step time: 0.2516\n",
      "138/281, train_loss: 0.7136, step time: 0.2523\n",
      "139/281, train_loss: 0.7405, step time: 0.2510\n",
      "140/281, train_loss: 0.7493, step time: 0.2553\n",
      "141/281, train_loss: 0.7357, step time: 0.2547\n",
      "142/281, train_loss: 0.8150, step time: 0.2537\n",
      "143/281, train_loss: 0.7279, step time: 0.2547\n",
      "144/281, train_loss: 0.6825, step time: 0.2577\n",
      "145/281, train_loss: 0.8039, step time: 0.2578\n",
      "146/281, train_loss: 0.7267, step time: 0.2590\n",
      "147/281, train_loss: 0.7904, step time: 0.2525\n",
      "148/281, train_loss: 0.8379, step time: 0.2493\n",
      "149/281, train_loss: 0.6509, step time: 0.2540\n",
      "150/281, train_loss: 0.7375, step time: 0.2638\n",
      "151/281, train_loss: 0.7577, step time: 0.2632\n",
      "152/281, train_loss: 0.7198, step time: 0.2512\n",
      "153/281, train_loss: 0.7934, step time: 0.2571\n",
      "154/281, train_loss: 0.8248, step time: 0.2582\n",
      "155/281, train_loss: 0.6618, step time: 0.2589\n",
      "156/281, train_loss: 0.7062, step time: 0.2536\n",
      "157/281, train_loss: 0.6982, step time: 0.2517\n",
      "158/281, train_loss: 0.7294, step time: 0.2572\n",
      "159/281, train_loss: 0.7908, step time: 0.2571\n",
      "160/281, train_loss: 0.8227, step time: 0.2544\n",
      "161/281, train_loss: 0.6843, step time: 0.2559\n",
      "162/281, train_loss: 0.7353, step time: 0.2622\n",
      "163/281, train_loss: 0.7367, step time: 0.2585\n",
      "164/281, train_loss: 0.7503, step time: 0.2645\n",
      "165/281, train_loss: 0.7009, step time: 0.2596\n",
      "166/281, train_loss: 0.6848, step time: 0.2518\n",
      "167/281, train_loss: 0.7720, step time: 0.2511\n",
      "168/281, train_loss: 0.6904, step time: 0.2491\n",
      "169/281, train_loss: 0.8265, step time: 0.2518\n",
      "170/281, train_loss: 0.7629, step time: 0.2547\n",
      "171/281, train_loss: 0.7013, step time: 0.2548\n",
      "172/281, train_loss: 0.6879, step time: 0.2593\n",
      "173/281, train_loss: 0.6952, step time: 0.2609\n",
      "174/281, train_loss: 0.6971, step time: 0.2459\n",
      "175/281, train_loss: 0.7608, step time: 0.2517\n",
      "176/281, train_loss: 0.7167, step time: 0.2549\n",
      "177/281, train_loss: 0.6930, step time: 0.2508\n",
      "178/281, train_loss: 0.6823, step time: 0.2591\n",
      "179/281, train_loss: 0.7943, step time: 0.2621\n",
      "180/281, train_loss: 0.6962, step time: 0.2609\n",
      "181/281, train_loss: 0.8061, step time: 0.2506\n",
      "182/281, train_loss: 0.8285, step time: 0.2533\n",
      "183/281, train_loss: 0.7097, step time: 0.2568\n",
      "184/281, train_loss: 0.7294, step time: 0.2562\n",
      "185/281, train_loss: 0.7252, step time: 0.2595\n",
      "186/281, train_loss: 0.7262, step time: 0.2481\n",
      "187/281, train_loss: 0.7308, step time: 0.2553\n",
      "188/281, train_loss: 0.7554, step time: 0.2582\n",
      "189/281, train_loss: 0.6574, step time: 0.2500\n",
      "190/281, train_loss: 0.7549, step time: 0.2570\n",
      "191/281, train_loss: 0.7373, step time: 0.2557\n",
      "192/281, train_loss: 0.7226, step time: 0.2622\n",
      "193/281, train_loss: 0.7067, step time: 0.2529\n",
      "194/281, train_loss: 0.6673, step time: 0.2538\n",
      "195/281, train_loss: 0.7171, step time: 0.2548\n",
      "196/281, train_loss: 0.6844, step time: 0.2467\n",
      "197/281, train_loss: 0.7299, step time: 0.2565\n",
      "198/281, train_loss: 0.7569, step time: 0.2508\n",
      "199/281, train_loss: 0.7037, step time: 0.2530\n",
      "200/281, train_loss: 0.7172, step time: 0.2479\n",
      "201/281, train_loss: 0.7663, step time: 0.2565\n",
      "202/281, train_loss: 0.7249, step time: 0.2528\n",
      "203/281, train_loss: 0.6673, step time: 0.2524\n",
      "204/281, train_loss: 0.6894, step time: 0.2465\n",
      "205/281, train_loss: 0.8221, step time: 0.2512\n",
      "206/281, train_loss: 0.7446, step time: 0.2529\n",
      "207/281, train_loss: 0.7175, step time: 0.2554\n",
      "208/281, train_loss: 0.7074, step time: 0.2536\n",
      "209/281, train_loss: 0.7022, step time: 0.2545\n",
      "210/281, train_loss: 0.6766, step time: 0.2524\n",
      "211/281, train_loss: 0.8417, step time: 0.2600\n",
      "212/281, train_loss: 0.6973, step time: 0.2583\n",
      "213/281, train_loss: 0.6664, step time: 0.2589\n",
      "214/281, train_loss: 0.7373, step time: 0.2588\n",
      "215/281, train_loss: 0.7509, step time: 0.2573\n",
      "216/281, train_loss: 0.6915, step time: 0.2589\n",
      "217/281, train_loss: 0.7425, step time: 0.2601\n",
      "218/281, train_loss: 0.7680, step time: 0.2486\n",
      "219/281, train_loss: 0.6914, step time: 0.2512\n",
      "220/281, train_loss: 0.7809, step time: 0.2569\n",
      "221/281, train_loss: 0.7189, step time: 0.2621\n",
      "222/281, train_loss: 0.7534, step time: 0.2556\n",
      "223/281, train_loss: 0.7882, step time: 0.2523\n",
      "224/281, train_loss: 0.6632, step time: 0.2527\n",
      "225/281, train_loss: 0.7109, step time: 0.2472\n",
      "226/281, train_loss: 0.6688, step time: 0.2564\n",
      "227/281, train_loss: 0.6899, step time: 0.2569\n",
      "228/281, train_loss: 0.7342, step time: 0.2590\n",
      "229/281, train_loss: 0.7911, step time: 0.2615\n",
      "230/281, train_loss: 0.6632, step time: 0.2521\n",
      "231/281, train_loss: 0.6869, step time: 0.2570\n",
      "232/281, train_loss: 0.7015, step time: 0.2588\n",
      "233/281, train_loss: 0.6335, step time: 0.2575\n",
      "234/281, train_loss: 0.6894, step time: 0.2481\n",
      "235/281, train_loss: 0.6720, step time: 0.2524\n",
      "236/281, train_loss: 0.7252, step time: 0.2477\n",
      "237/281, train_loss: 0.6955, step time: 0.2539\n",
      "238/281, train_loss: 0.6924, step time: 0.2580\n",
      "239/281, train_loss: 0.6854, step time: 0.2584\n",
      "240/281, train_loss: 0.6772, step time: 0.2559\n",
      "241/281, train_loss: 0.7203, step time: 0.2535\n",
      "242/281, train_loss: 0.7561, step time: 0.2560\n",
      "243/281, train_loss: 0.7107, step time: 0.2533\n",
      "244/281, train_loss: 0.7652, step time: 0.2497\n",
      "245/281, train_loss: 0.7020, step time: 0.2538\n",
      "246/281, train_loss: 0.6649, step time: 0.2606\n",
      "247/281, train_loss: 0.6875, step time: 0.2585\n",
      "248/281, train_loss: 0.6797, step time: 0.2585\n",
      "249/281, train_loss: 0.6693, step time: 0.2604\n",
      "250/281, train_loss: 0.6942, step time: 0.2571\n",
      "251/281, train_loss: 0.8339, step time: 0.2565\n",
      "252/281, train_loss: 0.6758, step time: 0.2592\n",
      "253/281, train_loss: 0.7007, step time: 0.2566\n",
      "254/281, train_loss: 0.6893, step time: 0.2721\n",
      "255/281, train_loss: 0.7087, step time: 0.2595\n",
      "256/281, train_loss: 0.7238, step time: 0.2611\n",
      "257/281, train_loss: 0.7451, step time: 0.2543\n",
      "258/281, train_loss: 0.7017, step time: 0.2523\n",
      "259/281, train_loss: 0.6648, step time: 0.2504\n",
      "260/281, train_loss: 0.8939, step time: 0.2524\n",
      "261/281, train_loss: 0.7151, step time: 0.2526\n",
      "262/281, train_loss: 0.6880, step time: 0.2553\n",
      "263/281, train_loss: 0.6636, step time: 0.2539\n",
      "264/281, train_loss: 0.6625, step time: 0.2546\n",
      "265/281, train_loss: 0.7445, step time: 0.2481\n",
      "266/281, train_loss: 0.8058, step time: 0.2604\n",
      "267/281, train_loss: 0.6850, step time: 0.2564\n",
      "268/281, train_loss: 0.7396, step time: 0.2551\n",
      "269/281, train_loss: 0.6950, step time: 0.2557\n",
      "270/281, train_loss: 0.7789, step time: 0.2538\n",
      "271/281, train_loss: 0.7326, step time: 0.2585\n",
      "272/281, train_loss: 0.7765, step time: 0.2533\n",
      "273/281, train_loss: 0.7033, step time: 0.2510\n",
      "274/281, train_loss: 0.6627, step time: 0.2570\n",
      "275/281, train_loss: 0.6508, step time: 0.2562\n",
      "276/281, train_loss: 0.7118, step time: 0.2592\n",
      "277/281, train_loss: 0.6840, step time: 0.2520\n",
      "278/281, train_loss: 0.6626, step time: 0.2529\n",
      "279/281, train_loss: 0.6399, step time: 0.2547\n",
      "280/281, train_loss: 0.6878, step time: 0.2526\n",
      "281/281, train_loss: 0.6264, step time: 0.2517\n",
      "282/281, train_loss: 0.6949, step time: 0.1510\n",
      "epoch 7 average loss: 0.7357\n",
      "saved new best metric model\n",
      "current epoch: 7 current mean dice: 0.4999 tc: 0.0090 wt: 0.8835 et: 0.6236\n",
      "best mean dice: 0.4999 at epoch: 7\n",
      "time consuming of epoch 7 is: 268.6077\n",
      "----------\n",
      "epoch 8/200\n",
      "1/281, train_loss: 0.7129, step time: 0.2680\n",
      "2/281, train_loss: 0.7186, step time: 0.2605\n",
      "3/281, train_loss: 0.6306, step time: 0.2617\n",
      "4/281, train_loss: 0.7410, step time: 0.2577\n",
      "5/281, train_loss: 0.6617, step time: 0.2496\n",
      "6/281, train_loss: 0.6455, step time: 0.2527\n",
      "7/281, train_loss: 0.7339, step time: 0.2585\n",
      "8/281, train_loss: 0.7571, step time: 0.2567\n",
      "9/281, train_loss: 0.7567, step time: 0.2610\n",
      "10/281, train_loss: 0.6966, step time: 0.2506\n",
      "11/281, train_loss: 0.7738, step time: 0.2557\n",
      "12/281, train_loss: 0.7017, step time: 0.2591\n",
      "13/281, train_loss: 0.7343, step time: 0.2497\n",
      "14/281, train_loss: 0.6653, step time: 0.2606\n",
      "15/281, train_loss: 0.7365, step time: 0.2502\n",
      "16/281, train_loss: 0.6443, step time: 0.2554\n",
      "17/281, train_loss: 0.7118, step time: 0.2493\n",
      "18/281, train_loss: 0.7367, step time: 0.2580\n",
      "19/281, train_loss: 0.6917, step time: 0.2540\n",
      "20/281, train_loss: 0.7056, step time: 0.2526\n",
      "21/281, train_loss: 0.6838, step time: 0.2540\n",
      "22/281, train_loss: 0.7267, step time: 0.2656\n",
      "23/281, train_loss: 0.6839, step time: 0.2578\n",
      "24/281, train_loss: 0.6682, step time: 0.2538\n",
      "25/281, train_loss: 0.7591, step time: 0.2587\n",
      "26/281, train_loss: 0.7532, step time: 0.2526\n",
      "27/281, train_loss: 0.6525, step time: 0.2617\n",
      "28/281, train_loss: 0.7097, step time: 0.2565\n",
      "29/281, train_loss: 0.7756, step time: 0.2586\n",
      "30/281, train_loss: 0.7071, step time: 0.2526\n",
      "31/281, train_loss: 0.6865, step time: 0.2514\n",
      "32/281, train_loss: 0.6047, step time: 0.2577\n",
      "33/281, train_loss: 0.7362, step time: 0.2630\n",
      "34/281, train_loss: 0.7090, step time: 0.2649\n",
      "35/281, train_loss: 0.7164, step time: 0.2557\n",
      "36/281, train_loss: 0.7345, step time: 0.2481\n",
      "37/281, train_loss: 0.6805, step time: 0.2534\n",
      "38/281, train_loss: 0.6367, step time: 0.2601\n",
      "39/281, train_loss: 0.8364, step time: 0.2620\n",
      "40/281, train_loss: 0.6512, step time: 0.2586\n",
      "41/281, train_loss: 0.6577, step time: 0.2613\n",
      "42/281, train_loss: 0.7578, step time: 0.2588\n",
      "43/281, train_loss: 0.6291, step time: 0.2609\n",
      "44/281, train_loss: 0.7416, step time: 0.2575\n",
      "45/281, train_loss: 0.6282, step time: 0.2608\n",
      "46/281, train_loss: 0.7071, step time: 0.2510\n",
      "47/281, train_loss: 0.6911, step time: 0.2541\n",
      "48/281, train_loss: 0.7214, step time: 0.2506\n",
      "49/281, train_loss: 0.7540, step time: 0.2520\n",
      "50/281, train_loss: 0.6760, step time: 0.2571\n",
      "51/281, train_loss: 0.7200, step time: 0.2550\n",
      "52/281, train_loss: 0.7140, step time: 0.2558\n",
      "53/281, train_loss: 0.7896, step time: 0.2556\n",
      "54/281, train_loss: 0.6548, step time: 0.2555\n",
      "55/281, train_loss: 0.6878, step time: 0.2544\n",
      "56/281, train_loss: 0.6073, step time: 0.2605\n",
      "57/281, train_loss: 0.6651, step time: 0.2550\n",
      "58/281, train_loss: 0.6939, step time: 0.2537\n",
      "59/281, train_loss: 0.7104, step time: 0.2590\n",
      "60/281, train_loss: 0.7069, step time: 0.2510\n",
      "61/281, train_loss: 0.7034, step time: 0.2538\n",
      "62/281, train_loss: 0.8346, step time: 0.2557\n",
      "63/281, train_loss: 0.7422, step time: 0.2562\n",
      "64/281, train_loss: 0.6831, step time: 0.2615\n",
      "65/281, train_loss: 0.7154, step time: 0.2629\n",
      "66/281, train_loss: 0.6375, step time: 0.2565\n",
      "67/281, train_loss: 0.6588, step time: 0.2587\n",
      "68/281, train_loss: 0.7188, step time: 0.2593\n",
      "69/281, train_loss: 0.6751, step time: 0.2540\n",
      "70/281, train_loss: 0.7511, step time: 0.2602\n",
      "71/281, train_loss: 0.7416, step time: 0.2548\n",
      "72/281, train_loss: 0.6485, step time: 0.2518\n",
      "73/281, train_loss: 0.7159, step time: 0.2509\n",
      "74/281, train_loss: 0.7184, step time: 0.2493\n",
      "75/281, train_loss: 0.6178, step time: 0.2433\n",
      "76/281, train_loss: 0.7302, step time: 0.2449\n",
      "77/281, train_loss: 0.6974, step time: 0.2528\n",
      "78/281, train_loss: 0.7278, step time: 0.2531\n",
      "79/281, train_loss: 0.6739, step time: 0.2508\n",
      "80/281, train_loss: 0.7742, step time: 0.2554\n",
      "81/281, train_loss: 0.7096, step time: 0.2531\n",
      "82/281, train_loss: 0.6364, step time: 0.2527\n",
      "83/281, train_loss: 0.6724, step time: 0.2610\n",
      "84/281, train_loss: 0.7140, step time: 0.2693\n",
      "85/281, train_loss: 0.6923, step time: 0.2572\n",
      "86/281, train_loss: 0.6814, step time: 0.2560\n",
      "87/281, train_loss: 0.6641, step time: 0.2545\n",
      "88/281, train_loss: 0.6501, step time: 0.2545\n",
      "89/281, train_loss: 0.6550, step time: 0.2516\n",
      "90/281, train_loss: 0.6367, step time: 0.2537\n",
      "91/281, train_loss: 0.7159, step time: 0.2620\n",
      "92/281, train_loss: 0.5965, step time: 0.2593\n",
      "93/281, train_loss: 0.6989, step time: 0.2541\n",
      "94/281, train_loss: 0.6593, step time: 0.2531\n",
      "95/281, train_loss: 0.7066, step time: 0.2564\n",
      "96/281, train_loss: 0.7365, step time: 0.2516\n",
      "97/281, train_loss: 0.6046, step time: 0.2527\n",
      "98/281, train_loss: 0.8767, step time: 0.2563\n",
      "99/281, train_loss: 0.6582, step time: 0.2601\n",
      "100/281, train_loss: 0.6678, step time: 0.2577\n",
      "101/281, train_loss: 0.6310, step time: 0.2564\n",
      "102/281, train_loss: 0.5684, step time: 0.2551\n",
      "103/281, train_loss: 0.6253, step time: 0.2549\n",
      "104/281, train_loss: 0.6633, step time: 0.2550\n",
      "105/281, train_loss: 0.6632, step time: 0.2593\n",
      "106/281, train_loss: 0.7433, step time: 0.2507\n",
      "107/281, train_loss: 0.6468, step time: 0.2566\n",
      "108/281, train_loss: 0.7576, step time: 0.2566\n",
      "109/281, train_loss: 0.6925, step time: 0.2549\n",
      "110/281, train_loss: 0.6655, step time: 0.2703\n",
      "111/281, train_loss: 0.6839, step time: 0.2546\n",
      "112/281, train_loss: 0.6809, step time: 0.2584\n",
      "113/281, train_loss: 0.6574, step time: 0.2532\n",
      "114/281, train_loss: 0.6896, step time: 0.2606\n",
      "115/281, train_loss: 0.7577, step time: 0.2530\n",
      "116/281, train_loss: 0.5806, step time: 0.2568\n",
      "117/281, train_loss: 0.6946, step time: 0.2532\n",
      "118/281, train_loss: 0.5953, step time: 0.2775\n",
      "119/281, train_loss: 0.6675, step time: 0.2546\n",
      "120/281, train_loss: 0.7221, step time: 0.2533\n",
      "121/281, train_loss: 0.6205, step time: 0.2477\n",
      "122/281, train_loss: 0.6485, step time: 0.2551\n",
      "123/281, train_loss: 0.6637, step time: 0.2603\n",
      "124/281, train_loss: 0.6528, step time: 0.2618\n",
      "125/281, train_loss: 0.7056, step time: 0.2829\n",
      "126/281, train_loss: 0.5950, step time: 0.2520\n",
      "127/281, train_loss: 0.7228, step time: 0.2521\n",
      "128/281, train_loss: 0.6744, step time: 0.2550\n",
      "129/281, train_loss: 0.6124, step time: 0.2538\n",
      "130/281, train_loss: 0.6096, step time: 0.2540\n",
      "131/281, train_loss: 0.6592, step time: 0.2552\n",
      "132/281, train_loss: 0.7364, step time: 0.2575\n",
      "133/281, train_loss: 0.6689, step time: 0.2544\n",
      "134/281, train_loss: 0.6561, step time: 0.2538\n",
      "135/281, train_loss: 0.6758, step time: 0.2556\n",
      "136/281, train_loss: 0.6801, step time: 0.2588\n",
      "137/281, train_loss: 0.6065, step time: 0.2496\n",
      "138/281, train_loss: 0.6433, step time: 0.2507\n",
      "139/281, train_loss: 0.6770, step time: 0.2586\n",
      "140/281, train_loss: 0.5464, step time: 0.2579\n",
      "141/281, train_loss: 0.6621, step time: 0.2598\n",
      "142/281, train_loss: 0.6274, step time: 0.2541\n",
      "143/281, train_loss: 0.6421, step time: 0.2565\n",
      "144/281, train_loss: 0.7805, step time: 0.2523\n",
      "145/281, train_loss: 0.7493, step time: 0.2551\n",
      "146/281, train_loss: 0.6394, step time: 0.2535\n",
      "147/281, train_loss: 0.7699, step time: 0.2502\n",
      "148/281, train_loss: 0.7131, step time: 0.2582\n",
      "149/281, train_loss: 0.6322, step time: 0.2545\n",
      "150/281, train_loss: 0.6920, step time: 0.2551\n",
      "151/281, train_loss: 0.7553, step time: 0.2574\n",
      "152/281, train_loss: 0.7135, step time: 0.2593\n",
      "153/281, train_loss: 0.7238, step time: 0.2552\n",
      "154/281, train_loss: 0.6765, step time: 0.2528\n",
      "155/281, train_loss: 0.6953, step time: 0.2545\n",
      "156/281, train_loss: 0.7493, step time: 0.2514\n",
      "157/281, train_loss: 0.6015, step time: 0.2535\n",
      "158/281, train_loss: 0.7107, step time: 0.2522\n",
      "159/281, train_loss: 0.7524, step time: 0.2560\n",
      "160/281, train_loss: 0.6849, step time: 0.2577\n",
      "161/281, train_loss: 0.7123, step time: 0.2542\n",
      "162/281, train_loss: 0.6122, step time: 0.2527\n",
      "163/281, train_loss: 0.6710, step time: 0.2521\n",
      "164/281, train_loss: 0.7019, step time: 0.2527\n",
      "165/281, train_loss: 0.6577, step time: 0.2477\n",
      "166/281, train_loss: 0.6258, step time: 0.2541\n",
      "167/281, train_loss: 0.7048, step time: 0.2599\n",
      "168/281, train_loss: 0.7158, step time: 0.2612\n",
      "169/281, train_loss: 0.6951, step time: 0.2537\n",
      "170/281, train_loss: 0.5793, step time: 0.2546\n",
      "171/281, train_loss: 0.6231, step time: 0.2555\n",
      "172/281, train_loss: 0.6638, step time: 0.2613\n",
      "173/281, train_loss: 0.6828, step time: 0.2489\n",
      "174/281, train_loss: 0.6547, step time: 0.2514\n",
      "175/281, train_loss: 0.6458, step time: 0.2514\n",
      "176/281, train_loss: 0.6514, step time: 0.2511\n",
      "177/281, train_loss: 0.6756, step time: 0.2533\n",
      "178/281, train_loss: 0.6564, step time: 0.2567\n",
      "179/281, train_loss: 0.6292, step time: 0.2717\n",
      "180/281, train_loss: 0.7176, step time: 0.2567\n",
      "181/281, train_loss: 0.6721, step time: 0.2559\n",
      "182/281, train_loss: 0.6080, step time: 0.2527\n",
      "183/281, train_loss: 0.7186, step time: 0.2543\n",
      "184/281, train_loss: 0.6944, step time: 0.2593\n",
      "185/281, train_loss: 0.7242, step time: 0.2517\n",
      "186/281, train_loss: 0.6962, step time: 0.2584\n",
      "187/281, train_loss: 0.6840, step time: 0.2586\n",
      "188/281, train_loss: 0.6387, step time: 0.2735\n",
      "189/281, train_loss: 0.7075, step time: 0.2497\n",
      "190/281, train_loss: 0.6170, step time: 0.2460\n",
      "191/281, train_loss: 0.7483, step time: 0.2499\n",
      "192/281, train_loss: 0.6387, step time: 0.2544\n",
      "193/281, train_loss: 0.6803, step time: 0.2584\n",
      "194/281, train_loss: 0.6927, step time: 0.2508\n",
      "195/281, train_loss: 0.7322, step time: 0.2510\n",
      "196/281, train_loss: 0.7452, step time: 0.2488\n",
      "197/281, train_loss: 0.6969, step time: 0.2630\n",
      "198/281, train_loss: 0.7096, step time: 0.2534\n",
      "199/281, train_loss: 0.7100, step time: 0.2799\n",
      "200/281, train_loss: 0.6278, step time: 0.2792\n",
      "201/281, train_loss: 0.6990, step time: 0.2855\n",
      "202/281, train_loss: 0.6089, step time: 0.2643\n",
      "203/281, train_loss: 0.6355, step time: 0.2564\n",
      "204/281, train_loss: 0.6069, step time: 0.2603\n",
      "205/281, train_loss: 0.6256, step time: 0.2580\n",
      "206/281, train_loss: 0.6539, step time: 0.2554\n",
      "207/281, train_loss: 0.6864, step time: 0.2561\n",
      "208/281, train_loss: 0.6841, step time: 0.2517\n",
      "209/281, train_loss: 0.7213, step time: 0.2508\n",
      "210/281, train_loss: 0.7264, step time: 0.2561\n",
      "211/281, train_loss: 0.5206, step time: 0.2496\n",
      "212/281, train_loss: 0.5947, step time: 0.2618\n",
      "213/281, train_loss: 0.7351, step time: 0.2649\n",
      "214/281, train_loss: 0.6381, step time: 0.2725\n",
      "215/281, train_loss: 0.7951, step time: 0.2649\n",
      "216/281, train_loss: 0.6139, step time: 0.2555\n",
      "217/281, train_loss: 0.7334, step time: 0.2680\n",
      "218/281, train_loss: 0.7027, step time: 0.2561\n",
      "219/281, train_loss: 0.7760, step time: 0.2590\n",
      "220/281, train_loss: 0.7000, step time: 0.2596\n",
      "221/281, train_loss: 0.6704, step time: 0.2485\n",
      "222/281, train_loss: 0.6478, step time: 0.2528\n",
      "223/281, train_loss: 0.6627, step time: 0.2527\n",
      "224/281, train_loss: 0.6708, step time: 0.2530\n",
      "225/281, train_loss: 0.6232, step time: 0.2660\n",
      "226/281, train_loss: 0.7239, step time: 0.2839\n",
      "227/281, train_loss: 0.7428, step time: 0.2500\n",
      "228/281, train_loss: 0.6068, step time: 0.2454\n",
      "229/281, train_loss: 0.6695, step time: 0.2516\n",
      "230/281, train_loss: 0.7032, step time: 0.2543\n",
      "231/281, train_loss: 0.6669, step time: 0.2496\n",
      "232/281, train_loss: 0.6263, step time: 0.2476\n",
      "233/281, train_loss: 0.7072, step time: 0.2514\n",
      "234/281, train_loss: 0.6421, step time: 0.2583\n",
      "235/281, train_loss: 0.7493, step time: 0.2531\n",
      "236/281, train_loss: 0.6660, step time: 0.2561\n",
      "237/281, train_loss: 0.6681, step time: 0.2534\n",
      "238/281, train_loss: 0.6336, step time: 0.2489\n",
      "239/281, train_loss: 0.6900, step time: 0.2489\n",
      "240/281, train_loss: 0.5969, step time: 0.2543\n",
      "241/281, train_loss: 0.6079, step time: 0.2491\n",
      "242/281, train_loss: 0.7368, step time: 0.2579\n",
      "243/281, train_loss: 0.7052, step time: 0.2456\n",
      "244/281, train_loss: 0.6316, step time: 0.2474\n",
      "245/281, train_loss: 0.6349, step time: 0.2487\n",
      "246/281, train_loss: 0.5992, step time: 0.2564\n",
      "247/281, train_loss: 0.6325, step time: 0.2506\n",
      "248/281, train_loss: 0.5396, step time: 0.2496\n",
      "249/281, train_loss: 0.6421, step time: 0.2472\n",
      "250/281, train_loss: 0.6097, step time: 0.2522\n",
      "251/281, train_loss: 0.6596, step time: 0.2554\n",
      "252/281, train_loss: 0.5319, step time: 0.2577\n",
      "253/281, train_loss: 0.6514, step time: 0.2825\n",
      "254/281, train_loss: 0.6571, step time: 0.2529\n",
      "255/281, train_loss: 0.6436, step time: 0.2541\n",
      "256/281, train_loss: 0.6470, step time: 0.2510\n",
      "257/281, train_loss: 0.6257, step time: 0.2547\n",
      "258/281, train_loss: 0.6015, step time: 0.2685\n",
      "259/281, train_loss: 0.5929, step time: 0.2506\n",
      "260/281, train_loss: 0.6221, step time: 0.2539\n",
      "261/281, train_loss: 0.6451, step time: 0.2506\n",
      "262/281, train_loss: 0.5993, step time: 0.2549\n",
      "263/281, train_loss: 0.7291, step time: 0.2577\n",
      "264/281, train_loss: 0.6064, step time: 0.2557\n",
      "265/281, train_loss: 0.5597, step time: 0.2546\n",
      "266/281, train_loss: 0.6328, step time: 0.2555\n",
      "267/281, train_loss: 0.6633, step time: 0.2575\n",
      "268/281, train_loss: 0.6003, step time: 0.2589\n",
      "269/281, train_loss: 0.5674, step time: 0.2590\n",
      "270/281, train_loss: 0.6211, step time: 0.2473\n",
      "271/281, train_loss: 0.7231, step time: 0.2456\n",
      "272/281, train_loss: 0.6936, step time: 0.2468\n",
      "273/281, train_loss: 0.5692, step time: 0.2501\n",
      "274/281, train_loss: 0.6155, step time: 0.2600\n",
      "275/281, train_loss: 0.6502, step time: 0.2662\n",
      "276/281, train_loss: 0.5807, step time: 0.2808\n",
      "277/281, train_loss: 0.6202, step time: 0.2569\n",
      "278/281, train_loss: 0.6425, step time: 0.2511\n",
      "279/281, train_loss: 0.5455, step time: 0.2506\n",
      "280/281, train_loss: 0.5743, step time: 0.2545\n",
      "281/281, train_loss: 0.6474, step time: 0.2512\n",
      "282/281, train_loss: 0.5988, step time: 0.1502\n",
      "epoch 8 average loss: 0.6755\n",
      "saved new best metric model\n",
      "current epoch: 8 current mean dice: 0.5002 tc: 0.0090 wt: 0.8882 et: 0.6180\n",
      "best mean dice: 0.5002 at epoch: 8\n",
      "time consuming of epoch 8 is: 242.6157\n",
      "----------\n",
      "epoch 9/200\n",
      "1/281, train_loss: 0.5811, step time: 0.2600\n",
      "2/281, train_loss: 0.5702, step time: 0.2631\n",
      "3/281, train_loss: 0.5502, step time: 0.2578\n",
      "4/281, train_loss: 0.6371, step time: 0.2619\n",
      "5/281, train_loss: 0.5725, step time: 0.2523\n",
      "6/281, train_loss: 0.6170, step time: 0.2512\n",
      "7/281, train_loss: 0.6333, step time: 0.2486\n",
      "8/281, train_loss: 0.5602, step time: 0.2479\n",
      "9/281, train_loss: 0.5618, step time: 0.2524\n",
      "10/281, train_loss: 0.6058, step time: 0.2557\n",
      "11/281, train_loss: 0.6335, step time: 0.2596\n",
      "12/281, train_loss: 0.6539, step time: 0.2531\n",
      "13/281, train_loss: 0.6167, step time: 0.2484\n",
      "14/281, train_loss: 0.6910, step time: 0.2559\n",
      "15/281, train_loss: 0.5558, step time: 0.2573\n",
      "16/281, train_loss: 0.7580, step time: 0.2603\n",
      "17/281, train_loss: 0.6153, step time: 0.2611\n",
      "18/281, train_loss: 0.6264, step time: 0.2554\n",
      "19/281, train_loss: 0.7237, step time: 0.2541\n",
      "20/281, train_loss: 0.6327, step time: 0.2508\n",
      "21/281, train_loss: 0.5416, step time: 0.2527\n",
      "22/281, train_loss: 0.6250, step time: 0.2577\n",
      "23/281, train_loss: 0.6556, step time: 0.2596\n",
      "24/281, train_loss: 0.6860, step time: 0.2584\n",
      "25/281, train_loss: 0.6086, step time: 0.2560\n",
      "26/281, train_loss: 0.6797, step time: 0.2548\n",
      "27/281, train_loss: 0.5446, step time: 0.2537\n",
      "28/281, train_loss: 0.7267, step time: 0.2617\n",
      "29/281, train_loss: 0.6550, step time: 0.2587\n",
      "30/281, train_loss: 0.6438, step time: 0.2516\n",
      "31/281, train_loss: 0.6047, step time: 0.2521\n",
      "32/281, train_loss: 0.6679, step time: 0.2477\n",
      "33/281, train_loss: 0.5491, step time: 0.2532\n",
      "34/281, train_loss: 0.7230, step time: 0.2505\n",
      "35/281, train_loss: 0.7392, step time: 0.2540\n",
      "36/281, train_loss: 0.6036, step time: 0.2514\n",
      "37/281, train_loss: 0.6099, step time: 0.2511\n",
      "38/281, train_loss: 0.5175, step time: 0.2558\n",
      "39/281, train_loss: 0.6952, step time: 0.2596\n",
      "40/281, train_loss: 0.6385, step time: 0.2547\n",
      "41/281, train_loss: 0.5275, step time: 0.2542\n",
      "42/281, train_loss: 0.5672, step time: 0.2543\n",
      "43/281, train_loss: 0.5684, step time: 0.2506\n",
      "44/281, train_loss: 0.5834, step time: 0.2479\n",
      "45/281, train_loss: 0.6426, step time: 0.2585\n",
      "46/281, train_loss: 0.6689, step time: 0.2580\n",
      "47/281, train_loss: 0.6808, step time: 0.2512\n",
      "48/281, train_loss: 0.6441, step time: 0.2538\n",
      "49/281, train_loss: 0.6403, step time: 0.2535\n",
      "50/281, train_loss: 0.6036, step time: 0.2522\n",
      "51/281, train_loss: 0.5811, step time: 0.2484\n",
      "52/281, train_loss: 0.6576, step time: 0.2602\n",
      "53/281, train_loss: 0.5712, step time: 0.2544\n",
      "54/281, train_loss: 0.7147, step time: 0.2477\n",
      "55/281, train_loss: 0.5636, step time: 0.2587\n",
      "56/281, train_loss: 0.6258, step time: 0.2643\n",
      "57/281, train_loss: 0.7353, step time: 0.2582\n",
      "58/281, train_loss: 0.6414, step time: 0.2786\n",
      "59/281, train_loss: 0.6159, step time: 0.2617\n",
      "60/281, train_loss: 0.5795, step time: 0.2527\n",
      "61/281, train_loss: 0.7029, step time: 0.2539\n",
      "62/281, train_loss: 0.7085, step time: 0.2564\n",
      "63/281, train_loss: 0.6151, step time: 0.2568\n",
      "64/281, train_loss: 0.7096, step time: 0.2549\n",
      "65/281, train_loss: 0.6952, step time: 0.2484\n",
      "66/281, train_loss: 0.5347, step time: 0.2514\n",
      "67/281, train_loss: 0.6697, step time: 0.2589\n",
      "68/281, train_loss: 0.5297, step time: 0.2537\n",
      "69/281, train_loss: 0.7120, step time: 0.2570\n",
      "70/281, train_loss: 0.5911, step time: 0.2557\n",
      "71/281, train_loss: 0.5647, step time: 0.2547\n",
      "72/281, train_loss: 0.5419, step time: 0.2530\n",
      "73/281, train_loss: 0.5805, step time: 0.2583\n",
      "74/281, train_loss: 0.6113, step time: 0.2580\n",
      "75/281, train_loss: 0.6237, step time: 0.2562\n",
      "76/281, train_loss: 0.5461, step time: 0.2564\n",
      "77/281, train_loss: 0.5430, step time: 0.2567\n",
      "78/281, train_loss: 0.7181, step time: 0.2547\n",
      "79/281, train_loss: 0.5864, step time: 0.2572\n",
      "80/281, train_loss: 0.6304, step time: 0.2543\n",
      "81/281, train_loss: 0.5548, step time: 0.2494\n",
      "82/281, train_loss: 0.5486, step time: 0.2534\n",
      "83/281, train_loss: 0.5500, step time: 0.2627\n",
      "84/281, train_loss: 0.6355, step time: 0.2597\n",
      "85/281, train_loss: 0.6299, step time: 0.2543\n",
      "86/281, train_loss: 0.5861, step time: 0.2558\n",
      "87/281, train_loss: 0.5345, step time: 0.2537\n",
      "88/281, train_loss: 0.6942, step time: 0.2538\n",
      "89/281, train_loss: 0.5765, step time: 0.2516\n",
      "90/281, train_loss: 0.6253, step time: 0.2534\n",
      "91/281, train_loss: 0.5854, step time: 0.2581\n",
      "92/281, train_loss: 0.5788, step time: 0.2593\n",
      "93/281, train_loss: 0.5656, step time: 0.2633\n",
      "94/281, train_loss: 0.5711, step time: 0.2681\n",
      "95/281, train_loss: 0.7232, step time: 0.2633\n",
      "96/281, train_loss: 0.6284, step time: 0.2573\n",
      "97/281, train_loss: 0.6999, step time: 0.2569\n",
      "98/281, train_loss: 0.6476, step time: 0.2520\n",
      "99/281, train_loss: 0.7007, step time: 0.2496\n",
      "100/281, train_loss: 0.6554, step time: 0.2573\n",
      "101/281, train_loss: 0.5662, step time: 0.2507\n",
      "102/281, train_loss: 0.5844, step time: 0.2630\n",
      "103/281, train_loss: 0.6699, step time: 0.2587\n",
      "104/281, train_loss: 0.4938, step time: 0.2568\n",
      "105/281, train_loss: 0.5981, step time: 0.2629\n",
      "106/281, train_loss: 0.5823, step time: 0.2617\n",
      "107/281, train_loss: 0.5618, step time: 0.2588\n",
      "108/281, train_loss: 0.4953, step time: 0.2525\n",
      "109/281, train_loss: 0.7038, step time: 0.2577\n",
      "110/281, train_loss: 0.5337, step time: 0.2621\n",
      "111/281, train_loss: 0.5199, step time: 0.2585\n",
      "112/281, train_loss: 0.7086, step time: 0.2602\n",
      "113/281, train_loss: 0.7333, step time: 0.2538\n",
      "114/281, train_loss: 0.6400, step time: 0.2537\n",
      "115/281, train_loss: 0.6488, step time: 0.2641\n",
      "116/281, train_loss: 0.6316, step time: 0.2651\n",
      "117/281, train_loss: 0.6863, step time: 0.2669\n",
      "118/281, train_loss: 0.6050, step time: 0.2581\n",
      "119/281, train_loss: 0.5525, step time: 0.2564\n",
      "120/281, train_loss: 0.6264, step time: 0.2589\n",
      "121/281, train_loss: 0.6774, step time: 0.2596\n",
      "122/281, train_loss: 0.4840, step time: 0.2524\n",
      "123/281, train_loss: 0.6768, step time: 0.2568\n",
      "124/281, train_loss: 0.5796, step time: 0.2607\n",
      "125/281, train_loss: 0.6387, step time: 0.2580\n",
      "126/281, train_loss: 0.6761, step time: 0.2583\n",
      "127/281, train_loss: 0.5533, step time: 0.2633\n",
      "128/281, train_loss: 0.5353, step time: 0.2654\n",
      "129/281, train_loss: 0.5221, step time: 0.2570\n",
      "130/281, train_loss: 0.6073, step time: 0.2580\n",
      "131/281, train_loss: 0.5359, step time: 0.2545\n",
      "132/281, train_loss: 0.6299, step time: 0.2535\n",
      "133/281, train_loss: 0.6957, step time: 0.2505\n",
      "134/281, train_loss: 0.5943, step time: 0.2567\n",
      "135/281, train_loss: 0.4954, step time: 0.2564\n",
      "136/281, train_loss: 0.5220, step time: 0.2576\n",
      "137/281, train_loss: 0.6078, step time: 0.2599\n",
      "138/281, train_loss: 0.6641, step time: 0.2660\n",
      "139/281, train_loss: 0.6944, step time: 0.2616\n",
      "140/281, train_loss: 0.5943, step time: 0.2558\n",
      "141/281, train_loss: 0.5656, step time: 0.2518\n",
      "142/281, train_loss: 0.5846, step time: 0.2573\n",
      "143/281, train_loss: 0.6268, step time: 0.2582\n",
      "144/281, train_loss: 0.6391, step time: 0.2626\n",
      "145/281, train_loss: 0.5576, step time: 0.2533\n",
      "146/281, train_loss: 0.5971, step time: 0.2579\n",
      "147/281, train_loss: 0.7011, step time: 0.2563\n",
      "148/281, train_loss: 0.5325, step time: 0.2593\n",
      "149/281, train_loss: 0.6422, step time: 0.2567\n",
      "150/281, train_loss: 0.5870, step time: 0.2566\n",
      "151/281, train_loss: 0.5752, step time: 0.2610\n",
      "152/281, train_loss: 0.6364, step time: 0.2602\n",
      "153/281, train_loss: 0.5540, step time: 0.2579\n",
      "154/281, train_loss: 0.6050, step time: 0.2668\n",
      "155/281, train_loss: 0.5627, step time: 0.2588\n",
      "156/281, train_loss: 0.7329, step time: 0.2597\n",
      "157/281, train_loss: 0.5596, step time: 0.2570\n",
      "158/281, train_loss: 0.5867, step time: 0.2610\n",
      "159/281, train_loss: 0.6025, step time: 0.2647\n",
      "160/281, train_loss: 0.6389, step time: 0.2611\n",
      "161/281, train_loss: 0.5331, step time: 0.2584\n",
      "162/281, train_loss: 0.7514, step time: 0.2583\n",
      "163/281, train_loss: 0.6496, step time: 0.2575\n",
      "164/281, train_loss: 0.5519, step time: 0.2545\n",
      "165/281, train_loss: 0.5473, step time: 0.2596\n",
      "166/281, train_loss: 0.5637, step time: 0.2558\n",
      "167/281, train_loss: 0.4762, step time: 0.2595\n",
      "168/281, train_loss: 0.6328, step time: 0.2850\n",
      "169/281, train_loss: 0.6414, step time: 0.2772\n",
      "170/281, train_loss: 0.5497, step time: 0.2566\n",
      "171/281, train_loss: 0.5319, step time: 0.2573\n",
      "172/281, train_loss: 0.5798, step time: 0.2605\n",
      "173/281, train_loss: 0.5740, step time: 0.2572\n",
      "174/281, train_loss: 0.6501, step time: 0.2538\n",
      "175/281, train_loss: 0.6084, step time: 0.2595\n",
      "176/281, train_loss: 0.5200, step time: 0.2582\n",
      "177/281, train_loss: 0.5950, step time: 0.2597\n",
      "178/281, train_loss: 0.6376, step time: 0.2636\n",
      "179/281, train_loss: 0.5735, step time: 0.2659\n",
      "180/281, train_loss: 0.6016, step time: 0.2625\n",
      "181/281, train_loss: 0.5573, step time: 0.2586\n",
      "182/281, train_loss: 0.5833, step time: 0.2602\n",
      "183/281, train_loss: 0.7319, step time: 0.2608\n",
      "184/281, train_loss: 0.5776, step time: 0.2594\n",
      "185/281, train_loss: 0.4846, step time: 0.2616\n",
      "186/281, train_loss: 0.5738, step time: 0.2560\n",
      "187/281, train_loss: 0.7135, step time: 0.2610\n",
      "188/281, train_loss: 0.6217, step time: 0.2574\n",
      "189/281, train_loss: 0.6263, step time: 0.2555\n",
      "190/281, train_loss: 0.5865, step time: 0.2554\n",
      "191/281, train_loss: 0.6152, step time: 0.2557\n",
      "192/281, train_loss: 0.5040, step time: 0.2578\n",
      "193/281, train_loss: 0.5401, step time: 0.2581\n",
      "194/281, train_loss: 0.4851, step time: 0.2578\n",
      "195/281, train_loss: 0.7204, step time: 0.2552\n",
      "196/281, train_loss: 0.6063, step time: 0.2536\n",
      "197/281, train_loss: 0.6516, step time: 0.2585\n",
      "198/281, train_loss: 0.5846, step time: 0.2569\n",
      "199/281, train_loss: 0.5965, step time: 0.2574\n",
      "200/281, train_loss: 0.5390, step time: 0.2578\n",
      "201/281, train_loss: 0.4814, step time: 0.2593\n",
      "202/281, train_loss: 0.6156, step time: 0.2546\n",
      "203/281, train_loss: 0.7079, step time: 0.2537\n",
      "204/281, train_loss: 0.7182, step time: 0.2514\n",
      "205/281, train_loss: 0.5204, step time: 0.2529\n",
      "206/281, train_loss: 0.5077, step time: 0.2609\n",
      "207/281, train_loss: 0.7414, step time: 0.2618\n",
      "208/281, train_loss: 0.5886, step time: 0.2572\n",
      "209/281, train_loss: 0.6704, step time: 0.2580\n",
      "210/281, train_loss: 0.6077, step time: 0.2649\n",
      "211/281, train_loss: 0.5747, step time: 0.2578\n",
      "212/281, train_loss: 0.5405, step time: 0.2573\n",
      "213/281, train_loss: 0.6497, step time: 0.2573\n",
      "214/281, train_loss: 0.6268, step time: 0.2568\n",
      "215/281, train_loss: 0.5923, step time: 0.2567\n",
      "216/281, train_loss: 0.6521, step time: 0.2581\n",
      "217/281, train_loss: 0.6204, step time: 0.2578\n",
      "218/281, train_loss: 0.7006, step time: 0.2539\n",
      "219/281, train_loss: 0.6255, step time: 0.2523\n",
      "220/281, train_loss: 0.5459, step time: 0.2555\n",
      "221/281, train_loss: 0.5681, step time: 0.2608\n",
      "222/281, train_loss: 0.6176, step time: 0.2721\n",
      "223/281, train_loss: 0.6040, step time: 0.2578\n",
      "224/281, train_loss: 0.6186, step time: 0.2576\n",
      "225/281, train_loss: 0.5760, step time: 0.2582\n",
      "226/281, train_loss: 0.4850, step time: 0.2610\n",
      "227/281, train_loss: 0.6421, step time: 0.2559\n",
      "228/281, train_loss: 0.6134, step time: 0.2459\n",
      "229/281, train_loss: 0.5998, step time: 0.2508\n",
      "230/281, train_loss: 0.5982, step time: 0.2598\n",
      "231/281, train_loss: 0.4704, step time: 0.2578\n",
      "232/281, train_loss: 0.5420, step time: 0.2550\n",
      "233/281, train_loss: 0.5395, step time: 0.2530\n",
      "234/281, train_loss: 0.6390, step time: 0.2575\n",
      "235/281, train_loss: 0.5346, step time: 0.2523\n",
      "236/281, train_loss: 0.5772, step time: 0.2490\n",
      "237/281, train_loss: 0.5312, step time: 0.2512\n",
      "238/281, train_loss: 0.6676, step time: 0.2581\n",
      "239/281, train_loss: 0.6144, step time: 0.2557\n",
      "240/281, train_loss: 0.5457, step time: 0.2573\n",
      "241/281, train_loss: 0.4925, step time: 0.2505\n",
      "242/281, train_loss: 0.6078, step time: 0.2497\n",
      "243/281, train_loss: 0.4978, step time: 0.2512\n",
      "244/281, train_loss: 0.5244, step time: 0.2488\n",
      "245/281, train_loss: 0.6126, step time: 0.2520\n",
      "246/281, train_loss: 0.6289, step time: 0.2548\n",
      "247/281, train_loss: 0.5896, step time: 0.2547\n",
      "248/281, train_loss: 0.5966, step time: 0.2513\n",
      "249/281, train_loss: 0.6698, step time: 0.2517\n",
      "250/281, train_loss: 0.6537, step time: 0.2553\n",
      "251/281, train_loss: 0.4945, step time: 0.2501\n",
      "252/281, train_loss: 0.6112, step time: 0.2529\n",
      "253/281, train_loss: 0.6082, step time: 0.2577\n",
      "254/281, train_loss: 0.6027, step time: 0.2638\n",
      "255/281, train_loss: 0.6816, step time: 0.2632\n",
      "256/281, train_loss: 0.5600, step time: 0.2612\n",
      "257/281, train_loss: 0.6639, step time: 0.2672\n",
      "258/281, train_loss: 0.5071, step time: 0.2627\n",
      "259/281, train_loss: 0.5373, step time: 0.2587\n",
      "260/281, train_loss: 0.5274, step time: 0.2551\n",
      "261/281, train_loss: 0.7177, step time: 0.2547\n",
      "262/281, train_loss: 0.6127, step time: 0.2610\n",
      "263/281, train_loss: 0.6988, step time: 0.2582\n",
      "264/281, train_loss: 0.6733, step time: 0.2584\n",
      "265/281, train_loss: 0.5710, step time: 0.2586\n",
      "266/281, train_loss: 0.5538, step time: 0.2616\n",
      "267/281, train_loss: 0.4853, step time: 0.2588\n",
      "268/281, train_loss: 0.5992, step time: 0.2589\n",
      "269/281, train_loss: 0.5943, step time: 0.2593\n",
      "270/281, train_loss: 0.4789, step time: 0.2574\n",
      "271/281, train_loss: 0.6367, step time: 0.2607\n",
      "272/281, train_loss: 0.5137, step time: 0.2590\n",
      "273/281, train_loss: 0.5248, step time: 0.2619\n",
      "274/281, train_loss: 0.6780, step time: 0.2596\n",
      "275/281, train_loss: 0.5315, step time: 0.2586\n",
      "276/281, train_loss: 0.5016, step time: 0.2577\n",
      "277/281, train_loss: 0.5912, step time: 0.2587\n",
      "278/281, train_loss: 0.4925, step time: 0.2604\n",
      "279/281, train_loss: 0.6287, step time: 0.2593\n",
      "280/281, train_loss: 0.6049, step time: 0.2582\n",
      "281/281, train_loss: 0.5550, step time: 0.2582\n",
      "282/281, train_loss: 0.5096, step time: 0.1518\n",
      "epoch 9 average loss: 0.6043\n",
      "saved new best metric model\n",
      "current epoch: 9 current mean dice: 0.5361 tc: 0.0090 wt: 0.8905 et: 0.7317\n",
      "best mean dice: 0.5361 at epoch: 9\n",
      "time consuming of epoch 9 is: 257.2703\n",
      "----------\n",
      "epoch 10/200\n",
      "1/281, train_loss: 0.5777, step time: 0.2914\n",
      "2/281, train_loss: 0.4517, step time: 0.2662\n",
      "3/281, train_loss: 0.6990, step time: 0.2662\n",
      "4/281, train_loss: 0.6029, step time: 0.2663\n",
      "5/281, train_loss: 0.6057, step time: 0.2682\n",
      "6/281, train_loss: 0.4631, step time: 0.2663\n",
      "7/281, train_loss: 0.7002, step time: 0.2575\n",
      "8/281, train_loss: 0.5202, step time: 0.2568\n",
      "9/281, train_loss: 0.5754, step time: 0.2556\n",
      "10/281, train_loss: 0.5591, step time: 0.2590\n",
      "11/281, train_loss: 0.5249, step time: 0.2587\n",
      "12/281, train_loss: 0.4835, step time: 0.2667\n",
      "13/281, train_loss: 0.6032, step time: 0.2591\n",
      "14/281, train_loss: 0.5883, step time: 0.2570\n",
      "15/281, train_loss: 0.4873, step time: 0.2671\n",
      "16/281, train_loss: 0.6975, step time: 0.2608\n",
      "17/281, train_loss: 0.6001, step time: 0.2563\n",
      "18/281, train_loss: 0.4790, step time: 0.2646\n",
      "19/281, train_loss: 0.6654, step time: 0.2639\n",
      "20/281, train_loss: 0.5581, step time: 0.2772\n",
      "21/281, train_loss: 0.4796, step time: 0.2545\n",
      "22/281, train_loss: 0.6347, step time: 0.2611\n",
      "23/281, train_loss: 0.5905, step time: 0.2608\n",
      "24/281, train_loss: 0.7077, step time: 0.2645\n",
      "25/281, train_loss: 0.4803, step time: 0.2633\n",
      "26/281, train_loss: 0.6613, step time: 0.2587\n",
      "27/281, train_loss: 0.4977, step time: 0.2618\n",
      "28/281, train_loss: 0.4384, step time: 0.2631\n",
      "29/281, train_loss: 0.5843, step time: 0.2612\n",
      "30/281, train_loss: 0.5945, step time: 0.2618\n",
      "31/281, train_loss: 0.5416, step time: 0.2614\n",
      "32/281, train_loss: 0.5566, step time: 0.2624\n",
      "33/281, train_loss: 0.5375, step time: 0.2628\n",
      "34/281, train_loss: 0.4987, step time: 0.2960\n",
      "35/281, train_loss: 0.4570, step time: 0.2698\n",
      "36/281, train_loss: 0.5063, step time: 0.2594\n",
      "37/281, train_loss: 0.4425, step time: 0.2616\n",
      "38/281, train_loss: 0.5317, step time: 0.2655\n",
      "39/281, train_loss: 0.5443, step time: 0.2627\n",
      "40/281, train_loss: 0.6613, step time: 0.2521\n",
      "41/281, train_loss: 0.5602, step time: 0.2649\n",
      "42/281, train_loss: 0.6772, step time: 0.2641\n",
      "43/281, train_loss: 0.5415, step time: 0.2611\n",
      "44/281, train_loss: 0.5432, step time: 0.2615\n",
      "45/281, train_loss: 0.5857, step time: 0.2603\n",
      "46/281, train_loss: 0.6481, step time: 0.2587\n",
      "47/281, train_loss: 0.5723, step time: 0.2571\n",
      "48/281, train_loss: 0.5995, step time: 0.2589\n",
      "49/281, train_loss: 0.5071, step time: 0.2579\n",
      "50/281, train_loss: 0.4902, step time: 0.2581\n",
      "51/281, train_loss: 0.6370, step time: 0.2615\n",
      "52/281, train_loss: 0.6746, step time: 0.2701\n",
      "53/281, train_loss: 0.5974, step time: 0.2616\n",
      "54/281, train_loss: 0.6663, step time: 0.2617\n",
      "55/281, train_loss: 0.5936, step time: 0.2618\n",
      "56/281, train_loss: 0.4835, step time: 0.2595\n",
      "57/281, train_loss: 0.5189, step time: 0.2572\n",
      "58/281, train_loss: 0.4428, step time: 0.2518\n",
      "59/281, train_loss: 0.5051, step time: 0.2583\n",
      "60/281, train_loss: 0.6103, step time: 0.2605\n",
      "61/281, train_loss: 0.5958, step time: 0.2586\n",
      "62/281, train_loss: 0.4625, step time: 0.2564\n",
      "63/281, train_loss: 0.5542, step time: 0.2581\n",
      "64/281, train_loss: 0.6425, step time: 0.2592\n",
      "65/281, train_loss: 0.6199, step time: 0.2647\n",
      "66/281, train_loss: 0.5820, step time: 0.2529\n",
      "67/281, train_loss: 0.6191, step time: 0.2531\n",
      "68/281, train_loss: 0.5494, step time: 0.2545\n",
      "69/281, train_loss: 0.4459, step time: 0.2568\n",
      "70/281, train_loss: 0.4981, step time: 0.2579\n",
      "71/281, train_loss: 0.4671, step time: 0.2569\n",
      "72/281, train_loss: 0.4869, step time: 0.2581\n",
      "73/281, train_loss: 0.4907, step time: 0.2538\n",
      "74/281, train_loss: 0.6092, step time: 0.2573\n",
      "75/281, train_loss: 0.5465, step time: 0.2580\n",
      "76/281, train_loss: 0.5426, step time: 0.2521\n",
      "77/281, train_loss: 0.6020, step time: 0.2569\n",
      "78/281, train_loss: 0.4862, step time: 0.2639\n",
      "79/281, train_loss: 0.6636, step time: 0.2566\n",
      "80/281, train_loss: 0.5652, step time: 0.2556\n",
      "81/281, train_loss: 0.4954, step time: 0.2548\n",
      "82/281, train_loss: 0.6438, step time: 0.2572\n",
      "83/281, train_loss: 0.5143, step time: 0.2606\n",
      "84/281, train_loss: 0.5155, step time: 0.2659\n",
      "85/281, train_loss: 0.6719, step time: 0.2641\n",
      "86/281, train_loss: 0.4794, step time: 0.2593\n",
      "87/281, train_loss: 0.5697, step time: 0.2565\n",
      "88/281, train_loss: 0.4964, step time: 0.2587\n",
      "89/281, train_loss: 0.6017, step time: 0.2589\n",
      "90/281, train_loss: 0.4851, step time: 0.2617\n",
      "91/281, train_loss: 0.5265, step time: 0.2567\n",
      "92/281, train_loss: 0.5733, step time: 0.2590\n",
      "93/281, train_loss: 0.5684, step time: 0.2600\n",
      "94/281, train_loss: 0.5435, step time: 0.2663\n",
      "95/281, train_loss: 0.4875, step time: 0.2613\n",
      "96/281, train_loss: 0.6952, step time: 0.2930\n",
      "97/281, train_loss: 0.4976, step time: 0.2546\n",
      "98/281, train_loss: 0.5675, step time: 0.2596\n",
      "99/281, train_loss: 0.4582, step time: 0.2590\n",
      "100/281, train_loss: 0.6338, step time: 0.2530\n",
      "101/281, train_loss: 0.6323, step time: 0.2535\n",
      "102/281, train_loss: 0.6888, step time: 0.2585\n",
      "103/281, train_loss: 0.5315, step time: 0.2647\n",
      "104/281, train_loss: 0.5098, step time: 0.2766\n",
      "105/281, train_loss: 0.6121, step time: 0.2566\n",
      "106/281, train_loss: 0.6010, step time: 0.2541\n",
      "107/281, train_loss: 0.5814, step time: 0.2581\n",
      "108/281, train_loss: 0.6147, step time: 0.2579\n",
      "109/281, train_loss: 0.4789, step time: 0.2599\n",
      "110/281, train_loss: 0.4706, step time: 0.2558\n",
      "111/281, train_loss: 0.5353, step time: 0.2602\n",
      "112/281, train_loss: 0.6174, step time: 0.2624\n",
      "113/281, train_loss: 0.5470, step time: 0.2583\n",
      "114/281, train_loss: 0.6082, step time: 0.2513\n",
      "115/281, train_loss: 0.5075, step time: 0.2544\n",
      "116/281, train_loss: 0.5769, step time: 0.2529\n",
      "117/281, train_loss: 0.5744, step time: 0.2577\n",
      "118/281, train_loss: 0.6249, step time: 0.2600\n",
      "119/281, train_loss: 0.6404, step time: 0.2563\n",
      "120/281, train_loss: 0.4787, step time: 0.2557\n",
      "121/281, train_loss: 0.4931, step time: 0.2597\n",
      "122/281, train_loss: 0.4891, step time: 0.2591\n",
      "123/281, train_loss: 0.5408, step time: 0.2600\n",
      "124/281, train_loss: 0.5070, step time: 0.2556\n",
      "125/281, train_loss: 0.6192, step time: 0.2578\n",
      "126/281, train_loss: 0.6118, step time: 0.2554\n",
      "127/281, train_loss: 0.4934, step time: 0.2538\n",
      "128/281, train_loss: 0.4665, step time: 0.2610\n",
      "129/281, train_loss: 0.5198, step time: 0.2633\n",
      "130/281, train_loss: 0.5509, step time: 0.2628\n",
      "131/281, train_loss: 0.4254, step time: 0.2610\n",
      "132/281, train_loss: 0.5445, step time: 0.2554\n",
      "133/281, train_loss: 0.4923, step time: 0.2577\n",
      "134/281, train_loss: 0.5756, step time: 0.2600\n",
      "135/281, train_loss: 0.5120, step time: 0.2609\n",
      "136/281, train_loss: 0.4899, step time: 0.2573\n",
      "137/281, train_loss: 0.4796, step time: 0.2608\n",
      "138/281, train_loss: 0.4643, step time: 0.2596\n",
      "139/281, train_loss: 0.5195, step time: 0.2544\n",
      "140/281, train_loss: 0.4452, step time: 0.2597\n",
      "141/281, train_loss: 0.5101, step time: 0.2588\n",
      "142/281, train_loss: 0.5512, step time: 0.2554\n",
      "143/281, train_loss: 0.5185, step time: 0.2579\n",
      "144/281, train_loss: 0.4530, step time: 0.2620\n",
      "145/281, train_loss: 0.5512, step time: 0.2570\n",
      "146/281, train_loss: 0.5593, step time: 0.2583\n",
      "147/281, train_loss: 0.5736, step time: 0.2639\n",
      "148/281, train_loss: 0.4254, step time: 0.2601\n",
      "149/281, train_loss: 0.5827, step time: 0.2602\n",
      "150/281, train_loss: 0.5523, step time: 0.2572\n",
      "151/281, train_loss: 0.6451, step time: 0.2545\n",
      "152/281, train_loss: 0.4740, step time: 0.2584\n",
      "153/281, train_loss: 0.4869, step time: 0.2665\n",
      "154/281, train_loss: 0.5457, step time: 0.2602\n",
      "155/281, train_loss: 0.5779, step time: 0.2581\n",
      "156/281, train_loss: 0.5948, step time: 0.2577\n",
      "157/281, train_loss: 0.5581, step time: 0.2820\n",
      "158/281, train_loss: 0.4819, step time: 0.2897\n",
      "159/281, train_loss: 0.5035, step time: 0.2721\n",
      "160/281, train_loss: 0.5687, step time: 0.2528\n",
      "161/281, train_loss: 0.4615, step time: 0.2534\n",
      "162/281, train_loss: 0.7051, step time: 0.2548\n",
      "163/281, train_loss: 0.5102, step time: 0.2565\n",
      "164/281, train_loss: 0.5862, step time: 0.2594\n",
      "165/281, train_loss: 0.5495, step time: 0.2601\n",
      "166/281, train_loss: 0.4804, step time: 0.2587\n",
      "167/281, train_loss: 0.5791, step time: 0.2567\n",
      "168/281, train_loss: 0.6200, step time: 0.2629\n",
      "169/281, train_loss: 0.5138, step time: 0.2605\n",
      "170/281, train_loss: 0.4501, step time: 0.2659\n",
      "171/281, train_loss: 0.4413, step time: 0.2632\n",
      "172/281, train_loss: 0.5093, step time: 0.2597\n",
      "173/281, train_loss: 0.4931, step time: 0.2589\n",
      "174/281, train_loss: 0.5016, step time: 0.2568\n",
      "175/281, train_loss: 0.5733, step time: 0.2598\n",
      "176/281, train_loss: 0.4739, step time: 0.2582\n",
      "177/281, train_loss: 0.6446, step time: 0.2662\n",
      "178/281, train_loss: 0.5089, step time: 0.2585\n",
      "179/281, train_loss: 0.5048, step time: 0.2586\n",
      "180/281, train_loss: 0.4947, step time: 0.2587\n",
      "181/281, train_loss: 0.6526, step time: 0.2513\n",
      "182/281, train_loss: 0.7114, step time: 0.2516\n",
      "183/281, train_loss: 0.4611, step time: 0.2474\n",
      "184/281, train_loss: 0.4551, step time: 0.2515\n",
      "185/281, train_loss: 0.5747, step time: 0.2531\n",
      "186/281, train_loss: 0.4598, step time: 0.2548\n",
      "187/281, train_loss: 0.6029, step time: 0.2537\n",
      "188/281, train_loss: 0.5470, step time: 0.2586\n",
      "189/281, train_loss: 0.5447, step time: 0.2557\n",
      "190/281, train_loss: 0.5694, step time: 0.2524\n",
      "191/281, train_loss: 0.5211, step time: 0.2514\n",
      "192/281, train_loss: 0.7139, step time: 0.2599\n",
      "193/281, train_loss: 0.4646, step time: 0.2530\n",
      "194/281, train_loss: 0.4439, step time: 0.2534\n",
      "195/281, train_loss: 0.7105, step time: 0.2508\n",
      "196/281, train_loss: 0.5157, step time: 0.2512\n",
      "197/281, train_loss: 0.5366, step time: 0.2451\n",
      "198/281, train_loss: 0.5614, step time: 0.2502\n",
      "199/281, train_loss: 0.4897, step time: 0.2488\n",
      "200/281, train_loss: 0.4922, step time: 0.2518\n",
      "201/281, train_loss: 0.4978, step time: 0.2480\n",
      "202/281, train_loss: 0.6294, step time: 0.2512\n",
      "203/281, train_loss: 0.5597, step time: 0.2476\n",
      "204/281, train_loss: 0.4890, step time: 0.2475\n",
      "205/281, train_loss: 0.4624, step time: 0.2640\n",
      "206/281, train_loss: 0.5397, step time: 0.2586\n",
      "207/281, train_loss: 0.5876, step time: 0.2586\n",
      "208/281, train_loss: 0.7129, step time: 0.2522\n",
      "209/281, train_loss: 0.4599, step time: 0.2461\n",
      "210/281, train_loss: 0.4778, step time: 0.2499\n",
      "211/281, train_loss: 0.5553, step time: 0.2520\n",
      "212/281, train_loss: 0.5250, step time: 0.2476\n",
      "213/281, train_loss: 0.5702, step time: 0.2550\n",
      "214/281, train_loss: 0.5748, step time: 0.2535\n",
      "215/281, train_loss: 0.4658, step time: 0.2537\n",
      "216/281, train_loss: 0.5434, step time: 0.2535\n",
      "217/281, train_loss: 0.5612, step time: 0.2470\n",
      "218/281, train_loss: 0.5795, step time: 0.2462\n",
      "219/281, train_loss: 0.4662, step time: 0.2535\n",
      "220/281, train_loss: 0.4494, step time: 0.2536\n",
      "221/281, train_loss: 0.4429, step time: 0.2472\n",
      "222/281, train_loss: 0.5993, step time: 0.2502\n",
      "223/281, train_loss: 0.5623, step time: 0.2515\n",
      "224/281, train_loss: 0.4644, step time: 0.2511\n",
      "225/281, train_loss: 0.4687, step time: 0.2517\n",
      "226/281, train_loss: 0.6290, step time: 0.2483\n",
      "227/281, train_loss: 0.4850, step time: 0.2489\n",
      "228/281, train_loss: 0.5955, step time: 0.2492\n",
      "229/281, train_loss: 0.4498, step time: 0.2569\n",
      "230/281, train_loss: 0.5475, step time: 0.2486\n",
      "231/281, train_loss: 0.5622, step time: 0.2524\n",
      "232/281, train_loss: 0.6265, step time: 0.2462\n",
      "233/281, train_loss: 0.6358, step time: 0.2486\n",
      "234/281, train_loss: 0.4855, step time: 0.2451\n",
      "235/281, train_loss: 0.5297, step time: 0.2498\n",
      "236/281, train_loss: 0.5507, step time: 0.2494\n",
      "237/281, train_loss: 0.5498, step time: 0.2523\n",
      "238/281, train_loss: 0.6658, step time: 0.2507\n",
      "239/281, train_loss: 0.6210, step time: 0.2453\n",
      "240/281, train_loss: 0.5619, step time: 0.2516\n",
      "241/281, train_loss: 0.6274, step time: 0.2501\n",
      "242/281, train_loss: 0.5926, step time: 0.2516\n",
      "243/281, train_loss: 0.6202, step time: 0.2576\n",
      "244/281, train_loss: 0.5676, step time: 0.2544\n",
      "245/281, train_loss: 0.5683, step time: 0.2546\n",
      "246/281, train_loss: 0.4682, step time: 0.2571\n",
      "247/281, train_loss: 0.5274, step time: 0.2533\n",
      "248/281, train_loss: 0.5025, step time: 0.2540\n",
      "249/281, train_loss: 0.4283, step time: 0.2511\n",
      "250/281, train_loss: 0.6090, step time: 0.2546\n",
      "251/281, train_loss: 0.4566, step time: 0.2539\n",
      "252/281, train_loss: 0.5760, step time: 0.2613\n",
      "253/281, train_loss: 0.4417, step time: 0.2544\n",
      "254/281, train_loss: 0.4254, step time: 0.2552\n",
      "255/281, train_loss: 0.5049, step time: 0.2587\n",
      "256/281, train_loss: 0.5498, step time: 0.2520\n",
      "257/281, train_loss: 0.6263, step time: 0.2499\n",
      "258/281, train_loss: 0.4738, step time: 0.2504\n",
      "259/281, train_loss: 0.4637, step time: 0.2471\n",
      "260/281, train_loss: 0.4395, step time: 0.2519\n",
      "261/281, train_loss: 0.5857, step time: 0.2500\n",
      "262/281, train_loss: 0.4864, step time: 0.2479\n",
      "263/281, train_loss: 0.5052, step time: 0.2501\n",
      "264/281, train_loss: 0.4308, step time: 0.2563\n",
      "265/281, train_loss: 0.4629, step time: 0.2493\n",
      "266/281, train_loss: 0.4350, step time: 0.2496\n",
      "267/281, train_loss: 0.5140, step time: 0.2482\n",
      "268/281, train_loss: 0.5329, step time: 0.2664\n",
      "269/281, train_loss: 0.5619, step time: 0.2528\n",
      "270/281, train_loss: 0.6008, step time: 0.2513\n",
      "271/281, train_loss: 0.4995, step time: 0.2496\n",
      "272/281, train_loss: 0.5688, step time: 0.2542\n",
      "273/281, train_loss: 0.6070, step time: 0.2568\n",
      "274/281, train_loss: 0.5422, step time: 0.2484\n",
      "275/281, train_loss: 0.5282, step time: 0.2473\n",
      "276/281, train_loss: 0.5256, step time: 0.2490\n",
      "277/281, train_loss: 0.6387, step time: 0.2507\n",
      "278/281, train_loss: 0.5535, step time: 0.2502\n",
      "279/281, train_loss: 0.4757, step time: 0.2515\n",
      "280/281, train_loss: 0.4636, step time: 0.2520\n",
      "281/281, train_loss: 0.4825, step time: 0.2429\n",
      "282/281, train_loss: 0.4480, step time: 0.1475\n",
      "epoch 10 average loss: 0.5446\n",
      "current epoch: 10 current mean dice: 0.5353 tc: 0.0090 wt: 0.8962 et: 0.7223\n",
      "best mean dice: 0.5361 at epoch: 9\n",
      "time consuming of epoch 10 is: 266.2409\n",
      "----------\n",
      "epoch 11/200\n",
      "1/281, train_loss: 0.5314, step time: 0.2588\n",
      "2/281, train_loss: 0.5674, step time: 0.2531\n",
      "3/281, train_loss: 0.4466, step time: 0.2561\n",
      "4/281, train_loss: 0.5277, step time: 0.2497\n",
      "5/281, train_loss: 0.4615, step time: 0.2566\n",
      "6/281, train_loss: 0.4481, step time: 0.2534\n",
      "7/281, train_loss: 0.6442, step time: 0.2512\n",
      "8/281, train_loss: 0.4289, step time: 0.2593\n",
      "9/281, train_loss: 0.5026, step time: 0.2590\n",
      "10/281, train_loss: 0.5487, step time: 0.2544\n",
      "11/281, train_loss: 0.5544, step time: 0.2526\n",
      "12/281, train_loss: 0.5614, step time: 0.2665\n",
      "13/281, train_loss: 0.4743, step time: 0.2488\n",
      "14/281, train_loss: 0.5010, step time: 0.2600\n",
      "15/281, train_loss: 0.5846, step time: 0.2598\n",
      "16/281, train_loss: 0.5273, step time: 0.2539\n",
      "17/281, train_loss: 0.4573, step time: 0.2497\n",
      "18/281, train_loss: 0.4558, step time: 0.2487\n",
      "19/281, train_loss: 0.4748, step time: 0.2472\n",
      "20/281, train_loss: 0.5479, step time: 0.2513\n",
      "21/281, train_loss: 0.6184, step time: 0.2570\n",
      "22/281, train_loss: 0.4817, step time: 0.2550\n",
      "23/281, train_loss: 0.6115, step time: 0.2509\n",
      "24/281, train_loss: 0.5063, step time: 0.2524\n",
      "25/281, train_loss: 0.4559, step time: 0.2586\n",
      "26/281, train_loss: 0.4716, step time: 0.2478\n",
      "27/281, train_loss: 0.5987, step time: 0.2528\n",
      "28/281, train_loss: 0.5037, step time: 0.2782\n",
      "29/281, train_loss: 0.5753, step time: 0.2564\n",
      "30/281, train_loss: 0.4913, step time: 0.2513\n",
      "31/281, train_loss: 0.5478, step time: 0.2520\n",
      "32/281, train_loss: 0.5895, step time: 0.2518\n",
      "33/281, train_loss: 0.4720, step time: 0.2688\n",
      "34/281, train_loss: 0.4380, step time: 0.2578\n",
      "35/281, train_loss: 0.4885, step time: 0.2554\n",
      "36/281, train_loss: 0.4937, step time: 0.2506\n",
      "37/281, train_loss: 0.4395, step time: 0.2491\n",
      "38/281, train_loss: 0.5053, step time: 0.2497\n",
      "39/281, train_loss: 0.5430, step time: 0.2499\n",
      "40/281, train_loss: 0.5555, step time: 0.2585\n",
      "41/281, train_loss: 0.4292, step time: 0.2513\n",
      "42/281, train_loss: 0.5314, step time: 0.2461\n",
      "43/281, train_loss: 0.4952, step time: 0.2479\n",
      "44/281, train_loss: 0.4902, step time: 0.2520\n",
      "45/281, train_loss: 0.4624, step time: 0.2552\n",
      "46/281, train_loss: 0.5873, step time: 0.2527\n",
      "47/281, train_loss: 0.4989, step time: 0.2488\n",
      "48/281, train_loss: 0.5652, step time: 0.2475\n",
      "49/281, train_loss: 0.4975, step time: 0.2539\n",
      "50/281, train_loss: 0.4578, step time: 0.2475\n",
      "51/281, train_loss: 0.4682, step time: 0.2510\n",
      "52/281, train_loss: 0.4806, step time: 0.2533\n",
      "53/281, train_loss: 0.5084, step time: 0.2518\n",
      "54/281, train_loss: 0.6291, step time: 0.2558\n",
      "55/281, train_loss: 0.5068, step time: 0.2575\n",
      "56/281, train_loss: 0.4828, step time: 0.2499\n",
      "57/281, train_loss: 0.4548, step time: 0.2501\n",
      "58/281, train_loss: 0.5543, step time: 0.2498\n",
      "59/281, train_loss: 0.5257, step time: 0.2519\n",
      "60/281, train_loss: 0.4502, step time: 0.2433\n",
      "61/281, train_loss: 0.5336, step time: 0.2637\n",
      "62/281, train_loss: 0.6870, step time: 0.2583\n",
      "63/281, train_loss: 0.4258, step time: 0.2552\n",
      "64/281, train_loss: 0.5610, step time: 0.2481\n",
      "65/281, train_loss: 0.5185, step time: 0.2790\n",
      "66/281, train_loss: 0.4301, step time: 0.2490\n",
      "67/281, train_loss: 0.4229, step time: 0.2501\n",
      "68/281, train_loss: 0.4601, step time: 0.2557\n",
      "69/281, train_loss: 0.5770, step time: 0.2502\n",
      "70/281, train_loss: 0.4221, step time: 0.2537\n",
      "71/281, train_loss: 0.4596, step time: 0.2623\n",
      "72/281, train_loss: 0.4734, step time: 0.2627\n",
      "73/281, train_loss: 0.5243, step time: 0.2618\n",
      "74/281, train_loss: 0.6561, step time: 0.2640\n",
      "75/281, train_loss: 0.4631, step time: 0.2590\n",
      "76/281, train_loss: 0.4411, step time: 0.2582\n",
      "77/281, train_loss: 0.6118, step time: 0.2573\n",
      "78/281, train_loss: 0.5664, step time: 0.2550\n",
      "79/281, train_loss: 0.5947, step time: 0.2573\n",
      "80/281, train_loss: 0.6503, step time: 0.2527\n",
      "81/281, train_loss: 0.4546, step time: 0.2452\n",
      "82/281, train_loss: 0.6536, step time: 0.2468\n",
      "83/281, train_loss: 0.4775, step time: 0.2528\n",
      "84/281, train_loss: 0.5591, step time: 0.2521\n",
      "85/281, train_loss: 0.6686, step time: 0.2499\n",
      "86/281, train_loss: 0.4598, step time: 0.2486\n",
      "87/281, train_loss: 0.6065, step time: 0.2452\n",
      "88/281, train_loss: 0.5066, step time: 0.2489\n",
      "89/281, train_loss: 0.5723, step time: 0.2517\n",
      "90/281, train_loss: 0.5751, step time: 0.2540\n",
      "91/281, train_loss: 0.5280, step time: 0.2467\n",
      "92/281, train_loss: 0.5960, step time: 0.2464\n",
      "93/281, train_loss: 0.5354, step time: 0.2486\n",
      "94/281, train_loss: 0.4997, step time: 0.2453\n",
      "95/281, train_loss: 0.4693, step time: 0.2491\n",
      "96/281, train_loss: 0.5637, step time: 0.2512\n",
      "97/281, train_loss: 0.5860, step time: 0.2495\n",
      "98/281, train_loss: 0.4566, step time: 0.2470\n",
      "99/281, train_loss: 0.4504, step time: 0.2507\n",
      "100/281, train_loss: 0.6279, step time: 0.2562\n",
      "101/281, train_loss: 0.6827, step time: 0.2521\n",
      "102/281, train_loss: 0.5084, step time: 0.2485\n",
      "103/281, train_loss: 0.4905, step time: 0.2466\n",
      "104/281, train_loss: 0.5995, step time: 0.2519\n",
      "105/281, train_loss: 0.6491, step time: 0.2520\n",
      "106/281, train_loss: 0.5144, step time: 0.2536\n",
      "107/281, train_loss: 0.5043, step time: 0.2539\n",
      "108/281, train_loss: 0.4429, step time: 0.2588\n",
      "109/281, train_loss: 0.5946, step time: 0.2620\n",
      "110/281, train_loss: 0.4876, step time: 0.2572\n",
      "111/281, train_loss: 0.5219, step time: 0.2615\n",
      "112/281, train_loss: 0.5592, step time: 0.2600\n",
      "113/281, train_loss: 0.6693, step time: 0.2563\n",
      "114/281, train_loss: 0.4353, step time: 0.2602\n",
      "115/281, train_loss: 0.5243, step time: 0.2556\n",
      "116/281, train_loss: 0.5611, step time: 0.2511\n",
      "117/281, train_loss: 0.5709, step time: 0.2554\n",
      "118/281, train_loss: 0.5936, step time: 0.2541\n",
      "119/281, train_loss: 0.4364, step time: 0.2566\n",
      "120/281, train_loss: 0.5736, step time: 0.2514\n",
      "121/281, train_loss: 0.4627, step time: 0.2530\n",
      "122/281, train_loss: 0.5995, step time: 0.2626\n",
      "123/281, train_loss: 0.5001, step time: 0.2654\n",
      "124/281, train_loss: 0.4219, step time: 0.2574\n",
      "125/281, train_loss: 0.4568, step time: 0.2549\n",
      "126/281, train_loss: 0.5957, step time: 0.2500\n",
      "127/281, train_loss: 0.4598, step time: 0.2524\n",
      "128/281, train_loss: 0.5553, step time: 0.2516\n",
      "129/281, train_loss: 0.6294, step time: 0.2604\n",
      "130/281, train_loss: 0.4557, step time: 0.2813\n",
      "131/281, train_loss: 0.6025, step time: 0.2558\n",
      "132/281, train_loss: 0.4540, step time: 0.2564\n",
      "133/281, train_loss: 0.4968, step time: 0.2559\n",
      "134/281, train_loss: 0.7110, step time: 0.2622\n",
      "135/281, train_loss: 0.4310, step time: 0.2593\n",
      "136/281, train_loss: 0.4967, step time: 0.2574\n",
      "137/281, train_loss: 0.4506, step time: 0.2477\n",
      "138/281, train_loss: 0.6046, step time: 0.2556\n",
      "139/281, train_loss: 0.5819, step time: 0.2551\n",
      "140/281, train_loss: 0.4444, step time: 0.2524\n",
      "141/281, train_loss: 0.4388, step time: 0.2610\n",
      "142/281, train_loss: 0.5662, step time: 0.2512\n",
      "143/281, train_loss: 0.4594, step time: 0.2528\n",
      "144/281, train_loss: 0.5326, step time: 0.2488\n",
      "145/281, train_loss: 0.5418, step time: 0.2544\n",
      "146/281, train_loss: 0.5052, step time: 0.2484\n",
      "147/281, train_loss: 0.4312, step time: 0.2557\n",
      "148/281, train_loss: 0.5512, step time: 0.2589\n",
      "149/281, train_loss: 0.4507, step time: 0.2527\n",
      "150/281, train_loss: 0.5602, step time: 0.2520\n",
      "151/281, train_loss: 0.4212, step time: 0.2556\n",
      "152/281, train_loss: 0.4766, step time: 0.2597\n",
      "153/281, train_loss: 0.4868, step time: 0.2629\n",
      "154/281, train_loss: 0.4442, step time: 0.2585\n",
      "155/281, train_loss: 0.4586, step time: 0.2602\n",
      "156/281, train_loss: 0.5316, step time: 0.2523\n",
      "157/281, train_loss: 0.4760, step time: 0.2499\n",
      "158/281, train_loss: 0.5951, step time: 0.2532\n",
      "159/281, train_loss: 0.4126, step time: 0.2561\n",
      "160/281, train_loss: 0.4987, step time: 0.2575\n",
      "161/281, train_loss: 0.4605, step time: 0.2587\n",
      "162/281, train_loss: 0.4491, step time: 0.2542\n",
      "163/281, train_loss: 0.4177, step time: 0.2462\n",
      "164/281, train_loss: 0.4814, step time: 0.2478\n",
      "165/281, train_loss: 0.5938, step time: 0.2469\n",
      "166/281, train_loss: 0.4883, step time: 0.2505\n",
      "167/281, train_loss: 0.4610, step time: 0.2567\n",
      "168/281, train_loss: 0.4938, step time: 0.2555\n",
      "169/281, train_loss: 0.4653, step time: 0.2526\n",
      "170/281, train_loss: 0.5462, step time: 0.2536\n",
      "171/281, train_loss: 0.4455, step time: 0.2534\n",
      "172/281, train_loss: 0.4887, step time: 0.2515\n",
      "173/281, train_loss: 0.5050, step time: 0.2552\n",
      "174/281, train_loss: 0.4526, step time: 0.2485\n",
      "175/281, train_loss: 0.4483, step time: 0.2555\n",
      "176/281, train_loss: 0.5829, step time: 0.2559\n",
      "177/281, train_loss: 0.4561, step time: 0.2597\n",
      "178/281, train_loss: 0.5664, step time: 0.2540\n",
      "179/281, train_loss: 0.5644, step time: 0.2537\n",
      "180/281, train_loss: 0.4577, step time: 0.2560\n",
      "181/281, train_loss: 0.5340, step time: 0.2558\n",
      "182/281, train_loss: 0.4550, step time: 0.2782\n",
      "183/281, train_loss: 0.5469, step time: 0.2531\n",
      "184/281, train_loss: 0.4673, step time: 0.2508\n",
      "185/281, train_loss: 0.4829, step time: 0.2523\n",
      "186/281, train_loss: 0.4433, step time: 0.2528\n",
      "187/281, train_loss: 0.4219, step time: 0.2551\n",
      "188/281, train_loss: 0.4832, step time: 0.2553\n",
      "189/281, train_loss: 0.5341, step time: 0.2514\n",
      "190/281, train_loss: 0.4669, step time: 0.2835\n",
      "191/281, train_loss: 0.4775, step time: 0.2696\n",
      "192/281, train_loss: 0.5342, step time: 0.2538\n",
      "193/281, train_loss: 0.5201, step time: 0.2525\n",
      "194/281, train_loss: 0.5533, step time: 0.2498\n",
      "195/281, train_loss: 0.4296, step time: 0.2539\n",
      "196/281, train_loss: 0.6569, step time: 0.2567\n",
      "197/281, train_loss: 0.4870, step time: 0.2592\n",
      "198/281, train_loss: 0.5580, step time: 0.2514\n",
      "199/281, train_loss: 0.5613, step time: 0.2548\n",
      "200/281, train_loss: 0.4520, step time: 0.2560\n",
      "201/281, train_loss: 0.5964, step time: 0.2537\n",
      "202/281, train_loss: 0.4300, step time: 0.2535\n",
      "203/281, train_loss: 0.4416, step time: 0.2527\n",
      "204/281, train_loss: 0.5905, step time: 0.2492\n",
      "205/281, train_loss: 0.5822, step time: 0.2553\n",
      "206/281, train_loss: 0.4250, step time: 0.2592\n",
      "207/281, train_loss: 0.4627, step time: 0.2518\n",
      "208/281, train_loss: 0.6080, step time: 0.2474\n",
      "209/281, train_loss: 0.5992, step time: 0.2482\n",
      "210/281, train_loss: 0.4666, step time: 0.2513\n",
      "211/281, train_loss: 0.4284, step time: 0.2528\n",
      "212/281, train_loss: 0.4330, step time: 0.2544\n",
      "213/281, train_loss: 0.5524, step time: 0.2530\n",
      "214/281, train_loss: 0.5018, step time: 0.2532\n",
      "215/281, train_loss: 0.5506, step time: 0.2542\n",
      "216/281, train_loss: 0.4887, step time: 0.2585\n",
      "217/281, train_loss: 0.4609, step time: 0.2592\n",
      "218/281, train_loss: 0.4312, step time: 0.2754\n",
      "219/281, train_loss: 0.4604, step time: 0.2613\n",
      "220/281, train_loss: 0.5215, step time: 0.2516\n",
      "221/281, train_loss: 0.5971, step time: 0.2534\n",
      "222/281, train_loss: 0.4323, step time: 0.2546\n",
      "223/281, train_loss: 0.4668, step time: 0.2588\n",
      "224/281, train_loss: 0.4952, step time: 0.2542\n",
      "225/281, train_loss: 0.4702, step time: 0.2581\n",
      "226/281, train_loss: 0.5397, step time: 0.2633\n",
      "227/281, train_loss: 0.4478, step time: 0.2678\n",
      "228/281, train_loss: 0.4626, step time: 0.2602\n",
      "229/281, train_loss: 0.4177, step time: 0.2560\n",
      "230/281, train_loss: 0.4895, step time: 0.2571\n",
      "231/281, train_loss: 0.5067, step time: 0.2585\n",
      "232/281, train_loss: 0.4612, step time: 0.2596\n",
      "233/281, train_loss: 0.4552, step time: 0.2590\n",
      "234/281, train_loss: 0.5591, step time: 0.2603\n",
      "235/281, train_loss: 0.4486, step time: 0.2581\n",
      "236/281, train_loss: 0.5829, step time: 0.2567\n",
      "237/281, train_loss: 0.4233, step time: 0.2544\n",
      "238/281, train_loss: 0.4803, step time: 0.2590\n",
      "239/281, train_loss: 0.4255, step time: 0.2617\n",
      "240/281, train_loss: 0.6211, step time: 0.2540\n",
      "241/281, train_loss: 0.4138, step time: 0.2522\n",
      "242/281, train_loss: 0.4532, step time: 0.2553\n",
      "243/281, train_loss: 0.4161, step time: 0.2545\n",
      "244/281, train_loss: 0.4995, step time: 0.2580\n",
      "245/281, train_loss: 0.5475, step time: 0.2567\n",
      "246/281, train_loss: 0.4885, step time: 0.2545\n",
      "247/281, train_loss: 0.4729, step time: 0.2567\n",
      "248/281, train_loss: 0.6554, step time: 0.2577\n",
      "249/281, train_loss: 0.4511, step time: 0.2535\n",
      "250/281, train_loss: 0.7119, step time: 0.2540\n",
      "251/281, train_loss: 0.4031, step time: 0.2527\n",
      "252/281, train_loss: 0.4408, step time: 0.2549\n",
      "253/281, train_loss: 0.4870, step time: 0.2576\n",
      "254/281, train_loss: 0.5728, step time: 0.2581\n",
      "255/281, train_loss: 0.7019, step time: 0.2583\n",
      "256/281, train_loss: 0.4538, step time: 0.2529\n",
      "257/281, train_loss: 0.4602, step time: 0.2556\n",
      "258/281, train_loss: 0.4472, step time: 0.2598\n",
      "259/281, train_loss: 0.4531, step time: 0.2586\n",
      "260/281, train_loss: 0.5004, step time: 0.2592\n",
      "261/281, train_loss: 0.4608, step time: 0.2596\n",
      "262/281, train_loss: 0.4549, step time: 0.2593\n",
      "263/281, train_loss: 0.4461, step time: 0.2554\n",
      "264/281, train_loss: 0.4222, step time: 0.2564\n",
      "265/281, train_loss: 0.4659, step time: 0.2564\n",
      "266/281, train_loss: 0.5525, step time: 0.2534\n",
      "267/281, train_loss: 0.4217, step time: 0.2569\n",
      "268/281, train_loss: 0.5459, step time: 0.2639\n",
      "269/281, train_loss: 0.5314, step time: 0.2584\n",
      "270/281, train_loss: 0.4356, step time: 0.2546\n",
      "271/281, train_loss: 0.5695, step time: 0.2563\n",
      "272/281, train_loss: 0.6987, step time: 0.2564\n",
      "273/281, train_loss: 0.5488, step time: 0.2581\n",
      "274/281, train_loss: 0.4700, step time: 0.2565\n",
      "275/281, train_loss: 0.6066, step time: 0.2570\n",
      "276/281, train_loss: 0.5425, step time: 0.2579\n",
      "277/281, train_loss: 0.4526, step time: 0.2560\n",
      "278/281, train_loss: 0.4479, step time: 0.2576\n",
      "279/281, train_loss: 0.5648, step time: 0.2506\n",
      "280/281, train_loss: 0.5926, step time: 0.2548\n",
      "281/281, train_loss: 0.4880, step time: 0.2552\n",
      "282/281, train_loss: 0.4024, step time: 0.1492\n",
      "epoch 11 average loss: 0.5108\n",
      "saved new best metric model\n",
      "current epoch: 11 current mean dice: 0.5491 tc: 0.0090 wt: 0.9013 et: 0.7608\n",
      "best mean dice: 0.5491 at epoch: 11\n",
      "time consuming of epoch 11 is: 274.0975\n",
      "----------\n",
      "epoch 12/200\n",
      "1/281, train_loss: 0.4235, step time: 0.2638\n",
      "2/281, train_loss: 0.6767, step time: 0.2567\n",
      "3/281, train_loss: 0.4182, step time: 0.2606\n",
      "4/281, train_loss: 0.4207, step time: 0.2662\n",
      "5/281, train_loss: 0.5633, step time: 0.2556\n",
      "6/281, train_loss: 0.4664, step time: 0.2592\n",
      "7/281, train_loss: 0.4777, step time: 0.2579\n",
      "8/281, train_loss: 0.4107, step time: 0.2589\n",
      "9/281, train_loss: 0.5161, step time: 0.2597\n",
      "10/281, train_loss: 0.4279, step time: 0.2580\n",
      "11/281, train_loss: 0.5226, step time: 0.2607\n",
      "12/281, train_loss: 0.6181, step time: 0.2574\n",
      "13/281, train_loss: 0.6609, step time: 0.2591\n",
      "14/281, train_loss: 0.5425, step time: 0.2554\n",
      "15/281, train_loss: 0.4260, step time: 0.2587\n",
      "16/281, train_loss: 0.4631, step time: 0.2569\n",
      "17/281, train_loss: 0.5030, step time: 0.2586\n",
      "18/281, train_loss: 0.4105, step time: 0.2540\n",
      "19/281, train_loss: 0.4362, step time: 0.2552\n",
      "20/281, train_loss: 0.4757, step time: 0.2587\n",
      "21/281, train_loss: 0.4170, step time: 0.2565\n",
      "22/281, train_loss: 0.4612, step time: 0.2617\n",
      "23/281, train_loss: 0.6104, step time: 0.2524\n",
      "24/281, train_loss: 0.5692, step time: 0.2574\n",
      "25/281, train_loss: 0.5029, step time: 0.2546\n",
      "26/281, train_loss: 0.6271, step time: 0.2552\n",
      "27/281, train_loss: 0.4571, step time: 0.2563\n",
      "28/281, train_loss: 0.6399, step time: 0.2524\n",
      "29/281, train_loss: 0.4581, step time: 0.2592\n",
      "30/281, train_loss: 0.5758, step time: 0.2579\n",
      "31/281, train_loss: 0.4607, step time: 0.2755\n",
      "32/281, train_loss: 0.4468, step time: 0.2590\n",
      "33/281, train_loss: 0.4814, step time: 0.2600\n",
      "34/281, train_loss: 0.5736, step time: 0.2630\n",
      "35/281, train_loss: 0.4448, step time: 0.2592\n",
      "36/281, train_loss: 0.4130, step time: 0.2581\n",
      "37/281, train_loss: 0.4343, step time: 0.2561\n",
      "38/281, train_loss: 0.7078, step time: 0.2571\n",
      "39/281, train_loss: 0.4474, step time: 0.2570\n",
      "40/281, train_loss: 0.4917, step time: 0.2559\n",
      "41/281, train_loss: 0.3938, step time: 0.2527\n",
      "42/281, train_loss: 0.4399, step time: 0.2539\n",
      "43/281, train_loss: 0.5808, step time: 0.2500\n",
      "44/281, train_loss: 0.5343, step time: 0.2515\n",
      "45/281, train_loss: 0.5823, step time: 0.2601\n",
      "46/281, train_loss: 0.4857, step time: 0.2587\n",
      "47/281, train_loss: 0.5508, step time: 0.2582\n",
      "48/281, train_loss: 0.5106, step time: 0.2588\n",
      "49/281, train_loss: 0.4550, step time: 0.2558\n",
      "50/281, train_loss: 0.3988, step time: 0.2608\n",
      "51/281, train_loss: 0.7242, step time: 0.2615\n",
      "52/281, train_loss: 0.4298, step time: 0.2663\n",
      "53/281, train_loss: 0.4768, step time: 0.2569\n",
      "54/281, train_loss: 0.4870, step time: 0.2581\n",
      "55/281, train_loss: 0.4290, step time: 0.2600\n",
      "56/281, train_loss: 0.5684, step time: 0.2561\n",
      "57/281, train_loss: 0.5749, step time: 0.2556\n",
      "58/281, train_loss: 0.4270, step time: 0.2601\n",
      "59/281, train_loss: 0.4249, step time: 0.2686\n",
      "60/281, train_loss: 0.6695, step time: 0.2589\n",
      "61/281, train_loss: 0.5943, step time: 0.2518\n",
      "62/281, train_loss: 0.4308, step time: 0.2612\n",
      "63/281, train_loss: 0.4139, step time: 0.2920\n",
      "64/281, train_loss: 0.4509, step time: 0.2622\n",
      "65/281, train_loss: 0.4632, step time: 0.2559\n",
      "66/281, train_loss: 0.4413, step time: 0.2555\n",
      "67/281, train_loss: 0.4163, step time: 0.2619\n",
      "68/281, train_loss: 0.4822, step time: 0.2586\n",
      "69/281, train_loss: 0.4445, step time: 0.2621\n",
      "70/281, train_loss: 0.4042, step time: 0.2548\n",
      "71/281, train_loss: 0.4958, step time: 0.2530\n",
      "72/281, train_loss: 0.4193, step time: 0.2600\n",
      "73/281, train_loss: 0.4486, step time: 0.2530\n",
      "74/281, train_loss: 0.4491, step time: 0.2585\n",
      "75/281, train_loss: 0.4448, step time: 0.2544\n",
      "76/281, train_loss: 0.4419, step time: 0.2543\n",
      "77/281, train_loss: 0.5943, step time: 0.2568\n",
      "78/281, train_loss: 0.5643, step time: 0.2494\n",
      "79/281, train_loss: 0.4252, step time: 0.2508\n",
      "80/281, train_loss: 0.4261, step time: 0.2557\n",
      "81/281, train_loss: 0.4771, step time: 0.2510\n",
      "82/281, train_loss: 0.4768, step time: 0.2490\n",
      "83/281, train_loss: 0.4948, step time: 0.2578\n",
      "84/281, train_loss: 0.4226, step time: 0.2584\n",
      "85/281, train_loss: 0.4207, step time: 0.2596\n",
      "86/281, train_loss: 0.5374, step time: 0.2547\n",
      "87/281, train_loss: 0.4701, step time: 0.2593\n",
      "88/281, train_loss: 0.4233, step time: 0.2566\n",
      "89/281, train_loss: 0.3965, step time: 0.2537\n",
      "90/281, train_loss: 0.5193, step time: 0.2520\n",
      "91/281, train_loss: 0.4348, step time: 0.2590\n",
      "92/281, train_loss: 0.5306, step time: 0.2535\n",
      "93/281, train_loss: 0.6117, step time: 0.2518\n",
      "94/281, train_loss: 0.4469, step time: 0.2559\n",
      "95/281, train_loss: 0.4613, step time: 0.2551\n",
      "96/281, train_loss: 0.5728, step time: 0.2527\n",
      "97/281, train_loss: 0.5095, step time: 0.2573\n",
      "98/281, train_loss: 0.5871, step time: 0.2582\n",
      "99/281, train_loss: 0.5547, step time: 0.2531\n",
      "100/281, train_loss: 0.4247, step time: 0.2598\n",
      "101/281, train_loss: 0.5726, step time: 0.2585\n",
      "102/281, train_loss: 0.4549, step time: 0.2578\n",
      "103/281, train_loss: 0.5999, step time: 0.2528\n",
      "104/281, train_loss: 0.4467, step time: 0.2542\n",
      "105/281, train_loss: 0.4233, step time: 0.2493\n",
      "106/281, train_loss: 0.5373, step time: 0.2490\n",
      "107/281, train_loss: 0.4678, step time: 0.2595\n",
      "108/281, train_loss: 0.5707, step time: 0.2788\n",
      "109/281, train_loss: 0.5954, step time: 0.2532\n",
      "110/281, train_loss: 0.4516, step time: 0.2519\n",
      "111/281, train_loss: 0.4555, step time: 0.2522\n",
      "112/281, train_loss: 0.4626, step time: 0.2521\n",
      "113/281, train_loss: 0.4112, step time: 0.2524\n",
      "114/281, train_loss: 0.4675, step time: 0.2529\n",
      "115/281, train_loss: 0.5728, step time: 0.2512\n",
      "116/281, train_loss: 0.4332, step time: 0.2498\n",
      "117/281, train_loss: 0.4390, step time: 0.2520\n",
      "118/281, train_loss: 0.4545, step time: 0.2557\n",
      "119/281, train_loss: 0.4165, step time: 0.2525\n",
      "120/281, train_loss: 0.5730, step time: 0.2477\n",
      "121/281, train_loss: 0.5304, step time: 0.2518\n",
      "122/281, train_loss: 0.5316, step time: 0.2526\n",
      "123/281, train_loss: 0.5500, step time: 0.2601\n",
      "124/281, train_loss: 0.4659, step time: 0.2536\n",
      "125/281, train_loss: 0.4707, step time: 0.2583\n",
      "126/281, train_loss: 0.5424, step time: 0.2580\n",
      "127/281, train_loss: 0.5131, step time: 0.2517\n",
      "128/281, train_loss: 0.4183, step time: 0.2593\n",
      "129/281, train_loss: 0.4601, step time: 0.2545\n",
      "130/281, train_loss: 0.5320, step time: 0.2726\n",
      "131/281, train_loss: 0.4543, step time: 0.2583\n",
      "132/281, train_loss: 0.5026, step time: 0.2594\n",
      "133/281, train_loss: 0.4878, step time: 0.2649\n",
      "134/281, train_loss: 0.4273, step time: 0.2625\n",
      "135/281, train_loss: 0.5242, step time: 0.2552\n",
      "136/281, train_loss: 0.6123, step time: 0.2578\n",
      "137/281, train_loss: 0.5063, step time: 0.2569\n",
      "138/281, train_loss: 0.6402, step time: 0.2570\n",
      "139/281, train_loss: 0.5886, step time: 0.2618\n",
      "140/281, train_loss: 0.4173, step time: 0.2580\n",
      "141/281, train_loss: 0.5062, step time: 0.2592\n",
      "142/281, train_loss: 0.5903, step time: 0.2587\n",
      "143/281, train_loss: 0.5037, step time: 0.2573\n",
      "144/281, train_loss: 0.6023, step time: 0.2601\n",
      "145/281, train_loss: 0.4337, step time: 0.2597\n",
      "146/281, train_loss: 0.4270, step time: 0.2533\n",
      "147/281, train_loss: 0.5111, step time: 0.2566\n",
      "148/281, train_loss: 0.6384, step time: 0.2597\n",
      "149/281, train_loss: 0.4266, step time: 0.2523\n",
      "150/281, train_loss: 0.5096, step time: 0.2607\n",
      "151/281, train_loss: 0.6224, step time: 0.2752\n",
      "152/281, train_loss: 0.5940, step time: 0.2626\n",
      "153/281, train_loss: 0.4407, step time: 0.2576\n",
      "154/281, train_loss: 0.4378, step time: 0.2553\n",
      "155/281, train_loss: 0.5557, step time: 0.2589\n",
      "156/281, train_loss: 0.5674, step time: 0.2677\n",
      "157/281, train_loss: 0.4611, step time: 0.2640\n",
      "158/281, train_loss: 0.4517, step time: 0.2588\n",
      "159/281, train_loss: 0.4668, step time: 0.2557\n",
      "160/281, train_loss: 0.4573, step time: 0.2559\n",
      "161/281, train_loss: 0.4475, step time: 0.2570\n",
      "162/281, train_loss: 0.5731, step time: 0.2515\n",
      "163/281, train_loss: 0.4508, step time: 0.2598\n",
      "164/281, train_loss: 0.5083, step time: 0.2614\n",
      "165/281, train_loss: 0.4242, step time: 0.2509\n",
      "166/281, train_loss: 0.4474, step time: 0.2513\n",
      "167/281, train_loss: 0.4197, step time: 0.2567\n",
      "168/281, train_loss: 0.4839, step time: 0.2529\n",
      "169/281, train_loss: 0.4400, step time: 0.2559\n",
      "170/281, train_loss: 0.4754, step time: 0.2605\n",
      "171/281, train_loss: 0.4285, step time: 0.2569\n",
      "172/281, train_loss: 0.4594, step time: 0.2571\n",
      "173/281, train_loss: 0.4579, step time: 0.2526\n",
      "174/281, train_loss: 0.4575, step time: 0.2508\n",
      "175/281, train_loss: 0.4204, step time: 0.2551\n",
      "176/281, train_loss: 0.4691, step time: 0.2575\n",
      "177/281, train_loss: 0.4705, step time: 0.2521\n",
      "178/281, train_loss: 0.6261, step time: 0.2506\n",
      "179/281, train_loss: 0.4523, step time: 0.2588\n",
      "180/281, train_loss: 0.5089, step time: 0.2955\n",
      "181/281, train_loss: 0.4548, step time: 0.2615\n",
      "182/281, train_loss: 0.5716, step time: 0.2536\n",
      "183/281, train_loss: 0.4378, step time: 0.2556\n",
      "184/281, train_loss: 0.5856, step time: 0.2581\n",
      "185/281, train_loss: 0.4471, step time: 0.2525\n",
      "186/281, train_loss: 0.4875, step time: 0.2506\n",
      "187/281, train_loss: 0.4179, step time: 0.2505\n",
      "188/281, train_loss: 0.5565, step time: 0.2599\n",
      "189/281, train_loss: 0.5303, step time: 0.2586\n",
      "190/281, train_loss: 0.5897, step time: 0.2512\n",
      "191/281, train_loss: 0.4315, step time: 0.2514\n",
      "192/281, train_loss: 0.4328, step time: 0.2495\n",
      "193/281, train_loss: 0.5679, step time: 0.2523\n",
      "194/281, train_loss: 0.6609, step time: 0.2518\n",
      "195/281, train_loss: 0.4230, step time: 0.2541\n",
      "196/281, train_loss: 0.4932, step time: 0.2550\n",
      "197/281, train_loss: 0.5331, step time: 0.2551\n",
      "198/281, train_loss: 0.4518, step time: 0.2590\n",
      "199/281, train_loss: 0.6227, step time: 0.2519\n",
      "200/281, train_loss: 0.5892, step time: 0.2546\n",
      "201/281, train_loss: 0.4457, step time: 0.2509\n",
      "202/281, train_loss: 0.4303, step time: 0.2518\n",
      "203/281, train_loss: 0.4681, step time: 0.2496\n",
      "204/281, train_loss: 0.4568, step time: 0.2499\n",
      "205/281, train_loss: 0.4465, step time: 0.2527\n",
      "206/281, train_loss: 0.4843, step time: 0.2511\n",
      "207/281, train_loss: 0.6192, step time: 0.2593\n",
      "208/281, train_loss: 0.5665, step time: 0.2592\n",
      "209/281, train_loss: 0.5211, step time: 0.2589\n",
      "210/281, train_loss: 0.5929, step time: 0.2605\n",
      "211/281, train_loss: 0.4598, step time: 0.2596\n",
      "212/281, train_loss: 0.4560, step time: 0.2548\n",
      "213/281, train_loss: 0.4767, step time: 0.2574\n",
      "214/281, train_loss: 0.4561, step time: 0.2559\n",
      "215/281, train_loss: 0.5337, step time: 0.2568\n",
      "216/281, train_loss: 0.5093, step time: 0.2580\n",
      "217/281, train_loss: 0.4387, step time: 0.2537\n",
      "218/281, train_loss: 0.4365, step time: 0.2495\n",
      "219/281, train_loss: 0.4238, step time: 0.2509\n",
      "220/281, train_loss: 0.4377, step time: 0.2527\n",
      "221/281, train_loss: 0.4840, step time: 0.2550\n",
      "222/281, train_loss: 0.4339, step time: 0.2851\n",
      "223/281, train_loss: 0.4190, step time: 0.2476\n",
      "224/281, train_loss: 0.5569, step time: 0.2468\n",
      "225/281, train_loss: 0.5234, step time: 0.2535\n",
      "226/281, train_loss: 0.4388, step time: 0.2521\n",
      "227/281, train_loss: 0.5897, step time: 0.2555\n",
      "228/281, train_loss: 0.4349, step time: 0.2562\n",
      "229/281, train_loss: 0.4126, step time: 0.2582\n",
      "230/281, train_loss: 0.5377, step time: 0.2557\n",
      "231/281, train_loss: 0.6475, step time: 0.2552\n",
      "232/281, train_loss: 0.3978, step time: 0.2544\n",
      "233/281, train_loss: 0.6306, step time: 0.2547\n",
      "234/281, train_loss: 0.4919, step time: 0.2533\n",
      "235/281, train_loss: 0.5381, step time: 0.2497\n",
      "236/281, train_loss: 0.4646, step time: 0.2494\n",
      "237/281, train_loss: 0.4431, step time: 0.2568\n",
      "238/281, train_loss: 0.5606, step time: 0.2537\n",
      "239/281, train_loss: 0.4279, step time: 0.2582\n",
      "240/281, train_loss: 0.4627, step time: 0.2550\n",
      "241/281, train_loss: 0.5898, step time: 0.2624\n",
      "242/281, train_loss: 0.4423, step time: 0.2590\n",
      "243/281, train_loss: 0.4346, step time: 0.2528\n",
      "244/281, train_loss: 0.5074, step time: 0.2527\n",
      "245/281, train_loss: 0.4892, step time: 0.2597\n",
      "246/281, train_loss: 0.4002, step time: 0.2563\n",
      "247/281, train_loss: 0.4827, step time: 0.2583\n",
      "248/281, train_loss: 0.5769, step time: 0.2526\n",
      "249/281, train_loss: 0.4906, step time: 0.2564\n",
      "250/281, train_loss: 0.4288, step time: 0.2529\n",
      "251/281, train_loss: 0.6167, step time: 0.2499\n",
      "252/281, train_loss: 0.5475, step time: 0.2533\n",
      "253/281, train_loss: 0.4708, step time: 0.2539\n",
      "254/281, train_loss: 0.5473, step time: 0.2495\n",
      "255/281, train_loss: 0.4414, step time: 0.2521\n",
      "256/281, train_loss: 0.4327, step time: 0.2546\n",
      "257/281, train_loss: 0.4497, step time: 0.2541\n",
      "258/281, train_loss: 0.4250, step time: 0.2511\n",
      "259/281, train_loss: 0.4554, step time: 0.2513\n",
      "260/281, train_loss: 0.4454, step time: 0.2571\n",
      "261/281, train_loss: 0.4368, step time: 0.2553\n",
      "262/281, train_loss: 0.5423, step time: 0.2566\n",
      "263/281, train_loss: 0.5781, step time: 0.2514\n",
      "264/281, train_loss: 0.4473, step time: 0.2581\n",
      "265/281, train_loss: 0.6078, step time: 0.2568\n",
      "266/281, train_loss: 0.4484, step time: 0.2529\n",
      "267/281, train_loss: 0.5681, step time: 0.2621\n",
      "268/281, train_loss: 0.4482, step time: 0.2577\n",
      "269/281, train_loss: 0.4522, step time: 0.2578\n",
      "270/281, train_loss: 0.4373, step time: 0.2588\n",
      "271/281, train_loss: 0.5406, step time: 0.2549\n",
      "272/281, train_loss: 0.4529, step time: 0.2512\n",
      "273/281, train_loss: 0.5671, step time: 0.2535\n",
      "274/281, train_loss: 0.4021, step time: 0.2533\n",
      "275/281, train_loss: 0.4480, step time: 0.2594\n",
      "276/281, train_loss: 0.5632, step time: 0.2553\n",
      "277/281, train_loss: 0.5898, step time: 0.2556\n",
      "278/281, train_loss: 0.7114, step time: 0.2556\n",
      "279/281, train_loss: 0.4197, step time: 0.2533\n",
      "280/281, train_loss: 0.4163, step time: 0.2491\n",
      "281/281, train_loss: 0.4895, step time: 0.2487\n",
      "282/281, train_loss: 0.4322, step time: 0.1492\n",
      "epoch 12 average loss: 0.4942\n",
      "current epoch: 12 current mean dice: 0.5411 tc: 0.0090 wt: 0.8948 et: 0.7449\n",
      "best mean dice: 0.5491 at epoch: 11\n",
      "time consuming of epoch 12 is: 355.3488\n",
      "----------\n",
      "epoch 13/200\n",
      "1/281, train_loss: 0.5881, step time: 0.2572\n",
      "2/281, train_loss: 0.5874, step time: 0.2483\n",
      "3/281, train_loss: 0.5024, step time: 0.2547\n",
      "4/281, train_loss: 0.5736, step time: 0.2516\n",
      "5/281, train_loss: 0.5723, step time: 0.2536\n",
      "6/281, train_loss: 0.4714, step time: 0.2543\n",
      "7/281, train_loss: 0.4814, step time: 0.2526\n",
      "8/281, train_loss: 0.5509, step time: 0.2544\n",
      "9/281, train_loss: 0.4344, step time: 0.2550\n",
      "10/281, train_loss: 0.4475, step time: 0.2528\n",
      "11/281, train_loss: 0.4217, step time: 0.2547\n",
      "12/281, train_loss: 0.4779, step time: 0.2581\n",
      "13/281, train_loss: 0.5115, step time: 0.2556\n",
      "14/281, train_loss: 0.4353, step time: 0.2502\n",
      "15/281, train_loss: 0.5826, step time: 0.2434\n",
      "16/281, train_loss: 0.6024, step time: 0.2483\n",
      "17/281, train_loss: 0.5951, step time: 0.2568\n",
      "18/281, train_loss: 0.5181, step time: 0.2503\n",
      "19/281, train_loss: 0.5084, step time: 0.2546\n",
      "20/281, train_loss: 0.4899, step time: 0.2610\n",
      "21/281, train_loss: 0.4317, step time: 0.2640\n",
      "22/281, train_loss: 0.5170, step time: 0.2570\n",
      "23/281, train_loss: 0.4323, step time: 0.2542\n",
      "24/281, train_loss: 0.4639, step time: 0.2539\n",
      "25/281, train_loss: 0.4667, step time: 0.2545\n",
      "26/281, train_loss: 0.4553, step time: 0.2610\n",
      "27/281, train_loss: 0.3979, step time: 0.2578\n",
      "28/281, train_loss: 0.4289, step time: 0.2541\n",
      "29/281, train_loss: 0.4127, step time: 0.2584\n",
      "30/281, train_loss: 0.4169, step time: 0.2620\n",
      "31/281, train_loss: 0.5540, step time: 0.2588\n",
      "32/281, train_loss: 0.4828, step time: 0.2537\n",
      "33/281, train_loss: 0.4564, step time: 0.2441\n",
      "34/281, train_loss: 0.4124, step time: 0.2534\n",
      "35/281, train_loss: 0.4496, step time: 0.2590\n",
      "36/281, train_loss: 0.4095, step time: 0.2555\n",
      "37/281, train_loss: 0.4982, step time: 0.2522\n",
      "38/281, train_loss: 0.5432, step time: 0.2543\n",
      "39/281, train_loss: 0.4883, step time: 0.2567\n",
      "40/281, train_loss: 0.4659, step time: 0.2516\n",
      "41/281, train_loss: 0.5886, step time: 0.2488\n",
      "42/281, train_loss: 0.4570, step time: 0.2544\n",
      "43/281, train_loss: 0.3965, step time: 0.2525\n",
      "44/281, train_loss: 0.5315, step time: 0.2523\n",
      "45/281, train_loss: 0.5354, step time: 0.2543\n",
      "46/281, train_loss: 0.6027, step time: 0.2587\n",
      "47/281, train_loss: 0.4301, step time: 0.2612\n",
      "48/281, train_loss: 0.4917, step time: 0.2561\n",
      "49/281, train_loss: 0.4375, step time: 0.2594\n",
      "50/281, train_loss: 0.4795, step time: 0.2490\n",
      "51/281, train_loss: 0.6188, step time: 0.2532\n",
      "52/281, train_loss: 0.4360, step time: 0.2517\n",
      "53/281, train_loss: 0.4119, step time: 0.2517\n",
      "54/281, train_loss: 0.5284, step time: 0.2527\n",
      "55/281, train_loss: 0.4453, step time: 0.2541\n",
      "56/281, train_loss: 0.4152, step time: 0.2539\n",
      "57/281, train_loss: 0.4598, step time: 0.2627\n",
      "58/281, train_loss: 0.4725, step time: 0.2587\n",
      "59/281, train_loss: 0.4330, step time: 0.2570\n",
      "60/281, train_loss: 0.6361, step time: 0.2612\n",
      "61/281, train_loss: 0.4697, step time: 0.2577\n",
      "62/281, train_loss: 0.4750, step time: 0.2541\n",
      "63/281, train_loss: 0.5685, step time: 0.2552\n",
      "64/281, train_loss: 0.5568, step time: 0.2583\n",
      "65/281, train_loss: 0.6196, step time: 0.2618\n",
      "66/281, train_loss: 0.6087, step time: 0.2538\n",
      "67/281, train_loss: 0.4014, step time: 0.2542\n",
      "68/281, train_loss: 0.4213, step time: 0.2563\n",
      "69/281, train_loss: 0.4794, step time: 0.2594\n",
      "70/281, train_loss: 0.4288, step time: 0.2565\n",
      "71/281, train_loss: 0.4788, step time: 0.2590\n",
      "72/281, train_loss: 0.4412, step time: 0.2511\n",
      "73/281, train_loss: 0.4582, step time: 0.2618\n",
      "74/281, train_loss: 0.5912, step time: 0.2553\n",
      "75/281, train_loss: 0.4203, step time: 0.2546\n",
      "76/281, train_loss: 0.3968, step time: 0.2473\n",
      "77/281, train_loss: 0.4106, step time: 0.2446\n",
      "78/281, train_loss: 0.4241, step time: 0.2565\n",
      "79/281, train_loss: 0.5586, step time: 0.2563\n",
      "80/281, train_loss: 0.5790, step time: 0.2539\n",
      "81/281, train_loss: 0.5894, step time: 0.2515\n",
      "82/281, train_loss: 0.4124, step time: 0.2559\n",
      "83/281, train_loss: 0.5861, step time: 0.2545\n",
      "84/281, train_loss: 0.4415, step time: 0.2484\n",
      "85/281, train_loss: 0.4321, step time: 0.2487\n",
      "86/281, train_loss: 0.4905, step time: 0.2627\n",
      "87/281, train_loss: 0.4838, step time: 0.2478\n",
      "88/281, train_loss: 0.5354, step time: 0.2540\n",
      "89/281, train_loss: 0.4209, step time: 0.2522\n",
      "90/281, train_loss: 0.4267, step time: 0.2526\n",
      "91/281, train_loss: 0.4736, step time: 0.2527\n",
      "92/281, train_loss: 0.6018, step time: 0.2533\n",
      "93/281, train_loss: 0.4835, step time: 0.2483\n",
      "94/281, train_loss: 0.4356, step time: 0.2502\n",
      "95/281, train_loss: 0.6030, step time: 0.2546\n",
      "96/281, train_loss: 0.4185, step time: 0.2535\n",
      "97/281, train_loss: 0.5104, step time: 0.2537\n",
      "98/281, train_loss: 0.4428, step time: 0.2562\n",
      "99/281, train_loss: 0.4463, step time: 0.2512\n",
      "100/281, train_loss: 0.5312, step time: 0.2510\n",
      "101/281, train_loss: 0.4877, step time: 0.2575\n",
      "102/281, train_loss: 0.4384, step time: 0.2569\n",
      "103/281, train_loss: 0.4591, step time: 0.2561\n",
      "104/281, train_loss: 0.5001, step time: 0.2508\n",
      "105/281, train_loss: 0.5927, step time: 0.2532\n",
      "106/281, train_loss: 0.5954, step time: 0.2523\n",
      "107/281, train_loss: 0.3944, step time: 0.2517\n",
      "108/281, train_loss: 0.4798, step time: 0.2536\n",
      "109/281, train_loss: 0.4802, step time: 0.2441\n",
      "110/281, train_loss: 0.4563, step time: 0.2454\n",
      "111/281, train_loss: 0.4164, step time: 0.2467\n",
      "112/281, train_loss: 0.5619, step time: 0.2478\n",
      "113/281, train_loss: 0.5771, step time: 0.2441\n",
      "114/281, train_loss: 0.4669, step time: 0.2446\n",
      "115/281, train_loss: 0.4349, step time: 0.2450\n",
      "116/281, train_loss: 0.4196, step time: 0.2507\n",
      "117/281, train_loss: 0.4001, step time: 0.2550\n",
      "118/281, train_loss: 0.4503, step time: 0.2541\n",
      "119/281, train_loss: 0.4107, step time: 0.2521\n",
      "120/281, train_loss: 0.6276, step time: 0.2609\n",
      "121/281, train_loss: 0.4027, step time: 0.2986\n",
      "122/281, train_loss: 0.5193, step time: 0.2525\n",
      "123/281, train_loss: 0.4164, step time: 0.2497\n",
      "124/281, train_loss: 0.4102, step time: 0.2526\n",
      "125/281, train_loss: 0.5661, step time: 0.2560\n",
      "126/281, train_loss: 0.4214, step time: 0.2577\n",
      "127/281, train_loss: 0.4914, step time: 0.2535\n",
      "128/281, train_loss: 0.5059, step time: 0.2528\n",
      "129/281, train_loss: 0.4403, step time: 0.2543\n",
      "130/281, train_loss: 0.5773, step time: 0.2542\n",
      "131/281, train_loss: 0.5851, step time: 0.2507\n",
      "132/281, train_loss: 0.4641, step time: 0.2510\n",
      "133/281, train_loss: 0.4512, step time: 0.2490\n",
      "134/281, train_loss: 0.4470, step time: 0.2549\n",
      "135/281, train_loss: 0.4314, step time: 0.2574\n",
      "136/281, train_loss: 0.4573, step time: 0.2551\n",
      "137/281, train_loss: 0.3824, step time: 0.2565\n",
      "138/281, train_loss: 0.4188, step time: 0.2581\n",
      "139/281, train_loss: 0.4355, step time: 0.2537\n",
      "140/281, train_loss: 0.5160, step time: 0.2535\n",
      "141/281, train_loss: 0.4173, step time: 0.2526\n",
      "142/281, train_loss: 0.5663, step time: 0.2502\n",
      "143/281, train_loss: 0.4040, step time: 0.2570\n",
      "144/281, train_loss: 0.4947, step time: 0.2512\n",
      "145/281, train_loss: 0.4750, step time: 0.2532\n",
      "146/281, train_loss: 0.4050, step time: 0.2495\n",
      "147/281, train_loss: 0.6510, step time: 0.2559\n",
      "148/281, train_loss: 0.4823, step time: 0.2515\n",
      "149/281, train_loss: 0.4507, step time: 0.2528\n",
      "150/281, train_loss: 0.4525, step time: 0.2469\n",
      "151/281, train_loss: 0.5196, step time: 0.2501\n",
      "152/281, train_loss: 0.4221, step time: 0.2540\n",
      "153/281, train_loss: 0.4126, step time: 0.2539\n",
      "154/281, train_loss: 0.4464, step time: 0.2525\n",
      "155/281, train_loss: 0.4106, step time: 0.2570\n",
      "156/281, train_loss: 0.4905, step time: 0.2558\n",
      "157/281, train_loss: 0.4449, step time: 0.2573\n",
      "158/281, train_loss: 0.5648, step time: 0.2543\n",
      "159/281, train_loss: 0.4409, step time: 0.2525\n",
      "160/281, train_loss: 0.4356, step time: 0.2535\n",
      "161/281, train_loss: 0.4300, step time: 0.2501\n",
      "162/281, train_loss: 0.5570, step time: 0.2526\n",
      "163/281, train_loss: 0.5371, step time: 0.2506\n",
      "164/281, train_loss: 0.4206, step time: 0.2522\n",
      "165/281, train_loss: 0.3945, step time: 0.2536\n",
      "166/281, train_loss: 0.3974, step time: 0.2474\n",
      "167/281, train_loss: 0.4128, step time: 0.2515\n",
      "168/281, train_loss: 0.4943, step time: 0.2513\n",
      "169/281, train_loss: 0.4185, step time: 0.2550\n",
      "170/281, train_loss: 0.4679, step time: 0.2575\n",
      "171/281, train_loss: 0.7236, step time: 0.2572\n",
      "172/281, train_loss: 0.6106, step time: 0.2562\n",
      "173/281, train_loss: 0.4400, step time: 0.2501\n",
      "174/281, train_loss: 0.4535, step time: 0.2471\n",
      "175/281, train_loss: 0.5274, step time: 0.2440\n",
      "176/281, train_loss: 0.4506, step time: 0.2452\n",
      "177/281, train_loss: 0.4395, step time: 0.2551\n",
      "178/281, train_loss: 0.4263, step time: 0.2545\n",
      "179/281, train_loss: 0.4752, step time: 0.2685\n",
      "180/281, train_loss: 0.4539, step time: 0.2531\n",
      "181/281, train_loss: 0.5779, step time: 0.2618\n",
      "182/281, train_loss: 0.4353, step time: 0.2528\n",
      "183/281, train_loss: 0.5625, step time: 0.2534\n",
      "184/281, train_loss: 0.4400, step time: 0.2537\n",
      "185/281, train_loss: 0.4495, step time: 0.2527\n",
      "186/281, train_loss: 0.6146, step time: 0.2443\n",
      "187/281, train_loss: 0.5596, step time: 0.2475\n",
      "188/281, train_loss: 0.4137, step time: 0.2448\n",
      "189/281, train_loss: 0.5008, step time: 0.2426\n",
      "190/281, train_loss: 0.5901, step time: 0.2433\n",
      "191/281, train_loss: 0.5075, step time: 0.2463\n",
      "192/281, train_loss: 0.4588, step time: 0.2555\n",
      "193/281, train_loss: 0.4006, step time: 0.2524\n",
      "194/281, train_loss: 0.4305, step time: 0.2537\n",
      "195/281, train_loss: 0.5671, step time: 0.2471\n",
      "196/281, train_loss: 0.4314, step time: 0.2547\n",
      "197/281, train_loss: 0.5289, step time: 0.2519\n",
      "198/281, train_loss: 0.4970, step time: 0.2485\n",
      "199/281, train_loss: 0.4133, step time: 0.2526\n",
      "200/281, train_loss: 0.4269, step time: 0.2522\n",
      "201/281, train_loss: 0.4212, step time: 0.2547\n",
      "202/281, train_loss: 0.4977, step time: 0.2518\n",
      "203/281, train_loss: 0.6289, step time: 0.2459\n",
      "204/281, train_loss: 0.4427, step time: 0.2557\n",
      "205/281, train_loss: 0.4233, step time: 0.2428\n",
      "206/281, train_loss: 0.5845, step time: 0.2453\n",
      "207/281, train_loss: 0.5962, step time: 0.2505\n",
      "208/281, train_loss: 0.4265, step time: 0.2544\n",
      "209/281, train_loss: 0.4975, step time: 0.2493\n",
      "210/281, train_loss: 0.5652, step time: 0.2484\n",
      "211/281, train_loss: 0.5344, step time: 0.2531\n",
      "212/281, train_loss: 0.4257, step time: 0.2696\n",
      "213/281, train_loss: 0.5528, step time: 0.2517\n",
      "214/281, train_loss: 0.4863, step time: 0.2503\n",
      "215/281, train_loss: 0.4325, step time: 0.2489\n",
      "216/281, train_loss: 0.4285, step time: 0.2465\n",
      "217/281, train_loss: 0.6120, step time: 0.2507\n",
      "218/281, train_loss: 0.5544, step time: 0.2487\n",
      "219/281, train_loss: 0.4169, step time: 0.2441\n",
      "220/281, train_loss: 0.5643, step time: 0.2509\n",
      "221/281, train_loss: 0.4195, step time: 0.2540\n",
      "222/281, train_loss: 0.4484, step time: 0.2515\n",
      "223/281, train_loss: 0.4180, step time: 0.2476\n",
      "224/281, train_loss: 0.4798, step time: 0.2475\n",
      "225/281, train_loss: 0.4641, step time: 0.2507\n",
      "226/281, train_loss: 0.5663, step time: 0.2485\n",
      "227/281, train_loss: 0.5513, step time: 0.2441\n",
      "228/281, train_loss: 0.4265, step time: 0.2499\n",
      "229/281, train_loss: 0.5316, step time: 0.2514\n",
      "230/281, train_loss: 0.4274, step time: 0.2510\n",
      "231/281, train_loss: 0.6164, step time: 0.2542\n",
      "232/281, train_loss: 0.4572, step time: 0.2564\n",
      "233/281, train_loss: 0.5233, step time: 0.2585\n",
      "234/281, train_loss: 0.4045, step time: 0.2541\n",
      "235/281, train_loss: 0.5686, step time: 0.2474\n",
      "236/281, train_loss: 0.4179, step time: 0.2489\n",
      "237/281, train_loss: 0.6482, step time: 0.2562\n",
      "238/281, train_loss: 0.4409, step time: 0.2549\n",
      "239/281, train_loss: 0.4706, step time: 0.2484\n",
      "240/281, train_loss: 0.4835, step time: 0.2546\n",
      "241/281, train_loss: 0.4575, step time: 0.2600\n",
      "242/281, train_loss: 0.5820, step time: 0.2572\n",
      "243/281, train_loss: 0.4237, step time: 0.2509\n",
      "244/281, train_loss: 0.5171, step time: 0.2552\n",
      "245/281, train_loss: 0.4738, step time: 0.2480\n",
      "246/281, train_loss: 0.4115, step time: 0.2491\n",
      "247/281, train_loss: 0.4706, step time: 0.2550\n",
      "248/281, train_loss: 0.7024, step time: 0.2586\n",
      "249/281, train_loss: 0.6148, step time: 0.2524\n",
      "250/281, train_loss: 0.4738, step time: 0.2545\n",
      "251/281, train_loss: 0.6634, step time: 0.2488\n",
      "252/281, train_loss: 0.4427, step time: 0.2529\n",
      "253/281, train_loss: 0.4001, step time: 0.2560\n",
      "254/281, train_loss: 0.4396, step time: 0.2550\n",
      "255/281, train_loss: 0.4190, step time: 0.2468\n",
      "256/281, train_loss: 0.4344, step time: 0.2527\n",
      "257/281, train_loss: 0.4123, step time: 0.2536\n",
      "258/281, train_loss: 0.5222, step time: 0.2538\n",
      "259/281, train_loss: 0.5120, step time: 0.2468\n",
      "260/281, train_loss: 0.5129, step time: 0.2558\n",
      "261/281, train_loss: 0.5523, step time: 0.2515\n",
      "262/281, train_loss: 0.4915, step time: 0.2535\n",
      "263/281, train_loss: 0.5526, step time: 0.2493\n",
      "264/281, train_loss: 0.4570, step time: 0.2472\n",
      "265/281, train_loss: 0.5603, step time: 0.2575\n",
      "266/281, train_loss: 0.4244, step time: 0.2566\n",
      "267/281, train_loss: 0.4782, step time: 0.2549\n",
      "268/281, train_loss: 0.4522, step time: 0.2533\n",
      "269/281, train_loss: 0.4834, step time: 0.2532\n",
      "270/281, train_loss: 0.4053, step time: 0.2531\n",
      "271/281, train_loss: 0.5163, step time: 0.2499\n",
      "272/281, train_loss: 0.4241, step time: 0.2536\n",
      "273/281, train_loss: 0.6102, step time: 0.2570\n",
      "274/281, train_loss: 0.4951, step time: 0.2564\n",
      "275/281, train_loss: 0.4666, step time: 0.2545\n",
      "276/281, train_loss: 0.4721, step time: 0.2506\n",
      "277/281, train_loss: 0.4603, step time: 0.2523\n",
      "278/281, train_loss: 0.4190, step time: 0.2515\n",
      "279/281, train_loss: 0.4026, step time: 0.2514\n",
      "280/281, train_loss: 0.4320, step time: 0.2550\n",
      "281/281, train_loss: 0.4087, step time: 0.2488\n",
      "282/281, train_loss: 0.4554, step time: 0.1499\n",
      "epoch 13 average loss: 0.4850\n",
      "saved new best metric model\n",
      "current epoch: 13 current mean dice: 0.5555 tc: 0.0090 wt: 0.8975 et: 0.7876\n",
      "best mean dice: 0.5555 at epoch: 13\n",
      "time consuming of epoch 13 is: 331.2193\n",
      "----------\n",
      "epoch 14/200\n",
      "1/281, train_loss: 0.4012, step time: 0.2607\n",
      "2/281, train_loss: 0.4240, step time: 0.2518\n",
      "3/281, train_loss: 0.4105, step time: 0.2555\n",
      "4/281, train_loss: 0.4277, step time: 0.2578\n",
      "5/281, train_loss: 0.5684, step time: 0.2724\n",
      "6/281, train_loss: 0.4103, step time: 0.2529\n",
      "7/281, train_loss: 0.4104, step time: 0.2554\n",
      "8/281, train_loss: 0.4390, step time: 0.2547\n",
      "9/281, train_loss: 0.5506, step time: 0.2503\n",
      "10/281, train_loss: 0.4435, step time: 0.2566\n",
      "11/281, train_loss: 0.4633, step time: 0.2563\n",
      "12/281, train_loss: 0.4958, step time: 0.2560\n",
      "13/281, train_loss: 0.7135, step time: 0.2526\n",
      "14/281, train_loss: 0.4260, step time: 0.2608\n",
      "15/281, train_loss: 0.4257, step time: 0.2629\n",
      "16/281, train_loss: 0.4295, step time: 0.2580\n",
      "17/281, train_loss: 0.4226, step time: 0.2564\n",
      "18/281, train_loss: 0.4906, step time: 0.2547\n",
      "19/281, train_loss: 0.5043, step time: 0.2609\n",
      "20/281, train_loss: 0.4270, step time: 0.2686\n",
      "21/281, train_loss: 0.5162, step time: 0.2564\n",
      "22/281, train_loss: 0.4649, step time: 0.2569\n",
      "23/281, train_loss: 0.4026, step time: 0.2532\n",
      "24/281, train_loss: 0.4757, step time: 0.2617\n",
      "25/281, train_loss: 0.6069, step time: 0.2609\n",
      "26/281, train_loss: 0.4069, step time: 0.2638\n",
      "27/281, train_loss: 0.3905, step time: 0.2549\n",
      "28/281, train_loss: 0.3992, step time: 0.2511\n",
      "29/281, train_loss: 0.5191, step time: 0.2650\n",
      "30/281, train_loss: 0.4493, step time: 0.2691\n",
      "31/281, train_loss: 0.4031, step time: 0.2594\n",
      "32/281, train_loss: 0.4538, step time: 0.2537\n",
      "33/281, train_loss: 0.4387, step time: 0.2563\n",
      "34/281, train_loss: 0.4976, step time: 0.2582\n",
      "35/281, train_loss: 0.5375, step time: 0.2529\n",
      "36/281, train_loss: 0.5187, step time: 0.2525\n",
      "37/281, train_loss: 0.4564, step time: 0.2544\n",
      "38/281, train_loss: 0.4124, step time: 0.2578\n",
      "39/281, train_loss: 0.4549, step time: 0.2638\n",
      "40/281, train_loss: 0.5835, step time: 0.2581\n",
      "41/281, train_loss: 0.5001, step time: 0.2588\n",
      "42/281, train_loss: 0.5531, step time: 0.2595\n",
      "43/281, train_loss: 0.4546, step time: 0.2579\n",
      "44/281, train_loss: 0.5843, step time: 0.2541\n",
      "45/281, train_loss: 0.5298, step time: 0.2603\n",
      "46/281, train_loss: 0.4468, step time: 0.2548\n",
      "47/281, train_loss: 0.4390, step time: 0.2515\n",
      "48/281, train_loss: 0.5049, step time: 0.2554\n",
      "49/281, train_loss: 0.4283, step time: 0.2545\n",
      "50/281, train_loss: 0.4943, step time: 0.2578\n",
      "51/281, train_loss: 0.4032, step time: 0.2620\n",
      "52/281, train_loss: 0.4474, step time: 0.2536\n",
      "53/281, train_loss: 0.3990, step time: 0.2587\n",
      "54/281, train_loss: 0.4269, step time: 0.2599\n",
      "55/281, train_loss: 0.4172, step time: 0.2575\n",
      "56/281, train_loss: 0.4206, step time: 0.2559\n",
      "57/281, train_loss: 0.7053, step time: 0.2513\n",
      "58/281, train_loss: 0.3978, step time: 0.2529\n",
      "59/281, train_loss: 0.4514, step time: 0.2494\n",
      "60/281, train_loss: 0.4225, step time: 0.2606\n",
      "61/281, train_loss: 0.4292, step time: 0.2575\n",
      "62/281, train_loss: 0.4370, step time: 0.2614\n",
      "63/281, train_loss: 0.4383, step time: 0.2593\n",
      "64/281, train_loss: 0.4103, step time: 0.2550\n",
      "65/281, train_loss: 0.5662, step time: 0.2520\n",
      "66/281, train_loss: 0.5714, step time: 0.2522\n",
      "67/281, train_loss: 0.4701, step time: 0.2547\n",
      "68/281, train_loss: 0.4142, step time: 0.2555\n",
      "69/281, train_loss: 0.4673, step time: 0.2526\n",
      "70/281, train_loss: 0.4065, step time: 0.2669\n",
      "71/281, train_loss: 0.4731, step time: 0.2508\n",
      "72/281, train_loss: 0.4108, step time: 0.2560\n",
      "73/281, train_loss: 0.4170, step time: 0.2548\n",
      "74/281, train_loss: 0.6682, step time: 0.2550\n",
      "75/281, train_loss: 0.4180, step time: 0.2513\n",
      "76/281, train_loss: 0.4856, step time: 0.2608\n",
      "77/281, train_loss: 0.5450, step time: 0.2520\n",
      "78/281, train_loss: 0.5793, step time: 0.2551\n",
      "79/281, train_loss: 0.4670, step time: 0.2595\n",
      "80/281, train_loss: 0.4929, step time: 0.2514\n",
      "81/281, train_loss: 0.4804, step time: 0.2520\n",
      "82/281, train_loss: 0.5598, step time: 0.2530\n",
      "83/281, train_loss: 0.4186, step time: 0.2523\n",
      "84/281, train_loss: 0.4164, step time: 0.2586\n",
      "85/281, train_loss: 0.4338, step time: 0.2595\n",
      "86/281, train_loss: 0.4303, step time: 0.2555\n",
      "87/281, train_loss: 0.4892, step time: 0.2545\n",
      "88/281, train_loss: 0.4360, step time: 0.2541\n",
      "89/281, train_loss: 0.4114, step time: 0.2605\n",
      "90/281, train_loss: 0.4140, step time: 0.2596\n",
      "91/281, train_loss: 0.6640, step time: 0.2605\n",
      "92/281, train_loss: 0.5635, step time: 0.2517\n",
      "93/281, train_loss: 0.5769, step time: 0.2559\n",
      "94/281, train_loss: 0.4144, step time: 0.2519\n",
      "95/281, train_loss: 0.4340, step time: 0.2513\n",
      "96/281, train_loss: 0.6010, step time: 0.2580\n",
      "97/281, train_loss: 0.4781, step time: 0.2572\n",
      "98/281, train_loss: 0.4308, step time: 0.2578\n",
      "99/281, train_loss: 0.4353, step time: 0.2553\n",
      "100/281, train_loss: 0.5916, step time: 0.2566\n",
      "101/281, train_loss: 0.4973, step time: 0.2565\n",
      "102/281, train_loss: 0.4200, step time: 0.2597\n",
      "103/281, train_loss: 0.4334, step time: 0.2598\n",
      "104/281, train_loss: 0.5754, step time: 0.2552\n",
      "105/281, train_loss: 0.4250, step time: 0.2590\n",
      "106/281, train_loss: 0.4040, step time: 0.2578\n",
      "107/281, train_loss: 0.4158, step time: 0.2595\n",
      "108/281, train_loss: 0.4530, step time: 0.2577\n",
      "109/281, train_loss: 0.4227, step time: 0.2687\n",
      "110/281, train_loss: 0.5412, step time: 0.2594\n",
      "111/281, train_loss: 0.5526, step time: 0.2561\n",
      "112/281, train_loss: 0.4143, step time: 0.2512\n",
      "113/281, train_loss: 0.4917, step time: 0.2588\n",
      "114/281, train_loss: 0.5415, step time: 0.2519\n",
      "115/281, train_loss: 0.5359, step time: 0.2625\n",
      "116/281, train_loss: 0.4317, step time: 0.2619\n",
      "117/281, train_loss: 0.4744, step time: 0.2828\n",
      "118/281, train_loss: 0.5452, step time: 0.2561\n",
      "119/281, train_loss: 0.4098, step time: 0.2551\n",
      "120/281, train_loss: 0.4157, step time: 0.2543\n",
      "121/281, train_loss: 0.3943, step time: 0.2523\n",
      "122/281, train_loss: 0.3994, step time: 0.2604\n",
      "123/281, train_loss: 0.4548, step time: 0.2604\n",
      "124/281, train_loss: 0.4101, step time: 0.2596\n",
      "125/281, train_loss: 0.4235, step time: 0.2564\n",
      "126/281, train_loss: 0.4316, step time: 0.2547\n",
      "127/281, train_loss: 0.5331, step time: 0.2576\n",
      "128/281, train_loss: 0.4233, step time: 0.2587\n",
      "129/281, train_loss: 0.5631, step time: 0.2624\n",
      "130/281, train_loss: 0.4725, step time: 0.2507\n",
      "131/281, train_loss: 0.4209, step time: 0.2515\n",
      "132/281, train_loss: 0.4465, step time: 0.2524\n",
      "133/281, train_loss: 0.5117, step time: 0.2568\n",
      "134/281, train_loss: 0.4354, step time: 0.2545\n",
      "135/281, train_loss: 0.5609, step time: 0.2516\n",
      "136/281, train_loss: 0.5732, step time: 0.2517\n",
      "137/281, train_loss: 0.4128, step time: 0.2579\n",
      "138/281, train_loss: 0.4717, step time: 0.2559\n",
      "139/281, train_loss: 0.4211, step time: 0.2586\n",
      "140/281, train_loss: 0.4406, step time: 0.2492\n",
      "141/281, train_loss: 0.4138, step time: 0.2517\n",
      "142/281, train_loss: 0.4183, step time: 0.2570\n",
      "143/281, train_loss: 0.4125, step time: 0.2528\n",
      "144/281, train_loss: 0.4692, step time: 0.2557\n",
      "145/281, train_loss: 0.4844, step time: 0.2528\n",
      "146/281, train_loss: 0.4107, step time: 0.2526\n",
      "147/281, train_loss: 0.4163, step time: 0.2557\n",
      "148/281, train_loss: 0.4252, step time: 0.2524\n",
      "149/281, train_loss: 0.4282, step time: 0.2586\n",
      "150/281, train_loss: 0.3933, step time: 0.2625\n",
      "151/281, train_loss: 0.4519, step time: 0.2598\n",
      "152/281, train_loss: 0.4864, step time: 0.2659\n",
      "153/281, train_loss: 0.5905, step time: 0.2573\n",
      "154/281, train_loss: 0.4376, step time: 0.2563\n",
      "155/281, train_loss: 0.4986, step time: 0.2587\n",
      "156/281, train_loss: 0.5815, step time: 0.2634\n",
      "157/281, train_loss: 0.4572, step time: 0.2599\n",
      "158/281, train_loss: 0.5482, step time: 0.2542\n",
      "159/281, train_loss: 0.4576, step time: 0.2571\n",
      "160/281, train_loss: 0.5793, step time: 0.2580\n",
      "161/281, train_loss: 0.4787, step time: 0.2535\n",
      "162/281, train_loss: 0.4404, step time: 0.2595\n",
      "163/281, train_loss: 0.4233, step time: 0.2638\n",
      "164/281, train_loss: 0.4963, step time: 0.2579\n",
      "165/281, train_loss: 0.4416, step time: 0.2583\n",
      "166/281, train_loss: 0.4310, step time: 0.2574\n",
      "167/281, train_loss: 0.4151, step time: 0.2576\n",
      "168/281, train_loss: 0.4221, step time: 0.2575\n",
      "169/281, train_loss: 0.4366, step time: 0.2524\n",
      "170/281, train_loss: 0.5472, step time: 0.2553\n",
      "171/281, train_loss: 0.4666, step time: 0.2575\n",
      "172/281, train_loss: 0.4143, step time: 0.2586\n",
      "173/281, train_loss: 0.3909, step time: 0.2525\n",
      "174/281, train_loss: 0.4502, step time: 0.2560\n",
      "175/281, train_loss: 0.5171, step time: 0.2586\n",
      "176/281, train_loss: 0.4667, step time: 0.2598\n",
      "177/281, train_loss: 0.3933, step time: 0.2564\n",
      "178/281, train_loss: 0.5339, step time: 0.2584\n",
      "179/281, train_loss: 0.4692, step time: 0.2570\n",
      "180/281, train_loss: 0.4661, step time: 0.2565\n",
      "181/281, train_loss: 0.5338, step time: 0.2595\n",
      "182/281, train_loss: 0.4202, step time: 0.2544\n",
      "183/281, train_loss: 0.4285, step time: 0.2558\n",
      "184/281, train_loss: 0.4242, step time: 0.2574\n",
      "185/281, train_loss: 0.4087, step time: 0.2593\n",
      "186/281, train_loss: 0.4553, step time: 0.2565\n",
      "187/281, train_loss: 0.4336, step time: 0.2549\n",
      "188/281, train_loss: 0.5783, step time: 0.2589\n",
      "189/281, train_loss: 0.4398, step time: 0.2563\n",
      "190/281, train_loss: 0.4292, step time: 0.2601\n",
      "191/281, train_loss: 0.5789, step time: 0.2557\n",
      "192/281, train_loss: 0.4524, step time: 0.2588\n",
      "193/281, train_loss: 0.4298, step time: 0.2522\n",
      "194/281, train_loss: 0.5332, step time: 0.2554\n",
      "195/281, train_loss: 0.5637, step time: 0.2585\n",
      "196/281, train_loss: 0.4742, step time: 0.2580\n",
      "197/281, train_loss: 0.4372, step time: 0.2526\n",
      "198/281, train_loss: 0.4704, step time: 0.2581\n",
      "199/281, train_loss: 0.4188, step time: 0.2603\n",
      "200/281, train_loss: 0.4130, step time: 0.2592\n",
      "201/281, train_loss: 0.5862, step time: 0.2645\n",
      "202/281, train_loss: 0.4510, step time: 0.2687\n",
      "203/281, train_loss: 0.4178, step time: 0.2573\n",
      "204/281, train_loss: 0.4048, step time: 0.2598\n",
      "205/281, train_loss: 0.4078, step time: 0.2572\n",
      "206/281, train_loss: 0.5415, step time: 0.2504\n",
      "207/281, train_loss: 0.5698, step time: 0.2572\n",
      "208/281, train_loss: 0.4326, step time: 0.2572\n",
      "209/281, train_loss: 0.5688, step time: 0.2515\n",
      "210/281, train_loss: 0.6641, step time: 0.2583\n",
      "211/281, train_loss: 0.5907, step time: 0.2605\n",
      "212/281, train_loss: 0.5052, step time: 0.2560\n",
      "213/281, train_loss: 0.4526, step time: 0.2555\n",
      "214/281, train_loss: 0.4479, step time: 0.2579\n",
      "215/281, train_loss: 0.4316, step time: 0.2574\n",
      "216/281, train_loss: 0.4320, step time: 0.2520\n",
      "217/281, train_loss: 0.4896, step time: 0.2570\n",
      "218/281, train_loss: 0.5433, step time: 0.2595\n",
      "219/281, train_loss: 0.4844, step time: 0.2646\n",
      "220/281, train_loss: 0.4375, step time: 0.2565\n",
      "221/281, train_loss: 0.4624, step time: 0.2568\n",
      "222/281, train_loss: 0.4834, step time: 0.2602\n",
      "223/281, train_loss: 0.5097, step time: 0.2558\n",
      "224/281, train_loss: 0.6323, step time: 0.2566\n",
      "225/281, train_loss: 0.4318, step time: 0.2570\n",
      "226/281, train_loss: 0.4249, step time: 0.2560\n",
      "227/281, train_loss: 0.4376, step time: 0.2515\n",
      "228/281, train_loss: 0.4204, step time: 0.2492\n",
      "229/281, train_loss: 0.4358, step time: 0.2541\n",
      "230/281, train_loss: 0.4390, step time: 0.2512\n",
      "231/281, train_loss: 0.5360, step time: 0.2512\n",
      "232/281, train_loss: 0.6981, step time: 0.2511\n",
      "233/281, train_loss: 0.4912, step time: 0.2489\n",
      "234/281, train_loss: 0.4267, step time: 0.2519\n",
      "235/281, train_loss: 0.4310, step time: 0.2526\n",
      "236/281, train_loss: 0.4767, step time: 0.2555\n",
      "237/281, train_loss: 0.4296, step time: 0.2553\n",
      "238/281, train_loss: 0.3957, step time: 0.2535\n",
      "239/281, train_loss: 0.4295, step time: 0.2476\n",
      "240/281, train_loss: 0.4241, step time: 0.2435\n",
      "241/281, train_loss: 0.5446, step time: 0.2506\n",
      "242/281, train_loss: 0.5518, step time: 0.2457\n",
      "243/281, train_loss: 0.4666, step time: 0.2517\n",
      "244/281, train_loss: 0.4777, step time: 0.2478\n",
      "245/281, train_loss: 0.6097, step time: 0.2485\n",
      "246/281, train_loss: 0.5486, step time: 0.2520\n",
      "247/281, train_loss: 0.4248, step time: 0.2481\n",
      "248/281, train_loss: 0.5561, step time: 0.2507\n",
      "249/281, train_loss: 0.4674, step time: 0.2477\n",
      "250/281, train_loss: 0.5635, step time: 0.2517\n",
      "251/281, train_loss: 0.4426, step time: 0.2499\n",
      "252/281, train_loss: 0.4359, step time: 0.2470\n",
      "253/281, train_loss: 0.4374, step time: 0.2486\n",
      "254/281, train_loss: 0.4380, step time: 0.2503\n",
      "255/281, train_loss: 0.4010, step time: 0.2521\n",
      "256/281, train_loss: 0.4835, step time: 0.2491\n",
      "257/281, train_loss: 0.4217, step time: 0.2457\n",
      "258/281, train_loss: 0.4992, step time: 0.2512\n",
      "259/281, train_loss: 0.5772, step time: 0.2597\n",
      "260/281, train_loss: 0.6089, step time: 0.2606\n",
      "261/281, train_loss: 0.4795, step time: 0.2580\n",
      "262/281, train_loss: 0.4714, step time: 0.2555\n",
      "263/281, train_loss: 0.4143, step time: 0.2613\n",
      "264/281, train_loss: 0.5641, step time: 0.2580\n",
      "265/281, train_loss: 0.5356, step time: 0.2570\n",
      "266/281, train_loss: 0.5262, step time: 0.2551\n",
      "267/281, train_loss: 0.4355, step time: 0.2522\n",
      "268/281, train_loss: 0.4587, step time: 0.2532\n",
      "269/281, train_loss: 0.5097, step time: 0.2533\n",
      "270/281, train_loss: 0.3932, step time: 0.2583\n",
      "271/281, train_loss: 0.5371, step time: 0.2802\n",
      "272/281, train_loss: 0.5395, step time: 0.2750\n",
      "273/281, train_loss: 0.4336, step time: 0.2535\n",
      "274/281, train_loss: 0.5669, step time: 0.2556\n",
      "275/281, train_loss: 0.4089, step time: 0.2534\n",
      "276/281, train_loss: 0.4189, step time: 0.2543\n",
      "277/281, train_loss: 0.4414, step time: 0.2570\n",
      "278/281, train_loss: 0.4139, step time: 0.2553\n",
      "279/281, train_loss: 0.4033, step time: 0.2548\n",
      "280/281, train_loss: 0.4358, step time: 0.2531\n",
      "281/281, train_loss: 0.5579, step time: 0.2511\n",
      "282/281, train_loss: 0.7131, step time: 0.1510\n",
      "epoch 14 average loss: 0.4738\n",
      "current epoch: 14 current mean dice: 0.5554 tc: 0.0090 wt: 0.9001 et: 0.7853\n",
      "best mean dice: 0.5555 at epoch: 13\n",
      "time consuming of epoch 14 is: 353.7683\n",
      "----------\n",
      "epoch 15/200\n",
      "1/281, train_loss: 0.4670, step time: 0.2651\n",
      "2/281, train_loss: 0.4879, step time: 0.2602\n",
      "3/281, train_loss: 0.4387, step time: 0.2602\n",
      "4/281, train_loss: 0.5893, step time: 0.2535\n",
      "5/281, train_loss: 0.3961, step time: 0.2554\n",
      "6/281, train_loss: 0.3893, step time: 0.2550\n",
      "7/281, train_loss: 0.5488, step time: 0.2540\n",
      "8/281, train_loss: 0.5593, step time: 0.2502\n",
      "9/281, train_loss: 0.5793, step time: 0.2512\n",
      "10/281, train_loss: 0.4423, step time: 0.2546\n",
      "11/281, train_loss: 0.4245, step time: 0.2537\n",
      "12/281, train_loss: 0.4183, step time: 0.2511\n",
      "13/281, train_loss: 0.4202, step time: 0.2509\n",
      "14/281, train_loss: 0.4101, step time: 0.2594\n",
      "15/281, train_loss: 0.3995, step time: 0.2575\n",
      "16/281, train_loss: 0.4172, step time: 0.2566\n",
      "17/281, train_loss: 0.6617, step time: 0.2557\n",
      "18/281, train_loss: 0.4280, step time: 0.2580\n",
      "19/281, train_loss: 0.4208, step time: 0.2581\n",
      "20/281, train_loss: 0.5659, step time: 0.2588\n",
      "21/281, train_loss: 0.5272, step time: 0.2508\n",
      "22/281, train_loss: 0.5577, step time: 0.2569\n",
      "23/281, train_loss: 0.4112, step time: 0.2560\n",
      "24/281, train_loss: 0.3974, step time: 0.2543\n",
      "25/281, train_loss: 0.5609, step time: 0.2724\n",
      "26/281, train_loss: 0.4389, step time: 0.2524\n",
      "27/281, train_loss: 0.4288, step time: 0.2547\n",
      "28/281, train_loss: 0.4188, step time: 0.2496\n",
      "29/281, train_loss: 0.4555, step time: 0.2593\n",
      "30/281, train_loss: 0.3961, step time: 0.2544\n",
      "31/281, train_loss: 0.4891, step time: 0.2537\n",
      "32/281, train_loss: 0.5432, step time: 0.2522\n",
      "33/281, train_loss: 0.3934, step time: 0.2548\n",
      "34/281, train_loss: 0.4231, step time: 0.2625\n",
      "35/281, train_loss: 0.5425, step time: 0.2559\n",
      "36/281, train_loss: 0.4274, step time: 0.2566\n",
      "37/281, train_loss: 0.5395, step time: 0.2585\n",
      "38/281, train_loss: 0.4343, step time: 0.2544\n",
      "39/281, train_loss: 0.4647, step time: 0.2543\n",
      "40/281, train_loss: 0.4646, step time: 0.2554\n",
      "41/281, train_loss: 0.4017, step time: 0.2526\n",
      "42/281, train_loss: 0.5262, step time: 0.2553\n",
      "43/281, train_loss: 0.4552, step time: 0.2622\n",
      "44/281, train_loss: 0.4482, step time: 0.2537\n",
      "45/281, train_loss: 0.4150, step time: 0.2624\n",
      "46/281, train_loss: 0.4089, step time: 0.2509\n",
      "47/281, train_loss: 0.4283, step time: 0.2523\n",
      "48/281, train_loss: 0.5909, step time: 0.2463\n",
      "49/281, train_loss: 0.4202, step time: 0.2547\n",
      "50/281, train_loss: 0.4043, step time: 0.2569\n",
      "51/281, train_loss: 0.4318, step time: 0.2565\n",
      "52/281, train_loss: 0.4362, step time: 0.2628\n",
      "53/281, train_loss: 0.4641, step time: 0.2601\n",
      "54/281, train_loss: 0.6436, step time: 0.2612\n",
      "55/281, train_loss: 0.4247, step time: 0.2563\n",
      "56/281, train_loss: 0.4382, step time: 0.2580\n",
      "57/281, train_loss: 0.4175, step time: 0.2549\n",
      "58/281, train_loss: 0.4362, step time: 0.2501\n",
      "59/281, train_loss: 0.3909, step time: 0.2527\n",
      "60/281, train_loss: 0.4349, step time: 0.2536\n",
      "61/281, train_loss: 0.4453, step time: 0.2550\n",
      "62/281, train_loss: 0.5330, step time: 0.2571\n",
      "63/281, train_loss: 0.4248, step time: 0.2549\n",
      "64/281, train_loss: 0.5678, step time: 0.2596\n",
      "65/281, train_loss: 0.4140, step time: 0.2531\n",
      "66/281, train_loss: 0.4461, step time: 0.2558\n",
      "67/281, train_loss: 0.4714, step time: 0.2611\n",
      "68/281, train_loss: 0.4722, step time: 0.2601\n",
      "69/281, train_loss: 0.4211, step time: 0.2571\n",
      "70/281, train_loss: 0.3984, step time: 0.2548\n",
      "71/281, train_loss: 0.4584, step time: 0.2576\n",
      "72/281, train_loss: 0.4207, step time: 0.2563\n",
      "73/281, train_loss: 0.5456, step time: 0.2551\n",
      "74/281, train_loss: 0.4056, step time: 0.2546\n",
      "75/281, train_loss: 0.5506, step time: 0.2608\n",
      "76/281, train_loss: 0.4171, step time: 0.2561\n",
      "77/281, train_loss: 0.4681, step time: 0.2555\n",
      "78/281, train_loss: 0.3936, step time: 0.2582\n",
      "79/281, train_loss: 0.4277, step time: 0.2545\n",
      "80/281, train_loss: 0.3925, step time: 0.2544\n",
      "81/281, train_loss: 0.5375, step time: 0.2524\n",
      "82/281, train_loss: 0.4109, step time: 0.2543\n",
      "83/281, train_loss: 0.5536, step time: 0.2545\n",
      "84/281, train_loss: 0.5024, step time: 0.2535\n",
      "85/281, train_loss: 0.4190, step time: 0.2604\n",
      "86/281, train_loss: 0.4014, step time: 0.2612\n",
      "87/281, train_loss: 0.4616, step time: 0.2551\n",
      "88/281, train_loss: 0.4832, step time: 0.2611\n",
      "89/281, train_loss: 0.5318, step time: 0.2503\n",
      "90/281, train_loss: 0.5342, step time: 0.2539\n",
      "91/281, train_loss: 0.4505, step time: 0.2524\n",
      "92/281, train_loss: 0.4584, step time: 0.2593\n",
      "93/281, train_loss: 0.4567, step time: 0.2501\n",
      "94/281, train_loss: 0.4373, step time: 0.2517\n",
      "95/281, train_loss: 0.3901, step time: 0.2486\n",
      "96/281, train_loss: 0.4156, step time: 0.2542\n",
      "97/281, train_loss: 0.4578, step time: 0.2533\n",
      "98/281, train_loss: 0.4483, step time: 0.2503\n",
      "99/281, train_loss: 0.7002, step time: 0.2549\n",
      "100/281, train_loss: 0.4076, step time: 0.2582\n",
      "101/281, train_loss: 0.5487, step time: 0.2591\n",
      "102/281, train_loss: 0.5680, step time: 0.2571\n",
      "103/281, train_loss: 0.4866, step time: 0.2549\n",
      "104/281, train_loss: 0.5343, step time: 0.2615\n",
      "105/281, train_loss: 0.4069, step time: 0.2505\n",
      "106/281, train_loss: 0.4268, step time: 0.2546\n",
      "107/281, train_loss: 0.5641, step time: 0.2524\n",
      "108/281, train_loss: 0.5458, step time: 0.2504\n",
      "109/281, train_loss: 0.4919, step time: 0.2555\n",
      "110/281, train_loss: 0.4008, step time: 0.2530\n",
      "111/281, train_loss: 0.4121, step time: 0.2577\n",
      "112/281, train_loss: 0.4088, step time: 0.2541\n",
      "113/281, train_loss: 0.4138, step time: 0.2578\n",
      "114/281, train_loss: 0.4180, step time: 0.2555\n",
      "115/281, train_loss: 0.4296, step time: 0.2563\n",
      "116/281, train_loss: 0.5433, step time: 0.2577\n",
      "117/281, train_loss: 0.4206, step time: 0.2565\n",
      "118/281, train_loss: 0.5121, step time: 0.2560\n",
      "119/281, train_loss: 0.3874, step time: 0.2533\n",
      "120/281, train_loss: 0.4751, step time: 0.2529\n",
      "121/281, train_loss: 0.5033, step time: 0.2531\n",
      "122/281, train_loss: 0.4028, step time: 0.2557\n",
      "123/281, train_loss: 0.5430, step time: 0.2564\n",
      "124/281, train_loss: 0.4558, step time: 0.2626\n",
      "125/281, train_loss: 0.4008, step time: 0.2522\n",
      "126/281, train_loss: 0.4397, step time: 0.2546\n",
      "127/281, train_loss: 0.4562, step time: 0.2563\n",
      "128/281, train_loss: 0.4089, step time: 0.2542\n",
      "129/281, train_loss: 0.5422, step time: 0.2643\n",
      "130/281, train_loss: 0.4397, step time: 0.2594\n",
      "131/281, train_loss: 0.4301, step time: 0.2534\n",
      "132/281, train_loss: 0.4258, step time: 0.2532\n",
      "133/281, train_loss: 0.3926, step time: 0.2542\n",
      "134/281, train_loss: 0.4377, step time: 0.2509\n",
      "135/281, train_loss: 0.4405, step time: 0.2534\n",
      "136/281, train_loss: 0.4132, step time: 0.2545\n",
      "137/281, train_loss: 0.5357, step time: 0.2460\n",
      "138/281, train_loss: 0.4406, step time: 0.2548\n",
      "139/281, train_loss: 0.4208, step time: 0.2555\n",
      "140/281, train_loss: 0.4118, step time: 0.2479\n",
      "141/281, train_loss: 0.3990, step time: 0.2504\n",
      "142/281, train_loss: 0.5501, step time: 0.2509\n",
      "143/281, train_loss: 0.4816, step time: 0.2539\n",
      "144/281, train_loss: 0.4360, step time: 0.2534\n",
      "145/281, train_loss: 0.4054, step time: 0.2515\n",
      "146/281, train_loss: 0.6324, step time: 0.2503\n",
      "147/281, train_loss: 0.5567, step time: 0.2480\n",
      "148/281, train_loss: 0.5293, step time: 0.2446\n",
      "149/281, train_loss: 0.4193, step time: 0.2580\n",
      "150/281, train_loss: 0.4606, step time: 0.2321\n",
      "151/281, train_loss: 0.6164, step time: 0.2516\n",
      "152/281, train_loss: 0.4126, step time: 0.2526\n",
      "153/281, train_loss: 0.5208, step time: 0.2470\n",
      "154/281, train_loss: 0.4272, step time: 0.2471\n",
      "155/281, train_loss: 0.4118, step time: 0.2545\n",
      "156/281, train_loss: 0.4216, step time: 0.2597\n",
      "157/281, train_loss: 0.4033, step time: 0.2531\n",
      "158/281, train_loss: 0.4086, step time: 0.2567\n",
      "159/281, train_loss: 0.3903, step time: 0.2562\n",
      "160/281, train_loss: 0.4374, step time: 0.2557\n",
      "161/281, train_loss: 0.4496, step time: 0.2577\n",
      "162/281, train_loss: 0.6358, step time: 0.2530\n",
      "163/281, train_loss: 0.4688, step time: 0.2549\n",
      "164/281, train_loss: 0.4008, step time: 0.2492\n",
      "165/281, train_loss: 0.5088, step time: 0.2548\n",
      "166/281, train_loss: 0.4502, step time: 0.2489\n",
      "167/281, train_loss: 0.4300, step time: 0.2470\n",
      "168/281, train_loss: 0.4191, step time: 0.2541\n",
      "169/281, train_loss: 0.4379, step time: 0.2614\n",
      "170/281, train_loss: 0.4708, step time: 0.2704\n",
      "171/281, train_loss: 0.4107, step time: 0.2559\n",
      "172/281, train_loss: 0.4124, step time: 0.2541\n",
      "173/281, train_loss: 0.4117, step time: 0.2528\n",
      "174/281, train_loss: 0.4678, step time: 0.2530\n",
      "175/281, train_loss: 0.4252, step time: 0.2507\n",
      "176/281, train_loss: 0.5457, step time: 0.2536\n",
      "177/281, train_loss: 0.4802, step time: 0.2582\n",
      "178/281, train_loss: 0.4340, step time: 0.2514\n",
      "179/281, train_loss: 0.5770, step time: 0.2538\n",
      "180/281, train_loss: 0.4507, step time: 0.2530\n",
      "181/281, train_loss: 0.4369, step time: 0.2509\n",
      "182/281, train_loss: 0.5092, step time: 0.2546\n",
      "183/281, train_loss: 0.3876, step time: 0.2519\n",
      "184/281, train_loss: 0.5815, step time: 0.2572\n",
      "185/281, train_loss: 0.4389, step time: 0.2552\n",
      "186/281, train_loss: 0.5073, step time: 0.2525\n",
      "187/281, train_loss: 0.4441, step time: 0.2501\n",
      "188/281, train_loss: 0.4583, step time: 0.2474\n",
      "189/281, train_loss: 0.4764, step time: 0.2516\n",
      "190/281, train_loss: 0.4745, step time: 0.2510\n",
      "191/281, train_loss: 0.4414, step time: 0.2546\n",
      "192/281, train_loss: 0.4599, step time: 0.2482\n",
      "193/281, train_loss: 0.4105, step time: 0.2539\n",
      "194/281, train_loss: 0.4514, step time: 0.2515\n",
      "195/281, train_loss: 0.5142, step time: 0.2568\n",
      "196/281, train_loss: 0.5849, step time: 0.2496\n",
      "197/281, train_loss: 0.4837, step time: 0.2515\n",
      "198/281, train_loss: 0.4533, step time: 0.2522\n",
      "199/281, train_loss: 0.4333, step time: 0.2501\n",
      "200/281, train_loss: 0.4336, step time: 0.2549\n",
      "201/281, train_loss: 0.4575, step time: 0.2499\n",
      "202/281, train_loss: 0.4659, step time: 0.2487\n",
      "203/281, train_loss: 0.4473, step time: 0.2520\n",
      "204/281, train_loss: 0.4302, step time: 0.2507\n",
      "205/281, train_loss: 0.4228, step time: 0.2515\n",
      "206/281, train_loss: 0.4331, step time: 0.2456\n",
      "207/281, train_loss: 0.4949, step time: 0.2543\n",
      "208/281, train_loss: 0.5528, step time: 0.2549\n",
      "209/281, train_loss: 0.3924, step time: 0.2489\n",
      "210/281, train_loss: 0.5121, step time: 0.2462\n",
      "211/281, train_loss: 0.5999, step time: 0.2461\n",
      "212/281, train_loss: 0.5904, step time: 0.2504\n",
      "213/281, train_loss: 0.3863, step time: 0.2459\n",
      "214/281, train_loss: 0.4928, step time: 0.2543\n",
      "215/281, train_loss: 0.5414, step time: 0.2492\n",
      "216/281, train_loss: 0.4662, step time: 0.2444\n",
      "217/281, train_loss: 0.5100, step time: 0.2486\n",
      "218/281, train_loss: 0.5472, step time: 0.2764\n",
      "219/281, train_loss: 0.3915, step time: 0.2503\n",
      "220/281, train_loss: 0.4321, step time: 0.2519\n",
      "221/281, train_loss: 0.4298, step time: 0.2520\n",
      "222/281, train_loss: 0.4006, step time: 0.2528\n",
      "223/281, train_loss: 0.5723, step time: 0.2533\n",
      "224/281, train_loss: 0.5581, step time: 0.2458\n",
      "225/281, train_loss: 0.5000, step time: 0.2522\n",
      "226/281, train_loss: 0.5687, step time: 0.2505\n",
      "227/281, train_loss: 0.4211, step time: 0.2511\n",
      "228/281, train_loss: 0.4067, step time: 0.2498\n",
      "229/281, train_loss: 0.4572, step time: 0.2526\n",
      "230/281, train_loss: 0.4088, step time: 0.2453\n",
      "231/281, train_loss: 0.5607, step time: 0.2445\n",
      "232/281, train_loss: 0.5670, step time: 0.2443\n",
      "233/281, train_loss: 0.4207, step time: 0.2500\n",
      "234/281, train_loss: 0.4315, step time: 0.2523\n",
      "235/281, train_loss: 0.4483, step time: 0.2526\n",
      "236/281, train_loss: 0.5625, step time: 0.2539\n",
      "237/281, train_loss: 0.4669, step time: 0.2473\n",
      "238/281, train_loss: 0.4565, step time: 0.2500\n",
      "239/281, train_loss: 0.4458, step time: 0.2491\n",
      "240/281, train_loss: 0.5974, step time: 0.2437\n",
      "241/281, train_loss: 0.4502, step time: 0.2485\n",
      "242/281, train_loss: 0.4194, step time: 0.2482\n",
      "243/281, train_loss: 0.4353, step time: 0.2501\n",
      "244/281, train_loss: 0.4282, step time: 0.2443\n",
      "245/281, train_loss: 0.4359, step time: 0.2530\n",
      "246/281, train_loss: 0.4223, step time: 0.2540\n",
      "247/281, train_loss: 0.5780, step time: 0.2507\n",
      "248/281, train_loss: 0.4166, step time: 0.2502\n",
      "249/281, train_loss: 0.4243, step time: 0.2488\n",
      "250/281, train_loss: 0.4401, step time: 0.2497\n",
      "251/281, train_loss: 0.5687, step time: 0.2466\n",
      "252/281, train_loss: 0.4494, step time: 0.2515\n",
      "253/281, train_loss: 0.4052, step time: 0.2582\n",
      "254/281, train_loss: 0.4145, step time: 0.2517\n",
      "255/281, train_loss: 0.5742, step time: 0.2457\n",
      "256/281, train_loss: 0.3838, step time: 0.2472\n",
      "257/281, train_loss: 0.4433, step time: 0.2453\n",
      "258/281, train_loss: 0.4383, step time: 0.2487\n",
      "259/281, train_loss: 0.4346, step time: 0.2488\n",
      "260/281, train_loss: 0.3858, step time: 0.2444\n",
      "261/281, train_loss: 0.5417, step time: 0.2459\n",
      "262/281, train_loss: 0.4482, step time: 0.2498\n",
      "263/281, train_loss: 0.3950, step time: 0.2487\n",
      "264/281, train_loss: 0.5998, step time: 0.2481\n",
      "265/281, train_loss: 0.4238, step time: 0.2529\n",
      "266/281, train_loss: 0.4867, step time: 0.2507\n",
      "267/281, train_loss: 0.4114, step time: 0.2496\n",
      "268/281, train_loss: 0.4234, step time: 0.2527\n",
      "269/281, train_loss: 0.4309, step time: 0.2453\n",
      "270/281, train_loss: 0.5378, step time: 0.2478\n",
      "271/281, train_loss: 0.4215, step time: 0.2492\n",
      "272/281, train_loss: 0.5142, step time: 0.2468\n",
      "273/281, train_loss: 0.4431, step time: 0.2465\n",
      "274/281, train_loss: 0.4876, step time: 0.2495\n",
      "275/281, train_loss: 0.4470, step time: 0.2471\n",
      "276/281, train_loss: 0.4146, step time: 0.2473\n",
      "277/281, train_loss: 0.4246, step time: 0.2536\n",
      "278/281, train_loss: 0.3900, step time: 0.2485\n",
      "279/281, train_loss: 0.5702, step time: 0.2505\n",
      "280/281, train_loss: 0.4246, step time: 0.2540\n",
      "281/281, train_loss: 0.4715, step time: 0.2483\n",
      "282/281, train_loss: 0.6903, step time: 0.1483\n",
      "epoch 15 average loss: 0.4654\n",
      "saved new best metric model\n",
      "current epoch: 15 current mean dice: 0.5593 tc: 0.0090 wt: 0.9008 et: 0.7961\n",
      "best mean dice: 0.5593 at epoch: 15\n",
      "time consuming of epoch 15 is: 356.2546\n",
      "----------\n",
      "epoch 16/200\n",
      "1/281, train_loss: 0.4215, step time: 0.2510\n",
      "2/281, train_loss: 0.3901, step time: 0.2446\n",
      "3/281, train_loss: 0.4566, step time: 0.2451\n",
      "4/281, train_loss: 0.4027, step time: 0.2752\n",
      "5/281, train_loss: 0.5373, step time: 0.2527\n",
      "6/281, train_loss: 0.5554, step time: 0.2533\n",
      "7/281, train_loss: 0.3945, step time: 0.2544\n",
      "8/281, train_loss: 0.4578, step time: 0.2601\n",
      "9/281, train_loss: 0.7047, step time: 0.2493\n",
      "10/281, train_loss: 0.4112, step time: 0.2484\n",
      "11/281, train_loss: 0.5805, step time: 0.2524\n",
      "12/281, train_loss: 0.4717, step time: 0.2506\n",
      "13/281, train_loss: 0.4019, step time: 0.2564\n",
      "14/281, train_loss: 0.4026, step time: 0.2493\n",
      "15/281, train_loss: 0.4305, step time: 0.2479\n",
      "16/281, train_loss: 0.5405, step time: 0.2508\n",
      "17/281, train_loss: 0.4553, step time: 0.2485\n",
      "18/281, train_loss: 0.4963, step time: 0.2532\n",
      "19/281, train_loss: 0.5210, step time: 0.2512\n",
      "20/281, train_loss: 0.4779, step time: 0.2574\n",
      "21/281, train_loss: 0.3844, step time: 0.2488\n",
      "22/281, train_loss: 0.4511, step time: 0.2514\n",
      "23/281, train_loss: 0.4220, step time: 0.2486\n",
      "24/281, train_loss: 0.5817, step time: 0.2532\n",
      "25/281, train_loss: 0.4021, step time: 0.2569\n",
      "26/281, train_loss: 0.4000, step time: 0.2610\n",
      "27/281, train_loss: 0.4312, step time: 0.2561\n",
      "28/281, train_loss: 0.4084, step time: 0.2519\n",
      "29/281, train_loss: 0.4766, step time: 0.2679\n",
      "30/281, train_loss: 0.3967, step time: 0.2525\n",
      "31/281, train_loss: 0.4422, step time: 0.2541\n",
      "32/281, train_loss: 0.5773, step time: 0.2527\n",
      "33/281, train_loss: 0.4381, step time: 0.2536\n",
      "34/281, train_loss: 0.4888, step time: 0.2667\n",
      "35/281, train_loss: 0.3943, step time: 0.2703\n",
      "36/281, train_loss: 0.5069, step time: 0.2479\n",
      "37/281, train_loss: 0.4917, step time: 0.2498\n",
      "38/281, train_loss: 0.3935, step time: 0.2530\n",
      "39/281, train_loss: 0.3973, step time: 0.2525\n",
      "40/281, train_loss: 0.4672, step time: 0.2491\n",
      "41/281, train_loss: 0.4459, step time: 0.2466\n",
      "42/281, train_loss: 0.4266, step time: 0.2512\n",
      "43/281, train_loss: 0.3990, step time: 0.2520\n",
      "44/281, train_loss: 0.4273, step time: 0.2557\n",
      "45/281, train_loss: 0.4655, step time: 0.2484\n",
      "46/281, train_loss: 0.3913, step time: 0.2490\n",
      "47/281, train_loss: 0.4042, step time: 0.2541\n",
      "48/281, train_loss: 0.4281, step time: 0.2871\n",
      "49/281, train_loss: 0.7046, step time: 0.2751\n",
      "50/281, train_loss: 0.4492, step time: 0.2516\n",
      "51/281, train_loss: 0.5672, step time: 0.2512\n",
      "52/281, train_loss: 0.6035, step time: 0.2556\n",
      "53/281, train_loss: 0.4438, step time: 0.2500\n",
      "54/281, train_loss: 0.4886, step time: 0.2594\n",
      "55/281, train_loss: 0.4044, step time: 0.2556\n",
      "56/281, train_loss: 0.5620, step time: 0.2601\n",
      "57/281, train_loss: 0.5309, step time: 0.2570\n",
      "58/281, train_loss: 0.4218, step time: 0.2539\n",
      "59/281, train_loss: 0.4482, step time: 0.2490\n",
      "60/281, train_loss: 0.4108, step time: 0.2477\n",
      "61/281, train_loss: 0.4147, step time: 0.2516\n",
      "62/281, train_loss: 0.5576, step time: 0.2512\n",
      "63/281, train_loss: 0.7104, step time: 0.2506\n",
      "64/281, train_loss: 0.4779, step time: 0.2489\n",
      "65/281, train_loss: 0.4263, step time: 0.2497\n",
      "66/281, train_loss: 0.4743, step time: 0.2529\n",
      "67/281, train_loss: 0.3982, step time: 0.2530\n",
      "68/281, train_loss: 0.4624, step time: 0.2535\n",
      "69/281, train_loss: 0.5107, step time: 0.2509\n",
      "70/281, train_loss: 0.3928, step time: 0.2483\n",
      "71/281, train_loss: 0.4270, step time: 0.2556\n",
      "72/281, train_loss: 0.4204, step time: 0.2559\n",
      "73/281, train_loss: 0.4343, step time: 0.2489\n",
      "74/281, train_loss: 0.4281, step time: 0.2478\n",
      "75/281, train_loss: 0.4107, step time: 0.2469\n",
      "76/281, train_loss: 0.3912, step time: 0.2515\n",
      "77/281, train_loss: 0.4382, step time: 0.2492\n",
      "78/281, train_loss: 0.4747, step time: 0.2433\n",
      "79/281, train_loss: 0.4262, step time: 0.2457\n",
      "80/281, train_loss: 0.4656, step time: 0.2520\n",
      "81/281, train_loss: 0.4048, step time: 0.2505\n",
      "82/281, train_loss: 0.4271, step time: 0.2519\n",
      "83/281, train_loss: 0.4871, step time: 0.2568\n",
      "84/281, train_loss: 0.5585, step time: 0.2538\n",
      "85/281, train_loss: 0.4109, step time: 0.2489\n",
      "86/281, train_loss: 0.4051, step time: 0.2534\n",
      "87/281, train_loss: 0.4767, step time: 0.2516\n",
      "88/281, train_loss: 0.4423, step time: 0.2521\n",
      "89/281, train_loss: 0.3965, step time: 0.2549\n",
      "90/281, train_loss: 0.4578, step time: 0.2529\n",
      "91/281, train_loss: 0.3912, step time: 0.2575\n",
      "92/281, train_loss: 0.4046, step time: 0.2540\n",
      "93/281, train_loss: 0.3953, step time: 0.2509\n",
      "94/281, train_loss: 0.4119, step time: 0.2597\n",
      "95/281, train_loss: 0.4248, step time: 0.2551\n",
      "96/281, train_loss: 0.5190, step time: 0.2508\n",
      "97/281, train_loss: 0.4132, step time: 0.2533\n",
      "98/281, train_loss: 0.4139, step time: 0.2527\n",
      "99/281, train_loss: 0.4699, step time: 0.2541\n",
      "100/281, train_loss: 0.4288, step time: 0.2516\n",
      "101/281, train_loss: 0.4592, step time: 0.2549\n",
      "102/281, train_loss: 0.5398, step time: 0.2509\n",
      "103/281, train_loss: 0.4473, step time: 0.2520\n",
      "104/281, train_loss: 0.5333, step time: 0.2531\n",
      "105/281, train_loss: 0.4299, step time: 0.2490\n",
      "106/281, train_loss: 0.4138, step time: 0.2547\n",
      "107/281, train_loss: 0.4201, step time: 0.2535\n",
      "108/281, train_loss: 0.4550, step time: 0.2573\n",
      "109/281, train_loss: 0.4164, step time: 0.2566\n",
      "110/281, train_loss: 0.4069, step time: 0.2530\n",
      "111/281, train_loss: 0.5226, step time: 0.2547\n",
      "112/281, train_loss: 0.6130, step time: 0.2591\n",
      "113/281, train_loss: 0.4093, step time: 0.2570\n",
      "114/281, train_loss: 0.5361, step time: 0.2585\n",
      "115/281, train_loss: 0.4095, step time: 0.2541\n",
      "116/281, train_loss: 0.5810, step time: 0.2539\n",
      "117/281, train_loss: 0.4546, step time: 0.2492\n",
      "118/281, train_loss: 0.4174, step time: 0.2516\n",
      "119/281, train_loss: 0.5937, step time: 0.2587\n",
      "120/281, train_loss: 0.4564, step time: 0.2655\n",
      "121/281, train_loss: 0.4016, step time: 0.2618\n",
      "122/281, train_loss: 0.4672, step time: 0.2570\n",
      "123/281, train_loss: 0.4121, step time: 0.2577\n",
      "124/281, train_loss: 0.4061, step time: 0.2583\n",
      "125/281, train_loss: 0.4205, step time: 0.2581\n",
      "126/281, train_loss: 0.4573, step time: 0.2566\n",
      "127/281, train_loss: 0.5464, step time: 0.2582\n",
      "128/281, train_loss: 0.4411, step time: 0.2540\n",
      "129/281, train_loss: 0.4688, step time: 0.2586\n",
      "130/281, train_loss: 0.4254, step time: 0.2603\n",
      "131/281, train_loss: 0.4454, step time: 0.2613\n",
      "132/281, train_loss: 0.5625, step time: 0.2521\n",
      "133/281, train_loss: 0.4392, step time: 0.2583\n",
      "134/281, train_loss: 0.4557, step time: 0.2573\n",
      "135/281, train_loss: 0.4250, step time: 0.2617\n",
      "136/281, train_loss: 0.4166, step time: 0.2526\n",
      "137/281, train_loss: 0.4702, step time: 0.2556\n",
      "138/281, train_loss: 0.4331, step time: 0.2516\n",
      "139/281, train_loss: 0.4797, step time: 0.2557\n",
      "140/281, train_loss: 0.5647, step time: 0.2582\n",
      "141/281, train_loss: 0.4215, step time: 0.2551\n",
      "142/281, train_loss: 0.4481, step time: 0.2588\n",
      "143/281, train_loss: 0.4390, step time: 0.2538\n",
      "144/281, train_loss: 0.4100, step time: 0.2540\n",
      "145/281, train_loss: 0.4655, step time: 0.2560\n",
      "146/281, train_loss: 0.4148, step time: 0.2575\n",
      "147/281, train_loss: 0.4474, step time: 0.2608\n",
      "148/281, train_loss: 0.4907, step time: 0.2567\n",
      "149/281, train_loss: 0.4214, step time: 0.2584\n",
      "150/281, train_loss: 0.3874, step time: 0.2568\n",
      "151/281, train_loss: 0.4445, step time: 0.2544\n",
      "152/281, train_loss: 0.5554, step time: 0.2561\n",
      "153/281, train_loss: 0.5738, step time: 0.2721\n",
      "154/281, train_loss: 0.4018, step time: 0.2554\n",
      "155/281, train_loss: 0.4236, step time: 0.2592\n",
      "156/281, train_loss: 0.4669, step time: 0.2559\n",
      "157/281, train_loss: 0.5546, step time: 0.2534\n",
      "158/281, train_loss: 0.4574, step time: 0.2501\n",
      "159/281, train_loss: 0.4504, step time: 0.2559\n",
      "160/281, train_loss: 0.5335, step time: 0.2598\n",
      "161/281, train_loss: 0.4680, step time: 0.2596\n",
      "162/281, train_loss: 0.5674, step time: 0.2649\n",
      "163/281, train_loss: 0.4457, step time: 0.2554\n",
      "164/281, train_loss: 0.3920, step time: 0.2570\n",
      "165/281, train_loss: 0.5670, step time: 0.2586\n",
      "166/281, train_loss: 0.4008, step time: 0.2589\n",
      "167/281, train_loss: 0.4313, step time: 0.2567\n",
      "168/281, train_loss: 0.4679, step time: 0.2567\n",
      "169/281, train_loss: 0.4295, step time: 0.2544\n",
      "170/281, train_loss: 0.4450, step time: 0.2549\n",
      "171/281, train_loss: 0.3974, step time: 0.2609\n",
      "172/281, train_loss: 0.3904, step time: 0.2574\n",
      "173/281, train_loss: 0.3900, step time: 0.2578\n",
      "174/281, train_loss: 0.4792, step time: 0.2579\n",
      "175/281, train_loss: 0.4224, step time: 0.2536\n",
      "176/281, train_loss: 0.6025, step time: 0.2575\n",
      "177/281, train_loss: 0.4094, step time: 0.2566\n",
      "178/281, train_loss: 0.4587, step time: 0.2571\n",
      "179/281, train_loss: 0.5666, step time: 0.2563\n",
      "180/281, train_loss: 0.4137, step time: 0.2568\n",
      "181/281, train_loss: 0.4486, step time: 0.2615\n",
      "182/281, train_loss: 0.4935, step time: 0.2600\n",
      "183/281, train_loss: 0.4240, step time: 0.2593\n",
      "184/281, train_loss: 0.5533, step time: 0.2562\n",
      "185/281, train_loss: 0.5302, step time: 0.2650\n",
      "186/281, train_loss: 0.4141, step time: 0.2540\n",
      "187/281, train_loss: 0.4215, step time: 0.2575\n",
      "188/281, train_loss: 0.4760, step time: 0.2611\n",
      "189/281, train_loss: 0.4316, step time: 0.2584\n",
      "190/281, train_loss: 0.4696, step time: 0.2558\n",
      "191/281, train_loss: 0.4543, step time: 0.2539\n",
      "192/281, train_loss: 0.4660, step time: 0.2619\n",
      "193/281, train_loss: 0.4774, step time: 0.2559\n",
      "194/281, train_loss: 0.4030, step time: 0.2534\n",
      "195/281, train_loss: 0.4494, step time: 0.2576\n",
      "196/281, train_loss: 0.4766, step time: 0.2615\n",
      "197/281, train_loss: 0.4058, step time: 0.2591\n",
      "198/281, train_loss: 0.5240, step time: 0.2585\n",
      "199/281, train_loss: 0.4890, step time: 0.2565\n",
      "200/281, train_loss: 0.4224, step time: 0.2546\n",
      "201/281, train_loss: 0.4434, step time: 0.2559\n",
      "202/281, train_loss: 0.5108, step time: 0.2537\n",
      "203/281, train_loss: 0.4043, step time: 0.2496\n",
      "204/281, train_loss: 0.4668, step time: 0.2582\n",
      "205/281, train_loss: 0.4779, step time: 0.2579\n",
      "206/281, train_loss: 0.4202, step time: 0.2521\n",
      "207/281, train_loss: 0.4744, step time: 0.2546\n",
      "208/281, train_loss: 0.4668, step time: 0.2520\n",
      "209/281, train_loss: 0.5461, step time: 0.2584\n",
      "210/281, train_loss: 0.4221, step time: 0.2590\n",
      "211/281, train_loss: 0.4222, step time: 0.2523\n",
      "212/281, train_loss: 0.4077, step time: 0.2583\n",
      "213/281, train_loss: 0.4075, step time: 0.2558\n",
      "214/281, train_loss: 0.4320, step time: 0.2533\n",
      "215/281, train_loss: 0.5776, step time: 0.2576\n",
      "216/281, train_loss: 0.4304, step time: 0.2586\n",
      "217/281, train_loss: 0.4227, step time: 0.2592\n",
      "218/281, train_loss: 0.3889, step time: 0.2537\n",
      "219/281, train_loss: 0.5739, step time: 0.2542\n",
      "220/281, train_loss: 0.4678, step time: 0.2590\n",
      "221/281, train_loss: 0.4470, step time: 0.2602\n",
      "222/281, train_loss: 0.4955, step time: 0.2514\n",
      "223/281, train_loss: 0.4382, step time: 0.2551\n",
      "224/281, train_loss: 0.4675, step time: 0.2579\n",
      "225/281, train_loss: 0.4181, step time: 0.2546\n",
      "226/281, train_loss: 0.5401, step time: 0.2537\n",
      "227/281, train_loss: 0.5179, step time: 0.2567\n",
      "228/281, train_loss: 0.3941, step time: 0.2594\n",
      "229/281, train_loss: 0.5081, step time: 0.2547\n",
      "230/281, train_loss: 0.4400, step time: 0.2507\n",
      "231/281, train_loss: 0.4275, step time: 0.2514\n",
      "232/281, train_loss: 0.4044, step time: 0.2579\n",
      "233/281, train_loss: 0.4804, step time: 0.2554\n",
      "234/281, train_loss: 0.4077, step time: 0.2556\n",
      "235/281, train_loss: 0.3942, step time: 0.2498\n",
      "236/281, train_loss: 0.5388, step time: 0.2520\n",
      "237/281, train_loss: 0.4164, step time: 0.2580\n",
      "238/281, train_loss: 0.5893, step time: 0.2519\n",
      "239/281, train_loss: 0.4300, step time: 0.2514\n",
      "240/281, train_loss: 0.4360, step time: 0.2626\n",
      "241/281, train_loss: 0.5652, step time: 0.2563\n",
      "242/281, train_loss: 0.4213, step time: 0.2570\n",
      "243/281, train_loss: 0.4196, step time: 0.2572\n",
      "244/281, train_loss: 0.5451, step time: 0.2556\n",
      "245/281, train_loss: 0.5773, step time: 0.2567\n",
      "246/281, train_loss: 0.4743, step time: 0.2567\n",
      "247/281, train_loss: 0.5145, step time: 0.2511\n",
      "248/281, train_loss: 0.3717, step time: 0.2501\n",
      "249/281, train_loss: 0.4185, step time: 0.2488\n",
      "250/281, train_loss: 0.3988, step time: 0.2555\n",
      "251/281, train_loss: 0.4217, step time: 0.2510\n",
      "252/281, train_loss: 0.6389, step time: 0.2542\n",
      "253/281, train_loss: 0.4632, step time: 0.2557\n",
      "254/281, train_loss: 0.5738, step time: 0.2497\n",
      "255/281, train_loss: 0.4232, step time: 0.2527\n",
      "256/281, train_loss: 0.3997, step time: 0.2502\n",
      "257/281, train_loss: 0.4782, step time: 0.2572\n",
      "258/281, train_loss: 0.3952, step time: 0.2576\n",
      "259/281, train_loss: 0.4737, step time: 0.2560\n",
      "260/281, train_loss: 0.5852, step time: 0.2497\n",
      "261/281, train_loss: 0.6049, step time: 0.2584\n",
      "262/281, train_loss: 0.4407, step time: 0.2565\n",
      "263/281, train_loss: 0.6465, step time: 0.2504\n",
      "264/281, train_loss: 0.5402, step time: 0.2505\n",
      "265/281, train_loss: 0.4316, step time: 0.2510\n",
      "266/281, train_loss: 0.5556, step time: 0.2521\n",
      "267/281, train_loss: 0.4017, step time: 0.2518\n",
      "268/281, train_loss: 0.4460, step time: 0.2541\n",
      "269/281, train_loss: 0.4235, step time: 0.2595\n",
      "270/281, train_loss: 0.5664, step time: 0.2544\n",
      "271/281, train_loss: 0.4568, step time: 0.2516\n",
      "272/281, train_loss: 0.4769, step time: 0.2551\n",
      "273/281, train_loss: 0.5199, step time: 0.2564\n",
      "274/281, train_loss: 0.4141, step time: 0.2508\n",
      "275/281, train_loss: 0.3910, step time: 0.2511\n",
      "276/281, train_loss: 0.4698, step time: 0.2554\n",
      "277/281, train_loss: 0.5082, step time: 0.2551\n",
      "278/281, train_loss: 0.4269, step time: 0.2520\n",
      "279/281, train_loss: 0.4032, step time: 0.2570\n",
      "280/281, train_loss: 0.5402, step time: 0.2518\n",
      "281/281, train_loss: 0.4168, step time: 0.2482\n",
      "282/281, train_loss: 0.3986, step time: 0.1525\n",
      "epoch 16 average loss: 0.4624\n",
      "saved new best metric model\n",
      "current epoch: 16 current mean dice: 0.5608 tc: 0.0090 wt: 0.9003 et: 0.8024\n",
      "best mean dice: 0.5608 at epoch: 16\n",
      "time consuming of epoch 16 is: 395.8401\n",
      "----------\n",
      "epoch 17/200\n",
      "1/281, train_loss: 0.4405, step time: 0.2618\n",
      "2/281, train_loss: 0.3941, step time: 0.2613\n",
      "3/281, train_loss: 0.5821, step time: 0.2628\n",
      "4/281, train_loss: 0.4520, step time: 0.2591\n",
      "5/281, train_loss: 0.5613, step time: 0.2579\n",
      "6/281, train_loss: 0.4442, step time: 0.2582\n",
      "7/281, train_loss: 0.4037, step time: 0.2631\n",
      "8/281, train_loss: 0.4405, step time: 0.2540\n",
      "9/281, train_loss: 0.4459, step time: 0.2589\n",
      "10/281, train_loss: 0.4166, step time: 0.2591\n",
      "11/281, train_loss: 0.5028, step time: 0.2631\n",
      "12/281, train_loss: 0.4332, step time: 0.2582\n",
      "13/281, train_loss: 0.4264, step time: 0.2634\n",
      "14/281, train_loss: 0.5957, step time: 0.2629\n",
      "15/281, train_loss: 0.4042, step time: 0.2536\n",
      "16/281, train_loss: 0.4575, step time: 0.2523\n",
      "17/281, train_loss: 0.4498, step time: 0.2717\n",
      "18/281, train_loss: 0.4393, step time: 0.2576\n",
      "19/281, train_loss: 0.4760, step time: 0.2583\n",
      "20/281, train_loss: 0.4350, step time: 0.2504\n",
      "21/281, train_loss: 0.3950, step time: 0.2577\n",
      "22/281, train_loss: 0.3874, step time: 0.2574\n",
      "23/281, train_loss: 0.4106, step time: 0.2606\n",
      "24/281, train_loss: 0.5764, step time: 0.2579\n",
      "25/281, train_loss: 0.4411, step time: 0.2625\n",
      "26/281, train_loss: 0.5270, step time: 0.2593\n",
      "27/281, train_loss: 0.3937, step time: 0.2563\n",
      "28/281, train_loss: 0.4079, step time: 0.2579\n",
      "29/281, train_loss: 0.4508, step time: 0.2605\n",
      "30/281, train_loss: 0.4286, step time: 0.3173\n",
      "31/281, train_loss: 0.4673, step time: 0.2552\n",
      "32/281, train_loss: 0.4244, step time: 0.2571\n",
      "33/281, train_loss: 0.5166, step time: 0.2552\n",
      "34/281, train_loss: 0.5379, step time: 0.2543\n",
      "35/281, train_loss: 0.4746, step time: 0.2480\n",
      "36/281, train_loss: 0.4521, step time: 0.2495\n",
      "37/281, train_loss: 0.4744, step time: 0.2541\n",
      "38/281, train_loss: 0.4506, step time: 0.2530\n",
      "39/281, train_loss: 0.4229, step time: 0.2525\n",
      "40/281, train_loss: 0.5118, step time: 0.2540\n",
      "41/281, train_loss: 0.5533, step time: 0.2671\n",
      "42/281, train_loss: 0.4398, step time: 0.2659\n",
      "43/281, train_loss: 0.4082, step time: 0.2630\n",
      "44/281, train_loss: 0.4778, step time: 0.2564\n",
      "45/281, train_loss: 0.4808, step time: 0.2620\n",
      "46/281, train_loss: 0.4368, step time: 0.2613\n",
      "47/281, train_loss: 0.4141, step time: 0.2594\n",
      "48/281, train_loss: 0.4089, step time: 0.2552\n",
      "49/281, train_loss: 0.5109, step time: 0.2543\n",
      "50/281, train_loss: 0.4640, step time: 0.2576\n",
      "51/281, train_loss: 0.4405, step time: 0.2589\n",
      "52/281, train_loss: 0.4996, step time: 0.2670\n",
      "53/281, train_loss: 0.3944, step time: 0.2580\n",
      "54/281, train_loss: 0.4703, step time: 0.2540\n",
      "55/281, train_loss: 0.4125, step time: 0.2568\n",
      "56/281, train_loss: 0.4352, step time: 0.2564\n",
      "57/281, train_loss: 0.4103, step time: 0.2617\n",
      "58/281, train_loss: 0.5472, step time: 0.2593\n",
      "59/281, train_loss: 0.3996, step time: 0.2533\n",
      "60/281, train_loss: 0.4066, step time: 0.2513\n",
      "61/281, train_loss: 0.4462, step time: 0.2533\n",
      "62/281, train_loss: 0.4330, step time: 0.2549\n",
      "63/281, train_loss: 0.3963, step time: 0.2523\n",
      "64/281, train_loss: 0.3728, step time: 0.2567\n",
      "65/281, train_loss: 0.4731, step time: 0.2583\n",
      "66/281, train_loss: 0.4633, step time: 0.2596\n",
      "67/281, train_loss: 0.4281, step time: 0.2573\n",
      "68/281, train_loss: 0.4287, step time: 0.2616\n",
      "69/281, train_loss: 0.4181, step time: 0.2686\n",
      "70/281, train_loss: 0.4324, step time: 0.2620\n",
      "71/281, train_loss: 0.4010, step time: 0.2634\n",
      "72/281, train_loss: 0.4094, step time: 0.2508\n",
      "73/281, train_loss: 0.4407, step time: 0.2765\n",
      "74/281, train_loss: 0.4485, step time: 0.2993\n",
      "75/281, train_loss: 0.4312, step time: 0.2604\n",
      "76/281, train_loss: 0.4102, step time: 0.2578\n",
      "77/281, train_loss: 0.4151, step time: 0.2576\n",
      "78/281, train_loss: 0.5766, step time: 0.2592\n",
      "79/281, train_loss: 0.4475, step time: 0.2525\n",
      "80/281, train_loss: 0.5015, step time: 0.2536\n",
      "81/281, train_loss: 0.4452, step time: 0.2541\n",
      "82/281, train_loss: 0.4773, step time: 0.2561\n",
      "83/281, train_loss: 0.4060, step time: 0.2587\n",
      "84/281, train_loss: 0.4846, step time: 0.2556\n",
      "85/281, train_loss: 0.4172, step time: 0.2572\n",
      "86/281, train_loss: 0.4662, step time: 0.2593\n",
      "87/281, train_loss: 0.4397, step time: 0.2596\n",
      "88/281, train_loss: 0.4110, step time: 0.2563\n",
      "89/281, train_loss: 0.5560, step time: 0.2495\n",
      "90/281, train_loss: 0.3963, step time: 0.2520\n",
      "91/281, train_loss: 0.4605, step time: 0.2556\n",
      "92/281, train_loss: 0.4079, step time: 0.2486\n",
      "93/281, train_loss: 0.5693, step time: 0.2504\n",
      "94/281, train_loss: 0.5462, step time: 0.2571\n",
      "95/281, train_loss: 0.4108, step time: 0.2557\n",
      "96/281, train_loss: 0.4145, step time: 0.2501\n",
      "97/281, train_loss: 0.5781, step time: 0.2483\n",
      "98/281, train_loss: 0.5536, step time: 0.2523\n",
      "99/281, train_loss: 0.4288, step time: 0.2512\n",
      "100/281, train_loss: 0.3899, step time: 0.2476\n",
      "101/281, train_loss: 0.5435, step time: 0.2486\n",
      "102/281, train_loss: 0.5666, step time: 0.2525\n",
      "103/281, train_loss: 0.4213, step time: 0.2517\n",
      "104/281, train_loss: 0.4316, step time: 0.2451\n",
      "105/281, train_loss: 0.3952, step time: 0.2490\n",
      "106/281, train_loss: 0.4506, step time: 0.2505\n",
      "107/281, train_loss: 0.4591, step time: 0.2962\n",
      "108/281, train_loss: 0.4878, step time: 0.2518\n",
      "109/281, train_loss: 0.4501, step time: 0.2511\n",
      "110/281, train_loss: 0.4208, step time: 0.2520\n",
      "111/281, train_loss: 0.5582, step time: 0.2486\n",
      "112/281, train_loss: 0.4020, step time: 0.2562\n",
      "113/281, train_loss: 0.4371, step time: 0.2546\n",
      "114/281, train_loss: 0.5343, step time: 0.2515\n",
      "115/281, train_loss: 0.4257, step time: 0.2551\n",
      "116/281, train_loss: 0.4258, step time: 0.2579\n",
      "117/281, train_loss: 0.4124, step time: 0.2603\n",
      "118/281, train_loss: 0.4325, step time: 0.2605\n",
      "119/281, train_loss: 0.5435, step time: 0.2605\n",
      "120/281, train_loss: 0.4950, step time: 0.2514\n",
      "121/281, train_loss: 0.4539, step time: 0.2519\n",
      "122/281, train_loss: 0.4085, step time: 0.2584\n",
      "123/281, train_loss: 0.4128, step time: 0.2574\n",
      "124/281, train_loss: 0.4353, step time: 0.2654\n",
      "125/281, train_loss: 0.4775, step time: 0.2577\n",
      "126/281, train_loss: 0.5340, step time: 0.2560\n",
      "127/281, train_loss: 0.4042, step time: 0.2591\n",
      "128/281, train_loss: 0.4105, step time: 0.2596\n",
      "129/281, train_loss: 0.5132, step time: 0.2544\n",
      "130/281, train_loss: 0.5582, step time: 0.2530\n",
      "131/281, train_loss: 0.4789, step time: 0.2513\n",
      "132/281, train_loss: 0.4212, step time: 0.2559\n",
      "133/281, train_loss: 0.3855, step time: 0.2579\n",
      "134/281, train_loss: 0.4143, step time: 0.2590\n",
      "135/281, train_loss: 0.3831, step time: 0.2520\n",
      "136/281, train_loss: 0.6911, step time: 0.2534\n",
      "137/281, train_loss: 0.4114, step time: 0.2546\n",
      "138/281, train_loss: 0.4323, step time: 0.2578\n",
      "139/281, train_loss: 0.3969, step time: 0.2570\n",
      "140/281, train_loss: 0.4143, step time: 0.2537\n",
      "141/281, train_loss: 0.4039, step time: 0.2724\n",
      "142/281, train_loss: 0.4602, step time: 0.2528\n",
      "143/281, train_loss: 0.5413, step time: 0.2508\n",
      "144/281, train_loss: 0.4007, step time: 0.2544\n",
      "145/281, train_loss: 0.4415, step time: 0.2595\n",
      "146/281, train_loss: 0.5829, step time: 0.2886\n",
      "147/281, train_loss: 0.7028, step time: 0.2615\n",
      "148/281, train_loss: 0.4337, step time: 0.2523\n",
      "149/281, train_loss: 0.5818, step time: 0.2540\n",
      "150/281, train_loss: 0.4119, step time: 0.2593\n",
      "151/281, train_loss: 0.4483, step time: 0.2532\n",
      "152/281, train_loss: 0.4009, step time: 0.2628\n",
      "153/281, train_loss: 0.4318, step time: 0.2615\n",
      "154/281, train_loss: 0.3996, step time: 0.2597\n",
      "155/281, train_loss: 0.3835, step time: 0.2597\n",
      "156/281, train_loss: 0.4122, step time: 0.2548\n",
      "157/281, train_loss: 0.4725, step time: 0.2556\n",
      "158/281, train_loss: 0.4177, step time: 0.2549\n",
      "159/281, train_loss: 0.5365, step time: 0.2586\n",
      "160/281, train_loss: 0.4635, step time: 0.2585\n",
      "161/281, train_loss: 0.5801, step time: 0.2568\n",
      "162/281, train_loss: 0.4379, step time: 0.2510\n",
      "163/281, train_loss: 0.4389, step time: 0.2556\n",
      "164/281, train_loss: 0.5418, step time: 0.2569\n",
      "165/281, train_loss: 0.4130, step time: 0.2579\n",
      "166/281, train_loss: 0.4911, step time: 0.2614\n",
      "167/281, train_loss: 0.4029, step time: 0.2518\n",
      "168/281, train_loss: 0.4095, step time: 0.2521\n",
      "169/281, train_loss: 0.3900, step time: 0.2531\n",
      "170/281, train_loss: 0.3985, step time: 0.2490\n",
      "171/281, train_loss: 0.6007, step time: 0.2553\n",
      "172/281, train_loss: 0.4800, step time: 0.2542\n",
      "173/281, train_loss: 0.5306, step time: 0.2588\n",
      "174/281, train_loss: 0.4502, step time: 0.2585\n",
      "175/281, train_loss: 0.4297, step time: 0.2584\n",
      "176/281, train_loss: 0.4270, step time: 0.2554\n",
      "177/281, train_loss: 0.5063, step time: 0.2559\n",
      "178/281, train_loss: 0.4209, step time: 0.2593\n",
      "179/281, train_loss: 0.4508, step time: 0.2592\n",
      "180/281, train_loss: 0.4420, step time: 0.2541\n",
      "181/281, train_loss: 0.3913, step time: 0.2574\n",
      "182/281, train_loss: 0.4127, step time: 0.2602\n",
      "183/281, train_loss: 0.5417, step time: 0.2560\n",
      "184/281, train_loss: 0.4611, step time: 0.2605\n",
      "185/281, train_loss: 0.4282, step time: 0.2606\n",
      "186/281, train_loss: 0.4524, step time: 0.2573\n",
      "187/281, train_loss: 0.4588, step time: 0.2590\n",
      "188/281, train_loss: 0.4447, step time: 0.2531\n",
      "189/281, train_loss: 0.3976, step time: 0.2519\n",
      "190/281, train_loss: 0.4128, step time: 0.2595\n",
      "191/281, train_loss: 0.4377, step time: 0.2923\n",
      "192/281, train_loss: 0.4762, step time: 0.2525\n",
      "193/281, train_loss: 0.4063, step time: 0.2558\n",
      "194/281, train_loss: 0.4021, step time: 0.2510\n",
      "195/281, train_loss: 0.4698, step time: 0.2555\n",
      "196/281, train_loss: 0.5131, step time: 0.2530\n",
      "197/281, train_loss: 0.5634, step time: 0.2520\n",
      "198/281, train_loss: 0.5590, step time: 0.2491\n",
      "199/281, train_loss: 0.5349, step time: 0.2531\n",
      "200/281, train_loss: 0.4176, step time: 0.2518\n",
      "201/281, train_loss: 0.4013, step time: 0.2562\n",
      "202/281, train_loss: 0.4426, step time: 0.2658\n",
      "203/281, train_loss: 0.4184, step time: 0.2690\n",
      "204/281, train_loss: 0.4135, step time: 0.2572\n",
      "205/281, train_loss: 0.5627, step time: 0.2565\n",
      "206/281, train_loss: 0.4025, step time: 0.2513\n",
      "207/281, train_loss: 0.4524, step time: 0.2500\n",
      "208/281, train_loss: 0.4191, step time: 0.2608\n",
      "209/281, train_loss: 0.3814, step time: 0.2572\n",
      "210/281, train_loss: 0.3925, step time: 0.2587\n",
      "211/281, train_loss: 0.4385, step time: 0.2557\n",
      "212/281, train_loss: 0.4288, step time: 0.2550\n",
      "213/281, train_loss: 0.5876, step time: 0.2554\n",
      "214/281, train_loss: 0.4385, step time: 0.2497\n",
      "215/281, train_loss: 0.5643, step time: 0.2509\n",
      "216/281, train_loss: 0.4079, step time: 0.2524\n",
      "217/281, train_loss: 0.6104, step time: 0.2526\n",
      "218/281, train_loss: 0.4326, step time: 0.2464\n",
      "219/281, train_loss: 0.4412, step time: 0.2460\n",
      "220/281, train_loss: 0.4671, step time: 0.2495\n",
      "221/281, train_loss: 0.3992, step time: 0.2546\n",
      "222/281, train_loss: 0.4272, step time: 0.2503\n",
      "223/281, train_loss: 0.4790, step time: 0.2532\n",
      "224/281, train_loss: 0.5828, step time: 0.2540\n",
      "225/281, train_loss: 0.4287, step time: 0.2522\n",
      "226/281, train_loss: 0.5924, step time: 0.2538\n",
      "227/281, train_loss: 0.4292, step time: 0.2625\n",
      "228/281, train_loss: 0.5887, step time: 0.2501\n",
      "229/281, train_loss: 0.3998, step time: 0.2543\n",
      "230/281, train_loss: 0.4012, step time: 0.2552\n",
      "231/281, train_loss: 0.4432, step time: 0.2513\n",
      "232/281, train_loss: 0.3940, step time: 0.2491\n",
      "233/281, train_loss: 0.4190, step time: 0.2567\n",
      "234/281, train_loss: 0.5825, step time: 0.2570\n",
      "235/281, train_loss: 0.4982, step time: 0.2534\n",
      "236/281, train_loss: 0.5912, step time: 0.2559\n",
      "237/281, train_loss: 0.4085, step time: 0.2520\n",
      "238/281, train_loss: 0.4042, step time: 0.2531\n",
      "239/281, train_loss: 0.5550, step time: 0.2492\n",
      "240/281, train_loss: 0.4442, step time: 0.2541\n",
      "241/281, train_loss: 0.4407, step time: 0.2548\n",
      "242/281, train_loss: 0.4028, step time: 0.2523\n",
      "243/281, train_loss: 0.4219, step time: 0.2510\n",
      "244/281, train_loss: 0.4556, step time: 0.2488\n",
      "245/281, train_loss: 0.4671, step time: 0.2549\n",
      "246/281, train_loss: 0.4770, step time: 0.2521\n",
      "247/281, train_loss: 0.3804, step time: 0.2527\n",
      "248/281, train_loss: 0.5828, step time: 0.2578\n",
      "249/281, train_loss: 0.3985, step time: 0.2521\n",
      "250/281, train_loss: 0.4136, step time: 0.2522\n",
      "251/281, train_loss: 0.4214, step time: 0.2502\n",
      "252/281, train_loss: 0.4017, step time: 0.2507\n",
      "253/281, train_loss: 0.4268, step time: 0.2519\n",
      "254/281, train_loss: 0.4600, step time: 0.2525\n",
      "255/281, train_loss: 0.7017, step time: 0.2530\n",
      "256/281, train_loss: 0.3940, step time: 0.2499\n",
      "257/281, train_loss: 0.4263, step time: 0.2585\n",
      "258/281, train_loss: 0.5093, step time: 0.2561\n",
      "259/281, train_loss: 0.4329, step time: 0.2534\n",
      "260/281, train_loss: 0.3827, step time: 0.2554\n",
      "261/281, train_loss: 0.3808, step time: 0.2505\n",
      "262/281, train_loss: 0.3950, step time: 0.2554\n",
      "263/281, train_loss: 0.4618, step time: 0.2550\n",
      "264/281, train_loss: 0.3868, step time: 0.2533\n",
      "265/281, train_loss: 0.3910, step time: 0.2493\n",
      "266/281, train_loss: 0.4586, step time: 0.2571\n",
      "267/281, train_loss: 0.4417, step time: 0.2500\n",
      "268/281, train_loss: 0.3959, step time: 0.2521\n",
      "269/281, train_loss: 0.4264, step time: 0.2496\n",
      "270/281, train_loss: 0.3994, step time: 0.2565\n",
      "271/281, train_loss: 0.4560, step time: 0.2547\n",
      "272/281, train_loss: 0.5511, step time: 0.2596\n",
      "273/281, train_loss: 0.5751, step time: 0.2538\n",
      "274/281, train_loss: 0.4440, step time: 0.2525\n",
      "275/281, train_loss: 0.4584, step time: 0.2549\n",
      "276/281, train_loss: 0.4112, step time: 0.2510\n",
      "277/281, train_loss: 0.4510, step time: 0.2523\n",
      "278/281, train_loss: 0.4420, step time: 0.2542\n",
      "279/281, train_loss: 0.4300, step time: 0.2493\n",
      "280/281, train_loss: 0.4094, step time: 0.2493\n",
      "281/281, train_loss: 0.3935, step time: 0.2515\n",
      "282/281, train_loss: 0.3800, step time: 0.1540\n",
      "epoch 17 average loss: 0.4560\n",
      "saved new best metric model\n",
      "current epoch: 17 current mean dice: 0.5651 tc: 0.0089 wt: 0.9064 et: 0.8089\n",
      "best mean dice: 0.5651 at epoch: 17\n",
      "time consuming of epoch 17 is: 379.1185\n",
      "----------\n",
      "epoch 18/200\n",
      "1/281, train_loss: 0.5583, step time: 0.2595\n",
      "2/281, train_loss: 0.4569, step time: 0.2518\n",
      "3/281, train_loss: 0.5816, step time: 0.2506\n",
      "4/281, train_loss: 0.4512, step time: 0.2486\n",
      "5/281, train_loss: 0.3969, step time: 0.2436\n",
      "6/281, train_loss: 0.4864, step time: 0.2481\n",
      "7/281, train_loss: 0.3904, step time: 0.2578\n",
      "8/281, train_loss: 0.4150, step time: 0.2588\n",
      "9/281, train_loss: 0.7032, step time: 0.2564\n",
      "10/281, train_loss: 0.4304, step time: 0.2509\n",
      "11/281, train_loss: 0.5441, step time: 0.2516\n",
      "12/281, train_loss: 0.4083, step time: 0.2524\n",
      "13/281, train_loss: 0.4163, step time: 0.2519\n",
      "14/281, train_loss: 0.4296, step time: 0.2482\n",
      "15/281, train_loss: 0.4273, step time: 0.2531\n",
      "16/281, train_loss: 0.4254, step time: 0.2531\n",
      "17/281, train_loss: 0.5383, step time: 0.2562\n",
      "18/281, train_loss: 0.3987, step time: 0.2606\n",
      "19/281, train_loss: 0.4682, step time: 0.2479\n",
      "20/281, train_loss: 0.4438, step time: 0.2525\n",
      "21/281, train_loss: 0.4476, step time: 0.2506\n",
      "22/281, train_loss: 0.4272, step time: 0.2521\n",
      "23/281, train_loss: 0.3996, step time: 0.2563\n",
      "24/281, train_loss: 0.4045, step time: 0.2547\n",
      "25/281, train_loss: 0.5527, step time: 0.2472\n",
      "26/281, train_loss: 0.4662, step time: 0.2552\n",
      "27/281, train_loss: 0.4033, step time: 0.2576\n",
      "28/281, train_loss: 0.4737, step time: 0.2559\n",
      "29/281, train_loss: 0.4280, step time: 0.2557\n",
      "30/281, train_loss: 0.4185, step time: 0.2515\n",
      "31/281, train_loss: 0.4151, step time: 0.2571\n",
      "32/281, train_loss: 0.4559, step time: 0.2496\n",
      "33/281, train_loss: 0.4790, step time: 0.2436\n",
      "34/281, train_loss: 0.4259, step time: 0.2496\n",
      "35/281, train_loss: 0.4128, step time: 0.2597\n",
      "36/281, train_loss: 0.4199, step time: 0.2556\n",
      "37/281, train_loss: 0.4107, step time: 0.2587\n",
      "38/281, train_loss: 0.4018, step time: 0.2527\n",
      "39/281, train_loss: 0.4397, step time: 0.2500\n",
      "40/281, train_loss: 0.3794, step time: 0.2494\n",
      "41/281, train_loss: 0.3999, step time: 0.2489\n",
      "42/281, train_loss: 0.4353, step time: 0.2518\n",
      "43/281, train_loss: 0.4474, step time: 0.2580\n",
      "44/281, train_loss: 0.4287, step time: 0.2499\n",
      "45/281, train_loss: 0.3959, step time: 0.2503\n",
      "46/281, train_loss: 0.4321, step time: 0.2457\n",
      "47/281, train_loss: 0.4790, step time: 0.2505\n",
      "48/281, train_loss: 0.4522, step time: 0.2487\n",
      "49/281, train_loss: 0.4396, step time: 0.2532\n",
      "50/281, train_loss: 0.4364, step time: 0.2488\n",
      "51/281, train_loss: 0.4736, step time: 0.2531\n",
      "52/281, train_loss: 0.5569, step time: 0.2519\n",
      "53/281, train_loss: 0.4075, step time: 0.2550\n",
      "54/281, train_loss: 0.4228, step time: 0.2564\n",
      "55/281, train_loss: 0.3860, step time: 0.2581\n",
      "56/281, train_loss: 0.4527, step time: 0.2774\n",
      "57/281, train_loss: 0.5363, step time: 0.2540\n",
      "58/281, train_loss: 0.4459, step time: 0.2468\n",
      "59/281, train_loss: 0.4431, step time: 0.2565\n",
      "60/281, train_loss: 0.4908, step time: 0.2503\n",
      "61/281, train_loss: 0.4363, step time: 0.2549\n",
      "62/281, train_loss: 0.6065, step time: 0.2518\n",
      "63/281, train_loss: 0.5524, step time: 0.2533\n",
      "64/281, train_loss: 0.3942, step time: 0.2551\n",
      "65/281, train_loss: 0.3851, step time: 0.2550\n",
      "66/281, train_loss: 0.3979, step time: 0.2493\n",
      "67/281, train_loss: 0.5672, step time: 0.2495\n",
      "68/281, train_loss: 0.4512, step time: 0.2540\n",
      "69/281, train_loss: 0.4679, step time: 0.2655\n",
      "70/281, train_loss: 0.5192, step time: 0.2627\n",
      "71/281, train_loss: 0.3945, step time: 0.2526\n",
      "72/281, train_loss: 0.5561, step time: 0.2506\n",
      "73/281, train_loss: 0.5918, step time: 0.2519\n",
      "74/281, train_loss: 0.5039, step time: 0.2507\n",
      "75/281, train_loss: 0.4767, step time: 0.2517\n",
      "76/281, train_loss: 0.3901, step time: 0.2510\n",
      "77/281, train_loss: 0.4129, step time: 0.2492\n",
      "78/281, train_loss: 0.5804, step time: 0.2521\n",
      "79/281, train_loss: 0.4115, step time: 0.2439\n",
      "80/281, train_loss: 0.4161, step time: 0.2457\n",
      "81/281, train_loss: 0.7080, step time: 0.2506\n",
      "82/281, train_loss: 0.4026, step time: 0.2564\n",
      "83/281, train_loss: 0.3797, step time: 0.2526\n",
      "84/281, train_loss: 0.4774, step time: 0.2529\n",
      "85/281, train_loss: 0.4326, step time: 0.2479\n",
      "86/281, train_loss: 0.4825, step time: 0.2491\n",
      "87/281, train_loss: 0.3982, step time: 0.2534\n",
      "88/281, train_loss: 0.4301, step time: 0.2522\n",
      "89/281, train_loss: 0.4689, step time: 0.2701\n",
      "90/281, train_loss: 0.4666, step time: 0.2493\n",
      "91/281, train_loss: 0.4277, step time: 0.2534\n",
      "92/281, train_loss: 0.3898, step time: 0.2495\n",
      "93/281, train_loss: 0.4954, step time: 0.2555\n",
      "94/281, train_loss: 0.4150, step time: 0.2488\n",
      "95/281, train_loss: 0.5538, step time: 0.2501\n",
      "96/281, train_loss: 0.4060, step time: 0.2465\n",
      "97/281, train_loss: 0.4711, step time: 0.2512\n",
      "98/281, train_loss: 0.4155, step time: 0.2495\n",
      "99/281, train_loss: 0.4250, step time: 0.2577\n",
      "100/281, train_loss: 0.4009, step time: 0.2506\n",
      "101/281, train_loss: 0.4394, step time: 0.2492\n",
      "102/281, train_loss: 0.5714, step time: 0.2545\n",
      "103/281, train_loss: 0.4662, step time: 0.2520\n",
      "104/281, train_loss: 0.4768, step time: 0.2449\n",
      "105/281, train_loss: 0.4016, step time: 0.2436\n",
      "106/281, train_loss: 0.6065, step time: 0.2491\n",
      "107/281, train_loss: 0.5556, step time: 0.2530\n",
      "108/281, train_loss: 0.4054, step time: 0.2490\n",
      "109/281, train_loss: 0.4153, step time: 0.2511\n",
      "110/281, train_loss: 0.4519, step time: 0.2522\n",
      "111/281, train_loss: 0.4124, step time: 0.2565\n",
      "112/281, train_loss: 0.4171, step time: 0.2459\n",
      "113/281, train_loss: 0.4101, step time: 0.2462\n",
      "114/281, train_loss: 0.4174, step time: 0.2479\n",
      "115/281, train_loss: 0.4472, step time: 0.2528\n",
      "116/281, train_loss: 0.4111, step time: 0.2499\n",
      "117/281, train_loss: 0.4059, step time: 0.2538\n",
      "118/281, train_loss: 0.4605, step time: 0.2547\n",
      "119/281, train_loss: 0.4149, step time: 0.2468\n",
      "120/281, train_loss: 0.5867, step time: 0.2458\n",
      "121/281, train_loss: 0.4079, step time: 0.2459\n",
      "122/281, train_loss: 0.4046, step time: 0.2466\n",
      "123/281, train_loss: 0.4290, step time: 0.2521\n",
      "124/281, train_loss: 0.4371, step time: 0.2467\n",
      "125/281, train_loss: 0.3905, step time: 0.2504\n",
      "126/281, train_loss: 0.3917, step time: 0.2462\n",
      "127/281, train_loss: 0.3933, step time: 0.2431\n",
      "128/281, train_loss: 0.5423, step time: 0.2435\n",
      "129/281, train_loss: 0.4013, step time: 0.2449\n",
      "130/281, train_loss: 0.3906, step time: 0.2457\n",
      "131/281, train_loss: 0.4149, step time: 0.2478\n",
      "132/281, train_loss: 0.4137, step time: 0.2523\n",
      "133/281, train_loss: 0.4540, step time: 0.2525\n",
      "134/281, train_loss: 0.4381, step time: 0.2503\n",
      "135/281, train_loss: 0.4259, step time: 0.2497\n",
      "136/281, train_loss: 0.4332, step time: 0.2596\n",
      "137/281, train_loss: 0.4371, step time: 0.2525\n",
      "138/281, train_loss: 0.4231, step time: 0.2528\n",
      "139/281, train_loss: 0.4547, step time: 0.2542\n",
      "140/281, train_loss: 0.5439, step time: 0.2576\n",
      "141/281, train_loss: 0.3897, step time: 0.2559\n",
      "142/281, train_loss: 0.5461, step time: 0.2598\n",
      "143/281, train_loss: 0.5726, step time: 0.2515\n",
      "144/281, train_loss: 0.4062, step time: 0.2502\n",
      "145/281, train_loss: 0.4009, step time: 0.2514\n",
      "146/281, train_loss: 0.4404, step time: 0.2491\n",
      "147/281, train_loss: 0.5628, step time: 0.2491\n",
      "148/281, train_loss: 0.4488, step time: 0.2457\n",
      "149/281, train_loss: 0.4262, step time: 0.2526\n",
      "150/281, train_loss: 0.4185, step time: 0.2481\n",
      "151/281, train_loss: 0.4326, step time: 0.2532\n",
      "152/281, train_loss: 0.4361, step time: 0.2535\n",
      "153/281, train_loss: 0.4064, step time: 0.2548\n",
      "154/281, train_loss: 0.4287, step time: 0.2521\n",
      "155/281, train_loss: 0.3865, step time: 0.2502\n",
      "156/281, train_loss: 0.4543, step time: 0.2511\n",
      "157/281, train_loss: 0.4779, step time: 0.2534\n",
      "158/281, train_loss: 0.4069, step time: 0.2469\n",
      "159/281, train_loss: 0.4571, step time: 0.2476\n",
      "160/281, train_loss: 0.3965, step time: 0.2521\n",
      "161/281, train_loss: 0.4096, step time: 0.2553\n",
      "162/281, train_loss: 0.4319, step time: 0.2527\n",
      "163/281, train_loss: 0.3867, step time: 0.2487\n",
      "164/281, train_loss: 0.5099, step time: 0.2514\n",
      "165/281, train_loss: 0.5658, step time: 0.2514\n",
      "166/281, train_loss: 0.5686, step time: 0.2492\n",
      "167/281, train_loss: 0.3866, step time: 0.2478\n",
      "168/281, train_loss: 0.3898, step time: 0.2512\n",
      "169/281, train_loss: 0.5687, step time: 0.2526\n",
      "170/281, train_loss: 0.4646, step time: 0.2533\n",
      "171/281, train_loss: 0.4068, step time: 0.2489\n",
      "172/281, train_loss: 0.4416, step time: 0.2548\n",
      "173/281, train_loss: 0.3775, step time: 0.2502\n",
      "174/281, train_loss: 0.3998, step time: 0.2512\n",
      "175/281, train_loss: 0.4299, step time: 0.2482\n",
      "176/281, train_loss: 0.5783, step time: 0.2493\n",
      "177/281, train_loss: 0.4232, step time: 0.2487\n",
      "178/281, train_loss: 0.4351, step time: 0.2484\n",
      "179/281, train_loss: 0.4384, step time: 0.2467\n",
      "180/281, train_loss: 0.4783, step time: 0.2548\n",
      "181/281, train_loss: 0.4414, step time: 0.2508\n",
      "182/281, train_loss: 0.5012, step time: 0.2510\n",
      "183/281, train_loss: 0.4012, step time: 0.2473\n",
      "184/281, train_loss: 0.6195, step time: 0.2532\n",
      "185/281, train_loss: 0.4397, step time: 0.2515\n",
      "186/281, train_loss: 0.4396, step time: 0.2507\n",
      "187/281, train_loss: 0.4160, step time: 0.2555\n",
      "188/281, train_loss: 0.4168, step time: 0.2551\n",
      "189/281, train_loss: 0.4139, step time: 0.2513\n",
      "190/281, train_loss: 0.4351, step time: 0.2467\n",
      "191/281, train_loss: 0.4471, step time: 0.2501\n",
      "192/281, train_loss: 0.5850, step time: 0.2556\n",
      "193/281, train_loss: 0.3941, step time: 0.2544\n",
      "194/281, train_loss: 0.4408, step time: 0.2485\n",
      "195/281, train_loss: 0.5371, step time: 0.2507\n",
      "196/281, train_loss: 0.4358, step time: 0.2461\n",
      "197/281, train_loss: 0.4296, step time: 0.2466\n",
      "198/281, train_loss: 0.4166, step time: 0.2469\n",
      "199/281, train_loss: 0.4246, step time: 0.2513\n",
      "200/281, train_loss: 0.4113, step time: 0.2523\n",
      "201/281, train_loss: 0.4079, step time: 0.2501\n",
      "202/281, train_loss: 0.4105, step time: 0.2527\n",
      "203/281, train_loss: 0.4408, step time: 0.2510\n",
      "204/281, train_loss: 0.3908, step time: 0.2474\n",
      "205/281, train_loss: 0.4668, step time: 0.2474\n",
      "206/281, train_loss: 0.5594, step time: 0.2448\n",
      "207/281, train_loss: 0.4049, step time: 0.2430\n",
      "208/281, train_loss: 0.4590, step time: 0.2651\n",
      "209/281, train_loss: 0.3980, step time: 0.2491\n",
      "210/281, train_loss: 0.4087, step time: 0.2487\n",
      "211/281, train_loss: 0.4855, step time: 0.2450\n",
      "212/281, train_loss: 0.5801, step time: 0.2471\n",
      "213/281, train_loss: 0.4254, step time: 0.2542\n",
      "214/281, train_loss: 0.3993, step time: 0.2504\n",
      "215/281, train_loss: 0.4237, step time: 0.2484\n",
      "216/281, train_loss: 0.5657, step time: 0.2484\n",
      "217/281, train_loss: 0.4251, step time: 0.2513\n",
      "218/281, train_loss: 0.4071, step time: 0.2496\n",
      "219/281, train_loss: 0.4343, step time: 0.2519\n",
      "220/281, train_loss: 0.3980, step time: 0.2547\n",
      "221/281, train_loss: 0.4068, step time: 0.2501\n",
      "222/281, train_loss: 0.4194, step time: 0.2508\n",
      "223/281, train_loss: 0.4534, step time: 0.2542\n",
      "224/281, train_loss: 0.4676, step time: 0.2489\n",
      "225/281, train_loss: 0.4220, step time: 0.2545\n",
      "226/281, train_loss: 0.4198, step time: 0.2519\n",
      "227/281, train_loss: 0.5363, step time: 0.2489\n",
      "228/281, train_loss: 0.4003, step time: 0.2520\n",
      "229/281, train_loss: 0.4271, step time: 0.2487\n",
      "230/281, train_loss: 0.4733, step time: 0.2482\n",
      "231/281, train_loss: 0.5269, step time: 0.2490\n",
      "232/281, train_loss: 0.4378, step time: 0.2531\n",
      "233/281, train_loss: 0.4065, step time: 0.2501\n",
      "234/281, train_loss: 0.4330, step time: 0.2504\n",
      "235/281, train_loss: 0.4154, step time: 0.2511\n",
      "236/281, train_loss: 0.4251, step time: 0.2527\n",
      "237/281, train_loss: 0.3965, step time: 0.2497\n",
      "238/281, train_loss: 0.4223, step time: 0.2482\n",
      "239/281, train_loss: 0.3945, step time: 0.2532\n",
      "240/281, train_loss: 0.4211, step time: 0.2518\n",
      "241/281, train_loss: 0.3947, step time: 0.2485\n",
      "242/281, train_loss: 0.4710, step time: 0.2563\n",
      "243/281, train_loss: 0.5647, step time: 0.2508\n",
      "244/281, train_loss: 0.7464, step time: 0.2525\n",
      "245/281, train_loss: 0.5415, step time: 0.2535\n",
      "246/281, train_loss: 0.4140, step time: 0.2505\n",
      "247/281, train_loss: 0.4212, step time: 0.2540\n",
      "248/281, train_loss: 0.5373, step time: 0.2575\n",
      "249/281, train_loss: 0.4073, step time: 0.2519\n",
      "250/281, train_loss: 0.5666, step time: 0.2479\n",
      "251/281, train_loss: 0.4145, step time: 0.2479\n",
      "252/281, train_loss: 0.4512, step time: 0.2618\n",
      "253/281, train_loss: 0.4766, step time: 0.2536\n",
      "254/281, train_loss: 0.4340, step time: 0.2615\n",
      "255/281, train_loss: 0.5782, step time: 0.2583\n",
      "256/281, train_loss: 0.4481, step time: 0.2481\n",
      "257/281, train_loss: 0.4590, step time: 0.2451\n",
      "258/281, train_loss: 0.4652, step time: 0.2510\n",
      "259/281, train_loss: 0.5477, step time: 0.2542\n",
      "260/281, train_loss: 0.4491, step time: 0.2528\n",
      "261/281, train_loss: 0.4517, step time: 0.2507\n",
      "262/281, train_loss: 0.4203, step time: 0.2490\n",
      "263/281, train_loss: 0.4827, step time: 0.2504\n",
      "264/281, train_loss: 0.4196, step time: 0.2478\n",
      "265/281, train_loss: 0.4245, step time: 0.2479\n",
      "266/281, train_loss: 0.4612, step time: 0.2541\n",
      "267/281, train_loss: 0.4360, step time: 0.2435\n",
      "268/281, train_loss: 0.3872, step time: 0.2455\n",
      "269/281, train_loss: 0.5357, step time: 0.2513\n",
      "270/281, train_loss: 0.5405, step time: 0.2503\n",
      "271/281, train_loss: 0.4418, step time: 0.2494\n",
      "272/281, train_loss: 0.4445, step time: 0.2511\n",
      "273/281, train_loss: 0.3834, step time: 0.2499\n",
      "274/281, train_loss: 0.4219, step time: 0.2484\n",
      "275/281, train_loss: 0.4340, step time: 0.2516\n",
      "276/281, train_loss: 0.4227, step time: 0.2495\n",
      "277/281, train_loss: 0.5425, step time: 0.2548\n",
      "278/281, train_loss: 0.3944, step time: 0.2492\n",
      "279/281, train_loss: 0.5564, step time: 0.2522\n",
      "280/281, train_loss: 0.5625, step time: 0.2520\n",
      "281/281, train_loss: 0.4154, step time: 0.2523\n",
      "282/281, train_loss: 0.4196, step time: 0.1487\n",
      "epoch 18 average loss: 0.4531\n",
      "current epoch: 18 current mean dice: 0.5569 tc: 0.0091 wt: 0.8904 et: 0.8024\n",
      "best mean dice: 0.5651 at epoch: 17\n",
      "time consuming of epoch 18 is: 408.7647\n",
      "----------\n",
      "epoch 19/200\n",
      "1/281, train_loss: 0.3933, step time: 0.2534\n",
      "2/281, train_loss: 0.3972, step time: 0.2456\n",
      "3/281, train_loss: 0.6662, step time: 0.2433\n",
      "4/281, train_loss: 0.4275, step time: 0.2535\n",
      "5/281, train_loss: 0.4372, step time: 0.2823\n",
      "6/281, train_loss: 0.3906, step time: 0.2728\n",
      "7/281, train_loss: 0.3798, step time: 0.2790\n",
      "8/281, train_loss: 0.3971, step time: 0.2587\n",
      "9/281, train_loss: 0.4174, step time: 0.2530\n",
      "10/281, train_loss: 0.3963, step time: 0.2549\n",
      "11/281, train_loss: 0.4151, step time: 0.2578\n",
      "12/281, train_loss: 0.4249, step time: 0.2577\n",
      "13/281, train_loss: 0.4100, step time: 0.2552\n",
      "14/281, train_loss: 0.5623, step time: 0.2556\n",
      "15/281, train_loss: 0.5716, step time: 0.2488\n",
      "16/281, train_loss: 0.4939, step time: 0.2548\n",
      "17/281, train_loss: 0.4059, step time: 0.2543\n",
      "18/281, train_loss: 0.5509, step time: 0.2563\n",
      "19/281, train_loss: 0.5637, step time: 0.2592\n",
      "20/281, train_loss: 0.3918, step time: 0.2584\n",
      "21/281, train_loss: 0.4213, step time: 0.2560\n",
      "22/281, train_loss: 0.4224, step time: 0.2568\n",
      "23/281, train_loss: 0.4232, step time: 0.2506\n",
      "24/281, train_loss: 0.4020, step time: 0.2520\n",
      "25/281, train_loss: 0.4070, step time: 0.2523\n",
      "26/281, train_loss: 0.4191, step time: 0.2573\n",
      "27/281, train_loss: 0.4362, step time: 0.2591\n",
      "28/281, train_loss: 0.3938, step time: 0.2616\n",
      "29/281, train_loss: 0.4340, step time: 0.2877\n",
      "30/281, train_loss: 0.3983, step time: 0.2533\n",
      "31/281, train_loss: 0.5341, step time: 0.2530\n",
      "32/281, train_loss: 0.4303, step time: 0.2579\n",
      "33/281, train_loss: 0.4095, step time: 0.2612\n",
      "34/281, train_loss: 0.5065, step time: 0.2588\n",
      "35/281, train_loss: 0.3848, step time: 0.2608\n",
      "36/281, train_loss: 0.4262, step time: 0.2564\n",
      "37/281, train_loss: 0.5694, step time: 0.2652\n",
      "38/281, train_loss: 0.4229, step time: 0.2899\n",
      "39/281, train_loss: 0.5704, step time: 0.2612\n",
      "40/281, train_loss: 0.4018, step time: 0.2558\n",
      "41/281, train_loss: 0.4083, step time: 0.2581\n",
      "42/281, train_loss: 0.4398, step time: 0.2551\n",
      "43/281, train_loss: 0.4689, step time: 0.2573\n",
      "44/281, train_loss: 0.4148, step time: 0.2535\n",
      "45/281, train_loss: 0.5363, step time: 0.2518\n",
      "46/281, train_loss: 0.5159, step time: 0.2536\n",
      "47/281, train_loss: 0.4151, step time: 0.2529\n",
      "48/281, train_loss: 0.4196, step time: 0.2513\n",
      "49/281, train_loss: 0.4305, step time: 0.2643\n",
      "50/281, train_loss: 0.6300, step time: 0.2591\n",
      "51/281, train_loss: 0.4192, step time: 0.2578\n",
      "52/281, train_loss: 0.4214, step time: 0.2569\n",
      "53/281, train_loss: 0.4209, step time: 0.2604\n",
      "54/281, train_loss: 0.4216, step time: 0.2574\n",
      "55/281, train_loss: 0.4868, step time: 0.2570\n",
      "56/281, train_loss: 0.6000, step time: 0.2593\n",
      "57/281, train_loss: 0.4221, step time: 0.2604\n",
      "58/281, train_loss: 0.4200, step time: 0.2594\n",
      "59/281, train_loss: 0.5893, step time: 0.2622\n",
      "60/281, train_loss: 0.3980, step time: 0.2681\n",
      "61/281, train_loss: 0.3975, step time: 0.2816\n",
      "62/281, train_loss: 0.4318, step time: 0.2585\n",
      "63/281, train_loss: 0.4702, step time: 0.2564\n",
      "64/281, train_loss: 0.3837, step time: 0.2564\n",
      "65/281, train_loss: 0.4110, step time: 0.2659\n",
      "66/281, train_loss: 0.4098, step time: 0.2574\n",
      "67/281, train_loss: 0.4289, step time: 0.2580\n",
      "68/281, train_loss: 0.4300, step time: 0.2576\n",
      "69/281, train_loss: 0.5549, step time: 0.2552\n",
      "70/281, train_loss: 0.5415, step time: 0.2566\n",
      "71/281, train_loss: 0.4632, step time: 0.2559\n",
      "72/281, train_loss: 0.3948, step time: 0.2571\n",
      "73/281, train_loss: 0.4136, step time: 0.2699\n",
      "74/281, train_loss: 0.4109, step time: 0.2606\n",
      "75/281, train_loss: 0.5650, step time: 0.2523\n",
      "76/281, train_loss: 0.4177, step time: 0.2507\n",
      "77/281, train_loss: 0.4004, step time: 0.2623\n",
      "78/281, train_loss: 0.6114, step time: 0.2583\n",
      "79/281, train_loss: 0.4186, step time: 0.2580\n",
      "80/281, train_loss: 0.4388, step time: 0.2580\n",
      "81/281, train_loss: 0.4142, step time: 0.2597\n",
      "82/281, train_loss: 0.4074, step time: 0.2579\n",
      "83/281, train_loss: 0.4410, step time: 0.2596\n",
      "84/281, train_loss: 0.4961, step time: 0.2554\n",
      "85/281, train_loss: 0.5835, step time: 0.2599\n",
      "86/281, train_loss: 0.4680, step time: 0.2582\n",
      "87/281, train_loss: 0.3950, step time: 0.2601\n",
      "88/281, train_loss: 0.4364, step time: 0.2593\n",
      "89/281, train_loss: 0.6592, step time: 0.2579\n",
      "90/281, train_loss: 0.4270, step time: 0.2582\n",
      "91/281, train_loss: 0.5480, step time: 0.2926\n",
      "92/281, train_loss: 0.4151, step time: 0.2574\n",
      "93/281, train_loss: 0.3972, step time: 0.2538\n",
      "94/281, train_loss: 0.4166, step time: 0.2613\n",
      "95/281, train_loss: 0.3957, step time: 0.2587\n",
      "96/281, train_loss: 0.3969, step time: 0.2521\n",
      "97/281, train_loss: 0.3993, step time: 0.2606\n",
      "98/281, train_loss: 0.3985, step time: 0.2634\n",
      "99/281, train_loss: 0.4137, step time: 0.2592\n",
      "100/281, train_loss: 0.4208, step time: 0.2551\n",
      "101/281, train_loss: 0.4012, step time: 0.2549\n",
      "102/281, train_loss: 0.4405, step time: 0.2585\n",
      "103/281, train_loss: 0.4341, step time: 0.2582\n",
      "104/281, train_loss: 0.4249, step time: 0.2570\n",
      "105/281, train_loss: 0.5742, step time: 0.2623\n",
      "106/281, train_loss: 0.3866, step time: 0.2534\n",
      "107/281, train_loss: 0.5623, step time: 0.2586\n",
      "108/281, train_loss: 0.4330, step time: 0.2576\n",
      "109/281, train_loss: 0.3939, step time: 0.2515\n",
      "110/281, train_loss: 0.4241, step time: 0.2663\n",
      "111/281, train_loss: 0.4401, step time: 0.2591\n",
      "112/281, train_loss: 0.5692, step time: 0.2625\n",
      "113/281, train_loss: 0.4536, step time: 0.2522\n",
      "114/281, train_loss: 0.4180, step time: 0.2558\n",
      "115/281, train_loss: 0.4118, step time: 0.2612\n",
      "116/281, train_loss: 0.4209, step time: 0.2578\n",
      "117/281, train_loss: 0.3827, step time: 0.2575\n",
      "118/281, train_loss: 0.4178, step time: 0.2555\n",
      "119/281, train_loss: 0.3968, step time: 0.2515\n",
      "120/281, train_loss: 0.4616, step time: 0.2543\n",
      "121/281, train_loss: 0.4595, step time: 0.2537\n",
      "122/281, train_loss: 0.4122, step time: 0.2548\n",
      "123/281, train_loss: 0.4107, step time: 0.2531\n",
      "124/281, train_loss: 0.4884, step time: 0.2572\n",
      "125/281, train_loss: 0.4507, step time: 0.2556\n",
      "126/281, train_loss: 0.4160, step time: 0.2550\n",
      "127/281, train_loss: 0.4138, step time: 0.2563\n",
      "128/281, train_loss: 0.4533, step time: 0.2577\n",
      "129/281, train_loss: 0.5457, step time: 0.2549\n",
      "130/281, train_loss: 0.4544, step time: 0.2542\n",
      "131/281, train_loss: 0.4029, step time: 0.2567\n",
      "132/281, train_loss: 0.4299, step time: 0.2580\n",
      "133/281, train_loss: 0.4093, step time: 0.2576\n",
      "134/281, train_loss: 0.4203, step time: 0.2545\n",
      "135/281, train_loss: 0.5404, step time: 0.2539\n",
      "136/281, train_loss: 0.4336, step time: 0.2573\n",
      "137/281, train_loss: 0.4322, step time: 0.2469\n",
      "138/281, train_loss: 0.4401, step time: 0.2537\n",
      "139/281, train_loss: 0.4065, step time: 0.2534\n",
      "140/281, train_loss: 0.4894, step time: 0.2523\n",
      "141/281, train_loss: 0.4569, step time: 0.2476\n",
      "142/281, train_loss: 0.4074, step time: 0.2556\n",
      "143/281, train_loss: 0.3989, step time: 0.2524\n",
      "144/281, train_loss: 0.4130, step time: 0.2564\n",
      "145/281, train_loss: 0.4654, step time: 0.2554\n",
      "146/281, train_loss: 0.3947, step time: 0.2654\n",
      "147/281, train_loss: 0.5829, step time: 0.2587\n",
      "148/281, train_loss: 0.4366, step time: 0.2511\n",
      "149/281, train_loss: 0.5221, step time: 0.2565\n",
      "150/281, train_loss: 0.4237, step time: 0.2517\n",
      "151/281, train_loss: 0.3855, step time: 0.2475\n",
      "152/281, train_loss: 0.3911, step time: 0.2581\n",
      "153/281, train_loss: 0.5699, step time: 0.2579\n",
      "154/281, train_loss: 0.5892, step time: 0.2559\n",
      "155/281, train_loss: 0.5457, step time: 0.2576\n",
      "156/281, train_loss: 0.4157, step time: 0.2574\n",
      "157/281, train_loss: 0.4448, step time: 0.2547\n",
      "158/281, train_loss: 0.5710, step time: 0.2562\n",
      "159/281, train_loss: 0.4534, step time: 0.2591\n",
      "160/281, train_loss: 0.3898, step time: 0.2562\n",
      "161/281, train_loss: 0.3897, step time: 0.2578\n",
      "162/281, train_loss: 0.5436, step time: 0.2513\n",
      "163/281, train_loss: 0.4723, step time: 0.2516\n",
      "164/281, train_loss: 0.4525, step time: 0.2551\n",
      "165/281, train_loss: 0.4026, step time: 0.2592\n",
      "166/281, train_loss: 0.4548, step time: 0.2581\n",
      "167/281, train_loss: 0.4094, step time: 0.2579\n",
      "168/281, train_loss: 0.3919, step time: 0.2544\n",
      "169/281, train_loss: 0.5466, step time: 0.2587\n",
      "170/281, train_loss: 0.4689, step time: 0.2588\n",
      "171/281, train_loss: 0.4221, step time: 0.2578\n",
      "172/281, train_loss: 0.4493, step time: 0.2563\n",
      "173/281, train_loss: 0.4047, step time: 0.2535\n",
      "174/281, train_loss: 0.4112, step time: 0.2586\n",
      "175/281, train_loss: 0.3812, step time: 0.2544\n",
      "176/281, train_loss: 0.4023, step time: 0.2542\n",
      "177/281, train_loss: 0.3761, step time: 0.2582\n",
      "178/281, train_loss: 0.4731, step time: 0.2581\n",
      "179/281, train_loss: 0.4023, step time: 0.2548\n",
      "180/281, train_loss: 0.5438, step time: 0.2554\n",
      "181/281, train_loss: 0.4648, step time: 0.2573\n",
      "182/281, train_loss: 0.4012, step time: 0.2578\n",
      "183/281, train_loss: 0.4206, step time: 0.2586\n",
      "184/281, train_loss: 0.3950, step time: 0.2570\n",
      "185/281, train_loss: 0.5390, step time: 0.2544\n",
      "186/281, train_loss: 0.4466, step time: 0.2500\n",
      "187/281, train_loss: 0.3722, step time: 0.2505\n",
      "188/281, train_loss: 0.4655, step time: 0.2601\n",
      "189/281, train_loss: 0.4027, step time: 0.2564\n",
      "190/281, train_loss: 0.5175, step time: 0.2546\n",
      "191/281, train_loss: 0.4324, step time: 0.2549\n",
      "192/281, train_loss: 0.5148, step time: 0.2568\n",
      "193/281, train_loss: 0.4498, step time: 0.2575\n",
      "194/281, train_loss: 0.5090, step time: 0.2527\n",
      "195/281, train_loss: 0.3890, step time: 0.2513\n",
      "196/281, train_loss: 0.4356, step time: 0.2573\n",
      "197/281, train_loss: 0.5748, step time: 0.2557\n",
      "198/281, train_loss: 0.4338, step time: 0.2534\n",
      "199/281, train_loss: 0.4170, step time: 0.2531\n",
      "200/281, train_loss: 0.4218, step time: 0.2573\n",
      "201/281, train_loss: 0.4265, step time: 0.2580\n",
      "202/281, train_loss: 0.4666, step time: 0.2569\n",
      "203/281, train_loss: 0.5469, step time: 0.2586\n",
      "204/281, train_loss: 0.4052, step time: 0.2554\n",
      "205/281, train_loss: 0.3955, step time: 0.2590\n",
      "206/281, train_loss: 0.4025, step time: 0.2581\n",
      "207/281, train_loss: 0.3849, step time: 0.2509\n",
      "208/281, train_loss: 0.4364, step time: 0.2585\n",
      "209/281, train_loss: 0.4137, step time: 0.2585\n",
      "210/281, train_loss: 0.6070, step time: 0.2535\n",
      "211/281, train_loss: 0.4206, step time: 0.2560\n",
      "212/281, train_loss: 0.4030, step time: 0.2523\n",
      "213/281, train_loss: 0.4064, step time: 0.2511\n",
      "214/281, train_loss: 0.4227, step time: 0.2523\n",
      "215/281, train_loss: 0.4064, step time: 0.2574\n",
      "216/281, train_loss: 0.4162, step time: 0.2577\n",
      "217/281, train_loss: 0.4377, step time: 0.2577\n",
      "218/281, train_loss: 0.4094, step time: 0.2583\n",
      "219/281, train_loss: 0.4283, step time: 0.2529\n",
      "220/281, train_loss: 0.5511, step time: 0.2574\n",
      "221/281, train_loss: 0.5734, step time: 0.2581\n",
      "222/281, train_loss: 0.5480, step time: 0.2519\n",
      "223/281, train_loss: 0.4056, step time: 0.2520\n",
      "224/281, train_loss: 0.4522, step time: 0.2532\n",
      "225/281, train_loss: 0.4247, step time: 0.2544\n",
      "226/281, train_loss: 0.4054, step time: 0.2511\n",
      "227/281, train_loss: 0.4420, step time: 0.2523\n",
      "228/281, train_loss: 0.5391, step time: 0.2595\n",
      "229/281, train_loss: 0.3772, step time: 0.2598\n",
      "230/281, train_loss: 0.4023, step time: 0.2574\n",
      "231/281, train_loss: 0.5003, step time: 0.2555\n",
      "232/281, train_loss: 0.5445, step time: 0.2540\n",
      "233/281, train_loss: 0.4091, step time: 0.2528\n",
      "234/281, train_loss: 0.3998, step time: 0.2563\n",
      "235/281, train_loss: 0.5535, step time: 0.2574\n",
      "236/281, train_loss: 0.4952, step time: 0.2579\n",
      "237/281, train_loss: 0.4136, step time: 0.2525\n",
      "238/281, train_loss: 0.4097, step time: 0.2483\n",
      "239/281, train_loss: 0.4325, step time: 0.2524\n",
      "240/281, train_loss: 0.5687, step time: 0.2508\n",
      "241/281, train_loss: 0.5942, step time: 0.2552\n",
      "242/281, train_loss: 0.3907, step time: 0.2605\n",
      "243/281, train_loss: 0.4221, step time: 0.2558\n",
      "244/281, train_loss: 0.5486, step time: 0.2575\n",
      "245/281, train_loss: 0.4196, step time: 0.2550\n",
      "246/281, train_loss: 0.5434, step time: 0.2508\n",
      "247/281, train_loss: 0.4224, step time: 0.2587\n",
      "248/281, train_loss: 0.3952, step time: 0.2544\n",
      "249/281, train_loss: 0.4357, step time: 0.2578\n",
      "250/281, train_loss: 0.4409, step time: 0.2585\n",
      "251/281, train_loss: 0.4448, step time: 0.2568\n",
      "252/281, train_loss: 0.5634, step time: 0.2568\n",
      "253/281, train_loss: 0.4302, step time: 0.2559\n",
      "254/281, train_loss: 0.4410, step time: 0.2592\n",
      "255/281, train_loss: 0.4120, step time: 0.2570\n",
      "256/281, train_loss: 0.4462, step time: 0.2525\n",
      "257/281, train_loss: 0.4563, step time: 0.2547\n",
      "258/281, train_loss: 0.4055, step time: 0.2575\n",
      "259/281, train_loss: 0.4039, step time: 0.2540\n",
      "260/281, train_loss: 0.3857, step time: 0.2559\n",
      "261/281, train_loss: 0.4241, step time: 0.2560\n",
      "262/281, train_loss: 0.4209, step time: 0.2544\n",
      "263/281, train_loss: 0.4163, step time: 0.2565\n",
      "264/281, train_loss: 0.4231, step time: 0.2557\n",
      "265/281, train_loss: 0.4072, step time: 0.2516\n",
      "266/281, train_loss: 0.4134, step time: 0.2547\n",
      "267/281, train_loss: 0.4643, step time: 0.2571\n",
      "268/281, train_loss: 0.3915, step time: 0.2511\n",
      "269/281, train_loss: 0.4283, step time: 0.2574\n",
      "270/281, train_loss: 0.3907, step time: 0.2606\n",
      "271/281, train_loss: 0.5558, step time: 0.2581\n",
      "272/281, train_loss: 0.4211, step time: 0.2590\n",
      "273/281, train_loss: 0.4402, step time: 0.2548\n",
      "274/281, train_loss: 0.4654, step time: 0.2587\n",
      "275/281, train_loss: 0.5757, step time: 0.2580\n",
      "276/281, train_loss: 0.4492, step time: 0.2564\n",
      "277/281, train_loss: 0.5406, step time: 0.2554\n",
      "278/281, train_loss: 0.5170, step time: 0.2527\n",
      "279/281, train_loss: 0.5635, step time: 0.2495\n",
      "280/281, train_loss: 0.4161, step time: 0.2555\n",
      "281/281, train_loss: 0.5921, step time: 0.2525\n",
      "282/281, train_loss: 0.4236, step time: 0.1529\n",
      "epoch 19 average loss: 0.4509\n",
      "saved new best metric model\n",
      "current epoch: 19 current mean dice: 0.5661 tc: 0.0090 wt: 0.8997 et: 0.8212\n",
      "best mean dice: 0.5661 at epoch: 19\n",
      "time consuming of epoch 19 is: 405.8652\n",
      "----------\n",
      "epoch 20/200\n",
      "1/281, train_loss: 0.4583, step time: 0.2552\n",
      "2/281, train_loss: 0.5683, step time: 0.2525\n",
      "3/281, train_loss: 0.4156, step time: 0.2488\n",
      "4/281, train_loss: 0.3770, step time: 0.2546\n",
      "5/281, train_loss: 0.6961, step time: 0.2546\n",
      "6/281, train_loss: 0.3967, step time: 0.2564\n",
      "7/281, train_loss: 0.4914, step time: 0.2575\n",
      "8/281, train_loss: 0.5628, step time: 0.2650\n",
      "9/281, train_loss: 0.7113, step time: 0.2478\n",
      "10/281, train_loss: 0.3789, step time: 0.2547\n",
      "11/281, train_loss: 0.5519, step time: 0.2501\n",
      "12/281, train_loss: 0.4223, step time: 0.2563\n",
      "13/281, train_loss: 0.5069, step time: 0.2609\n",
      "14/281, train_loss: 0.4082, step time: 0.2585\n",
      "15/281, train_loss: 0.4586, step time: 0.2503\n",
      "16/281, train_loss: 0.4000, step time: 0.2563\n",
      "17/281, train_loss: 0.4583, step time: 0.2502\n",
      "18/281, train_loss: 0.4654, step time: 0.2546\n",
      "19/281, train_loss: 0.4663, step time: 0.2536\n",
      "20/281, train_loss: 0.3987, step time: 0.2524\n",
      "21/281, train_loss: 0.4335, step time: 0.2482\n",
      "22/281, train_loss: 0.6638, step time: 0.2552\n",
      "23/281, train_loss: 0.3874, step time: 0.2548\n",
      "24/281, train_loss: 0.4263, step time: 0.2542\n",
      "25/281, train_loss: 0.4293, step time: 0.2490\n",
      "26/281, train_loss: 0.4869, step time: 0.2536\n",
      "27/281, train_loss: 0.3844, step time: 0.2501\n",
      "28/281, train_loss: 0.4271, step time: 0.2539\n",
      "29/281, train_loss: 0.5356, step time: 0.2518\n",
      "30/281, train_loss: 0.5481, step time: 0.2511\n",
      "31/281, train_loss: 0.4240, step time: 0.2495\n",
      "32/281, train_loss: 0.5665, step time: 0.2530\n",
      "33/281, train_loss: 0.4051, step time: 0.2590\n",
      "34/281, train_loss: 0.5513, step time: 0.2521\n",
      "35/281, train_loss: 0.4192, step time: 0.2514\n",
      "36/281, train_loss: 0.4254, step time: 0.2527\n",
      "37/281, train_loss: 0.5445, step time: 0.2517\n",
      "38/281, train_loss: 0.4159, step time: 0.2479\n",
      "39/281, train_loss: 0.3885, step time: 0.2520\n",
      "40/281, train_loss: 0.3986, step time: 0.2460\n",
      "41/281, train_loss: 0.4669, step time: 0.2534\n",
      "42/281, train_loss: 0.4168, step time: 0.2538\n",
      "43/281, train_loss: 0.4134, step time: 0.2594\n",
      "44/281, train_loss: 0.4602, step time: 0.2576\n",
      "45/281, train_loss: 0.4110, step time: 0.2534\n",
      "46/281, train_loss: 0.4082, step time: 0.2529\n",
      "47/281, train_loss: 0.4098, step time: 0.2535\n",
      "48/281, train_loss: 0.5689, step time: 0.2559\n",
      "49/281, train_loss: 0.3779, step time: 0.2464\n",
      "50/281, train_loss: 0.4345, step time: 0.2498\n",
      "51/281, train_loss: 0.4586, step time: 0.2463\n",
      "52/281, train_loss: 0.3879, step time: 0.2444\n",
      "53/281, train_loss: 0.3990, step time: 0.2489\n",
      "54/281, train_loss: 0.3933, step time: 0.2513\n",
      "55/281, train_loss: 0.4037, step time: 0.2535\n",
      "56/281, train_loss: 0.3941, step time: 0.2500\n",
      "57/281, train_loss: 0.7175, step time: 0.2502\n",
      "58/281, train_loss: 0.3867, step time: 0.2489\n",
      "59/281, train_loss: 0.3814, step time: 0.2518\n",
      "60/281, train_loss: 0.3794, step time: 0.2462\n",
      "61/281, train_loss: 0.4221, step time: 0.2608\n",
      "62/281, train_loss: 0.4315, step time: 0.2562\n",
      "63/281, train_loss: 0.4320, step time: 0.2493\n",
      "64/281, train_loss: 0.5367, step time: 0.2527\n",
      "65/281, train_loss: 0.5465, step time: 0.2518\n",
      "66/281, train_loss: 0.4748, step time: 0.2549\n",
      "67/281, train_loss: 0.3937, step time: 0.2554\n",
      "68/281, train_loss: 0.4261, step time: 0.2569\n",
      "69/281, train_loss: 0.3990, step time: 0.2553\n",
      "70/281, train_loss: 0.5680, step time: 0.2550\n",
      "71/281, train_loss: 0.4992, step time: 0.2571\n",
      "72/281, train_loss: 0.3966, step time: 0.2527\n",
      "73/281, train_loss: 0.3958, step time: 0.2593\n",
      "74/281, train_loss: 0.4222, step time: 0.2606\n",
      "75/281, train_loss: 0.4110, step time: 0.2572\n",
      "76/281, train_loss: 0.3975, step time: 0.2541\n",
      "77/281, train_loss: 0.4382, step time: 0.2554\n",
      "78/281, train_loss: 0.4987, step time: 0.2555\n",
      "79/281, train_loss: 0.4249, step time: 0.2450\n",
      "80/281, train_loss: 0.4131, step time: 0.2475\n",
      "81/281, train_loss: 0.4084, step time: 0.2532\n",
      "82/281, train_loss: 0.4174, step time: 0.2492\n",
      "83/281, train_loss: 0.4512, step time: 0.2512\n",
      "84/281, train_loss: 0.3930, step time: 0.2490\n",
      "85/281, train_loss: 0.4031, step time: 0.2460\n",
      "86/281, train_loss: 0.5917, step time: 0.2542\n",
      "87/281, train_loss: 0.4197, step time: 0.2492\n",
      "88/281, train_loss: 0.4714, step time: 0.2550\n",
      "89/281, train_loss: 0.4644, step time: 0.2527\n",
      "90/281, train_loss: 0.5672, step time: 0.2564\n",
      "91/281, train_loss: 0.3901, step time: 0.2584\n",
      "92/281, train_loss: 0.4456, step time: 0.2585\n",
      "93/281, train_loss: 0.3848, step time: 0.2543\n",
      "94/281, train_loss: 0.4243, step time: 0.2530\n",
      "95/281, train_loss: 0.3917, step time: 0.2509\n",
      "96/281, train_loss: 0.4233, step time: 0.2505\n",
      "97/281, train_loss: 0.3975, step time: 0.2529\n",
      "98/281, train_loss: 0.4094, step time: 0.2468\n",
      "99/281, train_loss: 0.4616, step time: 0.2506\n",
      "100/281, train_loss: 0.4037, step time: 0.2602\n",
      "101/281, train_loss: 0.4538, step time: 0.2533\n",
      "102/281, train_loss: 0.3792, step time: 0.2495\n",
      "103/281, train_loss: 0.4246, step time: 0.2516\n",
      "104/281, train_loss: 0.3871, step time: 0.2483\n",
      "105/281, train_loss: 0.5453, step time: 0.2551\n",
      "106/281, train_loss: 0.4593, step time: 0.2543\n",
      "107/281, train_loss: 0.4150, step time: 0.2507\n",
      "108/281, train_loss: 0.4278, step time: 0.2495\n",
      "109/281, train_loss: 0.4064, step time: 0.2530\n",
      "110/281, train_loss: 0.5451, step time: 0.2557\n",
      "111/281, train_loss: 0.4481, step time: 0.2548\n",
      "112/281, train_loss: 0.5636, step time: 0.2523\n",
      "113/281, train_loss: 0.4270, step time: 0.2521\n",
      "114/281, train_loss: 0.4040, step time: 0.2528\n",
      "115/281, train_loss: 0.5882, step time: 0.2459\n",
      "116/281, train_loss: 0.4289, step time: 0.2488\n",
      "117/281, train_loss: 0.3725, step time: 0.2483\n",
      "118/281, train_loss: 0.3935, step time: 0.2510\n",
      "119/281, train_loss: 0.3879, step time: 0.2501\n",
      "120/281, train_loss: 0.5191, step time: 0.2586\n",
      "121/281, train_loss: 0.3847, step time: 0.2557\n",
      "122/281, train_loss: 0.4193, step time: 0.2517\n",
      "123/281, train_loss: 0.3817, step time: 0.2496\n",
      "124/281, train_loss: 0.4372, step time: 0.2552\n",
      "125/281, train_loss: 0.4191, step time: 0.2471\n",
      "126/281, train_loss: 0.4217, step time: 0.2485\n",
      "127/281, train_loss: 0.4552, step time: 0.2477\n",
      "128/281, train_loss: 0.4099, step time: 0.2501\n",
      "129/281, train_loss: 0.4393, step time: 0.2486\n",
      "130/281, train_loss: 0.3977, step time: 0.2467\n",
      "131/281, train_loss: 0.4873, step time: 0.2496\n",
      "132/281, train_loss: 0.4199, step time: 0.2485\n",
      "133/281, train_loss: 0.4434, step time: 0.2421\n",
      "134/281, train_loss: 0.4425, step time: 0.2449\n",
      "135/281, train_loss: 0.3885, step time: 0.2479\n",
      "136/281, train_loss: 0.3846, step time: 0.2559\n",
      "137/281, train_loss: 0.4038, step time: 0.2482\n",
      "138/281, train_loss: 0.5618, step time: 0.2486\n",
      "139/281, train_loss: 0.4089, step time: 0.2521\n",
      "140/281, train_loss: 0.3942, step time: 0.2535\n",
      "141/281, train_loss: 0.4262, step time: 0.2477\n",
      "142/281, train_loss: 0.4099, step time: 0.2495\n",
      "143/281, train_loss: 0.4552, step time: 0.2517\n",
      "144/281, train_loss: 0.4814, step time: 0.2521\n",
      "145/281, train_loss: 0.5549, step time: 0.2517\n",
      "146/281, train_loss: 0.3964, step time: 0.2525\n",
      "147/281, train_loss: 0.4234, step time: 0.2453\n",
      "148/281, train_loss: 0.5558, step time: 0.2461\n",
      "149/281, train_loss: 0.4004, step time: 0.2481\n",
      "150/281, train_loss: 0.3784, step time: 0.2535\n",
      "151/281, train_loss: 0.4233, step time: 0.2517\n",
      "152/281, train_loss: 0.5373, step time: 0.2538\n",
      "153/281, train_loss: 0.3823, step time: 0.2600\n",
      "154/281, train_loss: 0.4363, step time: 0.2559\n",
      "155/281, train_loss: 0.4352, step time: 0.2513\n",
      "156/281, train_loss: 0.4471, step time: 0.2503\n",
      "157/281, train_loss: 0.5478, step time: 0.2558\n",
      "158/281, train_loss: 0.4514, step time: 0.2462\n",
      "159/281, train_loss: 0.4128, step time: 0.2529\n",
      "160/281, train_loss: 0.4672, step time: 0.2480\n",
      "161/281, train_loss: 0.3960, step time: 0.2555\n",
      "162/281, train_loss: 0.5520, step time: 0.2508\n",
      "163/281, train_loss: 0.3905, step time: 0.2538\n",
      "164/281, train_loss: 0.3973, step time: 0.2478\n",
      "165/281, train_loss: 0.3951, step time: 0.2691\n",
      "166/281, train_loss: 0.4497, step time: 0.2493\n",
      "167/281, train_loss: 0.4124, step time: 0.2508\n",
      "168/281, train_loss: 0.5595, step time: 0.2527\n",
      "169/281, train_loss: 0.4435, step time: 0.2532\n",
      "170/281, train_loss: 0.3874, step time: 0.2556\n",
      "171/281, train_loss: 0.4189, step time: 0.2601\n",
      "172/281, train_loss: 0.4190, step time: 0.2567\n",
      "173/281, train_loss: 0.5440, step time: 0.2529\n",
      "174/281, train_loss: 0.4632, step time: 0.2491\n",
      "175/281, train_loss: 0.3851, step time: 0.2522\n",
      "176/281, train_loss: 0.5387, step time: 0.2470\n",
      "177/281, train_loss: 0.4204, step time: 0.2489\n",
      "178/281, train_loss: 0.4093, step time: 0.2452\n",
      "179/281, train_loss: 0.4293, step time: 0.2449\n",
      "180/281, train_loss: 0.5541, step time: 0.2466\n",
      "181/281, train_loss: 0.5084, step time: 0.2588\n",
      "182/281, train_loss: 0.4202, step time: 0.2529\n",
      "183/281, train_loss: 0.4008, step time: 0.2497\n",
      "184/281, train_loss: 0.4211, step time: 0.2541\n",
      "185/281, train_loss: 0.4553, step time: 0.2505\n",
      "186/281, train_loss: 0.4468, step time: 0.2523\n",
      "187/281, train_loss: 0.4141, step time: 0.2479\n",
      "188/281, train_loss: 0.5424, step time: 0.2501\n",
      "189/281, train_loss: 0.5519, step time: 0.2755\n",
      "190/281, train_loss: 0.4308, step time: 0.2477\n",
      "191/281, train_loss: 0.4517, step time: 0.2486\n",
      "192/281, train_loss: 0.6954, step time: 0.2509\n",
      "193/281, train_loss: 0.4084, step time: 0.2491\n",
      "194/281, train_loss: 0.3927, step time: 0.2508\n",
      "195/281, train_loss: 0.4275, step time: 0.2481\n",
      "196/281, train_loss: 0.4159, step time: 0.2473\n",
      "197/281, train_loss: 0.4668, step time: 0.2480\n",
      "198/281, train_loss: 0.4729, step time: 0.2502\n",
      "199/281, train_loss: 0.4495, step time: 0.2591\n",
      "200/281, train_loss: 0.4226, step time: 0.2568\n",
      "201/281, train_loss: 0.4125, step time: 0.2529\n",
      "202/281, train_loss: 0.3841, step time: 0.2487\n",
      "203/281, train_loss: 0.4445, step time: 0.2487\n",
      "204/281, train_loss: 0.4099, step time: 0.2550\n",
      "205/281, train_loss: 0.3892, step time: 0.2562\n",
      "206/281, train_loss: 0.4358, step time: 0.2612\n",
      "207/281, train_loss: 0.4094, step time: 0.2533\n",
      "208/281, train_loss: 0.4045, step time: 0.2546\n",
      "209/281, train_loss: 0.4141, step time: 0.2483\n",
      "210/281, train_loss: 0.3905, step time: 0.2514\n",
      "211/281, train_loss: 0.5455, step time: 0.2498\n",
      "212/281, train_loss: 0.3924, step time: 0.2492\n",
      "213/281, train_loss: 0.5330, step time: 0.2486\n",
      "214/281, train_loss: 0.5993, step time: 0.2511\n",
      "215/281, train_loss: 0.4016, step time: 0.2504\n",
      "216/281, train_loss: 0.5761, step time: 0.2485\n",
      "217/281, train_loss: 0.4078, step time: 0.2498\n",
      "218/281, train_loss: 0.4385, step time: 0.2486\n",
      "219/281, train_loss: 0.4617, step time: 0.2493\n",
      "220/281, train_loss: 0.5495, step time: 0.2431\n",
      "221/281, train_loss: 0.4605, step time: 0.2502\n",
      "222/281, train_loss: 0.3919, step time: 0.2463\n",
      "223/281, train_loss: 0.4123, step time: 0.2483\n",
      "224/281, train_loss: 0.5906, step time: 0.2495\n",
      "225/281, train_loss: 0.4177, step time: 0.2537\n",
      "226/281, train_loss: 0.4085, step time: 0.2540\n",
      "227/281, train_loss: 0.4212, step time: 0.2516\n",
      "228/281, train_loss: 0.4306, step time: 0.2480\n",
      "229/281, train_loss: 0.4212, step time: 0.2487\n",
      "230/281, train_loss: 0.4776, step time: 0.2520\n",
      "231/281, train_loss: 0.4140, step time: 0.2508\n",
      "232/281, train_loss: 0.5718, step time: 0.2509\n",
      "233/281, train_loss: 0.4898, step time: 0.2530\n",
      "234/281, train_loss: 0.5511, step time: 0.2528\n",
      "235/281, train_loss: 0.4214, step time: 0.2483\n",
      "236/281, train_loss: 0.4287, step time: 0.2519\n",
      "237/281, train_loss: 0.5683, step time: 0.2453\n",
      "238/281, train_loss: 0.4189, step time: 0.2494\n",
      "239/281, train_loss: 0.4132, step time: 0.2471\n",
      "240/281, train_loss: 0.5731, step time: 0.2451\n",
      "241/281, train_loss: 0.3888, step time: 0.2577\n",
      "242/281, train_loss: 0.4540, step time: 0.2540\n",
      "243/281, train_loss: 0.4493, step time: 0.2545\n",
      "244/281, train_loss: 0.3842, step time: 0.2571\n",
      "245/281, train_loss: 0.4341, step time: 0.2541\n",
      "246/281, train_loss: 0.3907, step time: 0.2512\n",
      "247/281, train_loss: 0.4150, step time: 0.2519\n",
      "248/281, train_loss: 0.4252, step time: 0.2473\n",
      "249/281, train_loss: 0.3982, step time: 0.2499\n",
      "250/281, train_loss: 0.4565, step time: 0.2492\n",
      "251/281, train_loss: 0.4140, step time: 0.2495\n",
      "252/281, train_loss: 0.4154, step time: 0.2431\n",
      "253/281, train_loss: 0.3922, step time: 0.2525\n",
      "254/281, train_loss: 0.3889, step time: 0.2517\n",
      "255/281, train_loss: 0.4330, step time: 0.2511\n",
      "256/281, train_loss: 0.3996, step time: 0.2473\n",
      "257/281, train_loss: 0.5005, step time: 0.2542\n",
      "258/281, train_loss: 0.3766, step time: 0.2469\n",
      "259/281, train_loss: 0.4079, step time: 0.2434\n",
      "260/281, train_loss: 0.5314, step time: 0.2456\n",
      "261/281, train_loss: 0.3901, step time: 0.2464\n",
      "262/281, train_loss: 0.5411, step time: 0.2493\n",
      "263/281, train_loss: 0.5120, step time: 0.2486\n",
      "264/281, train_loss: 0.4328, step time: 0.2594\n",
      "265/281, train_loss: 0.3924, step time: 0.2486\n",
      "266/281, train_loss: 0.3961, step time: 0.2519\n",
      "267/281, train_loss: 0.4671, step time: 0.2535\n",
      "268/281, train_loss: 0.5445, step time: 0.2575\n",
      "269/281, train_loss: 0.4761, step time: 0.2506\n",
      "270/281, train_loss: 0.4344, step time: 0.2519\n",
      "271/281, train_loss: 0.3958, step time: 0.2543\n",
      "272/281, train_loss: 0.4773, step time: 0.2523\n",
      "273/281, train_loss: 0.4257, step time: 0.2550\n",
      "274/281, train_loss: 0.3901, step time: 0.2518\n",
      "275/281, train_loss: 0.4530, step time: 0.2495\n",
      "276/281, train_loss: 0.4460, step time: 0.2504\n",
      "277/281, train_loss: 0.4260, step time: 0.2510\n",
      "278/281, train_loss: 0.4412, step time: 0.2496\n",
      "279/281, train_loss: 0.4008, step time: 0.2537\n",
      "280/281, train_loss: 0.4011, step time: 0.2539\n",
      "281/281, train_loss: 0.3933, step time: 0.2491\n",
      "282/281, train_loss: 0.4112, step time: 0.1487\n",
      "epoch 20 average loss: 0.4476\n",
      "current epoch: 20 current mean dice: 0.5608 tc: 0.0090 wt: 0.8995 et: 0.8027\n",
      "best mean dice: 0.5661 at epoch: 19\n",
      "time consuming of epoch 20 is: 391.0484\n",
      "----------\n",
      "epoch 21/200\n",
      "1/281, train_loss: 0.4216, step time: 0.2517\n",
      "2/281, train_loss: 0.4036, step time: 0.2569\n",
      "3/281, train_loss: 0.3940, step time: 0.2555\n",
      "4/281, train_loss: 0.4418, step time: 0.2591\n",
      "5/281, train_loss: 0.4840, step time: 0.2528\n",
      "6/281, train_loss: 0.3737, step time: 0.2521\n",
      "7/281, train_loss: 0.3957, step time: 0.2523\n",
      "8/281, train_loss: 0.3976, step time: 0.2600\n",
      "9/281, train_loss: 0.4515, step time: 0.2427\n",
      "10/281, train_loss: 0.4070, step time: 0.2515\n",
      "11/281, train_loss: 0.5688, step time: 0.2514\n",
      "12/281, train_loss: 0.4049, step time: 0.2492\n",
      "13/281, train_loss: 0.3978, step time: 0.2532\n",
      "14/281, train_loss: 0.3866, step time: 0.2466\n",
      "15/281, train_loss: 0.5794, step time: 0.2468\n",
      "16/281, train_loss: 0.3869, step time: 0.2474\n",
      "17/281, train_loss: 0.3998, step time: 0.2501\n",
      "18/281, train_loss: 0.4272, step time: 0.2543\n",
      "19/281, train_loss: 0.4232, step time: 0.2556\n",
      "20/281, train_loss: 0.4734, step time: 0.2420\n",
      "21/281, train_loss: 0.3954, step time: 0.2476\n",
      "22/281, train_loss: 0.4067, step time: 0.2428\n",
      "23/281, train_loss: 0.4694, step time: 0.2488\n",
      "24/281, train_loss: 0.4031, step time: 0.2587\n",
      "25/281, train_loss: 0.3872, step time: 0.2617\n",
      "26/281, train_loss: 0.4540, step time: 0.2541\n",
      "27/281, train_loss: 0.4105, step time: 0.2489\n",
      "28/281, train_loss: 0.4155, step time: 0.2584\n",
      "29/281, train_loss: 0.3903, step time: 0.2621\n",
      "30/281, train_loss: 0.4108, step time: 0.2514\n",
      "31/281, train_loss: 0.3971, step time: 0.2535\n",
      "32/281, train_loss: 0.4277, step time: 0.2559\n",
      "33/281, train_loss: 0.4380, step time: 0.2549\n",
      "34/281, train_loss: 0.4978, step time: 0.2499\n",
      "35/281, train_loss: 0.4510, step time: 0.2528\n",
      "36/281, train_loss: 0.5495, step time: 0.2457\n",
      "37/281, train_loss: 0.3959, step time: 0.2487\n",
      "38/281, train_loss: 0.3955, step time: 0.2527\n",
      "39/281, train_loss: 0.5389, step time: 0.2432\n",
      "40/281, train_loss: 0.3826, step time: 0.2509\n",
      "41/281, train_loss: 0.5015, step time: 0.2537\n",
      "42/281, train_loss: 0.4276, step time: 0.2599\n",
      "43/281, train_loss: 0.4335, step time: 0.2580\n",
      "44/281, train_loss: 0.4262, step time: 0.2543\n",
      "45/281, train_loss: 0.4420, step time: 0.2509\n",
      "46/281, train_loss: 0.5614, step time: 0.2482\n",
      "47/281, train_loss: 0.4238, step time: 0.2509\n",
      "48/281, train_loss: 0.4598, step time: 0.2521\n",
      "49/281, train_loss: 0.5708, step time: 0.2498\n",
      "50/281, train_loss: 0.6081, step time: 0.2468\n",
      "51/281, train_loss: 0.4323, step time: 0.2517\n",
      "52/281, train_loss: 0.3970, step time: 0.2506\n",
      "53/281, train_loss: 0.5304, step time: 0.2471\n",
      "54/281, train_loss: 0.4541, step time: 0.2592\n",
      "55/281, train_loss: 0.3950, step time: 0.2561\n",
      "56/281, train_loss: 0.4259, step time: 0.2495\n",
      "57/281, train_loss: 0.4359, step time: 0.2506\n",
      "58/281, train_loss: 0.4071, step time: 0.2534\n",
      "59/281, train_loss: 0.4128, step time: 0.2509\n",
      "60/281, train_loss: 0.3999, step time: 0.2530\n",
      "61/281, train_loss: 0.4271, step time: 0.2496\n",
      "62/281, train_loss: 0.4098, step time: 0.2539\n",
      "63/281, train_loss: 0.3780, step time: 0.2553\n",
      "64/281, train_loss: 0.4173, step time: 0.2564\n",
      "65/281, train_loss: 0.3964, step time: 0.2560\n",
      "66/281, train_loss: 0.4015, step time: 0.2559\n",
      "67/281, train_loss: 0.3681, step time: 0.2564\n",
      "68/281, train_loss: 0.3967, step time: 0.2533\n",
      "69/281, train_loss: 0.4135, step time: 0.2533\n",
      "70/281, train_loss: 0.3947, step time: 0.2631\n",
      "71/281, train_loss: 0.4274, step time: 0.2588\n",
      "72/281, train_loss: 0.4708, step time: 0.2588\n",
      "73/281, train_loss: 0.4389, step time: 0.2524\n",
      "74/281, train_loss: 0.4189, step time: 0.2557\n",
      "75/281, train_loss: 0.4027, step time: 0.2512\n",
      "76/281, train_loss: 0.5643, step time: 0.2552\n",
      "77/281, train_loss: 0.4300, step time: 0.2590\n",
      "78/281, train_loss: 0.4200, step time: 0.2572\n",
      "79/281, train_loss: 0.4848, step time: 0.2508\n",
      "80/281, train_loss: 0.4136, step time: 0.2516\n",
      "81/281, train_loss: 0.4947, step time: 0.2522\n",
      "82/281, train_loss: 0.3966, step time: 0.2566\n",
      "83/281, train_loss: 0.4308, step time: 0.2492\n",
      "84/281, train_loss: 0.3841, step time: 0.2479\n",
      "85/281, train_loss: 0.4007, step time: 0.2535\n",
      "86/281, train_loss: 0.4002, step time: 0.2497\n",
      "87/281, train_loss: 0.5643, step time: 0.2538\n",
      "88/281, train_loss: 0.3969, step time: 0.2525\n",
      "89/281, train_loss: 0.5051, step time: 0.2503\n",
      "90/281, train_loss: 0.5440, step time: 0.2586\n",
      "91/281, train_loss: 0.4079, step time: 0.2571\n",
      "92/281, train_loss: 0.5464, step time: 0.2544\n",
      "93/281, train_loss: 0.5204, step time: 0.2532\n",
      "94/281, train_loss: 0.5056, step time: 0.2510\n",
      "95/281, train_loss: 0.3939, step time: 0.2525\n",
      "96/281, train_loss: 0.4313, step time: 0.2531\n",
      "97/281, train_loss: 0.4346, step time: 0.2564\n",
      "98/281, train_loss: 0.4416, step time: 0.2519\n",
      "99/281, train_loss: 0.4526, step time: 0.2588\n",
      "100/281, train_loss: 0.3873, step time: 0.2581\n",
      "101/281, train_loss: 0.4237, step time: 0.2568\n",
      "102/281, train_loss: 0.4361, step time: 0.2566\n",
      "103/281, train_loss: 0.4244, step time: 0.2558\n",
      "104/281, train_loss: 0.4111, step time: 0.2544\n",
      "105/281, train_loss: 0.3873, step time: 0.2528\n",
      "106/281, train_loss: 0.5837, step time: 0.2624\n",
      "107/281, train_loss: 0.4242, step time: 0.2549\n",
      "108/281, train_loss: 0.4346, step time: 0.2547\n",
      "109/281, train_loss: 0.3931, step time: 0.2546\n",
      "110/281, train_loss: 0.4351, step time: 0.2540\n",
      "111/281, train_loss: 0.4532, step time: 0.2491\n",
      "112/281, train_loss: 0.4102, step time: 0.2531\n",
      "113/281, train_loss: 0.3888, step time: 0.2468\n",
      "114/281, train_loss: 0.5593, step time: 0.2525\n",
      "115/281, train_loss: 0.4471, step time: 0.2557\n",
      "116/281, train_loss: 0.4189, step time: 0.2493\n",
      "117/281, train_loss: 0.4565, step time: 0.2509\n",
      "118/281, train_loss: 0.4200, step time: 0.2535\n",
      "119/281, train_loss: 0.5844, step time: 0.2545\n",
      "120/281, train_loss: 0.5478, step time: 0.2515\n",
      "121/281, train_loss: 0.5821, step time: 0.2535\n",
      "122/281, train_loss: 0.4155, step time: 0.2516\n",
      "123/281, train_loss: 0.3921, step time: 0.2559\n",
      "124/281, train_loss: 0.4096, step time: 0.2551\n",
      "125/281, train_loss: 0.4420, step time: 0.2475\n",
      "126/281, train_loss: 0.5489, step time: 0.2533\n",
      "127/281, train_loss: 0.4364, step time: 0.2515\n",
      "128/281, train_loss: 0.5547, step time: 0.2506\n",
      "129/281, train_loss: 0.5350, step time: 0.2540\n",
      "130/281, train_loss: 0.4105, step time: 0.2556\n",
      "131/281, train_loss: 0.5633, step time: 0.2538\n",
      "132/281, train_loss: 0.3959, step time: 0.2490\n",
      "133/281, train_loss: 0.4852, step time: 0.2584\n",
      "134/281, train_loss: 0.4269, step time: 0.2507\n",
      "135/281, train_loss: 0.3938, step time: 0.2545\n",
      "136/281, train_loss: 0.5335, step time: 0.2538\n",
      "137/281, train_loss: 0.5823, step time: 0.2511\n",
      "138/281, train_loss: 0.4721, step time: 0.2537\n",
      "139/281, train_loss: 0.5446, step time: 0.2539\n",
      "140/281, train_loss: 0.4203, step time: 0.2602\n",
      "141/281, train_loss: 0.3774, step time: 0.2532\n",
      "142/281, train_loss: 0.4128, step time: 0.2638\n",
      "143/281, train_loss: 0.4744, step time: 0.2749\n",
      "144/281, train_loss: 0.4745, step time: 0.2544\n",
      "145/281, train_loss: 0.4281, step time: 0.2506\n",
      "146/281, train_loss: 0.4527, step time: 0.2536\n",
      "147/281, train_loss: 0.5769, step time: 0.2498\n",
      "148/281, train_loss: 0.4084, step time: 0.2493\n",
      "149/281, train_loss: 0.3956, step time: 0.2544\n",
      "150/281, train_loss: 0.5501, step time: 0.2516\n",
      "151/281, train_loss: 0.4067, step time: 0.2579\n",
      "152/281, train_loss: 0.4414, step time: 0.2481\n",
      "153/281, train_loss: 0.3740, step time: 0.2503\n",
      "154/281, train_loss: 0.4114, step time: 0.2508\n",
      "155/281, train_loss: 0.5845, step time: 0.2546\n",
      "156/281, train_loss: 0.5593, step time: 0.2539\n",
      "157/281, train_loss: 0.3972, step time: 0.2497\n",
      "158/281, train_loss: 0.3899, step time: 0.2515\n",
      "159/281, train_loss: 0.4531, step time: 0.2587\n",
      "160/281, train_loss: 0.4144, step time: 0.2543\n",
      "161/281, train_loss: 0.3934, step time: 0.2551\n",
      "162/281, train_loss: 0.3991, step time: 0.2557\n",
      "163/281, train_loss: 0.3982, step time: 0.2557\n",
      "164/281, train_loss: 0.5386, step time: 0.2539\n",
      "165/281, train_loss: 0.3925, step time: 0.2575\n",
      "166/281, train_loss: 0.4165, step time: 0.2562\n",
      "167/281, train_loss: 0.5185, step time: 0.2524\n",
      "168/281, train_loss: 0.4073, step time: 0.2567\n",
      "169/281, train_loss: 0.4255, step time: 0.2580\n",
      "170/281, train_loss: 0.4522, step time: 0.2577\n",
      "171/281, train_loss: 0.3951, step time: 0.2566\n",
      "172/281, train_loss: 0.5518, step time: 0.2576\n",
      "173/281, train_loss: 0.3974, step time: 0.2597\n",
      "174/281, train_loss: 0.4621, step time: 0.2586\n",
      "175/281, train_loss: 0.4363, step time: 0.2595\n",
      "176/281, train_loss: 0.4351, step time: 0.2561\n",
      "177/281, train_loss: 0.5128, step time: 0.2527\n",
      "178/281, train_loss: 0.3900, step time: 0.2544\n",
      "179/281, train_loss: 0.5669, step time: 0.2576\n",
      "180/281, train_loss: 0.4105, step time: 0.2585\n",
      "181/281, train_loss: 0.4212, step time: 0.2582\n",
      "182/281, train_loss: 0.4183, step time: 0.2590\n",
      "183/281, train_loss: 0.4240, step time: 0.2558\n",
      "184/281, train_loss: 0.4017, step time: 0.2596\n",
      "185/281, train_loss: 0.5089, step time: 0.2580\n",
      "186/281, train_loss: 0.4220, step time: 0.2537\n",
      "187/281, train_loss: 0.4763, step time: 0.2548\n",
      "188/281, train_loss: 0.4322, step time: 0.2558\n",
      "189/281, train_loss: 0.4168, step time: 0.2551\n",
      "190/281, train_loss: 0.5526, step time: 0.2548\n",
      "191/281, train_loss: 0.4594, step time: 0.2504\n",
      "192/281, train_loss: 0.5532, step time: 0.2562\n",
      "193/281, train_loss: 0.4527, step time: 0.2563\n",
      "194/281, train_loss: 0.4002, step time: 0.2568\n",
      "195/281, train_loss: 0.4150, step time: 0.2551\n",
      "196/281, train_loss: 0.4578, step time: 0.2562\n",
      "197/281, train_loss: 0.4405, step time: 0.2575\n",
      "198/281, train_loss: 0.4142, step time: 0.2527\n",
      "199/281, train_loss: 0.5457, step time: 0.2577\n",
      "200/281, train_loss: 0.5474, step time: 0.2531\n",
      "201/281, train_loss: 0.5446, step time: 0.2587\n",
      "202/281, train_loss: 0.4055, step time: 0.2514\n",
      "203/281, train_loss: 0.4208, step time: 0.2508\n",
      "204/281, train_loss: 0.4182, step time: 0.2550\n",
      "205/281, train_loss: 0.4177, step time: 0.2514\n",
      "206/281, train_loss: 0.3865, step time: 0.2515\n",
      "207/281, train_loss: 0.5831, step time: 0.2527\n",
      "208/281, train_loss: 0.5653, step time: 0.2618\n",
      "209/281, train_loss: 0.4447, step time: 0.2663\n",
      "210/281, train_loss: 0.3884, step time: 0.2546\n",
      "211/281, train_loss: 0.4091, step time: 0.2583\n",
      "212/281, train_loss: 0.4463, step time: 0.2660\n",
      "213/281, train_loss: 0.4418, step time: 0.2566\n",
      "214/281, train_loss: 0.4690, step time: 0.2572\n",
      "215/281, train_loss: 0.4049, step time: 0.2605\n",
      "216/281, train_loss: 0.4042, step time: 0.2582\n",
      "217/281, train_loss: 0.4375, step time: 0.2610\n",
      "218/281, train_loss: 0.4690, step time: 0.2554\n",
      "219/281, train_loss: 0.5351, step time: 0.2554\n",
      "220/281, train_loss: 0.3977, step time: 0.2588\n",
      "221/281, train_loss: 0.4457, step time: 0.2577\n",
      "222/281, train_loss: 0.4337, step time: 0.2570\n",
      "223/281, train_loss: 0.4133, step time: 0.2614\n",
      "224/281, train_loss: 0.4163, step time: 0.2554\n",
      "225/281, train_loss: 0.4196, step time: 0.2548\n",
      "226/281, train_loss: 0.4460, step time: 0.2515\n",
      "227/281, train_loss: 0.5890, step time: 0.2637\n",
      "228/281, train_loss: 0.3784, step time: 0.2584\n",
      "229/281, train_loss: 0.4141, step time: 0.2502\n",
      "230/281, train_loss: 0.3807, step time: 0.2537\n",
      "231/281, train_loss: 0.4289, step time: 0.2576\n",
      "232/281, train_loss: 0.4119, step time: 0.2554\n",
      "233/281, train_loss: 0.4110, step time: 0.2543\n",
      "234/281, train_loss: 0.5477, step time: 0.2536\n",
      "235/281, train_loss: 0.5572, step time: 0.2500\n",
      "236/281, train_loss: 0.4278, step time: 0.2513\n",
      "237/281, train_loss: 0.3800, step time: 0.2538\n",
      "238/281, train_loss: 0.6107, step time: 0.2562\n",
      "239/281, train_loss: 0.4070, step time: 0.2561\n",
      "240/281, train_loss: 0.4350, step time: 0.2496\n",
      "241/281, train_loss: 0.4362, step time: 0.2516\n",
      "242/281, train_loss: 0.5298, step time: 0.2566\n",
      "243/281, train_loss: 0.4190, step time: 0.2561\n",
      "244/281, train_loss: 0.5659, step time: 0.2619\n",
      "245/281, train_loss: 0.3776, step time: 0.2556\n",
      "246/281, train_loss: 0.4297, step time: 0.2546\n",
      "247/281, train_loss: 0.4303, step time: 0.2581\n",
      "248/281, train_loss: 0.4186, step time: 0.2521\n",
      "249/281, train_loss: 0.4037, step time: 0.2546\n",
      "250/281, train_loss: 0.4188, step time: 0.2530\n",
      "251/281, train_loss: 0.5439, step time: 0.2553\n",
      "252/281, train_loss: 0.3911, step time: 0.2570\n",
      "253/281, train_loss: 0.4125, step time: 0.2519\n",
      "254/281, train_loss: 0.3894, step time: 0.2528\n",
      "255/281, train_loss: 0.4657, step time: 0.2584\n",
      "256/281, train_loss: 0.4222, step time: 0.2570\n",
      "257/281, train_loss: 0.5743, step time: 0.2526\n",
      "258/281, train_loss: 0.3721, step time: 0.2493\n",
      "259/281, train_loss: 0.3985, step time: 0.2537\n",
      "260/281, train_loss: 0.4582, step time: 0.2568\n",
      "261/281, train_loss: 0.4280, step time: 0.2518\n",
      "262/281, train_loss: 0.4486, step time: 0.2544\n",
      "263/281, train_loss: 0.4104, step time: 0.2614\n",
      "264/281, train_loss: 0.4028, step time: 0.2565\n",
      "265/281, train_loss: 0.4077, step time: 0.2527\n",
      "266/281, train_loss: 0.3606, step time: 0.2505\n",
      "267/281, train_loss: 0.3902, step time: 0.2498\n",
      "268/281, train_loss: 0.4067, step time: 0.2481\n",
      "269/281, train_loss: 0.4634, step time: 0.2486\n",
      "270/281, train_loss: 0.3895, step time: 0.2520\n",
      "271/281, train_loss: 0.4221, step time: 0.2526\n",
      "272/281, train_loss: 0.5820, step time: 0.2525\n",
      "273/281, train_loss: 0.5486, step time: 0.2582\n",
      "274/281, train_loss: 0.4794, step time: 0.2571\n",
      "275/281, train_loss: 0.4163, step time: 0.2558\n",
      "276/281, train_loss: 0.3936, step time: 0.2540\n",
      "277/281, train_loss: 0.4396, step time: 0.2545\n",
      "278/281, train_loss: 0.4639, step time: 0.2508\n",
      "279/281, train_loss: 0.4866, step time: 0.2492\n",
      "280/281, train_loss: 0.4323, step time: 0.2500\n",
      "281/281, train_loss: 0.5328, step time: 0.2474\n",
      "282/281, train_loss: 0.3782, step time: 0.1513\n",
      "epoch 21 average loss: 0.4468\n",
      "current epoch: 21 current mean dice: 0.5634 tc: 0.0091 wt: 0.8946 et: 0.8179\n",
      "best mean dice: 0.5661 at epoch: 19\n",
      "time consuming of epoch 21 is: 441.2592\n",
      "----------\n",
      "epoch 22/200\n",
      "1/281, train_loss: 0.3980, step time: 0.2553\n",
      "2/281, train_loss: 0.3827, step time: 0.2555\n",
      "3/281, train_loss: 0.4228, step time: 0.2545\n",
      "4/281, train_loss: 0.4508, step time: 0.2550\n",
      "5/281, train_loss: 0.4165, step time: 0.2532\n",
      "6/281, train_loss: 0.3796, step time: 0.2560\n",
      "7/281, train_loss: 0.4049, step time: 0.2554\n",
      "8/281, train_loss: 0.4308, step time: 0.2578\n",
      "9/281, train_loss: 0.4146, step time: 0.2558\n",
      "10/281, train_loss: 0.3782, step time: 0.2500\n",
      "11/281, train_loss: 0.7043, step time: 0.2522\n",
      "12/281, train_loss: 0.3846, step time: 0.2507\n",
      "13/281, train_loss: 0.4323, step time: 0.2499\n",
      "14/281, train_loss: 0.4119, step time: 0.2555\n",
      "15/281, train_loss: 0.4569, step time: 0.2533\n",
      "16/281, train_loss: 0.4040, step time: 0.2567\n",
      "17/281, train_loss: 0.4211, step time: 0.2504\n",
      "18/281, train_loss: 0.3969, step time: 0.2492\n",
      "19/281, train_loss: 0.3837, step time: 0.2513\n",
      "20/281, train_loss: 0.4570, step time: 0.2616\n",
      "21/281, train_loss: 0.4353, step time: 0.2526\n",
      "22/281, train_loss: 0.4008, step time: 0.2531\n",
      "23/281, train_loss: 0.4156, step time: 0.2474\n",
      "24/281, train_loss: 0.5543, step time: 0.2517\n",
      "25/281, train_loss: 0.4999, step time: 0.2457\n",
      "26/281, train_loss: 0.4104, step time: 0.2437\n",
      "27/281, train_loss: 0.3756, step time: 0.2486\n",
      "28/281, train_loss: 0.3937, step time: 0.2445\n",
      "29/281, train_loss: 0.4245, step time: 0.2504\n",
      "30/281, train_loss: 0.4115, step time: 0.2535\n",
      "31/281, train_loss: 0.5635, step time: 0.2570\n",
      "32/281, train_loss: 0.4379, step time: 0.2524\n",
      "33/281, train_loss: 0.4054, step time: 0.2494\n",
      "34/281, train_loss: 0.4366, step time: 0.2523\n",
      "35/281, train_loss: 0.4515, step time: 0.2540\n",
      "36/281, train_loss: 0.4460, step time: 0.2489\n",
      "37/281, train_loss: 0.4317, step time: 0.2489\n",
      "38/281, train_loss: 0.4262, step time: 0.2566\n",
      "39/281, train_loss: 0.3907, step time: 0.2530\n",
      "40/281, train_loss: 0.3977, step time: 0.2543\n",
      "41/281, train_loss: 0.4179, step time: 0.2536\n",
      "42/281, train_loss: 0.3985, step time: 0.2508\n",
      "43/281, train_loss: 0.3899, step time: 0.2488\n",
      "44/281, train_loss: 0.4550, step time: 0.2526\n",
      "45/281, train_loss: 0.4021, step time: 0.2522\n",
      "46/281, train_loss: 0.5910, step time: 0.2622\n",
      "47/281, train_loss: 0.3922, step time: 0.2527\n",
      "48/281, train_loss: 0.5320, step time: 0.2515\n",
      "49/281, train_loss: 0.4980, step time: 0.2509\n",
      "50/281, train_loss: 0.4023, step time: 0.2490\n",
      "51/281, train_loss: 0.4396, step time: 0.2538\n",
      "52/281, train_loss: 0.4513, step time: 0.2548\n",
      "53/281, train_loss: 0.4378, step time: 0.2471\n",
      "54/281, train_loss: 0.5028, step time: 0.2507\n",
      "55/281, train_loss: 0.4495, step time: 0.2481\n",
      "56/281, train_loss: 0.4070, step time: 0.2535\n",
      "57/281, train_loss: 0.5512, step time: 0.2487\n",
      "58/281, train_loss: 0.4208, step time: 0.2505\n",
      "59/281, train_loss: 0.5450, step time: 0.2475\n",
      "60/281, train_loss: 0.4052, step time: 0.2524\n",
      "61/281, train_loss: 0.4521, step time: 0.2529\n",
      "62/281, train_loss: 0.5561, step time: 0.2501\n",
      "63/281, train_loss: 0.4041, step time: 0.2739\n",
      "64/281, train_loss: 0.5490, step time: 0.2583\n",
      "65/281, train_loss: 0.4179, step time: 0.2587\n",
      "66/281, train_loss: 0.4910, step time: 0.2654\n",
      "67/281, train_loss: 0.4142, step time: 0.2524\n",
      "68/281, train_loss: 0.6976, step time: 0.2558\n",
      "69/281, train_loss: 0.4278, step time: 0.2559\n",
      "70/281, train_loss: 0.4016, step time: 0.2583\n",
      "71/281, train_loss: 0.3823, step time: 0.2552\n",
      "72/281, train_loss: 0.3968, step time: 0.2531\n",
      "73/281, train_loss: 0.4242, step time: 0.2530\n",
      "74/281, train_loss: 0.4281, step time: 0.2534\n",
      "75/281, train_loss: 0.3911, step time: 0.2535\n",
      "76/281, train_loss: 0.4140, step time: 0.2518\n",
      "77/281, train_loss: 0.4320, step time: 0.2523\n",
      "78/281, train_loss: 0.4041, step time: 0.2502\n",
      "79/281, train_loss: 0.4161, step time: 0.2622\n",
      "80/281, train_loss: 0.4078, step time: 0.2566\n",
      "81/281, train_loss: 0.4112, step time: 0.2556\n",
      "82/281, train_loss: 0.3995, step time: 0.2531\n",
      "83/281, train_loss: 0.4068, step time: 0.2523\n",
      "84/281, train_loss: 0.4444, step time: 0.2559\n",
      "85/281, train_loss: 0.5560, step time: 0.2561\n",
      "86/281, train_loss: 0.3739, step time: 0.2501\n",
      "87/281, train_loss: 0.4532, step time: 0.2552\n",
      "88/281, train_loss: 0.4520, step time: 0.2571\n",
      "89/281, train_loss: 0.4131, step time: 0.2522\n",
      "90/281, train_loss: 0.4672, step time: 0.2512\n",
      "91/281, train_loss: 0.4869, step time: 0.2458\n",
      "92/281, train_loss: 0.4322, step time: 0.2539\n",
      "93/281, train_loss: 0.5363, step time: 0.2544\n",
      "94/281, train_loss: 0.4160, step time: 0.2511\n",
      "95/281, train_loss: 0.4044, step time: 0.2531\n",
      "96/281, train_loss: 0.5563, step time: 0.2614\n",
      "97/281, train_loss: 0.4575, step time: 0.2544\n",
      "98/281, train_loss: 0.3891, step time: 0.2513\n",
      "99/281, train_loss: 0.4293, step time: 0.2460\n",
      "100/281, train_loss: 0.4204, step time: 0.2527\n",
      "101/281, train_loss: 0.4489, step time: 0.2489\n",
      "102/281, train_loss: 0.3877, step time: 0.2529\n",
      "103/281, train_loss: 0.4235, step time: 0.2546\n",
      "104/281, train_loss: 0.4066, step time: 0.2533\n",
      "105/281, train_loss: 0.3998, step time: 0.2524\n",
      "106/281, train_loss: 0.4156, step time: 0.2496\n",
      "107/281, train_loss: 0.5286, step time: 0.2509\n",
      "108/281, train_loss: 0.4100, step time: 0.2505\n",
      "109/281, train_loss: 0.3780, step time: 0.2521\n",
      "110/281, train_loss: 0.3985, step time: 0.2542\n",
      "111/281, train_loss: 0.4058, step time: 0.2510\n",
      "112/281, train_loss: 0.4035, step time: 0.2494\n",
      "113/281, train_loss: 0.4199, step time: 0.2517\n",
      "114/281, train_loss: 0.4733, step time: 0.2562\n",
      "115/281, train_loss: 0.5638, step time: 0.2550\n",
      "116/281, train_loss: 0.4508, step time: 0.2530\n",
      "117/281, train_loss: 0.5512, step time: 0.2511\n",
      "118/281, train_loss: 0.3991, step time: 0.2529\n",
      "119/281, train_loss: 0.4314, step time: 0.2524\n",
      "120/281, train_loss: 0.3882, step time: 0.2552\n",
      "121/281, train_loss: 0.3825, step time: 0.2536\n",
      "122/281, train_loss: 0.4135, step time: 0.2545\n",
      "123/281, train_loss: 0.4153, step time: 0.2493\n",
      "124/281, train_loss: 0.4292, step time: 0.2514\n",
      "125/281, train_loss: 0.4105, step time: 0.2565\n",
      "126/281, train_loss: 0.5421, step time: 0.2548\n",
      "127/281, train_loss: 0.5310, step time: 0.2506\n",
      "128/281, train_loss: 0.4324, step time: 0.2564\n",
      "129/281, train_loss: 0.4225, step time: 0.2532\n",
      "130/281, train_loss: 0.4328, step time: 0.2475\n",
      "131/281, train_loss: 0.4433, step time: 0.2525\n",
      "132/281, train_loss: 0.3857, step time: 0.2536\n",
      "133/281, train_loss: 0.4408, step time: 0.2527\n",
      "134/281, train_loss: 0.4160, step time: 0.2503\n",
      "135/281, train_loss: 0.5140, step time: 0.2516\n",
      "136/281, train_loss: 0.4248, step time: 0.2541\n",
      "137/281, train_loss: 0.4194, step time: 0.2579\n",
      "138/281, train_loss: 0.4275, step time: 0.2582\n",
      "139/281, train_loss: 0.4194, step time: 0.2588\n",
      "140/281, train_loss: 0.3884, step time: 0.2535\n",
      "141/281, train_loss: 0.4056, step time: 0.2495\n",
      "142/281, train_loss: 0.5473, step time: 0.2443\n",
      "143/281, train_loss: 0.3990, step time: 0.2451\n",
      "144/281, train_loss: 0.5464, step time: 0.2514\n",
      "145/281, train_loss: 0.4094, step time: 0.2586\n",
      "146/281, train_loss: 0.5700, step time: 0.2530\n",
      "147/281, train_loss: 0.4055, step time: 0.2474\n",
      "148/281, train_loss: 0.5661, step time: 0.2503\n",
      "149/281, train_loss: 0.4144, step time: 0.2465\n",
      "150/281, train_loss: 0.3833, step time: 0.2441\n",
      "151/281, train_loss: 0.3936, step time: 0.2497\n",
      "152/281, train_loss: 0.4156, step time: 0.2546\n",
      "153/281, train_loss: 0.4966, step time: 0.2532\n",
      "154/281, train_loss: 0.4454, step time: 0.2450\n",
      "155/281, train_loss: 0.4041, step time: 0.2457\n",
      "156/281, train_loss: 0.5868, step time: 0.2482\n",
      "157/281, train_loss: 0.4050, step time: 0.2484\n",
      "158/281, train_loss: 0.4372, step time: 0.2477\n",
      "159/281, train_loss: 0.4314, step time: 0.2465\n",
      "160/281, train_loss: 0.4202, step time: 0.2499\n",
      "161/281, train_loss: 0.3876, step time: 0.2509\n",
      "162/281, train_loss: 0.4270, step time: 0.2534\n",
      "163/281, train_loss: 0.5771, step time: 0.2487\n",
      "164/281, train_loss: 0.3863, step time: 0.2543\n",
      "165/281, train_loss: 0.3835, step time: 0.2566\n",
      "166/281, train_loss: 0.4383, step time: 0.2507\n",
      "167/281, train_loss: 0.4369, step time: 0.2523\n",
      "168/281, train_loss: 0.4436, step time: 0.2533\n",
      "169/281, train_loss: 0.4163, step time: 0.2522\n",
      "170/281, train_loss: 0.4018, step time: 0.2510\n",
      "171/281, train_loss: 0.3950, step time: 0.2530\n",
      "172/281, train_loss: 0.3983, step time: 0.2516\n",
      "173/281, train_loss: 0.5012, step time: 0.2502\n",
      "174/281, train_loss: 0.4371, step time: 0.2492\n",
      "175/281, train_loss: 0.3947, step time: 0.2477\n",
      "176/281, train_loss: 0.4111, step time: 0.2479\n",
      "177/281, train_loss: 0.3887, step time: 0.2525\n",
      "178/281, train_loss: 0.3692, step time: 0.2519\n",
      "179/281, train_loss: 0.5881, step time: 0.2540\n",
      "180/281, train_loss: 0.4151, step time: 0.2480\n",
      "181/281, train_loss: 0.4138, step time: 0.2492\n",
      "182/281, train_loss: 0.5475, step time: 0.2488\n",
      "183/281, train_loss: 0.4176, step time: 0.2471\n",
      "184/281, train_loss: 0.5813, step time: 0.2551\n",
      "185/281, train_loss: 0.4337, step time: 0.2520\n",
      "186/281, train_loss: 0.5814, step time: 0.2519\n",
      "187/281, train_loss: 0.4188, step time: 0.2509\n",
      "188/281, train_loss: 0.4138, step time: 0.2502\n",
      "189/281, train_loss: 0.4049, step time: 0.2481\n",
      "190/281, train_loss: 0.6982, step time: 0.2489\n",
      "191/281, train_loss: 0.4807, step time: 0.2470\n",
      "192/281, train_loss: 0.3954, step time: 0.2513\n",
      "193/281, train_loss: 0.5527, step time: 0.2455\n",
      "194/281, train_loss: 0.4302, step time: 0.2457\n",
      "195/281, train_loss: 0.5420, step time: 0.2470\n",
      "196/281, train_loss: 0.3888, step time: 0.2507\n",
      "197/281, train_loss: 0.4036, step time: 0.2508\n",
      "198/281, train_loss: 0.3960, step time: 0.2535\n",
      "199/281, train_loss: 0.4083, step time: 0.2525\n",
      "200/281, train_loss: 0.4723, step time: 0.2558\n",
      "201/281, train_loss: 0.4079, step time: 0.2543\n",
      "202/281, train_loss: 0.5810, step time: 0.2455\n",
      "203/281, train_loss: 0.3971, step time: 0.2540\n",
      "204/281, train_loss: 0.3995, step time: 0.2471\n",
      "205/281, train_loss: 0.5902, step time: 0.2504\n",
      "206/281, train_loss: 0.3942, step time: 0.2521\n",
      "207/281, train_loss: 0.5917, step time: 0.2515\n",
      "208/281, train_loss: 0.3868, step time: 0.2483\n",
      "209/281, train_loss: 0.3890, step time: 0.2498\n",
      "210/281, train_loss: 0.5916, step time: 0.2513\n",
      "211/281, train_loss: 0.5663, step time: 0.2465\n",
      "212/281, train_loss: 0.4097, step time: 0.2470\n",
      "213/281, train_loss: 0.6039, step time: 0.2520\n",
      "214/281, train_loss: 0.3979, step time: 0.2566\n",
      "215/281, train_loss: 0.4051, step time: 0.2554\n",
      "216/281, train_loss: 0.3911, step time: 0.2480\n",
      "217/281, train_loss: 0.4227, step time: 0.2519\n",
      "218/281, train_loss: 0.4124, step time: 0.2474\n",
      "219/281, train_loss: 0.4069, step time: 0.2482\n",
      "220/281, train_loss: 0.6988, step time: 0.2594\n",
      "221/281, train_loss: 0.4157, step time: 0.2523\n",
      "222/281, train_loss: 0.4068, step time: 0.2487\n",
      "223/281, train_loss: 0.3820, step time: 0.2501\n",
      "224/281, train_loss: 0.4070, step time: 0.2607\n",
      "225/281, train_loss: 0.5522, step time: 0.2571\n",
      "226/281, train_loss: 0.5432, step time: 0.2503\n",
      "227/281, train_loss: 0.4158, step time: 0.2501\n",
      "228/281, train_loss: 0.5525, step time: 0.2526\n",
      "229/281, train_loss: 0.4329, step time: 0.2528\n",
      "230/281, train_loss: 0.4339, step time: 0.2479\n",
      "231/281, train_loss: 0.3787, step time: 0.2475\n",
      "232/281, train_loss: 0.4841, step time: 0.2664\n",
      "233/281, train_loss: 0.4111, step time: 0.2535\n",
      "234/281, train_loss: 0.4338, step time: 0.2521\n",
      "235/281, train_loss: 0.4388, step time: 0.2519\n",
      "236/281, train_loss: 0.4146, step time: 0.2520\n",
      "237/281, train_loss: 0.5387, step time: 0.2550\n",
      "238/281, train_loss: 0.4034, step time: 0.2559\n",
      "239/281, train_loss: 0.5360, step time: 0.2532\n",
      "240/281, train_loss: 0.4156, step time: 0.2520\n",
      "241/281, train_loss: 0.4155, step time: 0.2474\n",
      "242/281, train_loss: 0.4295, step time: 0.2482\n",
      "243/281, train_loss: 0.4077, step time: 0.2538\n",
      "244/281, train_loss: 0.4502, step time: 0.2535\n",
      "245/281, train_loss: 0.3987, step time: 0.2474\n",
      "246/281, train_loss: 0.5707, step time: 0.2456\n",
      "247/281, train_loss: 0.4667, step time: 0.2537\n",
      "248/281, train_loss: 0.3845, step time: 0.2533\n",
      "249/281, train_loss: 0.4028, step time: 0.2487\n",
      "250/281, train_loss: 0.4092, step time: 0.2547\n",
      "251/281, train_loss: 0.3747, step time: 0.2461\n",
      "252/281, train_loss: 0.4051, step time: 0.2485\n",
      "253/281, train_loss: 0.3744, step time: 0.2484\n",
      "254/281, train_loss: 0.3810, step time: 0.2453\n",
      "255/281, train_loss: 0.3956, step time: 0.2509\n",
      "256/281, train_loss: 0.4553, step time: 0.2452\n",
      "257/281, train_loss: 0.4336, step time: 0.2444\n",
      "258/281, train_loss: 0.4016, step time: 0.2467\n",
      "259/281, train_loss: 0.4083, step time: 0.2527\n",
      "260/281, train_loss: 0.4697, step time: 0.2548\n",
      "261/281, train_loss: 0.3744, step time: 0.2448\n",
      "262/281, train_loss: 0.4475, step time: 0.2468\n",
      "263/281, train_loss: 0.4726, step time: 0.2525\n",
      "264/281, train_loss: 0.4288, step time: 0.2523\n",
      "265/281, train_loss: 0.4415, step time: 0.2495\n",
      "266/281, train_loss: 0.4279, step time: 0.2514\n",
      "267/281, train_loss: 0.3985, step time: 0.2584\n",
      "268/281, train_loss: 0.3881, step time: 0.2577\n",
      "269/281, train_loss: 0.5442, step time: 0.2535\n",
      "270/281, train_loss: 0.4938, step time: 0.2505\n",
      "271/281, train_loss: 0.4224, step time: 0.2513\n",
      "272/281, train_loss: 0.4038, step time: 0.2505\n",
      "273/281, train_loss: 0.4187, step time: 0.2518\n",
      "274/281, train_loss: 0.3925, step time: 0.2506\n",
      "275/281, train_loss: 0.3867, step time: 0.2512\n",
      "276/281, train_loss: 0.5482, step time: 0.2515\n",
      "277/281, train_loss: 0.4006, step time: 0.2511\n",
      "278/281, train_loss: 0.3900, step time: 0.2479\n",
      "279/281, train_loss: 0.4917, step time: 0.2491\n",
      "280/281, train_loss: 0.5111, step time: 0.2485\n",
      "281/281, train_loss: 0.3998, step time: 0.2506\n",
      "282/281, train_loss: 0.4941, step time: 0.1514\n",
      "epoch 22 average loss: 0.4437\n",
      "current epoch: 22 current mean dice: 0.5593 tc: 0.0090 wt: 0.9073 et: 0.7892\n",
      "best mean dice: 0.5661 at epoch: 19\n",
      "time consuming of epoch 22 is: 400.0131\n",
      "----------\n",
      "epoch 23/200\n",
      "1/281, train_loss: 0.3987, step time: 0.2540\n",
      "2/281, train_loss: 0.4207, step time: 0.2483\n",
      "3/281, train_loss: 0.5653, step time: 0.2503\n",
      "4/281, train_loss: 0.5726, step time: 0.2502\n",
      "5/281, train_loss: 0.4081, step time: 0.2586\n",
      "6/281, train_loss: 0.4038, step time: 0.2489\n",
      "7/281, train_loss: 0.4200, step time: 0.2533\n",
      "8/281, train_loss: 0.3872, step time: 0.2507\n",
      "9/281, train_loss: 0.4041, step time: 0.2507\n",
      "10/281, train_loss: 0.4090, step time: 0.2497\n",
      "11/281, train_loss: 0.4205, step time: 0.2512\n",
      "12/281, train_loss: 0.4444, step time: 0.2494\n",
      "13/281, train_loss: 0.4106, step time: 0.2555\n",
      "14/281, train_loss: 0.4012, step time: 0.2513\n",
      "15/281, train_loss: 0.4109, step time: 0.2541\n",
      "16/281, train_loss: 0.5652, step time: 0.2586\n",
      "17/281, train_loss: 0.4697, step time: 0.2531\n",
      "18/281, train_loss: 0.4429, step time: 0.2508\n",
      "19/281, train_loss: 0.3973, step time: 0.2525\n",
      "20/281, train_loss: 0.4086, step time: 0.2582\n",
      "21/281, train_loss: 0.4379, step time: 0.2515\n",
      "22/281, train_loss: 0.4138, step time: 0.2488\n",
      "23/281, train_loss: 0.5585, step time: 0.2518\n",
      "24/281, train_loss: 0.3957, step time: 0.2508\n",
      "25/281, train_loss: 0.5377, step time: 0.2511\n",
      "26/281, train_loss: 0.3992, step time: 0.2487\n",
      "27/281, train_loss: 0.3903, step time: 0.2584\n",
      "28/281, train_loss: 0.3757, step time: 0.2566\n",
      "29/281, train_loss: 0.4126, step time: 0.2589\n",
      "30/281, train_loss: 0.4332, step time: 0.2549\n",
      "31/281, train_loss: 0.4071, step time: 0.2583\n",
      "32/281, train_loss: 0.3837, step time: 0.2575\n",
      "33/281, train_loss: 0.4575, step time: 0.2503\n",
      "34/281, train_loss: 0.4055, step time: 0.2559\n",
      "35/281, train_loss: 0.4724, step time: 0.2543\n",
      "36/281, train_loss: 0.4726, step time: 0.2502\n",
      "37/281, train_loss: 0.4031, step time: 0.2646\n",
      "38/281, train_loss: 0.4720, step time: 0.2744\n",
      "39/281, train_loss: 0.5313, step time: 0.2539\n",
      "40/281, train_loss: 0.4015, step time: 0.2523\n",
      "41/281, train_loss: 0.4020, step time: 0.2444\n",
      "42/281, train_loss: 0.4179, step time: 0.2510\n",
      "43/281, train_loss: 0.4016, step time: 0.2562\n",
      "44/281, train_loss: 0.5597, step time: 0.2517\n",
      "45/281, train_loss: 0.4081, step time: 0.2539\n",
      "46/281, train_loss: 0.5523, step time: 0.2557\n",
      "47/281, train_loss: 0.3862, step time: 0.2591\n",
      "48/281, train_loss: 0.5507, step time: 0.2607\n",
      "49/281, train_loss: 0.4116, step time: 0.2543\n",
      "50/281, train_loss: 0.3822, step time: 0.2545\n",
      "51/281, train_loss: 0.4409, step time: 0.2590\n",
      "52/281, train_loss: 0.4182, step time: 0.2559\n",
      "53/281, train_loss: 0.4427, step time: 0.2597\n",
      "54/281, train_loss: 0.6074, step time: 0.2546\n",
      "55/281, train_loss: 0.4234, step time: 0.2586\n",
      "56/281, train_loss: 0.4115, step time: 0.2584\n",
      "57/281, train_loss: 0.4721, step time: 0.2564\n",
      "58/281, train_loss: 0.4014, step time: 0.2641\n",
      "59/281, train_loss: 0.4408, step time: 0.2641\n",
      "60/281, train_loss: 0.4276, step time: 0.2640\n",
      "61/281, train_loss: 0.4116, step time: 0.2558\n",
      "62/281, train_loss: 0.3891, step time: 0.2585\n",
      "63/281, train_loss: 0.4166, step time: 0.2596\n",
      "64/281, train_loss: 0.4190, step time: 0.2585\n",
      "65/281, train_loss: 0.3783, step time: 0.2650\n",
      "66/281, train_loss: 0.4264, step time: 0.2588\n",
      "67/281, train_loss: 0.4096, step time: 0.2577\n",
      "68/281, train_loss: 0.4309, step time: 0.2582\n",
      "69/281, train_loss: 0.5621, step time: 0.2589\n",
      "70/281, train_loss: 0.4144, step time: 0.2595\n",
      "71/281, train_loss: 0.4386, step time: 0.2628\n",
      "72/281, train_loss: 0.4004, step time: 0.2686\n",
      "73/281, train_loss: 0.3961, step time: 0.2567\n",
      "74/281, train_loss: 0.3823, step time: 0.2575\n",
      "75/281, train_loss: 0.4195, step time: 0.2564\n",
      "76/281, train_loss: 0.4086, step time: 0.2512\n",
      "77/281, train_loss: 0.5945, step time: 0.2543\n",
      "78/281, train_loss: 0.4360, step time: 0.2576\n",
      "79/281, train_loss: 0.4255, step time: 0.2645\n",
      "80/281, train_loss: 0.4948, step time: 0.2571\n",
      "81/281, train_loss: 0.3785, step time: 0.2541\n",
      "82/281, train_loss: 0.4192, step time: 0.2596\n",
      "83/281, train_loss: 0.5782, step time: 0.2592\n",
      "84/281, train_loss: 0.4095, step time: 0.2593\n",
      "85/281, train_loss: 0.4043, step time: 0.2636\n",
      "86/281, train_loss: 0.3980, step time: 0.2689\n",
      "87/281, train_loss: 0.4054, step time: 0.2600\n",
      "88/281, train_loss: 0.4625, step time: 0.2598\n",
      "89/281, train_loss: 0.4145, step time: 0.2594\n",
      "90/281, train_loss: 0.3892, step time: 0.2631\n",
      "91/281, train_loss: 0.4251, step time: 0.2652\n",
      "92/281, train_loss: 0.4173, step time: 0.2583\n",
      "93/281, train_loss: 0.4287, step time: 0.2539\n",
      "94/281, train_loss: 0.4257, step time: 0.2552\n",
      "95/281, train_loss: 0.3899, step time: 0.2576\n",
      "96/281, train_loss: 0.4327, step time: 0.2576\n",
      "97/281, train_loss: 0.4090, step time: 0.2613\n",
      "98/281, train_loss: 0.3883, step time: 0.2595\n",
      "99/281, train_loss: 0.5009, step time: 0.2529\n",
      "100/281, train_loss: 0.5531, step time: 0.2569\n",
      "101/281, train_loss: 0.4702, step time: 0.2558\n",
      "102/281, train_loss: 0.4577, step time: 0.2552\n",
      "103/281, train_loss: 0.4063, step time: 0.2583\n",
      "104/281, train_loss: 0.4348, step time: 0.2576\n",
      "105/281, train_loss: 0.4148, step time: 0.2565\n",
      "106/281, train_loss: 0.5689, step time: 0.2657\n",
      "107/281, train_loss: 0.4018, step time: 0.2606\n",
      "108/281, train_loss: 0.4326, step time: 0.2554\n",
      "109/281, train_loss: 0.4134, step time: 0.2602\n",
      "110/281, train_loss: 0.4494, step time: 0.2597\n",
      "111/281, train_loss: 0.4170, step time: 0.2561\n",
      "112/281, train_loss: 0.3780, step time: 0.2581\n",
      "113/281, train_loss: 0.3893, step time: 0.2582\n",
      "114/281, train_loss: 0.4116, step time: 0.2627\n",
      "115/281, train_loss: 0.5498, step time: 0.2619\n",
      "116/281, train_loss: 0.3789, step time: 0.2595\n",
      "117/281, train_loss: 0.4199, step time: 0.2569\n",
      "118/281, train_loss: 0.3998, step time: 0.2540\n",
      "119/281, train_loss: 0.4595, step time: 0.2585\n",
      "120/281, train_loss: 0.4005, step time: 0.2534\n",
      "121/281, train_loss: 0.3997, step time: 0.2584\n",
      "122/281, train_loss: 0.4216, step time: 0.2638\n",
      "123/281, train_loss: 0.3781, step time: 0.2579\n",
      "124/281, train_loss: 0.4071, step time: 0.2546\n",
      "125/281, train_loss: 0.3789, step time: 0.2579\n",
      "126/281, train_loss: 0.4176, step time: 0.2567\n",
      "127/281, train_loss: 0.3862, step time: 0.2581\n",
      "128/281, train_loss: 0.5453, step time: 0.2565\n",
      "129/281, train_loss: 0.6938, step time: 0.2516\n",
      "130/281, train_loss: 0.3752, step time: 0.2497\n",
      "131/281, train_loss: 0.4144, step time: 0.2531\n",
      "132/281, train_loss: 0.4078, step time: 0.2556\n",
      "133/281, train_loss: 0.3720, step time: 0.2602\n",
      "134/281, train_loss: 0.5725, step time: 0.2533\n",
      "135/281, train_loss: 0.3899, step time: 0.2598\n",
      "136/281, train_loss: 0.4149, step time: 0.2519\n",
      "137/281, train_loss: 0.4187, step time: 0.2517\n",
      "138/281, train_loss: 0.4970, step time: 0.2525\n",
      "139/281, train_loss: 0.4680, step time: 0.2554\n",
      "140/281, train_loss: 0.4192, step time: 0.2593\n",
      "141/281, train_loss: 0.4233, step time: 0.2610\n",
      "142/281, train_loss: 0.5301, step time: 0.2598\n",
      "143/281, train_loss: 0.4414, step time: 0.2501\n",
      "144/281, train_loss: 0.3865, step time: 0.2543\n",
      "145/281, train_loss: 0.4152, step time: 0.2515\n",
      "146/281, train_loss: 0.4327, step time: 0.2524\n",
      "147/281, train_loss: 0.4135, step time: 0.2597\n",
      "148/281, train_loss: 0.4226, step time: 0.2586\n",
      "149/281, train_loss: 0.3932, step time: 0.2571\n",
      "150/281, train_loss: 0.5551, step time: 0.2566\n",
      "151/281, train_loss: 0.4133, step time: 0.2518\n",
      "152/281, train_loss: 0.3780, step time: 0.2569\n",
      "153/281, train_loss: 0.3781, step time: 0.2574\n",
      "154/281, train_loss: 0.3933, step time: 0.2599\n",
      "155/281, train_loss: 0.3942, step time: 0.2582\n",
      "156/281, train_loss: 0.5384, step time: 0.2513\n",
      "157/281, train_loss: 0.4588, step time: 0.2525\n",
      "158/281, train_loss: 0.4031, step time: 0.2517\n",
      "159/281, train_loss: 0.4201, step time: 0.2506\n",
      "160/281, train_loss: 0.4597, step time: 0.2604\n",
      "161/281, train_loss: 0.4714, step time: 0.2281\n",
      "162/281, train_loss: 0.4256, step time: 0.2540\n",
      "163/281, train_loss: 0.4219, step time: 0.2510\n",
      "164/281, train_loss: 0.5623, step time: 0.2543\n",
      "165/281, train_loss: 0.4081, step time: 0.2541\n",
      "166/281, train_loss: 0.3994, step time: 0.2600\n",
      "167/281, train_loss: 0.3918, step time: 0.2577\n",
      "168/281, train_loss: 0.3986, step time: 0.2548\n",
      "169/281, train_loss: 0.5375, step time: 0.2484\n",
      "170/281, train_loss: 0.5636, step time: 0.2495\n",
      "171/281, train_loss: 0.6183, step time: 0.2493\n",
      "172/281, train_loss: 0.4052, step time: 0.2527\n",
      "173/281, train_loss: 0.4583, step time: 0.2517\n",
      "174/281, train_loss: 0.5645, step time: 0.2566\n",
      "175/281, train_loss: 0.4191, step time: 0.2524\n",
      "176/281, train_loss: 0.3835, step time: 0.2532\n",
      "177/281, train_loss: 0.5450, step time: 0.2519\n",
      "178/281, train_loss: 0.3996, step time: 0.2570\n",
      "179/281, train_loss: 0.4036, step time: 0.2525\n",
      "180/281, train_loss: 0.4455, step time: 0.2566\n",
      "181/281, train_loss: 0.4121, step time: 0.2585\n",
      "182/281, train_loss: 0.4217, step time: 0.2538\n",
      "183/281, train_loss: 0.5592, step time: 0.2518\n",
      "184/281, train_loss: 0.3842, step time: 0.2566\n",
      "185/281, train_loss: 0.5033, step time: 0.2610\n",
      "186/281, train_loss: 0.4091, step time: 0.2596\n",
      "187/281, train_loss: 0.4533, step time: 0.2564\n",
      "188/281, train_loss: 0.3869, step time: 0.2549\n",
      "189/281, train_loss: 0.3916, step time: 0.2513\n",
      "190/281, train_loss: 0.4857, step time: 0.2516\n",
      "191/281, train_loss: 0.4425, step time: 0.2535\n",
      "192/281, train_loss: 0.4382, step time: 0.2619\n",
      "193/281, train_loss: 0.4701, step time: 0.2530\n",
      "194/281, train_loss: 0.4535, step time: 0.2511\n",
      "195/281, train_loss: 0.5412, step time: 0.2523\n",
      "196/281, train_loss: 0.4256, step time: 0.2541\n",
      "197/281, train_loss: 0.4531, step time: 0.2568\n",
      "198/281, train_loss: 0.5677, step time: 0.2580\n",
      "199/281, train_loss: 0.3827, step time: 0.2565\n",
      "200/281, train_loss: 0.4260, step time: 0.2531\n",
      "201/281, train_loss: 0.5479, step time: 0.2568\n",
      "202/281, train_loss: 0.5539, step time: 0.2582\n",
      "203/281, train_loss: 0.3960, step time: 0.2509\n",
      "204/281, train_loss: 0.3826, step time: 0.2566\n",
      "205/281, train_loss: 0.4137, step time: 0.2527\n",
      "206/281, train_loss: 0.3830, step time: 0.2526\n",
      "207/281, train_loss: 0.4226, step time: 0.2574\n",
      "208/281, train_loss: 0.3862, step time: 0.2575\n",
      "209/281, train_loss: 0.5629, step time: 0.2501\n",
      "210/281, train_loss: 0.4335, step time: 0.2528\n",
      "211/281, train_loss: 0.4193, step time: 0.2557\n",
      "212/281, train_loss: 0.4178, step time: 0.2488\n",
      "213/281, train_loss: 0.4256, step time: 0.2519\n",
      "214/281, train_loss: 0.4124, step time: 0.2507\n",
      "215/281, train_loss: 0.4027, step time: 0.2497\n",
      "216/281, train_loss: 0.6108, step time: 0.2557\n",
      "217/281, train_loss: 0.4539, step time: 0.2590\n",
      "218/281, train_loss: 0.5251, step time: 0.2539\n",
      "219/281, train_loss: 0.4198, step time: 0.2576\n",
      "220/281, train_loss: 0.5263, step time: 0.2546\n",
      "221/281, train_loss: 0.3889, step time: 0.2605\n",
      "222/281, train_loss: 0.4440, step time: 0.2603\n",
      "223/281, train_loss: 0.4143, step time: 0.2584\n",
      "224/281, train_loss: 0.4362, step time: 0.2538\n",
      "225/281, train_loss: 0.4149, step time: 0.2580\n",
      "226/281, train_loss: 0.4131, step time: 0.2587\n",
      "227/281, train_loss: 0.4080, step time: 0.2548\n",
      "228/281, train_loss: 0.5450, step time: 0.2566\n",
      "229/281, train_loss: 0.4160, step time: 0.2562\n",
      "230/281, train_loss: 0.5408, step time: 0.2600\n",
      "231/281, train_loss: 0.5595, step time: 0.2578\n",
      "232/281, train_loss: 0.4160, step time: 0.2579\n",
      "233/281, train_loss: 0.4071, step time: 0.2557\n",
      "234/281, train_loss: 0.3769, step time: 0.2546\n",
      "235/281, train_loss: 0.5415, step time: 0.2566\n",
      "236/281, train_loss: 0.4120, step time: 0.2547\n",
      "237/281, train_loss: 0.5563, step time: 0.2594\n",
      "238/281, train_loss: 0.4051, step time: 0.2598\n",
      "239/281, train_loss: 0.5445, step time: 0.2606\n",
      "240/281, train_loss: 0.4012, step time: 0.2570\n",
      "241/281, train_loss: 0.4139, step time: 0.2595\n",
      "242/281, train_loss: 0.4057, step time: 0.2536\n",
      "243/281, train_loss: 0.3858, step time: 0.2562\n",
      "244/281, train_loss: 0.3848, step time: 0.2581\n",
      "245/281, train_loss: 0.5392, step time: 0.2536\n",
      "246/281, train_loss: 0.3891, step time: 0.2572\n",
      "247/281, train_loss: 0.3693, step time: 0.2529\n",
      "248/281, train_loss: 0.3835, step time: 0.2520\n",
      "249/281, train_loss: 0.4271, step time: 0.2505\n",
      "250/281, train_loss: 0.3872, step time: 0.2504\n",
      "251/281, train_loss: 0.4198, step time: 0.2495\n",
      "252/281, train_loss: 0.4051, step time: 0.2508\n",
      "253/281, train_loss: 0.5510, step time: 0.2536\n",
      "254/281, train_loss: 0.4130, step time: 0.2580\n",
      "255/281, train_loss: 0.4228, step time: 0.2580\n",
      "256/281, train_loss: 0.4073, step time: 0.2520\n",
      "257/281, train_loss: 0.5961, step time: 0.2547\n",
      "258/281, train_loss: 0.4331, step time: 0.2573\n",
      "259/281, train_loss: 0.4130, step time: 0.2493\n",
      "260/281, train_loss: 0.4326, step time: 0.2506\n",
      "261/281, train_loss: 0.3868, step time: 0.2505\n",
      "262/281, train_loss: 0.5662, step time: 0.2490\n",
      "263/281, train_loss: 0.3937, step time: 0.2458\n",
      "264/281, train_loss: 0.5332, step time: 0.2476\n",
      "265/281, train_loss: 0.5710, step time: 0.2465\n",
      "266/281, train_loss: 0.4065, step time: 0.2499\n",
      "267/281, train_loss: 0.4131, step time: 0.2430\n",
      "268/281, train_loss: 0.4114, step time: 0.2547\n",
      "269/281, train_loss: 0.4150, step time: 0.2559\n",
      "270/281, train_loss: 0.3968, step time: 0.2554\n",
      "271/281, train_loss: 0.4153, step time: 0.2513\n",
      "272/281, train_loss: 0.3845, step time: 0.2527\n",
      "273/281, train_loss: 0.4164, step time: 0.2552\n",
      "274/281, train_loss: 0.3902, step time: 0.2488\n",
      "275/281, train_loss: 0.4087, step time: 0.2476\n",
      "276/281, train_loss: 0.4268, step time: 0.2496\n",
      "277/281, train_loss: 0.5554, step time: 0.2531\n",
      "278/281, train_loss: 0.3942, step time: 0.2532\n",
      "279/281, train_loss: 0.3811, step time: 0.2560\n",
      "280/281, train_loss: 0.4578, step time: 0.2479\n",
      "281/281, train_loss: 0.3863, step time: 0.2455\n",
      "282/281, train_loss: 0.4049, step time: 0.1488\n",
      "epoch 23 average loss: 0.4408\n",
      "saved new best metric model\n",
      "current epoch: 23 current mean dice: 0.5714 tc: 0.0092 wt: 0.9101 et: 0.8249\n",
      "best mean dice: 0.5714 at epoch: 23\n",
      "time consuming of epoch 23 is: 406.6127\n",
      "----------\n",
      "epoch 24/200\n",
      "1/281, train_loss: 0.5440, step time: 0.2556\n",
      "2/281, train_loss: 0.4479, step time: 0.2498\n",
      "3/281, train_loss: 0.3883, step time: 0.2528\n",
      "4/281, train_loss: 0.4290, step time: 0.2583\n",
      "5/281, train_loss: 0.4096, step time: 0.2557\n",
      "6/281, train_loss: 0.4051, step time: 0.2473\n",
      "7/281, train_loss: 0.3856, step time: 0.2483\n",
      "8/281, train_loss: 0.4078, step time: 0.2478\n",
      "9/281, train_loss: 0.3994, step time: 0.2506\n",
      "10/281, train_loss: 0.4535, step time: 0.2647\n",
      "11/281, train_loss: 0.4821, step time: 0.2510\n",
      "12/281, train_loss: 0.5502, step time: 0.2485\n",
      "13/281, train_loss: 0.5363, step time: 0.2467\n",
      "14/281, train_loss: 0.4097, step time: 0.2516\n",
      "15/281, train_loss: 0.3875, step time: 0.2581\n",
      "16/281, train_loss: 0.4013, step time: 0.2475\n",
      "17/281, train_loss: 0.5561, step time: 0.2464\n",
      "18/281, train_loss: 0.3859, step time: 0.2569\n",
      "19/281, train_loss: 0.3998, step time: 0.2522\n",
      "20/281, train_loss: 0.4386, step time: 0.2543\n",
      "21/281, train_loss: 0.4111, step time: 0.2449\n",
      "22/281, train_loss: 0.3849, step time: 0.2488\n",
      "23/281, train_loss: 0.3978, step time: 0.2432\n",
      "24/281, train_loss: 0.3889, step time: 0.2538\n",
      "25/281, train_loss: 0.4346, step time: 0.2531\n",
      "26/281, train_loss: 0.4275, step time: 0.2550\n",
      "27/281, train_loss: 0.4283, step time: 0.2538\n",
      "28/281, train_loss: 0.5398, step time: 0.2471\n",
      "29/281, train_loss: 0.4076, step time: 0.2464\n",
      "30/281, train_loss: 0.3933, step time: 0.2505\n",
      "31/281, train_loss: 0.4089, step time: 0.2502\n",
      "32/281, train_loss: 0.5522, step time: 0.2490\n",
      "33/281, train_loss: 0.4152, step time: 0.2556\n",
      "34/281, train_loss: 0.3807, step time: 0.2553\n",
      "35/281, train_loss: 0.5419, step time: 0.2505\n",
      "36/281, train_loss: 0.5536, step time: 0.2563\n",
      "37/281, train_loss: 0.5458, step time: 0.2551\n",
      "38/281, train_loss: 0.3855, step time: 0.2568\n",
      "39/281, train_loss: 0.4099, step time: 0.2437\n",
      "40/281, train_loss: 0.4392, step time: 0.2472\n",
      "41/281, train_loss: 0.4299, step time: 0.2490\n",
      "42/281, train_loss: 0.4456, step time: 0.2498\n",
      "43/281, train_loss: 0.4240, step time: 0.2539\n",
      "44/281, train_loss: 0.3960, step time: 0.2607\n",
      "45/281, train_loss: 0.5400, step time: 0.2572\n",
      "46/281, train_loss: 0.3964, step time: 0.2487\n",
      "47/281, train_loss: 0.4465, step time: 0.2537\n",
      "48/281, train_loss: 0.4255, step time: 0.2497\n",
      "49/281, train_loss: 0.4219, step time: 0.2473\n",
      "50/281, train_loss: 0.3921, step time: 0.2443\n",
      "51/281, train_loss: 0.6972, step time: 0.2493\n",
      "52/281, train_loss: 0.4003, step time: 0.2448\n",
      "53/281, train_loss: 0.4041, step time: 0.2458\n",
      "54/281, train_loss: 0.4051, step time: 0.2464\n",
      "55/281, train_loss: 0.4337, step time: 0.2480\n",
      "56/281, train_loss: 0.5004, step time: 0.2556\n",
      "57/281, train_loss: 0.3758, step time: 0.2577\n",
      "58/281, train_loss: 0.5191, step time: 0.2519\n",
      "59/281, train_loss: 0.4963, step time: 0.2537\n",
      "60/281, train_loss: 0.4257, step time: 0.2546\n",
      "61/281, train_loss: 0.4294, step time: 0.2549\n",
      "62/281, train_loss: 0.5417, step time: 0.2500\n",
      "63/281, train_loss: 0.3959, step time: 0.2533\n",
      "64/281, train_loss: 0.4567, step time: 0.2558\n",
      "65/281, train_loss: 0.4433, step time: 0.2540\n",
      "66/281, train_loss: 0.4633, step time: 0.2505\n",
      "67/281, train_loss: 0.4121, step time: 0.2512\n",
      "68/281, train_loss: 0.4579, step time: 0.2529\n",
      "69/281, train_loss: 0.3988, step time: 0.2476\n",
      "70/281, train_loss: 0.4036, step time: 0.2521\n",
      "71/281, train_loss: 0.3835, step time: 0.2519\n",
      "72/281, train_loss: 0.4057, step time: 0.2542\n",
      "73/281, train_loss: 0.4180, step time: 0.2539\n",
      "74/281, train_loss: 0.5673, step time: 0.2487\n",
      "75/281, train_loss: 0.3811, step time: 0.2528\n",
      "76/281, train_loss: 0.3807, step time: 0.2532\n",
      "77/281, train_loss: 0.3819, step time: 0.2506\n",
      "78/281, train_loss: 0.4728, step time: 0.2491\n",
      "79/281, train_loss: 0.4034, step time: 0.2530\n",
      "80/281, train_loss: 0.4175, step time: 0.2529\n",
      "81/281, train_loss: 0.4021, step time: 0.2472\n",
      "82/281, train_loss: 0.4007, step time: 0.2497\n",
      "83/281, train_loss: 0.4092, step time: 0.2481\n",
      "84/281, train_loss: 0.3813, step time: 0.2483\n",
      "85/281, train_loss: 0.3915, step time: 0.2460\n",
      "86/281, train_loss: 0.4318, step time: 0.2489\n",
      "87/281, train_loss: 0.3918, step time: 0.2475\n",
      "88/281, train_loss: 0.5237, step time: 0.2520\n",
      "89/281, train_loss: 0.4336, step time: 0.2519\n",
      "90/281, train_loss: 0.6813, step time: 0.2497\n",
      "91/281, train_loss: 0.4032, step time: 0.2517\n",
      "92/281, train_loss: 0.4027, step time: 0.2484\n",
      "93/281, train_loss: 0.3957, step time: 0.2470\n",
      "94/281, train_loss: 0.4055, step time: 0.2488\n",
      "95/281, train_loss: 0.4361, step time: 0.2520\n",
      "96/281, train_loss: 0.4556, step time: 0.2465\n",
      "97/281, train_loss: 0.4091, step time: 0.2440\n",
      "98/281, train_loss: 0.4244, step time: 0.2417\n",
      "99/281, train_loss: 0.4265, step time: 0.2513\n",
      "100/281, train_loss: 0.3910, step time: 0.2496\n",
      "101/281, train_loss: 0.5171, step time: 0.2549\n",
      "102/281, train_loss: 0.3998, step time: 0.2563\n",
      "103/281, train_loss: 0.4084, step time: 0.2420\n",
      "104/281, train_loss: 0.4264, step time: 0.2450\n",
      "105/281, train_loss: 0.4030, step time: 0.2570\n",
      "106/281, train_loss: 0.4217, step time: 0.2557\n",
      "107/281, train_loss: 0.3932, step time: 0.2608\n",
      "108/281, train_loss: 0.5642, step time: 0.2509\n",
      "109/281, train_loss: 0.5296, step time: 0.2512\n",
      "110/281, train_loss: 0.3820, step time: 0.2482\n",
      "111/281, train_loss: 0.4105, step time: 0.2492\n",
      "112/281, train_loss: 0.4981, step time: 0.2502\n",
      "113/281, train_loss: 0.3982, step time: 0.2460\n",
      "114/281, train_loss: 0.5527, step time: 0.2494\n",
      "115/281, train_loss: 0.5302, step time: 0.2533\n",
      "116/281, train_loss: 0.3807, step time: 0.2477\n",
      "117/281, train_loss: 0.4399, step time: 0.2540\n",
      "118/281, train_loss: 0.4037, step time: 0.2623\n",
      "119/281, train_loss: 0.5291, step time: 0.2489\n",
      "120/281, train_loss: 0.3986, step time: 0.2515\n",
      "121/281, train_loss: 0.3906, step time: 0.2472\n",
      "122/281, train_loss: 0.5213, step time: 0.2460\n",
      "123/281, train_loss: 0.4175, step time: 0.2522\n",
      "124/281, train_loss: 0.4330, step time: 0.2545\n",
      "125/281, train_loss: 0.4252, step time: 0.2479\n",
      "126/281, train_loss: 0.4364, step time: 0.2520\n",
      "127/281, train_loss: 0.5648, step time: 0.2496\n",
      "128/281, train_loss: 0.4211, step time: 0.2561\n",
      "129/281, train_loss: 0.5449, step time: 0.2552\n",
      "130/281, train_loss: 0.4184, step time: 0.2542\n",
      "131/281, train_loss: 0.5315, step time: 0.2476\n",
      "132/281, train_loss: 0.4073, step time: 0.2558\n",
      "133/281, train_loss: 0.4168, step time: 0.2573\n",
      "134/281, train_loss: 0.7035, step time: 0.2557\n",
      "135/281, train_loss: 0.4439, step time: 0.2493\n",
      "136/281, train_loss: 0.4008, step time: 0.2502\n",
      "137/281, train_loss: 0.4296, step time: 0.2478\n",
      "138/281, train_loss: 0.3981, step time: 0.2493\n",
      "139/281, train_loss: 0.5502, step time: 0.2498\n",
      "140/281, train_loss: 0.3985, step time: 0.2516\n",
      "141/281, train_loss: 0.3956, step time: 0.2503\n",
      "142/281, train_loss: 0.3954, step time: 0.2516\n",
      "143/281, train_loss: 0.4506, step time: 0.2540\n",
      "144/281, train_loss: 0.4040, step time: 0.2552\n",
      "145/281, train_loss: 0.3955, step time: 0.2571\n",
      "146/281, train_loss: 0.4609, step time: 0.2529\n",
      "147/281, train_loss: 0.4177, step time: 0.2553\n",
      "148/281, train_loss: 0.5485, step time: 0.2520\n",
      "149/281, train_loss: 0.3788, step time: 0.2461\n",
      "150/281, train_loss: 0.5609, step time: 0.2509\n",
      "151/281, train_loss: 0.4216, step time: 0.2508\n",
      "152/281, train_loss: 0.4753, step time: 0.2443\n",
      "153/281, train_loss: 0.4269, step time: 0.2444\n",
      "154/281, train_loss: 0.4091, step time: 0.2456\n",
      "155/281, train_loss: 0.5511, step time: 0.2461\n",
      "156/281, train_loss: 0.4451, step time: 0.2505\n",
      "157/281, train_loss: 0.5093, step time: 0.2477\n",
      "158/281, train_loss: 0.4188, step time: 0.2494\n",
      "159/281, train_loss: 0.4771, step time: 0.2507\n",
      "160/281, train_loss: 0.4875, step time: 0.2455\n",
      "161/281, train_loss: 0.3843, step time: 0.2465\n",
      "162/281, train_loss: 0.4179, step time: 0.2477\n",
      "163/281, train_loss: 0.4250, step time: 0.2517\n",
      "164/281, train_loss: 0.4529, step time: 0.2461\n",
      "165/281, train_loss: 0.3739, step time: 0.2498\n",
      "166/281, train_loss: 0.3810, step time: 0.2508\n",
      "167/281, train_loss: 0.4695, step time: 0.2503\n",
      "168/281, train_loss: 0.3965, step time: 0.2491\n",
      "169/281, train_loss: 0.3823, step time: 0.2492\n",
      "170/281, train_loss: 0.5425, step time: 0.2503\n",
      "171/281, train_loss: 0.5521, step time: 0.2559\n",
      "172/281, train_loss: 0.4588, step time: 0.2557\n",
      "173/281, train_loss: 0.5516, step time: 0.2536\n",
      "174/281, train_loss: 0.4104, step time: 0.2538\n",
      "175/281, train_loss: 0.3800, step time: 0.2565\n",
      "176/281, train_loss: 0.4118, step time: 0.2504\n",
      "177/281, train_loss: 0.3836, step time: 0.2498\n",
      "178/281, train_loss: 0.4260, step time: 0.2458\n",
      "179/281, train_loss: 0.3950, step time: 0.2539\n",
      "180/281, train_loss: 0.3918, step time: 0.2537\n",
      "181/281, train_loss: 0.4159, step time: 0.2491\n",
      "182/281, train_loss: 0.4934, step time: 0.2486\n",
      "183/281, train_loss: 0.3983, step time: 0.2450\n",
      "184/281, train_loss: 0.4472, step time: 0.2436\n",
      "185/281, train_loss: 0.4256, step time: 0.2459\n",
      "186/281, train_loss: 0.4165, step time: 0.2463\n",
      "187/281, train_loss: 0.4859, step time: 0.2458\n",
      "188/281, train_loss: 0.4083, step time: 0.2505\n",
      "189/281, train_loss: 0.4331, step time: 0.2510\n",
      "190/281, train_loss: 0.4465, step time: 0.2525\n",
      "191/281, train_loss: 0.5677, step time: 0.2551\n",
      "192/281, train_loss: 0.4003, step time: 0.2500\n",
      "193/281, train_loss: 0.5580, step time: 0.2489\n",
      "194/281, train_loss: 0.4076, step time: 0.2505\n",
      "195/281, train_loss: 0.4064, step time: 0.2470\n",
      "196/281, train_loss: 0.4335, step time: 0.2458\n",
      "197/281, train_loss: 0.3995, step time: 0.2497\n",
      "198/281, train_loss: 0.5459, step time: 0.2564\n",
      "199/281, train_loss: 0.3777, step time: 0.2518\n",
      "200/281, train_loss: 0.3912, step time: 0.2473\n",
      "201/281, train_loss: 0.4252, step time: 0.2454\n",
      "202/281, train_loss: 0.4173, step time: 0.2435\n",
      "203/281, train_loss: 0.5288, step time: 0.2481\n",
      "204/281, train_loss: 0.4119, step time: 0.2502\n",
      "205/281, train_loss: 0.4001, step time: 0.2456\n",
      "206/281, train_loss: 0.5799, step time: 0.2453\n",
      "207/281, train_loss: 0.4052, step time: 0.2514\n",
      "208/281, train_loss: 0.4378, step time: 0.2542\n",
      "209/281, train_loss: 0.4301, step time: 0.2506\n",
      "210/281, train_loss: 0.4511, step time: 0.2483\n",
      "211/281, train_loss: 0.4114, step time: 0.2520\n",
      "212/281, train_loss: 0.3974, step time: 0.2503\n",
      "213/281, train_loss: 0.3810, step time: 0.2496\n",
      "214/281, train_loss: 0.4000, step time: 0.2502\n",
      "215/281, train_loss: 0.4393, step time: 0.2461\n",
      "216/281, train_loss: 0.4270, step time: 0.2491\n",
      "217/281, train_loss: 0.4432, step time: 0.2486\n",
      "218/281, train_loss: 0.4019, step time: 0.2468\n",
      "219/281, train_loss: 0.4406, step time: 0.2509\n",
      "220/281, train_loss: 0.3825, step time: 0.2493\n",
      "221/281, train_loss: 0.4311, step time: 0.2509\n",
      "222/281, train_loss: 0.4417, step time: 0.2494\n",
      "223/281, train_loss: 0.4075, step time: 0.2508\n",
      "224/281, train_loss: 0.5903, step time: 0.2505\n",
      "225/281, train_loss: 0.5461, step time: 0.2794\n",
      "226/281, train_loss: 0.4339, step time: 0.2491\n",
      "227/281, train_loss: 0.3959, step time: 0.2503\n",
      "228/281, train_loss: 0.4252, step time: 0.2528\n",
      "229/281, train_loss: 0.4059, step time: 0.2481\n",
      "230/281, train_loss: 0.4064, step time: 0.2511\n",
      "231/281, train_loss: 0.4037, step time: 0.2480\n",
      "232/281, train_loss: 0.4509, step time: 0.2477\n",
      "233/281, train_loss: 0.4503, step time: 0.2476\n",
      "234/281, train_loss: 0.4093, step time: 0.2528\n",
      "235/281, train_loss: 0.4161, step time: 0.2579\n",
      "236/281, train_loss: 0.4094, step time: 0.2545\n",
      "237/281, train_loss: 0.4541, step time: 0.2558\n",
      "238/281, train_loss: 0.3737, step time: 0.2512\n",
      "239/281, train_loss: 0.5609, step time: 0.2504\n",
      "240/281, train_loss: 0.3866, step time: 0.2519\n",
      "241/281, train_loss: 0.5484, step time: 0.2517\n",
      "242/281, train_loss: 0.4312, step time: 0.2531\n",
      "243/281, train_loss: 0.4099, step time: 0.2531\n",
      "244/281, train_loss: 0.5727, step time: 0.2507\n",
      "245/281, train_loss: 0.5324, step time: 0.2529\n",
      "246/281, train_loss: 0.5417, step time: 0.2457\n",
      "247/281, train_loss: 0.5263, step time: 0.2494\n",
      "248/281, train_loss: 0.4301, step time: 0.2532\n",
      "249/281, train_loss: 0.4269, step time: 0.2522\n",
      "250/281, train_loss: 0.4570, step time: 0.2457\n",
      "251/281, train_loss: 0.4000, step time: 0.2492\n",
      "252/281, train_loss: 0.4445, step time: 0.2614\n",
      "253/281, train_loss: 0.4005, step time: 0.2557\n",
      "254/281, train_loss: 0.4198, step time: 0.2482\n",
      "255/281, train_loss: 0.4165, step time: 0.2608\n",
      "256/281, train_loss: 0.4644, step time: 0.2552\n",
      "257/281, train_loss: 0.4077, step time: 0.2516\n",
      "258/281, train_loss: 0.4332, step time: 0.2513\n",
      "259/281, train_loss: 0.4103, step time: 0.2465\n",
      "260/281, train_loss: 0.4024, step time: 0.2524\n",
      "261/281, train_loss: 0.3877, step time: 0.2476\n",
      "262/281, train_loss: 0.3979, step time: 0.2506\n",
      "263/281, train_loss: 0.3972, step time: 0.2553\n",
      "264/281, train_loss: 0.4092, step time: 0.2560\n",
      "265/281, train_loss: 0.3872, step time: 0.2524\n",
      "266/281, train_loss: 0.5758, step time: 0.2498\n",
      "267/281, train_loss: 0.4131, step time: 0.2460\n",
      "268/281, train_loss: 0.4407, step time: 0.2533\n",
      "269/281, train_loss: 0.4007, step time: 0.2519\n",
      "270/281, train_loss: 0.3943, step time: 0.2510\n",
      "271/281, train_loss: 0.4110, step time: 0.2540\n",
      "272/281, train_loss: 0.3813, step time: 0.2537\n",
      "273/281, train_loss: 0.3904, step time: 0.2508\n",
      "274/281, train_loss: 0.4367, step time: 0.2528\n",
      "275/281, train_loss: 0.5756, step time: 0.2505\n",
      "276/281, train_loss: 0.5554, step time: 0.2503\n",
      "277/281, train_loss: 0.4515, step time: 0.2522\n",
      "278/281, train_loss: 0.3982, step time: 0.2575\n",
      "279/281, train_loss: 0.4788, step time: 0.2531\n",
      "280/281, train_loss: 0.4457, step time: 0.2521\n",
      "281/281, train_loss: 0.4178, step time: 0.2487\n",
      "282/281, train_loss: 0.3993, step time: 0.1516\n",
      "epoch 24 average loss: 0.4418\n",
      "current epoch: 24 current mean dice: 0.5658 tc: 0.0100 wt: 0.9038 et: 0.8112\n",
      "best mean dice: 0.5714 at epoch: 23\n",
      "time consuming of epoch 24 is: 427.1919\n",
      "----------\n",
      "epoch 25/200\n",
      "1/281, train_loss: 0.5453, step time: 0.2591\n",
      "2/281, train_loss: 0.3957, step time: 0.2531\n",
      "3/281, train_loss: 0.4116, step time: 0.2595\n",
      "4/281, train_loss: 0.3754, step time: 0.2633\n",
      "5/281, train_loss: 0.4222, step time: 0.2576\n",
      "6/281, train_loss: 0.4187, step time: 0.2551\n",
      "7/281, train_loss: 0.3693, step time: 0.2564\n",
      "8/281, train_loss: 0.4007, step time: 0.2504\n",
      "9/281, train_loss: 0.3884, step time: 0.2493\n",
      "10/281, train_loss: 0.4308, step time: 0.2550\n",
      "11/281, train_loss: 0.5216, step time: 0.2562\n",
      "12/281, train_loss: 0.3844, step time: 0.2569\n",
      "13/281, train_loss: 0.3967, step time: 0.2504\n",
      "14/281, train_loss: 0.4736, step time: 0.2543\n",
      "15/281, train_loss: 0.4384, step time: 0.2513\n",
      "16/281, train_loss: 0.4430, step time: 0.2266\n",
      "17/281, train_loss: 0.4121, step time: 0.2473\n",
      "18/281, train_loss: 0.4144, step time: 0.2518\n",
      "19/281, train_loss: 0.4162, step time: 0.2499\n",
      "20/281, train_loss: 0.5354, step time: 0.2502\n",
      "21/281, train_loss: 0.4001, step time: 0.2514\n",
      "22/281, train_loss: 0.5852, step time: 0.2504\n",
      "23/281, train_loss: 0.4284, step time: 0.2506\n",
      "24/281, train_loss: 0.4214, step time: 0.2485\n",
      "25/281, train_loss: 0.4567, step time: 0.2621\n",
      "26/281, train_loss: 0.3829, step time: 0.2570\n",
      "27/281, train_loss: 0.4012, step time: 0.2559\n",
      "28/281, train_loss: 0.4572, step time: 0.2548\n",
      "29/281, train_loss: 0.4280, step time: 0.2560\n",
      "30/281, train_loss: 0.3974, step time: 0.2563\n",
      "31/281, train_loss: 0.4653, step time: 0.2568\n",
      "32/281, train_loss: 0.3975, step time: 0.2614\n",
      "33/281, train_loss: 0.3849, step time: 0.2636\n",
      "34/281, train_loss: 0.5465, step time: 0.2588\n",
      "35/281, train_loss: 0.5414, step time: 0.2642\n",
      "36/281, train_loss: 0.4132, step time: 0.2514\n",
      "37/281, train_loss: 0.4527, step time: 0.2537\n",
      "38/281, train_loss: 0.4019, step time: 0.2509\n",
      "39/281, train_loss: 0.5064, step time: 0.2507\n",
      "40/281, train_loss: 0.4106, step time: 0.2479\n",
      "41/281, train_loss: 0.3986, step time: 0.2562\n",
      "42/281, train_loss: 0.4325, step time: 0.2556\n",
      "43/281, train_loss: 0.5353, step time: 0.2586\n",
      "44/281, train_loss: 0.4081, step time: 0.2477\n",
      "45/281, train_loss: 0.4685, step time: 0.2599\n",
      "46/281, train_loss: 0.4159, step time: 0.2588\n",
      "47/281, train_loss: 0.4023, step time: 0.2598\n",
      "48/281, train_loss: 0.5479, step time: 0.2516\n",
      "49/281, train_loss: 0.4177, step time: 0.2546\n",
      "50/281, train_loss: 0.3906, step time: 0.2541\n",
      "51/281, train_loss: 0.3783, step time: 0.2549\n",
      "52/281, train_loss: 0.3880, step time: 0.2510\n",
      "53/281, train_loss: 0.4294, step time: 0.2571\n",
      "54/281, train_loss: 0.5434, step time: 0.2562\n",
      "55/281, train_loss: 0.3892, step time: 0.2529\n",
      "56/281, train_loss: 0.4319, step time: 0.2559\n",
      "57/281, train_loss: 0.4055, step time: 0.2509\n",
      "58/281, train_loss: 0.3919, step time: 0.2527\n",
      "59/281, train_loss: 0.4670, step time: 0.2504\n",
      "60/281, train_loss: 0.5468, step time: 0.2553\n",
      "61/281, train_loss: 0.4215, step time: 0.2511\n",
      "62/281, train_loss: 0.6283, step time: 0.2511\n",
      "63/281, train_loss: 0.4195, step time: 0.2511\n",
      "64/281, train_loss: 0.4138, step time: 0.2587\n",
      "65/281, train_loss: 0.4039, step time: 0.2598\n",
      "66/281, train_loss: 0.3790, step time: 0.2596\n",
      "67/281, train_loss: 0.4073, step time: 0.2575\n",
      "68/281, train_loss: 0.4372, step time: 0.2561\n",
      "69/281, train_loss: 0.5355, step time: 0.2585\n",
      "70/281, train_loss: 0.4571, step time: 0.2591\n",
      "71/281, train_loss: 0.4045, step time: 0.2527\n",
      "72/281, train_loss: 0.4339, step time: 0.2516\n",
      "73/281, train_loss: 0.4055, step time: 0.2559\n",
      "74/281, train_loss: 0.4070, step time: 0.2528\n",
      "75/281, train_loss: 0.3914, step time: 0.2552\n",
      "76/281, train_loss: 0.4539, step time: 0.2629\n",
      "77/281, train_loss: 0.4372, step time: 0.2577\n",
      "78/281, train_loss: 0.7144, step time: 0.2569\n",
      "79/281, train_loss: 0.4399, step time: 0.2564\n",
      "80/281, train_loss: 0.5549, step time: 0.2564\n",
      "81/281, train_loss: 0.4289, step time: 0.2670\n",
      "82/281, train_loss: 0.3711, step time: 0.2580\n",
      "83/281, train_loss: 0.4103, step time: 0.2573\n",
      "84/281, train_loss: 0.4012, step time: 0.2562\n",
      "85/281, train_loss: 0.4019, step time: 0.2572\n",
      "86/281, train_loss: 0.3806, step time: 0.2533\n",
      "87/281, train_loss: 0.3947, step time: 0.2563\n",
      "88/281, train_loss: 0.3909, step time: 0.2564\n",
      "89/281, train_loss: 0.3975, step time: 0.2526\n",
      "90/281, train_loss: 0.4403, step time: 0.2551\n",
      "91/281, train_loss: 0.4302, step time: 0.2708\n",
      "92/281, train_loss: 0.4078, step time: 0.2564\n",
      "93/281, train_loss: 0.4080, step time: 0.2527\n",
      "94/281, train_loss: 0.4369, step time: 0.2564\n",
      "95/281, train_loss: 0.5635, step time: 0.2562\n",
      "96/281, train_loss: 0.4894, step time: 0.2578\n",
      "97/281, train_loss: 0.4119, step time: 0.2586\n",
      "98/281, train_loss: 0.4243, step time: 0.2504\n",
      "99/281, train_loss: 0.5572, step time: 0.2513\n",
      "100/281, train_loss: 0.4328, step time: 0.2523\n",
      "101/281, train_loss: 0.3817, step time: 0.2585\n",
      "102/281, train_loss: 0.5563, step time: 0.2550\n",
      "103/281, train_loss: 0.4347, step time: 0.2538\n",
      "104/281, train_loss: 0.4063, step time: 0.2568\n",
      "105/281, train_loss: 0.4602, step time: 0.2581\n",
      "106/281, train_loss: 0.4556, step time: 0.2563\n",
      "107/281, train_loss: 0.4245, step time: 0.2585\n",
      "108/281, train_loss: 0.3951, step time: 0.2606\n",
      "109/281, train_loss: 0.5607, step time: 0.2561\n",
      "110/281, train_loss: 0.3986, step time: 0.2515\n",
      "111/281, train_loss: 0.5459, step time: 0.2529\n",
      "112/281, train_loss: 0.3889, step time: 0.2548\n",
      "113/281, train_loss: 0.3849, step time: 0.2568\n",
      "114/281, train_loss: 0.4098, step time: 0.2540\n",
      "115/281, train_loss: 0.4232, step time: 0.2562\n",
      "116/281, train_loss: 0.4157, step time: 0.2578\n",
      "117/281, train_loss: 0.4017, step time: 0.2556\n",
      "118/281, train_loss: 0.4041, step time: 0.2505\n",
      "119/281, train_loss: 0.3989, step time: 0.2515\n",
      "120/281, train_loss: 0.5439, step time: 0.2532\n",
      "121/281, train_loss: 0.4023, step time: 0.2568\n",
      "122/281, train_loss: 0.4094, step time: 0.2545\n",
      "123/281, train_loss: 0.5472, step time: 0.2586\n",
      "124/281, train_loss: 0.3814, step time: 0.2573\n",
      "125/281, train_loss: 0.3983, step time: 0.2566\n",
      "126/281, train_loss: 0.3927, step time: 0.2524\n",
      "127/281, train_loss: 0.3964, step time: 0.2583\n",
      "128/281, train_loss: 0.4676, step time: 0.2583\n",
      "129/281, train_loss: 0.4523, step time: 0.2532\n",
      "130/281, train_loss: 0.4269, step time: 0.2497\n",
      "131/281, train_loss: 0.4000, step time: 0.2513\n",
      "132/281, train_loss: 0.4363, step time: 0.2530\n",
      "133/281, train_loss: 0.5393, step time: 0.2558\n",
      "134/281, train_loss: 0.3870, step time: 0.2528\n",
      "135/281, train_loss: 0.5343, step time: 0.2512\n",
      "136/281, train_loss: 0.3933, step time: 0.2572\n",
      "137/281, train_loss: 0.4145, step time: 0.2575\n",
      "138/281, train_loss: 0.3943, step time: 0.2522\n",
      "139/281, train_loss: 0.4070, step time: 0.2517\n",
      "140/281, train_loss: 0.4435, step time: 0.2532\n",
      "141/281, train_loss: 0.3827, step time: 0.2565\n",
      "142/281, train_loss: 0.3882, step time: 0.2602\n",
      "143/281, train_loss: 0.4035, step time: 0.2731\n",
      "144/281, train_loss: 0.4192, step time: 0.2585\n",
      "145/281, train_loss: 0.7011, step time: 0.2554\n",
      "146/281, train_loss: 0.4153, step time: 0.2527\n",
      "147/281, train_loss: 0.4278, step time: 0.2514\n",
      "148/281, train_loss: 0.4332, step time: 0.2565\n",
      "149/281, train_loss: 0.4135, step time: 0.2610\n",
      "150/281, train_loss: 0.5439, step time: 0.2559\n",
      "151/281, train_loss: 0.3938, step time: 0.2543\n",
      "152/281, train_loss: 0.3921, step time: 0.2592\n",
      "153/281, train_loss: 0.3985, step time: 0.2563\n",
      "154/281, train_loss: 0.4040, step time: 0.2566\n",
      "155/281, train_loss: 0.4145, step time: 0.2538\n",
      "156/281, train_loss: 0.3876, step time: 0.2467\n",
      "157/281, train_loss: 0.4713, step time: 0.2505\n",
      "158/281, train_loss: 0.4348, step time: 0.2485\n",
      "159/281, train_loss: 0.3945, step time: 0.2506\n",
      "160/281, train_loss: 0.4293, step time: 0.2505\n",
      "161/281, train_loss: 0.4181, step time: 0.2552\n",
      "162/281, train_loss: 0.3894, step time: 0.2546\n",
      "163/281, train_loss: 0.4048, step time: 0.2500\n",
      "164/281, train_loss: 0.5441, step time: 0.2517\n",
      "165/281, train_loss: 0.3940, step time: 0.2568\n",
      "166/281, train_loss: 0.4496, step time: 0.2542\n",
      "167/281, train_loss: 0.3789, step time: 0.2566\n",
      "168/281, train_loss: 0.4081, step time: 0.2530\n",
      "169/281, train_loss: 0.4300, step time: 0.2571\n",
      "170/281, train_loss: 0.3741, step time: 0.2595\n",
      "171/281, train_loss: 0.6983, step time: 0.2556\n",
      "172/281, train_loss: 0.3871, step time: 0.2533\n",
      "173/281, train_loss: 0.4418, step time: 0.2572\n",
      "174/281, train_loss: 0.4483, step time: 0.2582\n",
      "175/281, train_loss: 0.4255, step time: 0.2556\n",
      "176/281, train_loss: 0.3941, step time: 0.2570\n",
      "177/281, train_loss: 0.4214, step time: 0.2561\n",
      "178/281, train_loss: 0.4008, step time: 0.2526\n",
      "179/281, train_loss: 0.4295, step time: 0.2577\n",
      "180/281, train_loss: 0.5701, step time: 0.2651\n",
      "181/281, train_loss: 0.3992, step time: 0.2553\n",
      "182/281, train_loss: 0.4111, step time: 0.2519\n",
      "183/281, train_loss: 0.4398, step time: 0.2507\n",
      "184/281, train_loss: 0.3986, step time: 0.2522\n",
      "185/281, train_loss: 0.4012, step time: 0.2542\n",
      "186/281, train_loss: 0.4556, step time: 0.2533\n",
      "187/281, train_loss: 0.3807, step time: 0.2523\n",
      "188/281, train_loss: 0.4204, step time: 0.2541\n",
      "189/281, train_loss: 0.4351, step time: 0.2578\n",
      "190/281, train_loss: 0.4125, step time: 0.2541\n",
      "191/281, train_loss: 0.4096, step time: 0.2599\n",
      "192/281, train_loss: 0.4194, step time: 0.2547\n",
      "193/281, train_loss: 0.4096, step time: 0.2511\n",
      "194/281, train_loss: 0.3715, step time: 0.2527\n",
      "195/281, train_loss: 0.4103, step time: 0.2560\n",
      "196/281, train_loss: 0.3982, step time: 0.2546\n",
      "197/281, train_loss: 0.4130, step time: 0.2552\n",
      "198/281, train_loss: 0.3706, step time: 0.2520\n",
      "199/281, train_loss: 0.4312, step time: 0.2536\n",
      "200/281, train_loss: 0.4182, step time: 0.2559\n",
      "201/281, train_loss: 0.4661, step time: 0.2559\n",
      "202/281, train_loss: 0.5794, step time: 0.2569\n",
      "203/281, train_loss: 0.3817, step time: 0.2575\n",
      "204/281, train_loss: 0.4447, step time: 0.2564\n",
      "205/281, train_loss: 0.4105, step time: 0.2540\n",
      "206/281, train_loss: 0.3897, step time: 0.2578\n",
      "207/281, train_loss: 0.4841, step time: 0.2562\n",
      "208/281, train_loss: 0.4324, step time: 0.2619\n",
      "209/281, train_loss: 0.6954, step time: 0.2587\n",
      "210/281, train_loss: 0.4344, step time: 0.2605\n",
      "211/281, train_loss: 0.4591, step time: 0.2565\n",
      "212/281, train_loss: 0.4227, step time: 0.2524\n",
      "213/281, train_loss: 0.4605, step time: 0.2550\n",
      "214/281, train_loss: 0.4152, step time: 0.2552\n",
      "215/281, train_loss: 0.4231, step time: 0.2541\n",
      "216/281, train_loss: 0.4371, step time: 0.2539\n",
      "217/281, train_loss: 0.4174, step time: 0.2549\n",
      "218/281, train_loss: 0.3961, step time: 0.2560\n",
      "219/281, train_loss: 0.4188, step time: 0.2543\n",
      "220/281, train_loss: 0.3918, step time: 0.2566\n",
      "221/281, train_loss: 0.3974, step time: 0.2588\n",
      "222/281, train_loss: 0.4112, step time: 0.2521\n",
      "223/281, train_loss: 0.4226, step time: 0.2507\n",
      "224/281, train_loss: 0.5000, step time: 0.2560\n",
      "225/281, train_loss: 0.4489, step time: 0.2565\n",
      "226/281, train_loss: 0.5192, step time: 0.2553\n",
      "227/281, train_loss: 0.4191, step time: 0.2530\n",
      "228/281, train_loss: 0.4468, step time: 0.2537\n",
      "229/281, train_loss: 0.5330, step time: 0.2567\n",
      "230/281, train_loss: 0.4575, step time: 0.2545\n",
      "231/281, train_loss: 0.3952, step time: 0.2550\n",
      "232/281, train_loss: 0.4057, step time: 0.2583\n",
      "233/281, train_loss: 0.4190, step time: 0.2573\n",
      "234/281, train_loss: 0.5215, step time: 0.2515\n",
      "235/281, train_loss: 0.4305, step time: 0.2607\n",
      "236/281, train_loss: 0.4571, step time: 0.2523\n",
      "237/281, train_loss: 0.3841, step time: 0.2491\n",
      "238/281, train_loss: 0.5482, step time: 0.2543\n",
      "239/281, train_loss: 0.3808, step time: 0.2512\n",
      "240/281, train_loss: 0.4443, step time: 0.2509\n",
      "241/281, train_loss: 0.5432, step time: 0.2533\n",
      "242/281, train_loss: 0.3969, step time: 0.2539\n",
      "243/281, train_loss: 0.4368, step time: 0.2580\n",
      "244/281, train_loss: 0.4211, step time: 0.2534\n",
      "245/281, train_loss: 0.5369, step time: 0.2516\n",
      "246/281, train_loss: 0.4006, step time: 0.2502\n",
      "247/281, train_loss: 0.5529, step time: 0.2455\n",
      "248/281, train_loss: 0.4218, step time: 0.2588\n",
      "249/281, train_loss: 0.4103, step time: 0.2547\n",
      "250/281, train_loss: 0.4136, step time: 0.2549\n",
      "251/281, train_loss: 0.3823, step time: 0.2501\n",
      "252/281, train_loss: 0.3994, step time: 0.2472\n",
      "253/281, train_loss: 0.5137, step time: 0.2525\n",
      "254/281, train_loss: 0.5263, step time: 0.2642\n",
      "255/281, train_loss: 0.4161, step time: 0.2573\n",
      "256/281, train_loss: 0.5288, step time: 0.2571\n",
      "257/281, train_loss: 0.3955, step time: 0.2556\n",
      "258/281, train_loss: 0.5558, step time: 0.2513\n",
      "259/281, train_loss: 0.4315, step time: 0.2484\n",
      "260/281, train_loss: 0.4351, step time: 0.2543\n",
      "261/281, train_loss: 0.5254, step time: 0.2548\n",
      "262/281, train_loss: 0.3850, step time: 0.2530\n",
      "263/281, train_loss: 0.4648, step time: 0.2564\n",
      "264/281, train_loss: 0.4058, step time: 0.2539\n",
      "265/281, train_loss: 0.5764, step time: 0.2546\n",
      "266/281, train_loss: 0.4037, step time: 0.2541\n",
      "267/281, train_loss: 0.4395, step time: 0.2501\n",
      "268/281, train_loss: 0.4145, step time: 0.2508\n",
      "269/281, train_loss: 0.4050, step time: 0.2470\n",
      "270/281, train_loss: 0.4139, step time: 0.2527\n",
      "271/281, train_loss: 0.4128, step time: 0.2455\n",
      "272/281, train_loss: 0.5546, step time: 0.2523\n",
      "273/281, train_loss: 0.4340, step time: 0.2511\n",
      "274/281, train_loss: 0.5857, step time: 0.2516\n",
      "275/281, train_loss: 0.4175, step time: 0.2519\n",
      "276/281, train_loss: 0.4023, step time: 0.2704\n",
      "277/281, train_loss: 0.3895, step time: 0.2538\n",
      "278/281, train_loss: 0.4023, step time: 0.2543\n",
      "279/281, train_loss: 0.4600, step time: 0.2520\n",
      "280/281, train_loss: 0.5382, step time: 0.2574\n",
      "281/281, train_loss: 0.4151, step time: 0.2527\n",
      "282/281, train_loss: 0.4236, step time: 0.1516\n",
      "epoch 25 average loss: 0.4396\n",
      "saved new best metric model\n",
      "current epoch: 25 current mean dice: 0.5719 tc: 0.0141 wt: 0.9085 et: 0.8224\n",
      "best mean dice: 0.5719 at epoch: 25\n",
      "time consuming of epoch 25 is: 400.1256\n",
      "----------\n",
      "epoch 26/200\n",
      "1/281, train_loss: 0.4082, step time: 0.2563\n",
      "2/281, train_loss: 0.4612, step time: 0.2573\n",
      "3/281, train_loss: 0.3843, step time: 0.2596\n",
      "4/281, train_loss: 0.3869, step time: 0.2598\n",
      "5/281, train_loss: 0.3922, step time: 0.2525\n",
      "6/281, train_loss: 0.5492, step time: 0.2536\n",
      "7/281, train_loss: 0.5682, step time: 0.2506\n",
      "8/281, train_loss: 0.4193, step time: 0.2558\n",
      "9/281, train_loss: 0.4433, step time: 0.2516\n",
      "10/281, train_loss: 0.5057, step time: 0.2547\n",
      "11/281, train_loss: 0.4095, step time: 0.2545\n",
      "12/281, train_loss: 0.3740, step time: 0.2559\n",
      "13/281, train_loss: 0.5413, step time: 0.2545\n",
      "14/281, train_loss: 0.3895, step time: 0.2563\n",
      "15/281, train_loss: 0.5257, step time: 0.2671\n",
      "16/281, train_loss: 0.4029, step time: 0.2789\n",
      "17/281, train_loss: 0.4213, step time: 0.2527\n",
      "18/281, train_loss: 0.3954, step time: 0.2613\n",
      "19/281, train_loss: 0.4047, step time: 0.2628\n",
      "20/281, train_loss: 0.3945, step time: 0.2490\n",
      "21/281, train_loss: 0.3954, step time: 0.2464\n",
      "22/281, train_loss: 0.6901, step time: 0.2442\n",
      "23/281, train_loss: 0.3689, step time: 0.2520\n",
      "24/281, train_loss: 0.5885, step time: 0.2486\n",
      "25/281, train_loss: 0.4197, step time: 0.2453\n",
      "26/281, train_loss: 0.6999, step time: 0.2487\n",
      "27/281, train_loss: 0.4365, step time: 0.2453\n",
      "28/281, train_loss: 0.4327, step time: 0.2481\n",
      "29/281, train_loss: 0.4065, step time: 0.2509\n",
      "30/281, train_loss: 0.3989, step time: 0.2447\n",
      "31/281, train_loss: 0.4046, step time: 0.2487\n",
      "32/281, train_loss: 0.6980, step time: 0.2478\n",
      "33/281, train_loss: 0.4016, step time: 0.2446\n",
      "34/281, train_loss: 0.4032, step time: 0.2483\n",
      "35/281, train_loss: 0.4214, step time: 0.2543\n",
      "36/281, train_loss: 0.4409, step time: 0.2542\n",
      "37/281, train_loss: 0.3749, step time: 0.2478\n",
      "38/281, train_loss: 0.3975, step time: 0.2529\n",
      "39/281, train_loss: 0.3833, step time: 0.2545\n",
      "40/281, train_loss: 0.5665, step time: 0.2426\n",
      "41/281, train_loss: 0.4218, step time: 0.2445\n",
      "42/281, train_loss: 0.4130, step time: 0.2506\n",
      "43/281, train_loss: 0.4141, step time: 0.2709\n",
      "44/281, train_loss: 0.6897, step time: 0.2627\n",
      "45/281, train_loss: 0.3826, step time: 0.2628\n",
      "46/281, train_loss: 0.4364, step time: 0.2463\n",
      "47/281, train_loss: 0.5562, step time: 0.2476\n",
      "48/281, train_loss: 0.4369, step time: 0.2474\n",
      "49/281, train_loss: 0.4022, step time: 0.2494\n",
      "50/281, train_loss: 0.4180, step time: 0.2469\n",
      "51/281, train_loss: 0.3984, step time: 0.2488\n",
      "52/281, train_loss: 0.3837, step time: 0.2501\n",
      "53/281, train_loss: 0.4258, step time: 0.2464\n",
      "54/281, train_loss: 0.3992, step time: 0.2538\n",
      "55/281, train_loss: 0.4019, step time: 0.2526\n",
      "56/281, train_loss: 0.3992, step time: 0.2494\n",
      "57/281, train_loss: 0.3913, step time: 0.2456\n",
      "58/281, train_loss: 0.4296, step time: 0.2517\n",
      "59/281, train_loss: 0.5482, step time: 0.2566\n",
      "60/281, train_loss: 0.3864, step time: 0.2571\n",
      "61/281, train_loss: 0.5753, step time: 0.2560\n",
      "62/281, train_loss: 0.3875, step time: 0.2527\n",
      "63/281, train_loss: 0.4384, step time: 0.2473\n",
      "64/281, train_loss: 0.4521, step time: 0.2457\n",
      "65/281, train_loss: 0.3948, step time: 0.2491\n",
      "66/281, train_loss: 0.3935, step time: 0.2495\n",
      "67/281, train_loss: 0.5085, step time: 0.2514\n",
      "68/281, train_loss: 0.4039, step time: 0.2504\n",
      "69/281, train_loss: 0.3719, step time: 0.2551\n",
      "70/281, train_loss: 0.4163, step time: 0.2472\n",
      "71/281, train_loss: 0.3801, step time: 0.2440\n",
      "72/281, train_loss: 0.3825, step time: 0.2466\n",
      "73/281, train_loss: 0.3922, step time: 0.2429\n",
      "74/281, train_loss: 0.3805, step time: 0.2499\n",
      "75/281, train_loss: 0.4634, step time: 0.2509\n",
      "76/281, train_loss: 0.4083, step time: 0.2511\n",
      "77/281, train_loss: 0.4480, step time: 0.2481\n",
      "78/281, train_loss: 0.3942, step time: 0.2571\n",
      "79/281, train_loss: 0.4702, step time: 0.2537\n",
      "80/281, train_loss: 0.4091, step time: 0.2548\n",
      "81/281, train_loss: 0.4273, step time: 0.2640\n",
      "82/281, train_loss: 0.4483, step time: 0.2521\n",
      "83/281, train_loss: 0.4066, step time: 0.2555\n",
      "84/281, train_loss: 0.5856, step time: 0.2559\n",
      "85/281, train_loss: 0.4125, step time: 0.2468\n",
      "86/281, train_loss: 0.5628, step time: 0.2482\n",
      "87/281, train_loss: 0.4114, step time: 0.2542\n",
      "88/281, train_loss: 0.3985, step time: 0.2500\n",
      "89/281, train_loss: 0.4029, step time: 0.2488\n",
      "90/281, train_loss: 0.3802, step time: 0.2512\n",
      "91/281, train_loss: 0.3910, step time: 0.2512\n",
      "92/281, train_loss: 0.4362, step time: 0.2551\n",
      "93/281, train_loss: 0.3777, step time: 0.2471\n",
      "94/281, train_loss: 0.4102, step time: 0.2579\n",
      "95/281, train_loss: 0.3779, step time: 0.2553\n",
      "96/281, train_loss: 0.3892, step time: 0.2545\n",
      "97/281, train_loss: 0.4142, step time: 0.2476\n",
      "98/281, train_loss: 0.4585, step time: 0.2503\n",
      "99/281, train_loss: 0.6032, step time: 0.2472\n",
      "100/281, train_loss: 0.5308, step time: 0.2492\n",
      "101/281, train_loss: 0.3807, step time: 0.2495\n",
      "102/281, train_loss: 0.5088, step time: 0.2478\n",
      "103/281, train_loss: 0.4184, step time: 0.2551\n",
      "104/281, train_loss: 0.3898, step time: 0.2529\n",
      "105/281, train_loss: 0.4627, step time: 0.2535\n",
      "106/281, train_loss: 0.4175, step time: 0.2635\n",
      "107/281, train_loss: 0.4652, step time: 0.2421\n",
      "108/281, train_loss: 0.4043, step time: 0.2446\n",
      "109/281, train_loss: 0.3866, step time: 0.2426\n",
      "110/281, train_loss: 0.4201, step time: 0.2501\n",
      "111/281, train_loss: 0.5625, step time: 0.2462\n",
      "112/281, train_loss: 0.4694, step time: 0.2481\n",
      "113/281, train_loss: 0.4623, step time: 0.2551\n",
      "114/281, train_loss: 0.4319, step time: 0.2496\n",
      "115/281, train_loss: 0.4105, step time: 0.2538\n",
      "116/281, train_loss: 0.4284, step time: 0.2524\n",
      "117/281, train_loss: 0.4279, step time: 0.2463\n",
      "118/281, train_loss: 0.4139, step time: 0.2481\n",
      "119/281, train_loss: 0.4382, step time: 0.2538\n",
      "120/281, train_loss: 0.3832, step time: 0.2486\n",
      "121/281, train_loss: 0.4410, step time: 0.2463\n",
      "122/281, train_loss: 0.3940, step time: 0.2521\n",
      "123/281, train_loss: 0.3797, step time: 0.2537\n",
      "124/281, train_loss: 0.3777, step time: 0.2528\n",
      "125/281, train_loss: 0.3876, step time: 0.2473\n",
      "126/281, train_loss: 0.4392, step time: 0.2486\n",
      "127/281, train_loss: 0.4283, step time: 0.2464\n",
      "128/281, train_loss: 0.4238, step time: 0.2454\n",
      "129/281, train_loss: 0.3849, step time: 0.2471\n",
      "130/281, train_loss: 0.4121, step time: 0.2539\n",
      "131/281, train_loss: 0.4178, step time: 0.2445\n",
      "132/281, train_loss: 0.4172, step time: 0.2475\n",
      "133/281, train_loss: 0.3970, step time: 0.2478\n",
      "134/281, train_loss: 0.4160, step time: 0.2635\n",
      "135/281, train_loss: 0.3977, step time: 0.2485\n",
      "136/281, train_loss: 0.4146, step time: 0.2509\n",
      "137/281, train_loss: 0.3993, step time: 0.2480\n",
      "138/281, train_loss: 0.5601, step time: 0.2621\n",
      "139/281, train_loss: 0.3863, step time: 0.2461\n",
      "140/281, train_loss: 0.4329, step time: 0.2470\n",
      "141/281, train_loss: 0.4020, step time: 0.2451\n",
      "142/281, train_loss: 0.4433, step time: 0.2511\n",
      "143/281, train_loss: 0.4027, step time: 0.2469\n",
      "144/281, train_loss: 0.3755, step time: 0.2496\n",
      "145/281, train_loss: 0.4084, step time: 0.2529\n",
      "146/281, train_loss: 0.4581, step time: 0.2505\n",
      "147/281, train_loss: 0.3804, step time: 0.2481\n",
      "148/281, train_loss: 0.3733, step time: 0.2495\n",
      "149/281, train_loss: 0.3795, step time: 0.2523\n",
      "150/281, train_loss: 0.4052, step time: 0.2519\n",
      "151/281, train_loss: 0.4118, step time: 0.2511\n",
      "152/281, train_loss: 0.4026, step time: 0.2541\n",
      "153/281, train_loss: 0.4051, step time: 0.2496\n",
      "154/281, train_loss: 0.3876, step time: 0.2468\n",
      "155/281, train_loss: 0.4028, step time: 0.2566\n",
      "156/281, train_loss: 0.3967, step time: 0.2463\n",
      "157/281, train_loss: 0.3845, step time: 0.2479\n",
      "158/281, train_loss: 0.5453, step time: 0.2485\n",
      "159/281, train_loss: 0.4322, step time: 0.2508\n",
      "160/281, train_loss: 0.3911, step time: 0.2486\n",
      "161/281, train_loss: 0.4149, step time: 0.2520\n",
      "162/281, train_loss: 0.4127, step time: 0.2524\n",
      "163/281, train_loss: 0.4132, step time: 0.2431\n",
      "164/281, train_loss: 0.4386, step time: 0.2494\n",
      "165/281, train_loss: 0.4016, step time: 0.2420\n",
      "166/281, train_loss: 0.4054, step time: 0.2421\n",
      "167/281, train_loss: 0.4067, step time: 0.2493\n",
      "168/281, train_loss: 0.3971, step time: 0.2509\n",
      "169/281, train_loss: 0.4238, step time: 0.2483\n",
      "170/281, train_loss: 0.5519, step time: 0.2494\n",
      "171/281, train_loss: 0.4239, step time: 0.2484\n",
      "172/281, train_loss: 0.4143, step time: 0.2471\n",
      "173/281, train_loss: 0.4037, step time: 0.2482\n",
      "174/281, train_loss: 0.4147, step time: 0.2492\n",
      "175/281, train_loss: 0.4101, step time: 0.2535\n",
      "176/281, train_loss: 0.5371, step time: 0.2532\n",
      "177/281, train_loss: 0.3882, step time: 0.2518\n",
      "178/281, train_loss: 0.5692, step time: 0.2550\n",
      "179/281, train_loss: 0.3927, step time: 0.2521\n",
      "180/281, train_loss: 0.4758, step time: 0.2502\n",
      "181/281, train_loss: 0.4838, step time: 0.2501\n",
      "182/281, train_loss: 0.3838, step time: 0.2498\n",
      "183/281, train_loss: 0.4209, step time: 0.2508\n",
      "184/281, train_loss: 0.3916, step time: 0.2560\n",
      "185/281, train_loss: 0.5271, step time: 0.2509\n",
      "186/281, train_loss: 0.5376, step time: 0.2471\n",
      "187/281, train_loss: 0.4301, step time: 0.2554\n",
      "188/281, train_loss: 0.3913, step time: 0.2519\n",
      "189/281, train_loss: 0.4014, step time: 0.2559\n",
      "190/281, train_loss: 0.4451, step time: 0.2532\n",
      "191/281, train_loss: 0.4482, step time: 0.2502\n",
      "192/281, train_loss: 0.5496, step time: 0.2512\n",
      "193/281, train_loss: 0.4485, step time: 0.2523\n",
      "194/281, train_loss: 0.5247, step time: 0.2486\n",
      "195/281, train_loss: 0.4285, step time: 0.2565\n",
      "196/281, train_loss: 0.3717, step time: 0.2573\n",
      "197/281, train_loss: 0.3946, step time: 0.2526\n",
      "198/281, train_loss: 0.3754, step time: 0.2519\n",
      "199/281, train_loss: 0.4084, step time: 0.2592\n",
      "200/281, train_loss: 0.3920, step time: 0.2611\n",
      "201/281, train_loss: 0.4325, step time: 0.2518\n",
      "202/281, train_loss: 0.4146, step time: 0.2543\n",
      "203/281, train_loss: 0.4871, step time: 0.2439\n",
      "204/281, train_loss: 0.5477, step time: 0.2498\n",
      "205/281, train_loss: 0.3760, step time: 0.2521\n",
      "206/281, train_loss: 0.5464, step time: 0.2537\n",
      "207/281, train_loss: 0.3805, step time: 0.2569\n",
      "208/281, train_loss: 0.4270, step time: 0.2487\n",
      "209/281, train_loss: 0.3967, step time: 0.2541\n",
      "210/281, train_loss: 0.6960, step time: 0.2562\n",
      "211/281, train_loss: 0.3776, step time: 0.2569\n",
      "212/281, train_loss: 0.4058, step time: 0.2523\n",
      "213/281, train_loss: 0.3974, step time: 0.2520\n",
      "214/281, train_loss: 0.4085, step time: 0.2534\n",
      "215/281, train_loss: 0.4663, step time: 0.2511\n",
      "216/281, train_loss: 0.3996, step time: 0.2508\n",
      "217/281, train_loss: 0.3982, step time: 0.2504\n",
      "218/281, train_loss: 0.4420, step time: 0.2533\n",
      "219/281, train_loss: 0.4272, step time: 0.2490\n",
      "220/281, train_loss: 0.4337, step time: 0.2527\n",
      "221/281, train_loss: 0.3873, step time: 0.2529\n",
      "222/281, train_loss: 0.5466, step time: 0.2604\n",
      "223/281, train_loss: 0.3737, step time: 0.2534\n",
      "224/281, train_loss: 0.3894, step time: 0.2489\n",
      "225/281, train_loss: 0.3877, step time: 0.2592\n",
      "226/281, train_loss: 0.4043, step time: 0.2867\n",
      "227/281, train_loss: 0.5854, step time: 0.2674\n",
      "228/281, train_loss: 0.3855, step time: 0.2435\n",
      "229/281, train_loss: 0.3921, step time: 0.2557\n",
      "230/281, train_loss: 0.4132, step time: 0.2537\n",
      "231/281, train_loss: 0.3875, step time: 0.2540\n",
      "232/281, train_loss: 0.3901, step time: 0.2541\n",
      "233/281, train_loss: 0.5546, step time: 0.2541\n",
      "234/281, train_loss: 0.5495, step time: 0.2545\n",
      "235/281, train_loss: 0.3837, step time: 0.2511\n",
      "236/281, train_loss: 0.5597, step time: 0.2567\n",
      "237/281, train_loss: 0.4687, step time: 0.2566\n",
      "238/281, train_loss: 0.4633, step time: 0.2545\n",
      "239/281, train_loss: 0.5584, step time: 0.2551\n",
      "240/281, train_loss: 0.3803, step time: 0.2507\n",
      "241/281, train_loss: 0.4366, step time: 0.2562\n",
      "242/281, train_loss: 0.3998, step time: 0.2569\n",
      "243/281, train_loss: 0.4290, step time: 0.2537\n",
      "244/281, train_loss: 0.3969, step time: 0.2537\n",
      "245/281, train_loss: 0.4290, step time: 0.2565\n",
      "246/281, train_loss: 0.5253, step time: 0.2589\n",
      "247/281, train_loss: 0.3827, step time: 0.2534\n",
      "248/281, train_loss: 0.4449, step time: 0.2519\n",
      "249/281, train_loss: 0.5407, step time: 0.2526\n",
      "250/281, train_loss: 0.3835, step time: 0.2529\n",
      "251/281, train_loss: 0.5544, step time: 0.2506\n",
      "252/281, train_loss: 0.4025, step time: 0.2528\n",
      "253/281, train_loss: 0.5439, step time: 0.2562\n",
      "254/281, train_loss: 0.4500, step time: 0.2494\n",
      "255/281, train_loss: 0.4068, step time: 0.2540\n",
      "256/281, train_loss: 0.4051, step time: 0.2493\n",
      "257/281, train_loss: 0.5374, step time: 0.2547\n",
      "258/281, train_loss: 0.3970, step time: 0.2544\n",
      "259/281, train_loss: 0.3876, step time: 0.2534\n",
      "260/281, train_loss: 0.3980, step time: 0.2541\n",
      "261/281, train_loss: 0.4160, step time: 0.2576\n",
      "262/281, train_loss: 0.4090, step time: 0.2529\n",
      "263/281, train_loss: 0.3934, step time: 0.2507\n",
      "264/281, train_loss: 0.4145, step time: 0.2516\n",
      "265/281, train_loss: 0.4140, step time: 0.2569\n",
      "266/281, train_loss: 0.4003, step time: 0.2550\n",
      "267/281, train_loss: 0.3877, step time: 0.2557\n",
      "268/281, train_loss: 0.3768, step time: 0.2587\n",
      "269/281, train_loss: 0.3828, step time: 0.2562\n",
      "270/281, train_loss: 0.4160, step time: 0.2501\n",
      "271/281, train_loss: 0.3977, step time: 0.2552\n",
      "272/281, train_loss: 0.3881, step time: 0.2527\n",
      "273/281, train_loss: 0.3938, step time: 0.2555\n",
      "274/281, train_loss: 0.3851, step time: 0.2563\n",
      "275/281, train_loss: 0.4150, step time: 0.2549\n",
      "276/281, train_loss: 0.4345, step time: 0.2577\n",
      "277/281, train_loss: 0.5403, step time: 0.2603\n",
      "278/281, train_loss: 0.3981, step time: 0.2525\n",
      "279/281, train_loss: 0.4090, step time: 0.2521\n",
      "280/281, train_loss: 0.4225, step time: 0.2550\n",
      "281/281, train_loss: 0.4099, step time: 0.2492\n",
      "282/281, train_loss: 0.4148, step time: 0.1504\n",
      "epoch 26 average loss: 0.4342\n",
      "saved new best metric model\n",
      "current epoch: 26 current mean dice: 0.5770 tc: 0.0175 wt: 0.9096 et: 0.8348\n",
      "best mean dice: 0.5770 at epoch: 26\n",
      "time consuming of epoch 26 is: 438.8425\n",
      "----------\n",
      "epoch 27/200\n",
      "1/281, train_loss: 0.3971, step time: 0.2578\n",
      "2/281, train_loss: 0.5591, step time: 0.2495\n",
      "3/281, train_loss: 0.3896, step time: 0.2512\n",
      "4/281, train_loss: 0.3666, step time: 0.2523\n",
      "5/281, train_loss: 0.4845, step time: 0.2529\n",
      "6/281, train_loss: 0.3956, step time: 0.2524\n",
      "7/281, train_loss: 0.4296, step time: 0.2565\n",
      "8/281, train_loss: 0.4105, step time: 0.2570\n",
      "9/281, train_loss: 0.3740, step time: 0.2558\n",
      "10/281, train_loss: 0.3816, step time: 0.2542\n",
      "11/281, train_loss: 0.3789, step time: 0.2548\n",
      "12/281, train_loss: 0.4711, step time: 0.2526\n",
      "13/281, train_loss: 0.4823, step time: 0.2514\n",
      "14/281, train_loss: 0.4094, step time: 0.2489\n",
      "15/281, train_loss: 0.4143, step time: 0.2524\n",
      "16/281, train_loss: 0.4096, step time: 0.2605\n",
      "17/281, train_loss: 0.4018, step time: 0.2514\n",
      "18/281, train_loss: 0.4076, step time: 0.2486\n",
      "19/281, train_loss: 0.4028, step time: 0.2533\n",
      "20/281, train_loss: 0.4115, step time: 0.2534\n",
      "21/281, train_loss: 0.4250, step time: 0.2523\n",
      "22/281, train_loss: 0.4348, step time: 0.2526\n",
      "23/281, train_loss: 0.4110, step time: 0.2580\n",
      "24/281, train_loss: 0.4146, step time: 0.2551\n",
      "25/281, train_loss: 0.3844, step time: 0.2494\n",
      "26/281, train_loss: 0.3869, step time: 0.2513\n",
      "27/281, train_loss: 0.3936, step time: 0.2555\n",
      "28/281, train_loss: 0.4142, step time: 0.2480\n",
      "29/281, train_loss: 0.4138, step time: 0.2498\n",
      "30/281, train_loss: 0.5432, step time: 0.2546\n",
      "31/281, train_loss: 0.4011, step time: 0.2532\n",
      "32/281, train_loss: 0.3757, step time: 0.2600\n",
      "33/281, train_loss: 0.3949, step time: 0.2501\n",
      "34/281, train_loss: 0.4193, step time: 0.2557\n",
      "35/281, train_loss: 0.6885, step time: 0.2564\n",
      "36/281, train_loss: 0.5302, step time: 0.2516\n",
      "37/281, train_loss: 0.3709, step time: 0.2516\n",
      "38/281, train_loss: 0.4027, step time: 0.2554\n",
      "39/281, train_loss: 0.3762, step time: 0.2539\n",
      "40/281, train_loss: 0.5118, step time: 0.2548\n",
      "41/281, train_loss: 0.3906, step time: 0.2578\n",
      "42/281, train_loss: 0.3876, step time: 0.2542\n",
      "43/281, train_loss: 0.4318, step time: 0.2611\n",
      "44/281, train_loss: 0.4207, step time: 0.2562\n",
      "45/281, train_loss: 0.4114, step time: 0.2523\n",
      "46/281, train_loss: 0.5585, step time: 0.2572\n",
      "47/281, train_loss: 0.3894, step time: 0.2584\n",
      "48/281, train_loss: 0.4305, step time: 0.2602\n",
      "49/281, train_loss: 0.3746, step time: 0.2823\n",
      "50/281, train_loss: 0.5484, step time: 0.2600\n",
      "51/281, train_loss: 0.5457, step time: 0.2599\n",
      "52/281, train_loss: 0.4748, step time: 0.2604\n",
      "53/281, train_loss: 0.3728, step time: 0.2541\n",
      "54/281, train_loss: 0.3931, step time: 0.2589\n",
      "55/281, train_loss: 0.4377, step time: 0.2667\n",
      "56/281, train_loss: 0.4184, step time: 0.2662\n",
      "57/281, train_loss: 0.3887, step time: 0.2511\n",
      "58/281, train_loss: 0.4028, step time: 0.2659\n",
      "59/281, train_loss: 0.4179, step time: 0.2594\n",
      "60/281, train_loss: 0.3918, step time: 0.2506\n",
      "61/281, train_loss: 0.4123, step time: 0.2495\n",
      "62/281, train_loss: 0.4450, step time: 0.2638\n",
      "63/281, train_loss: 0.4259, step time: 0.2581\n",
      "64/281, train_loss: 0.3781, step time: 0.2542\n",
      "65/281, train_loss: 0.3868, step time: 0.2563\n",
      "66/281, train_loss: 0.4014, step time: 0.2518\n",
      "67/281, train_loss: 0.3781, step time: 0.2589\n",
      "68/281, train_loss: 0.4132, step time: 0.2492\n",
      "69/281, train_loss: 0.3933, step time: 0.2508\n",
      "70/281, train_loss: 0.4271, step time: 0.2550\n",
      "71/281, train_loss: 0.4084, step time: 0.2575\n",
      "72/281, train_loss: 0.4102, step time: 0.2521\n",
      "73/281, train_loss: 0.4081, step time: 0.2510\n",
      "74/281, train_loss: 0.4109, step time: 0.2559\n",
      "75/281, train_loss: 0.4023, step time: 0.2564\n",
      "76/281, train_loss: 0.4055, step time: 0.2606\n",
      "77/281, train_loss: 0.4360, step time: 0.2576\n",
      "78/281, train_loss: 0.4205, step time: 0.2601\n",
      "79/281, train_loss: 0.4301, step time: 0.2578\n",
      "80/281, train_loss: 0.3965, step time: 0.2521\n",
      "81/281, train_loss: 0.3828, step time: 0.2553\n",
      "82/281, train_loss: 0.3873, step time: 0.2523\n",
      "83/281, train_loss: 0.4316, step time: 0.2519\n",
      "84/281, train_loss: 0.4043, step time: 0.2530\n",
      "85/281, train_loss: 0.4160, step time: 0.2552\n",
      "86/281, train_loss: 0.4020, step time: 0.2557\n",
      "87/281, train_loss: 0.4009, step time: 0.2656\n",
      "88/281, train_loss: 0.4207, step time: 0.2555\n",
      "89/281, train_loss: 0.4133, step time: 0.2544\n",
      "90/281, train_loss: 0.3718, step time: 0.2578\n",
      "91/281, train_loss: 0.3800, step time: 0.2584\n",
      "92/281, train_loss: 0.4073, step time: 0.2555\n",
      "93/281, train_loss: 0.4103, step time: 0.2597\n",
      "94/281, train_loss: 0.3860, step time: 0.2567\n",
      "95/281, train_loss: 0.3873, step time: 0.2527\n",
      "96/281, train_loss: 0.3859, step time: 0.2523\n",
      "97/281, train_loss: 0.6114, step time: 0.2533\n",
      "98/281, train_loss: 0.4114, step time: 0.2577\n",
      "99/281, train_loss: 0.3954, step time: 0.2569\n",
      "100/281, train_loss: 0.4092, step time: 0.2581\n",
      "101/281, train_loss: 0.5536, step time: 0.2475\n",
      "102/281, train_loss: 0.4430, step time: 0.2529\n",
      "103/281, train_loss: 0.4225, step time: 0.2511\n",
      "104/281, train_loss: 0.3976, step time: 0.2546\n",
      "105/281, train_loss: 0.3717, step time: 0.2497\n",
      "106/281, train_loss: 0.3960, step time: 0.2540\n",
      "107/281, train_loss: 0.4137, step time: 0.2522\n",
      "108/281, train_loss: 0.3906, step time: 0.2544\n",
      "109/281, train_loss: 0.3687, step time: 0.2569\n",
      "110/281, train_loss: 0.4182, step time: 0.2525\n",
      "111/281, train_loss: 0.3968, step time: 0.2568\n",
      "112/281, train_loss: 0.3949, step time: 0.2555\n",
      "113/281, train_loss: 0.4102, step time: 0.2545\n",
      "114/281, train_loss: 0.4065, step time: 0.2541\n",
      "115/281, train_loss: 0.4464, step time: 0.2513\n",
      "116/281, train_loss: 0.4041, step time: 0.2528\n",
      "117/281, train_loss: 0.4322, step time: 0.2521\n",
      "118/281, train_loss: 0.4106, step time: 0.2520\n",
      "119/281, train_loss: 0.3765, step time: 0.2535\n",
      "120/281, train_loss: 0.4142, step time: 0.2520\n",
      "121/281, train_loss: 0.4035, step time: 0.2603\n",
      "122/281, train_loss: 0.5603, step time: 0.2590\n",
      "123/281, train_loss: 0.3841, step time: 0.2539\n",
      "124/281, train_loss: 0.5388, step time: 0.2505\n",
      "125/281, train_loss: 0.4395, step time: 0.2583\n",
      "126/281, train_loss: 0.3821, step time: 0.2566\n",
      "127/281, train_loss: 0.3889, step time: 0.2543\n",
      "128/281, train_loss: 0.4157, step time: 0.2486\n",
      "129/281, train_loss: 0.5284, step time: 0.2533\n",
      "130/281, train_loss: 0.5480, step time: 0.2528\n",
      "131/281, train_loss: 0.5400, step time: 0.2515\n",
      "132/281, train_loss: 0.4084, step time: 0.2517\n",
      "133/281, train_loss: 0.4334, step time: 0.2553\n",
      "134/281, train_loss: 0.4692, step time: 0.2510\n",
      "135/281, train_loss: 0.3875, step time: 0.2519\n",
      "136/281, train_loss: 0.3972, step time: 0.2479\n",
      "137/281, train_loss: 0.6864, step time: 0.2531\n",
      "138/281, train_loss: 0.3907, step time: 0.2583\n",
      "139/281, train_loss: 0.5651, step time: 0.2541\n",
      "140/281, train_loss: 0.3830, step time: 0.2526\n",
      "141/281, train_loss: 0.4098, step time: 0.2510\n",
      "142/281, train_loss: 0.4371, step time: 0.2486\n",
      "143/281, train_loss: 0.4274, step time: 0.2475\n",
      "144/281, train_loss: 0.3870, step time: 0.2471\n",
      "145/281, train_loss: 0.3724, step time: 0.2540\n",
      "146/281, train_loss: 0.5770, step time: 0.2542\n",
      "147/281, train_loss: 0.3813, step time: 0.2481\n",
      "148/281, train_loss: 0.4552, step time: 0.2462\n",
      "149/281, train_loss: 0.4568, step time: 0.2531\n",
      "150/281, train_loss: 0.3963, step time: 0.2520\n",
      "151/281, train_loss: 0.4009, step time: 0.2563\n",
      "152/281, train_loss: 0.4151, step time: 0.2647\n",
      "153/281, train_loss: 0.4281, step time: 0.2575\n",
      "154/281, train_loss: 0.4130, step time: 0.2547\n",
      "155/281, train_loss: 0.3715, step time: 0.2570\n",
      "156/281, train_loss: 0.5289, step time: 0.2532\n",
      "157/281, train_loss: 0.3723, step time: 0.2604\n",
      "158/281, train_loss: 0.3895, step time: 0.2581\n",
      "159/281, train_loss: 0.3898, step time: 0.2568\n",
      "160/281, train_loss: 0.3997, step time: 0.2572\n",
      "161/281, train_loss: 0.3801, step time: 0.2599\n",
      "162/281, train_loss: 0.3950, step time: 0.2594\n",
      "163/281, train_loss: 0.4032, step time: 0.2532\n",
      "164/281, train_loss: 0.4203, step time: 0.2575\n",
      "165/281, train_loss: 0.5478, step time: 0.2547\n",
      "166/281, train_loss: 0.4207, step time: 0.2540\n",
      "167/281, train_loss: 0.4085, step time: 0.2561\n",
      "168/281, train_loss: 0.5305, step time: 0.2519\n",
      "169/281, train_loss: 0.3936, step time: 0.2538\n",
      "170/281, train_loss: 0.4371, step time: 0.2512\n",
      "171/281, train_loss: 0.5602, step time: 0.2516\n",
      "172/281, train_loss: 0.4052, step time: 0.2524\n",
      "173/281, train_loss: 0.3936, step time: 0.2554\n",
      "174/281, train_loss: 0.4335, step time: 0.2541\n",
      "175/281, train_loss: 0.3730, step time: 0.2570\n",
      "176/281, train_loss: 0.3936, step time: 0.2500\n",
      "177/281, train_loss: 0.5250, step time: 0.2616\n",
      "178/281, train_loss: 0.4480, step time: 0.2585\n",
      "179/281, train_loss: 0.3614, step time: 0.2488\n",
      "180/281, train_loss: 0.3844, step time: 0.2523\n",
      "181/281, train_loss: 0.4423, step time: 0.2553\n",
      "182/281, train_loss: 0.5424, step time: 0.2595\n",
      "183/281, train_loss: 0.3870, step time: 0.2566\n",
      "184/281, train_loss: 0.4187, step time: 0.2548\n",
      "185/281, train_loss: 0.5499, step time: 0.2555\n",
      "186/281, train_loss: 0.3747, step time: 0.2600\n",
      "187/281, train_loss: 0.5442, step time: 0.2588\n",
      "188/281, train_loss: 0.3870, step time: 0.2646\n",
      "189/281, train_loss: 0.4091, step time: 0.2552\n",
      "190/281, train_loss: 0.5381, step time: 0.2521\n",
      "191/281, train_loss: 0.3813, step time: 0.2579\n",
      "192/281, train_loss: 0.4045, step time: 0.2645\n",
      "193/281, train_loss: 0.5625, step time: 0.2601\n",
      "194/281, train_loss: 0.5552, step time: 0.2571\n",
      "195/281, train_loss: 0.4111, step time: 0.2570\n",
      "196/281, train_loss: 0.4059, step time: 0.2555\n",
      "197/281, train_loss: 0.3896, step time: 0.2492\n",
      "198/281, train_loss: 0.3830, step time: 0.2523\n",
      "199/281, train_loss: 0.3842, step time: 0.2497\n",
      "200/281, train_loss: 0.3940, step time: 0.2523\n",
      "201/281, train_loss: 0.5511, step time: 0.2508\n",
      "202/281, train_loss: 0.5902, step time: 0.2540\n",
      "203/281, train_loss: 0.4150, step time: 0.2536\n",
      "204/281, train_loss: 0.4465, step time: 0.2523\n",
      "205/281, train_loss: 0.4400, step time: 0.2543\n",
      "206/281, train_loss: 0.5698, step time: 0.2543\n",
      "207/281, train_loss: 0.4145, step time: 0.2572\n",
      "208/281, train_loss: 0.5532, step time: 0.2605\n",
      "209/281, train_loss: 0.4459, step time: 0.2621\n",
      "210/281, train_loss: 0.4088, step time: 0.2511\n",
      "211/281, train_loss: 0.3787, step time: 0.2569\n",
      "212/281, train_loss: 0.3890, step time: 0.2537\n",
      "213/281, train_loss: 0.4119, step time: 0.2567\n",
      "214/281, train_loss: 0.5388, step time: 0.2574\n",
      "215/281, train_loss: 0.5587, step time: 0.2613\n",
      "216/281, train_loss: 0.4406, step time: 0.2565\n",
      "217/281, train_loss: 0.4500, step time: 0.2522\n",
      "218/281, train_loss: 0.3849, step time: 0.2542\n",
      "219/281, train_loss: 0.4071, step time: 0.2546\n",
      "220/281, train_loss: 0.4731, step time: 0.2583\n",
      "221/281, train_loss: 0.3881, step time: 0.2550\n",
      "222/281, train_loss: 0.5399, step time: 0.2492\n",
      "223/281, train_loss: 0.4080, step time: 0.2516\n",
      "224/281, train_loss: 0.3996, step time: 0.2506\n",
      "225/281, train_loss: 0.4354, step time: 0.2503\n",
      "226/281, train_loss: 0.4155, step time: 0.2501\n",
      "227/281, train_loss: 0.5651, step time: 0.2502\n",
      "228/281, train_loss: 0.4055, step time: 0.2507\n",
      "229/281, train_loss: 0.4092, step time: 0.2507\n",
      "230/281, train_loss: 0.3898, step time: 0.2533\n",
      "231/281, train_loss: 0.4082, step time: 0.2495\n",
      "232/281, train_loss: 0.3776, step time: 0.2544\n",
      "233/281, train_loss: 0.4086, step time: 0.2580\n",
      "234/281, train_loss: 0.4023, step time: 0.2547\n",
      "235/281, train_loss: 0.4253, step time: 0.2522\n",
      "236/281, train_loss: 0.3780, step time: 0.2525\n",
      "237/281, train_loss: 0.4226, step time: 0.2555\n",
      "238/281, train_loss: 0.4172, step time: 0.2554\n",
      "239/281, train_loss: 0.3756, step time: 0.2579\n",
      "240/281, train_loss: 0.3740, step time: 0.2536\n",
      "241/281, train_loss: 0.4121, step time: 0.2568\n",
      "242/281, train_loss: 0.4142, step time: 0.2470\n",
      "243/281, train_loss: 0.4783, step time: 0.2549\n",
      "244/281, train_loss: 0.3764, step time: 0.2533\n",
      "245/281, train_loss: 0.3847, step time: 0.2533\n",
      "246/281, train_loss: 0.4200, step time: 0.2533\n",
      "247/281, train_loss: 0.4293, step time: 0.2541\n",
      "248/281, train_loss: 0.4122, step time: 0.2517\n",
      "249/281, train_loss: 0.3989, step time: 0.2474\n",
      "250/281, train_loss: 0.3868, step time: 0.2507\n",
      "251/281, train_loss: 0.5773, step time: 0.2518\n",
      "252/281, train_loss: 0.4257, step time: 0.2524\n",
      "253/281, train_loss: 0.4352, step time: 0.2536\n",
      "254/281, train_loss: 0.3778, step time: 0.2517\n",
      "255/281, train_loss: 0.4300, step time: 0.2511\n",
      "256/281, train_loss: 0.6930, step time: 0.2558\n",
      "257/281, train_loss: 0.5459, step time: 0.2545\n",
      "258/281, train_loss: 0.5762, step time: 0.2510\n",
      "259/281, train_loss: 0.4154, step time: 0.2486\n",
      "260/281, train_loss: 0.4047, step time: 0.2535\n",
      "261/281, train_loss: 0.4147, step time: 0.2511\n",
      "262/281, train_loss: 0.4287, step time: 0.2554\n",
      "263/281, train_loss: 0.3911, step time: 0.2525\n",
      "264/281, train_loss: 0.3811, step time: 0.2572\n",
      "265/281, train_loss: 0.5433, step time: 0.2538\n",
      "266/281, train_loss: 0.3987, step time: 0.2608\n",
      "267/281, train_loss: 0.3910, step time: 0.2516\n",
      "268/281, train_loss: 0.3857, step time: 0.2540\n",
      "269/281, train_loss: 0.4601, step time: 0.2542\n",
      "270/281, train_loss: 0.4120, step time: 0.2573\n",
      "271/281, train_loss: 0.3845, step time: 0.2494\n",
      "272/281, train_loss: 0.3919, step time: 0.2523\n",
      "273/281, train_loss: 0.4323, step time: 0.2640\n",
      "274/281, train_loss: 0.4009, step time: 0.2485\n",
      "275/281, train_loss: 0.4613, step time: 0.2550\n",
      "276/281, train_loss: 0.5766, step time: 0.2516\n",
      "277/281, train_loss: 0.3765, step time: 0.2520\n",
      "278/281, train_loss: 0.5521, step time: 0.2540\n",
      "279/281, train_loss: 0.5499, step time: 0.2512\n",
      "280/281, train_loss: 0.3926, step time: 0.2544\n",
      "281/281, train_loss: 0.4424, step time: 0.2507\n",
      "282/281, train_loss: 0.4137, step time: 0.1534\n",
      "epoch 27 average loss: 0.4314\n",
      "current epoch: 27 current mean dice: 0.5769 tc: 0.0198 wt: 0.9098 et: 0.8302\n",
      "best mean dice: 0.5770 at epoch: 26\n",
      "time consuming of epoch 27 is: 425.0872\n",
      "----------\n",
      "epoch 28/200\n",
      "1/281, train_loss: 0.4556, step time: 0.2840\n",
      "2/281, train_loss: 0.3768, step time: 0.2557\n",
      "3/281, train_loss: 0.4354, step time: 0.2655\n",
      "4/281, train_loss: 0.4380, step time: 0.2509\n",
      "5/281, train_loss: 0.3964, step time: 0.2482\n",
      "6/281, train_loss: 0.5498, step time: 0.2489\n",
      "7/281, train_loss: 0.3802, step time: 0.2475\n",
      "8/281, train_loss: 0.3911, step time: 0.2491\n",
      "9/281, train_loss: 0.4084, step time: 0.2478\n",
      "10/281, train_loss: 0.4114, step time: 0.2452\n",
      "11/281, train_loss: 0.4085, step time: 0.2474\n",
      "12/281, train_loss: 0.3960, step time: 0.2489\n",
      "13/281, train_loss: 0.5800, step time: 0.2481\n",
      "14/281, train_loss: 0.3991, step time: 0.2449\n",
      "15/281, train_loss: 0.4097, step time: 0.2463\n",
      "16/281, train_loss: 0.3992, step time: 0.2428\n",
      "17/281, train_loss: 0.3829, step time: 0.2544\n",
      "18/281, train_loss: 0.4116, step time: 0.2573\n",
      "19/281, train_loss: 0.5210, step time: 0.2575\n",
      "20/281, train_loss: 0.4635, step time: 0.2539\n",
      "21/281, train_loss: 0.4375, step time: 0.2515\n",
      "22/281, train_loss: 0.4220, step time: 0.2505\n",
      "23/281, train_loss: 0.4180, step time: 0.2544\n",
      "24/281, train_loss: 0.3858, step time: 0.2493\n",
      "25/281, train_loss: 0.3956, step time: 0.2576\n",
      "26/281, train_loss: 0.4139, step time: 0.2493\n",
      "27/281, train_loss: 0.5649, step time: 0.2496\n",
      "28/281, train_loss: 0.4014, step time: 0.2467\n",
      "29/281, train_loss: 0.3983, step time: 0.2500\n",
      "30/281, train_loss: 0.3967, step time: 0.2443\n",
      "31/281, train_loss: 0.4142, step time: 0.2433\n",
      "32/281, train_loss: 0.5278, step time: 0.2466\n",
      "33/281, train_loss: 0.3912, step time: 0.2553\n",
      "34/281, train_loss: 0.3900, step time: 0.2545\n",
      "35/281, train_loss: 0.3906, step time: 0.2484\n",
      "36/281, train_loss: 0.6023, step time: 0.2519\n",
      "37/281, train_loss: 0.4048, step time: 0.2521\n",
      "38/281, train_loss: 0.4043, step time: 0.2599\n",
      "39/281, train_loss: 0.4244, step time: 0.2627\n",
      "40/281, train_loss: 0.3893, step time: 0.2608\n",
      "41/281, train_loss: 0.4013, step time: 0.2522\n",
      "42/281, train_loss: 0.3882, step time: 0.2508\n",
      "43/281, train_loss: 0.4675, step time: 0.2513\n",
      "44/281, train_loss: 0.3930, step time: 0.2540\n",
      "45/281, train_loss: 0.5421, step time: 0.2485\n",
      "46/281, train_loss: 0.5487, step time: 0.2548\n",
      "47/281, train_loss: 0.3696, step time: 0.2574\n",
      "48/281, train_loss: 0.3764, step time: 0.2496\n",
      "49/281, train_loss: 0.4095, step time: 0.2533\n",
      "50/281, train_loss: 0.3819, step time: 0.2504\n",
      "51/281, train_loss: 0.4068, step time: 0.2507\n",
      "52/281, train_loss: 0.4011, step time: 0.2484\n",
      "53/281, train_loss: 0.4045, step time: 0.2506\n",
      "54/281, train_loss: 0.4027, step time: 0.2576\n",
      "55/281, train_loss: 0.4127, step time: 0.2612\n",
      "56/281, train_loss: 0.4834, step time: 0.2582\n",
      "57/281, train_loss: 0.4123, step time: 0.2554\n",
      "58/281, train_loss: 0.4251, step time: 0.2499\n",
      "59/281, train_loss: 0.4307, step time: 0.2435\n",
      "60/281, train_loss: 0.3687, step time: 0.2442\n",
      "61/281, train_loss: 0.5423, step time: 0.2478\n",
      "62/281, train_loss: 0.4890, step time: 0.2490\n",
      "63/281, train_loss: 0.4339, step time: 0.2527\n",
      "64/281, train_loss: 0.4151, step time: 0.2474\n",
      "65/281, train_loss: 0.3715, step time: 0.2516\n",
      "66/281, train_loss: 0.4269, step time: 0.2547\n",
      "67/281, train_loss: 0.4264, step time: 0.2516\n",
      "68/281, train_loss: 0.4128, step time: 0.2541\n",
      "69/281, train_loss: 0.3978, step time: 0.2538\n",
      "70/281, train_loss: 0.4208, step time: 0.2508\n",
      "71/281, train_loss: 0.4193, step time: 0.2478\n",
      "72/281, train_loss: 0.4209, step time: 0.2526\n",
      "73/281, train_loss: 0.4413, step time: 0.2552\n",
      "74/281, train_loss: 0.5272, step time: 0.2533\n",
      "75/281, train_loss: 0.4323, step time: 0.2533\n",
      "76/281, train_loss: 0.3691, step time: 0.2559\n",
      "77/281, train_loss: 0.4191, step time: 0.2456\n",
      "78/281, train_loss: 0.5362, step time: 0.2439\n",
      "79/281, train_loss: 0.5416, step time: 0.2474\n",
      "80/281, train_loss: 0.5640, step time: 0.2523\n",
      "81/281, train_loss: 0.4011, step time: 0.2433\n",
      "82/281, train_loss: 0.3971, step time: 0.2455\n",
      "83/281, train_loss: 0.4113, step time: 0.2462\n",
      "84/281, train_loss: 0.4154, step time: 0.2441\n",
      "85/281, train_loss: 0.4322, step time: 0.2570\n",
      "86/281, train_loss: 0.3881, step time: 0.2605\n",
      "87/281, train_loss: 0.4118, step time: 0.2570\n",
      "88/281, train_loss: 0.4000, step time: 0.2574\n",
      "89/281, train_loss: 0.3848, step time: 0.2514\n",
      "90/281, train_loss: 0.3707, step time: 0.2577\n",
      "91/281, train_loss: 0.3927, step time: 0.2479\n",
      "92/281, train_loss: 0.3808, step time: 0.2470\n",
      "93/281, train_loss: 0.3731, step time: 0.2598\n",
      "94/281, train_loss: 0.4230, step time: 0.2504\n",
      "95/281, train_loss: 0.3966, step time: 0.2574\n",
      "96/281, train_loss: 0.5521, step time: 0.2818\n",
      "97/281, train_loss: 0.4035, step time: 0.2495\n",
      "98/281, train_loss: 0.5272, step time: 0.2470\n",
      "99/281, train_loss: 0.3822, step time: 0.2448\n",
      "100/281, train_loss: 0.5355, step time: 0.2453\n",
      "101/281, train_loss: 0.3794, step time: 0.2442\n",
      "102/281, train_loss: 0.3899, step time: 0.2457\n",
      "103/281, train_loss: 0.4554, step time: 0.2469\n",
      "104/281, train_loss: 0.5628, step time: 0.2533\n",
      "105/281, train_loss: 0.4223, step time: 0.2573\n",
      "106/281, train_loss: 0.5689, step time: 0.2840\n",
      "107/281, train_loss: 0.4039, step time: 0.2511\n",
      "108/281, train_loss: 0.3982, step time: 0.2453\n",
      "109/281, train_loss: 0.3943, step time: 0.2566\n",
      "110/281, train_loss: 0.4051, step time: 0.2574\n",
      "111/281, train_loss: 0.4135, step time: 0.2562\n",
      "112/281, train_loss: 0.3977, step time: 0.2525\n",
      "113/281, train_loss: 0.4183, step time: 0.2598\n",
      "114/281, train_loss: 0.3994, step time: 0.2617\n",
      "115/281, train_loss: 0.4044, step time: 0.2617\n",
      "116/281, train_loss: 0.4041, step time: 0.2555\n",
      "117/281, train_loss: 0.5349, step time: 0.2525\n",
      "118/281, train_loss: 0.3826, step time: 0.2548\n",
      "119/281, train_loss: 0.3801, step time: 0.2599\n",
      "120/281, train_loss: 0.4242, step time: 0.2544\n",
      "121/281, train_loss: 0.5576, step time: 0.2529\n",
      "122/281, train_loss: 0.4098, step time: 0.2481\n",
      "123/281, train_loss: 0.4006, step time: 0.2442\n",
      "124/281, train_loss: 0.4408, step time: 0.2531\n",
      "125/281, train_loss: 0.5253, step time: 0.2475\n",
      "126/281, train_loss: 0.3909, step time: 0.2513\n",
      "127/281, train_loss: 0.3896, step time: 0.2504\n",
      "128/281, train_loss: 0.4580, step time: 0.2490\n",
      "129/281, train_loss: 0.3994, step time: 0.2523\n",
      "130/281, train_loss: 0.4673, step time: 0.2541\n",
      "131/281, train_loss: 0.3875, step time: 0.2523\n",
      "132/281, train_loss: 0.3997, step time: 0.2551\n",
      "133/281, train_loss: 0.4390, step time: 0.2532\n",
      "134/281, train_loss: 0.4868, step time: 0.2553\n",
      "135/281, train_loss: 0.3815, step time: 0.2544\n",
      "136/281, train_loss: 0.5651, step time: 0.2459\n",
      "137/281, train_loss: 0.4220, step time: 0.2462\n",
      "138/281, train_loss: 0.3961, step time: 0.2443\n",
      "139/281, train_loss: 0.5841, step time: 0.2415\n",
      "140/281, train_loss: 0.4048, step time: 0.2563\n",
      "141/281, train_loss: 0.3948, step time: 0.2499\n",
      "142/281, train_loss: 0.3979, step time: 0.2493\n",
      "143/281, train_loss: 0.4153, step time: 0.2468\n",
      "144/281, train_loss: 0.3793, step time: 0.2484\n",
      "145/281, train_loss: 0.5348, step time: 0.2557\n",
      "146/281, train_loss: 0.4438, step time: 0.2511\n",
      "147/281, train_loss: 0.3879, step time: 0.2502\n",
      "148/281, train_loss: 0.5543, step time: 0.2462\n",
      "149/281, train_loss: 0.4354, step time: 0.2488\n",
      "150/281, train_loss: 0.3838, step time: 0.2479\n",
      "151/281, train_loss: 0.5369, step time: 0.2503\n",
      "152/281, train_loss: 0.3918, step time: 0.2501\n",
      "153/281, train_loss: 0.5812, step time: 0.2496\n",
      "154/281, train_loss: 0.4007, step time: 0.2506\n",
      "155/281, train_loss: 0.3761, step time: 0.2498\n",
      "156/281, train_loss: 0.5348, step time: 0.2470\n",
      "157/281, train_loss: 0.4137, step time: 0.2492\n",
      "158/281, train_loss: 0.5416, step time: 0.2529\n",
      "159/281, train_loss: 0.4032, step time: 0.2548\n",
      "160/281, train_loss: 0.3952, step time: 0.2513\n",
      "161/281, train_loss: 0.4492, step time: 0.2561\n",
      "162/281, train_loss: 0.3968, step time: 0.2515\n",
      "163/281, train_loss: 0.4004, step time: 0.2494\n",
      "164/281, train_loss: 0.4054, step time: 0.2511\n",
      "165/281, train_loss: 0.3663, step time: 0.2468\n",
      "166/281, train_loss: 0.4059, step time: 0.2450\n",
      "167/281, train_loss: 0.4078, step time: 0.2447\n",
      "168/281, train_loss: 0.4284, step time: 0.2533\n",
      "169/281, train_loss: 0.4043, step time: 0.2480\n",
      "170/281, train_loss: 0.4191, step time: 0.2543\n",
      "171/281, train_loss: 0.3815, step time: 0.2513\n",
      "172/281, train_loss: 0.3841, step time: 0.2536\n",
      "173/281, train_loss: 0.3722, step time: 0.2517\n",
      "174/281, train_loss: 0.5252, step time: 0.2537\n",
      "175/281, train_loss: 0.3911, step time: 0.2533\n",
      "176/281, train_loss: 0.4216, step time: 0.2518\n",
      "177/281, train_loss: 0.3888, step time: 0.2540\n",
      "178/281, train_loss: 0.4102, step time: 0.2572\n",
      "179/281, train_loss: 0.4568, step time: 0.2555\n",
      "180/281, train_loss: 0.3859, step time: 0.2510\n",
      "181/281, train_loss: 0.4060, step time: 0.2517\n",
      "182/281, train_loss: 0.5416, step time: 0.2509\n",
      "183/281, train_loss: 0.4079, step time: 0.2545\n",
      "184/281, train_loss: 0.3984, step time: 0.2531\n",
      "185/281, train_loss: 0.4520, step time: 0.2496\n",
      "186/281, train_loss: 0.4322, step time: 0.2500\n",
      "187/281, train_loss: 0.5292, step time: 0.2489\n",
      "188/281, train_loss: 0.5374, step time: 0.2510\n",
      "189/281, train_loss: 0.4488, step time: 0.2545\n",
      "190/281, train_loss: 0.3924, step time: 0.2598\n",
      "191/281, train_loss: 0.4249, step time: 0.2590\n",
      "192/281, train_loss: 0.4457, step time: 0.2504\n",
      "193/281, train_loss: 0.3925, step time: 0.2500\n",
      "194/281, train_loss: 0.4431, step time: 0.2535\n",
      "195/281, train_loss: 0.4308, step time: 0.2501\n",
      "196/281, train_loss: 0.5237, step time: 0.2493\n",
      "197/281, train_loss: 0.4391, step time: 0.2552\n",
      "198/281, train_loss: 0.4040, step time: 0.2536\n",
      "199/281, train_loss: 0.5488, step time: 0.2547\n",
      "200/281, train_loss: 0.5479, step time: 0.2621\n",
      "201/281, train_loss: 0.4239, step time: 0.2551\n",
      "202/281, train_loss: 0.3876, step time: 0.2470\n",
      "203/281, train_loss: 0.3778, step time: 0.2499\n",
      "204/281, train_loss: 0.3998, step time: 0.2511\n",
      "205/281, train_loss: 0.5671, step time: 0.2536\n",
      "206/281, train_loss: 0.3879, step time: 0.2476\n",
      "207/281, train_loss: 0.3989, step time: 0.2464\n",
      "208/281, train_loss: 0.4469, step time: 0.2496\n",
      "209/281, train_loss: 0.3944, step time: 0.2521\n",
      "210/281, train_loss: 0.4146, step time: 0.2521\n",
      "211/281, train_loss: 0.3749, step time: 0.2510\n",
      "212/281, train_loss: 0.4100, step time: 0.2513\n",
      "213/281, train_loss: 0.4523, step time: 0.2541\n",
      "214/281, train_loss: 0.4187, step time: 0.2557\n",
      "215/281, train_loss: 0.3944, step time: 0.2554\n",
      "216/281, train_loss: 0.4188, step time: 0.2556\n",
      "217/281, train_loss: 0.3925, step time: 0.2517\n",
      "218/281, train_loss: 0.5541, step time: 0.2543\n",
      "219/281, train_loss: 0.4310, step time: 0.2597\n",
      "220/281, train_loss: 0.4126, step time: 0.2552\n",
      "221/281, train_loss: 0.4038, step time: 0.2567\n",
      "222/281, train_loss: 0.4115, step time: 0.2502\n",
      "223/281, train_loss: 0.4050, step time: 0.2520\n",
      "224/281, train_loss: 0.4114, step time: 0.2533\n",
      "225/281, train_loss: 0.4009, step time: 0.2548\n",
      "226/281, train_loss: 0.4165, step time: 0.2537\n",
      "227/281, train_loss: 0.4022, step time: 0.2552\n",
      "228/281, train_loss: 0.5439, step time: 0.2519\n",
      "229/281, train_loss: 0.3703, step time: 0.2561\n",
      "230/281, train_loss: 0.3802, step time: 0.2535\n",
      "231/281, train_loss: 0.3839, step time: 0.2499\n",
      "232/281, train_loss: 0.4872, step time: 0.2566\n",
      "233/281, train_loss: 0.4004, step time: 0.2518\n",
      "234/281, train_loss: 0.3770, step time: 0.2485\n",
      "235/281, train_loss: 0.3832, step time: 0.2508\n",
      "236/281, train_loss: 0.4098, step time: 0.2534\n",
      "237/281, train_loss: 0.4288, step time: 0.2535\n",
      "238/281, train_loss: 0.4321, step time: 0.2604\n",
      "239/281, train_loss: 0.3966, step time: 0.2483\n",
      "240/281, train_loss: 0.3981, step time: 0.2525\n",
      "241/281, train_loss: 0.4221, step time: 0.2533\n",
      "242/281, train_loss: 0.4094, step time: 0.2549\n",
      "243/281, train_loss: 0.4700, step time: 0.2577\n",
      "244/281, train_loss: 0.3724, step time: 0.2498\n",
      "245/281, train_loss: 0.3792, step time: 0.2598\n",
      "246/281, train_loss: 0.3790, step time: 0.2513\n",
      "247/281, train_loss: 0.5605, step time: 0.2555\n",
      "248/281, train_loss: 0.3850, step time: 0.2552\n",
      "249/281, train_loss: 0.5400, step time: 0.2483\n",
      "250/281, train_loss: 0.3901, step time: 0.2523\n",
      "251/281, train_loss: 0.3775, step time: 0.2539\n",
      "252/281, train_loss: 0.3968, step time: 0.2604\n",
      "253/281, train_loss: 0.3815, step time: 0.2497\n",
      "254/281, train_loss: 0.3703, step time: 0.2536\n",
      "255/281, train_loss: 0.5436, step time: 0.2528\n",
      "256/281, train_loss: 0.5723, step time: 0.2599\n",
      "257/281, train_loss: 0.4007, step time: 0.2513\n",
      "258/281, train_loss: 0.5280, step time: 0.2571\n",
      "259/281, train_loss: 0.5438, step time: 0.2536\n",
      "260/281, train_loss: 0.3869, step time: 0.2509\n",
      "261/281, train_loss: 0.3707, step time: 0.2562\n",
      "262/281, train_loss: 0.3929, step time: 0.2543\n",
      "263/281, train_loss: 0.5502, step time: 0.2566\n",
      "264/281, train_loss: 0.3966, step time: 0.2555\n",
      "265/281, train_loss: 0.3894, step time: 0.2553\n",
      "266/281, train_loss: 0.5718, step time: 0.2534\n",
      "267/281, train_loss: 0.5475, step time: 0.2531\n",
      "268/281, train_loss: 0.3838, step time: 0.2501\n",
      "269/281, train_loss: 0.5811, step time: 0.2596\n",
      "270/281, train_loss: 0.3840, step time: 0.2540\n",
      "271/281, train_loss: 0.4599, step time: 0.2530\n",
      "272/281, train_loss: 0.4151, step time: 0.2591\n",
      "273/281, train_loss: 0.3997, step time: 0.2585\n",
      "274/281, train_loss: 0.4072, step time: 0.2620\n",
      "275/281, train_loss: 0.3893, step time: 0.2563\n",
      "276/281, train_loss: 0.3923, step time: 0.2594\n",
      "277/281, train_loss: 0.4017, step time: 0.2585\n",
      "278/281, train_loss: 0.3877, step time: 0.2614\n",
      "279/281, train_loss: 0.4036, step time: 0.2578\n",
      "280/281, train_loss: 0.4247, step time: 0.2535\n",
      "281/281, train_loss: 0.3911, step time: 0.2496\n",
      "282/281, train_loss: 0.4206, step time: 0.1497\n",
      "epoch 28 average loss: 0.4314\n",
      "saved new best metric model\n",
      "current epoch: 28 current mean dice: 0.5776 tc: 0.0212 wt: 0.9088 et: 0.8340\n",
      "best mean dice: 0.5776 at epoch: 28\n",
      "time consuming of epoch 28 is: 414.0761\n",
      "----------\n",
      "epoch 29/200\n",
      "1/281, train_loss: 0.6337, step time: 0.2566\n",
      "2/281, train_loss: 0.4175, step time: 0.2506\n",
      "3/281, train_loss: 0.4087, step time: 0.2478\n",
      "4/281, train_loss: 0.4258, step time: 0.2453\n",
      "5/281, train_loss: 0.4331, step time: 0.2460\n",
      "6/281, train_loss: 0.3828, step time: 0.2493\n",
      "7/281, train_loss: 0.4118, step time: 0.2507\n",
      "8/281, train_loss: 0.3760, step time: 0.2500\n",
      "9/281, train_loss: 0.4416, step time: 0.2562\n",
      "10/281, train_loss: 0.5189, step time: 0.2491\n",
      "11/281, train_loss: 0.3802, step time: 0.2480\n",
      "12/281, train_loss: 0.3871, step time: 0.2492\n",
      "13/281, train_loss: 0.4397, step time: 0.2520\n",
      "14/281, train_loss: 0.4048, step time: 0.2609\n",
      "15/281, train_loss: 0.4894, step time: 0.2548\n",
      "16/281, train_loss: 0.5743, step time: 0.2531\n",
      "17/281, train_loss: 0.6375, step time: 0.2534\n",
      "18/281, train_loss: 0.5556, step time: 0.2521\n",
      "19/281, train_loss: 0.4252, step time: 0.2534\n",
      "20/281, train_loss: 0.4030, step time: 0.2542\n",
      "21/281, train_loss: 0.4035, step time: 0.2518\n",
      "22/281, train_loss: 0.4065, step time: 0.2498\n",
      "23/281, train_loss: 0.3694, step time: 0.2489\n",
      "24/281, train_loss: 0.4265, step time: 0.2502\n",
      "25/281, train_loss: 0.5369, step time: 0.2556\n",
      "26/281, train_loss: 0.4092, step time: 0.2655\n",
      "27/281, train_loss: 0.3943, step time: 0.2586\n",
      "28/281, train_loss: 0.4154, step time: 0.2570\n",
      "29/281, train_loss: 0.5277, step time: 0.2588\n",
      "30/281, train_loss: 0.3914, step time: 0.2589\n",
      "31/281, train_loss: 0.4126, step time: 0.2609\n",
      "32/281, train_loss: 0.3895, step time: 0.2559\n",
      "33/281, train_loss: 0.3845, step time: 0.2572\n",
      "34/281, train_loss: 0.4238, step time: 0.2591\n",
      "35/281, train_loss: 0.6886, step time: 0.2604\n",
      "36/281, train_loss: 0.3876, step time: 0.2539\n",
      "37/281, train_loss: 0.5469, step time: 0.2541\n",
      "38/281, train_loss: 0.3962, step time: 0.2567\n",
      "39/281, train_loss: 0.4030, step time: 0.2519\n",
      "40/281, train_loss: 0.4136, step time: 0.2525\n",
      "41/281, train_loss: 0.4068, step time: 0.2550\n",
      "42/281, train_loss: 0.3953, step time: 0.2600\n",
      "43/281, train_loss: 0.3869, step time: 0.2560\n",
      "44/281, train_loss: 0.4189, step time: 0.2558\n",
      "45/281, train_loss: 0.3764, step time: 0.2496\n",
      "46/281, train_loss: 0.3800, step time: 0.2596\n",
      "47/281, train_loss: 0.4360, step time: 0.2530\n",
      "48/281, train_loss: 0.4403, step time: 0.2483\n",
      "49/281, train_loss: 0.4208, step time: 0.2580\n",
      "50/281, train_loss: 0.3633, step time: 0.2591\n",
      "51/281, train_loss: 0.4114, step time: 0.2573\n",
      "52/281, train_loss: 0.4058, step time: 0.2579\n",
      "53/281, train_loss: 0.4166, step time: 0.2631\n",
      "54/281, train_loss: 0.4048, step time: 0.2595\n",
      "55/281, train_loss: 0.4195, step time: 0.2567\n",
      "56/281, train_loss: 0.4309, step time: 0.2521\n",
      "57/281, train_loss: 0.5874, step time: 0.2556\n",
      "58/281, train_loss: 0.5420, step time: 0.2877\n",
      "59/281, train_loss: 0.4149, step time: 0.2559\n",
      "60/281, train_loss: 0.4048, step time: 0.2591\n",
      "61/281, train_loss: 0.4150, step time: 0.2624\n",
      "62/281, train_loss: 0.4256, step time: 0.2538\n",
      "63/281, train_loss: 0.3997, step time: 0.2519\n",
      "64/281, train_loss: 0.3874, step time: 0.2485\n",
      "65/281, train_loss: 0.3928, step time: 0.2527\n",
      "66/281, train_loss: 0.4150, step time: 0.2562\n",
      "67/281, train_loss: 0.3668, step time: 0.2518\n",
      "68/281, train_loss: 0.3957, step time: 0.2564\n",
      "69/281, train_loss: 0.4008, step time: 0.2531\n",
      "70/281, train_loss: 0.4015, step time: 0.2522\n",
      "71/281, train_loss: 0.3946, step time: 0.2639\n",
      "72/281, train_loss: 0.3739, step time: 0.2577\n",
      "73/281, train_loss: 0.3986, step time: 0.2605\n",
      "74/281, train_loss: 0.4037, step time: 0.2641\n",
      "75/281, train_loss: 0.5372, step time: 0.2507\n",
      "76/281, train_loss: 0.5497, step time: 0.2852\n",
      "77/281, train_loss: 0.3955, step time: 0.2662\n",
      "78/281, train_loss: 0.3855, step time: 0.2571\n",
      "79/281, train_loss: 0.3758, step time: 0.2540\n",
      "80/281, train_loss: 0.3854, step time: 0.2586\n",
      "81/281, train_loss: 0.6865, step time: 0.2592\n",
      "82/281, train_loss: 0.4745, step time: 0.2586\n",
      "83/281, train_loss: 0.3955, step time: 0.2568\n",
      "84/281, train_loss: 0.6997, step time: 0.2511\n",
      "85/281, train_loss: 0.3828, step time: 0.2513\n",
      "86/281, train_loss: 0.3992, step time: 0.2487\n",
      "87/281, train_loss: 0.4060, step time: 0.2566\n",
      "88/281, train_loss: 0.4374, step time: 0.2577\n",
      "89/281, train_loss: 0.3906, step time: 0.2931\n",
      "90/281, train_loss: 0.3999, step time: 0.2656\n",
      "91/281, train_loss: 0.4112, step time: 0.2514\n",
      "92/281, train_loss: 0.3993, step time: 0.2504\n",
      "93/281, train_loss: 0.3933, step time: 0.2512\n",
      "94/281, train_loss: 0.3875, step time: 0.2583\n",
      "95/281, train_loss: 0.5473, step time: 0.2585\n",
      "96/281, train_loss: 0.4104, step time: 0.2607\n",
      "97/281, train_loss: 0.3987, step time: 0.2533\n",
      "98/281, train_loss: 0.3822, step time: 0.2492\n",
      "99/281, train_loss: 0.4488, step time: 0.2769\n",
      "100/281, train_loss: 0.3869, step time: 0.2870\n",
      "101/281, train_loss: 0.3822, step time: 0.2638\n",
      "102/281, train_loss: 0.3959, step time: 0.2584\n",
      "103/281, train_loss: 0.3731, step time: 0.2561\n",
      "104/281, train_loss: 0.3932, step time: 0.2524\n",
      "105/281, train_loss: 0.3912, step time: 0.2581\n",
      "106/281, train_loss: 0.4006, step time: 0.2581\n",
      "107/281, train_loss: 0.5315, step time: 0.2538\n",
      "108/281, train_loss: 0.4252, step time: 0.2535\n",
      "109/281, train_loss: 0.4112, step time: 0.2499\n",
      "110/281, train_loss: 0.3708, step time: 0.2514\n",
      "111/281, train_loss: 0.4350, step time: 0.2554\n",
      "112/281, train_loss: 0.3914, step time: 0.2598\n",
      "113/281, train_loss: 0.4057, step time: 0.2686\n",
      "114/281, train_loss: 0.4141, step time: 0.2545\n",
      "115/281, train_loss: 0.3930, step time: 0.2659\n",
      "116/281, train_loss: 0.5277, step time: 0.2537\n",
      "117/281, train_loss: 0.3772, step time: 0.2554\n",
      "118/281, train_loss: 0.4177, step time: 0.2512\n",
      "119/281, train_loss: 0.3835, step time: 0.2558\n",
      "120/281, train_loss: 0.4071, step time: 0.2513\n",
      "121/281, train_loss: 0.3871, step time: 0.2521\n",
      "122/281, train_loss: 0.4016, step time: 0.2599\n",
      "123/281, train_loss: 0.5554, step time: 0.2521\n",
      "124/281, train_loss: 0.3782, step time: 0.2500\n",
      "125/281, train_loss: 0.4074, step time: 0.2567\n",
      "126/281, train_loss: 0.3965, step time: 0.2516\n",
      "127/281, train_loss: 0.3700, step time: 0.2522\n",
      "128/281, train_loss: 0.5519, step time: 0.2508\n",
      "129/281, train_loss: 0.4493, step time: 0.2519\n",
      "130/281, train_loss: 0.4167, step time: 0.2582\n",
      "131/281, train_loss: 0.3876, step time: 0.2476\n",
      "132/281, train_loss: 0.3851, step time: 0.2460\n",
      "133/281, train_loss: 0.4037, step time: 0.2518\n",
      "134/281, train_loss: 0.3647, step time: 0.2520\n",
      "135/281, train_loss: 0.3781, step time: 0.2470\n",
      "136/281, train_loss: 0.5267, step time: 0.2503\n",
      "137/281, train_loss: 0.5165, step time: 0.2527\n",
      "138/281, train_loss: 0.3733, step time: 0.2490\n",
      "139/281, train_loss: 0.5381, step time: 0.2492\n",
      "140/281, train_loss: 0.4072, step time: 0.2504\n",
      "141/281, train_loss: 0.5412, step time: 0.2515\n",
      "142/281, train_loss: 0.3725, step time: 0.2522\n",
      "143/281, train_loss: 0.3969, step time: 0.2493\n",
      "144/281, train_loss: 0.4135, step time: 0.2548\n",
      "145/281, train_loss: 0.4078, step time: 0.2544\n",
      "146/281, train_loss: 0.3695, step time: 0.2786\n",
      "147/281, train_loss: 0.3874, step time: 0.2564\n",
      "148/281, train_loss: 0.3876, step time: 0.2607\n",
      "149/281, train_loss: 0.3740, step time: 0.2573\n",
      "150/281, train_loss: 0.3866, step time: 0.2521\n",
      "151/281, train_loss: 0.3981, step time: 0.2571\n",
      "152/281, train_loss: 0.5209, step time: 0.2486\n",
      "153/281, train_loss: 0.3703, step time: 0.2557\n",
      "154/281, train_loss: 0.4139, step time: 0.2584\n",
      "155/281, train_loss: 0.5535, step time: 0.2526\n",
      "156/281, train_loss: 0.3850, step time: 0.2540\n",
      "157/281, train_loss: 0.4096, step time: 0.2556\n",
      "158/281, train_loss: 0.3860, step time: 0.2496\n",
      "159/281, train_loss: 0.3745, step time: 0.2544\n",
      "160/281, train_loss: 0.5345, step time: 0.2567\n",
      "161/281, train_loss: 0.4202, step time: 0.2563\n",
      "162/281, train_loss: 0.3809, step time: 0.2516\n",
      "163/281, train_loss: 0.4049, step time: 0.2507\n",
      "164/281, train_loss: 0.4506, step time: 0.2559\n",
      "165/281, train_loss: 0.4402, step time: 0.2540\n",
      "166/281, train_loss: 0.3751, step time: 0.2600\n",
      "167/281, train_loss: 0.4201, step time: 0.2565\n",
      "168/281, train_loss: 0.3864, step time: 0.2571\n",
      "169/281, train_loss: 0.4126, step time: 0.2567\n",
      "170/281, train_loss: 0.4360, step time: 0.2566\n",
      "171/281, train_loss: 0.3929, step time: 0.2566\n",
      "172/281, train_loss: 0.5512, step time: 0.2604\n",
      "173/281, train_loss: 0.5485, step time: 0.2624\n",
      "174/281, train_loss: 0.3917, step time: 0.2569\n",
      "175/281, train_loss: 0.5542, step time: 0.2558\n",
      "176/281, train_loss: 0.4019, step time: 0.2561\n",
      "177/281, train_loss: 0.5243, step time: 0.2524\n",
      "178/281, train_loss: 0.4311, step time: 0.2594\n",
      "179/281, train_loss: 0.3924, step time: 0.2581\n",
      "180/281, train_loss: 0.3847, step time: 0.2652\n",
      "181/281, train_loss: 0.3958, step time: 0.2524\n",
      "182/281, train_loss: 0.3857, step time: 0.2534\n",
      "183/281, train_loss: 0.4135, step time: 0.2523\n",
      "184/281, train_loss: 0.3736, step time: 0.2582\n",
      "185/281, train_loss: 0.5852, step time: 0.2532\n",
      "186/281, train_loss: 0.4430, step time: 0.2574\n",
      "187/281, train_loss: 0.4015, step time: 0.2537\n",
      "188/281, train_loss: 0.4287, step time: 0.2558\n",
      "189/281, train_loss: 0.5233, step time: 0.2583\n",
      "190/281, train_loss: 0.3858, step time: 0.2526\n",
      "191/281, train_loss: 0.4278, step time: 0.2551\n",
      "192/281, train_loss: 0.4231, step time: 0.2551\n",
      "193/281, train_loss: 0.3658, step time: 0.2562\n",
      "194/281, train_loss: 0.4013, step time: 0.2502\n",
      "195/281, train_loss: 0.3941, step time: 0.2475\n",
      "196/281, train_loss: 0.4180, step time: 0.2545\n",
      "197/281, train_loss: 0.4322, step time: 0.2526\n",
      "198/281, train_loss: 0.4251, step time: 0.2556\n",
      "199/281, train_loss: 0.4351, step time: 0.2534\n",
      "200/281, train_loss: 0.3969, step time: 0.2545\n",
      "201/281, train_loss: 0.3738, step time: 0.2570\n",
      "202/281, train_loss: 0.4254, step time: 0.2589\n",
      "203/281, train_loss: 0.4215, step time: 0.2531\n",
      "204/281, train_loss: 0.3788, step time: 0.2554\n",
      "205/281, train_loss: 0.4032, step time: 0.2558\n",
      "206/281, train_loss: 0.3650, step time: 0.2587\n",
      "207/281, train_loss: 0.5429, step time: 0.2522\n",
      "208/281, train_loss: 0.4229, step time: 0.2535\n",
      "209/281, train_loss: 0.3821, step time: 0.2517\n",
      "210/281, train_loss: 0.3919, step time: 0.2519\n",
      "211/281, train_loss: 0.4029, step time: 0.2510\n",
      "212/281, train_loss: 0.3692, step time: 0.2546\n",
      "213/281, train_loss: 0.5385, step time: 0.2537\n",
      "214/281, train_loss: 0.4003, step time: 0.2516\n",
      "215/281, train_loss: 0.4058, step time: 0.2583\n",
      "216/281, train_loss: 0.4596, step time: 0.2574\n",
      "217/281, train_loss: 0.4292, step time: 0.2566\n",
      "218/281, train_loss: 0.3831, step time: 0.2534\n",
      "219/281, train_loss: 0.3954, step time: 0.2540\n",
      "220/281, train_loss: 0.4067, step time: 0.2506\n",
      "221/281, train_loss: 0.3811, step time: 0.2507\n",
      "222/281, train_loss: 0.4240, step time: 0.2520\n",
      "223/281, train_loss: 0.4032, step time: 0.2512\n",
      "224/281, train_loss: 0.3773, step time: 0.2606\n",
      "225/281, train_loss: 0.4247, step time: 0.2666\n",
      "226/281, train_loss: 0.4138, step time: 0.2521\n",
      "227/281, train_loss: 0.3908, step time: 0.2513\n",
      "228/281, train_loss: 0.3980, step time: 0.2565\n",
      "229/281, train_loss: 0.3978, step time: 0.2588\n",
      "230/281, train_loss: 0.5249, step time: 0.2536\n",
      "231/281, train_loss: 0.5524, step time: 0.2483\n",
      "232/281, train_loss: 0.4180, step time: 0.2542\n",
      "233/281, train_loss: 0.3869, step time: 0.2491\n",
      "234/281, train_loss: 0.3934, step time: 0.2510\n",
      "235/281, train_loss: 0.3974, step time: 0.2514\n",
      "236/281, train_loss: 0.5341, step time: 0.2630\n",
      "237/281, train_loss: 0.3850, step time: 0.2810\n",
      "238/281, train_loss: 0.3614, step time: 0.2652\n",
      "239/281, train_loss: 0.5648, step time: 0.2489\n",
      "240/281, train_loss: 0.3903, step time: 0.2519\n",
      "241/281, train_loss: 0.3866, step time: 0.2521\n",
      "242/281, train_loss: 0.3976, step time: 0.2504\n",
      "243/281, train_loss: 0.4018, step time: 0.2470\n",
      "244/281, train_loss: 0.3884, step time: 0.2465\n",
      "245/281, train_loss: 0.4310, step time: 0.2481\n",
      "246/281, train_loss: 0.3583, step time: 0.2499\n",
      "247/281, train_loss: 0.3910, step time: 0.2538\n",
      "248/281, train_loss: 0.4587, step time: 0.2513\n",
      "249/281, train_loss: 0.3886, step time: 0.2490\n",
      "250/281, train_loss: 0.3902, step time: 0.2519\n",
      "251/281, train_loss: 0.3859, step time: 0.2541\n",
      "252/281, train_loss: 0.4030, step time: 0.2572\n",
      "253/281, train_loss: 0.3870, step time: 0.2561\n",
      "254/281, train_loss: 0.4375, step time: 0.2524\n",
      "255/281, train_loss: 0.5431, step time: 0.2516\n",
      "256/281, train_loss: 0.4245, step time: 0.2576\n",
      "257/281, train_loss: 0.4018, step time: 0.2574\n",
      "258/281, train_loss: 0.4093, step time: 0.2575\n",
      "259/281, train_loss: 0.4185, step time: 0.2451\n",
      "260/281, train_loss: 0.3916, step time: 0.2504\n",
      "261/281, train_loss: 0.4209, step time: 0.2476\n",
      "262/281, train_loss: 0.5278, step time: 0.2489\n",
      "263/281, train_loss: 0.5357, step time: 0.2505\n",
      "264/281, train_loss: 0.4245, step time: 0.2494\n",
      "265/281, train_loss: 0.7057, step time: 0.2462\n",
      "266/281, train_loss: 0.4684, step time: 0.2472\n",
      "267/281, train_loss: 0.3826, step time: 0.2585\n",
      "268/281, train_loss: 0.4193, step time: 0.2478\n",
      "269/281, train_loss: 0.3868, step time: 0.2562\n",
      "270/281, train_loss: 0.4157, step time: 0.2550\n",
      "271/281, train_loss: 0.3871, step time: 0.2540\n",
      "272/281, train_loss: 0.3662, step time: 0.2469\n",
      "273/281, train_loss: 0.4161, step time: 0.2517\n",
      "274/281, train_loss: 0.4974, step time: 0.2486\n",
      "275/281, train_loss: 0.4260, step time: 0.2548\n",
      "276/281, train_loss: 0.5473, step time: 0.2548\n",
      "277/281, train_loss: 0.4159, step time: 0.2524\n",
      "278/281, train_loss: 0.4155, step time: 0.2570\n",
      "279/281, train_loss: 0.4074, step time: 0.2542\n",
      "280/281, train_loss: 0.3933, step time: 0.2501\n",
      "281/281, train_loss: 0.4201, step time: 0.2511\n",
      "282/281, train_loss: 0.3941, step time: 0.1486\n",
      "epoch 29 average loss: 0.4274\n",
      "saved new best metric model\n",
      "current epoch: 29 current mean dice: 0.7060 tc: 0.4083 wt: 0.9019 et: 0.8252\n",
      "best mean dice: 0.7060 at epoch: 29\n",
      "time consuming of epoch 29 is: 421.8501\n",
      "----------\n",
      "epoch 30/200\n",
      "1/281, train_loss: 0.3619, step time: 0.2555\n",
      "2/281, train_loss: 0.3995, step time: 0.2512\n",
      "3/281, train_loss: 0.3700, step time: 0.2477\n",
      "4/281, train_loss: 0.3923, step time: 0.2449\n",
      "5/281, train_loss: 0.3907, step time: 0.2477\n",
      "6/281, train_loss: 0.3635, step time: 0.2492\n",
      "7/281, train_loss: 0.4154, step time: 0.2405\n",
      "8/281, train_loss: 0.5748, step time: 0.2488\n",
      "9/281, train_loss: 0.3771, step time: 0.2523\n",
      "10/281, train_loss: 0.3839, step time: 0.2562\n",
      "11/281, train_loss: 0.4203, step time: 0.2525\n",
      "12/281, train_loss: 0.3781, step time: 0.2455\n",
      "13/281, train_loss: 0.5610, step time: 0.2518\n",
      "14/281, train_loss: 0.3826, step time: 0.2548\n",
      "15/281, train_loss: 0.4328, step time: 0.2517\n",
      "16/281, train_loss: 0.5432, step time: 0.2516\n",
      "17/281, train_loss: 0.4446, step time: 0.2514\n",
      "18/281, train_loss: 0.3940, step time: 0.2529\n",
      "19/281, train_loss: 0.3744, step time: 0.2536\n",
      "20/281, train_loss: 0.4084, step time: 0.2569\n",
      "21/281, train_loss: 0.4031, step time: 0.2523\n",
      "22/281, train_loss: 0.3989, step time: 0.2532\n",
      "23/281, train_loss: 0.4084, step time: 0.2520\n",
      "24/281, train_loss: 0.3937, step time: 0.2524\n",
      "25/281, train_loss: 0.3954, step time: 0.2501\n",
      "26/281, train_loss: 0.4574, step time: 0.2537\n",
      "27/281, train_loss: 0.3719, step time: 0.2503\n",
      "28/281, train_loss: 0.3931, step time: 0.2510\n",
      "29/281, train_loss: 0.4123, step time: 0.2520\n",
      "30/281, train_loss: 0.4131, step time: 0.2524\n",
      "31/281, train_loss: 0.4042, step time: 0.2484\n",
      "32/281, train_loss: 0.3899, step time: 0.2555\n",
      "33/281, train_loss: 0.3921, step time: 0.2557\n",
      "34/281, train_loss: 0.3632, step time: 0.2477\n",
      "35/281, train_loss: 0.3731, step time: 0.2462\n",
      "36/281, train_loss: 0.3823, step time: 0.2470\n",
      "37/281, train_loss: 0.5970, step time: 0.2450\n",
      "38/281, train_loss: 0.3811, step time: 0.2470\n",
      "39/281, train_loss: 0.3941, step time: 0.2459\n",
      "40/281, train_loss: 0.5001, step time: 0.2524\n",
      "41/281, train_loss: 0.3943, step time: 0.2558\n",
      "42/281, train_loss: 0.4056, step time: 0.2532\n",
      "43/281, train_loss: 0.5557, step time: 0.2541\n",
      "44/281, train_loss: 0.5064, step time: 0.2551\n",
      "45/281, train_loss: 0.3640, step time: 0.2516\n",
      "46/281, train_loss: 0.3904, step time: 0.2464\n",
      "47/281, train_loss: 0.3258, step time: 0.2477\n",
      "48/281, train_loss: 0.4833, step time: 0.2490\n",
      "49/281, train_loss: 0.3577, step time: 0.2507\n",
      "50/281, train_loss: 0.5077, step time: 0.2472\n",
      "51/281, train_loss: 0.3779, step time: 0.2801\n",
      "52/281, train_loss: 0.6482, step time: 0.2659\n",
      "53/281, train_loss: 0.4032, step time: 0.2487\n",
      "54/281, train_loss: 0.3962, step time: 0.2465\n",
      "55/281, train_loss: 0.6398, step time: 0.2451\n",
      "56/281, train_loss: 0.5677, step time: 0.2500\n",
      "57/281, train_loss: 0.3799, step time: 0.2474\n",
      "58/281, train_loss: 0.3530, step time: 0.2483\n",
      "59/281, train_loss: 0.4024, step time: 0.2503\n",
      "60/281, train_loss: 0.4020, step time: 0.2502\n",
      "61/281, train_loss: 0.4182, step time: 0.2473\n",
      "62/281, train_loss: 0.3465, step time: 0.2488\n",
      "63/281, train_loss: 0.5107, step time: 0.2541\n",
      "64/281, train_loss: 0.5063, step time: 0.2481\n",
      "65/281, train_loss: 0.3672, step time: 0.2445\n",
      "66/281, train_loss: 0.4840, step time: 0.2462\n",
      "67/281, train_loss: 0.4435, step time: 0.2446\n",
      "68/281, train_loss: 0.3861, step time: 0.2481\n",
      "69/281, train_loss: 0.3593, step time: 0.2544\n",
      "70/281, train_loss: 0.3389, step time: 0.2506\n",
      "71/281, train_loss: 0.3992, step time: 0.2461\n",
      "72/281, train_loss: 0.3822, step time: 0.2474\n",
      "73/281, train_loss: 0.5093, step time: 0.2542\n",
      "74/281, train_loss: 0.4098, step time: 0.2521\n",
      "75/281, train_loss: 0.3613, step time: 0.2450\n",
      "76/281, train_loss: 0.3661, step time: 0.2516\n",
      "77/281, train_loss: 0.3595, step time: 0.2497\n",
      "78/281, train_loss: 0.3738, step time: 0.2507\n",
      "79/281, train_loss: 0.3135, step time: 0.2503\n",
      "80/281, train_loss: 0.3408, step time: 0.2538\n",
      "81/281, train_loss: 0.3535, step time: 0.2476\n",
      "82/281, train_loss: 0.3113, step time: 0.2441\n",
      "83/281, train_loss: 0.3773, step time: 0.2574\n",
      "84/281, train_loss: 0.4738, step time: 0.2550\n",
      "85/281, train_loss: 0.3598, step time: 0.2499\n",
      "86/281, train_loss: 0.3975, step time: 0.2485\n",
      "87/281, train_loss: 0.4413, step time: 0.2510\n",
      "88/281, train_loss: 0.3796, step time: 0.2476\n",
      "89/281, train_loss: 0.3381, step time: 0.2449\n",
      "90/281, train_loss: 0.4799, step time: 0.2479\n",
      "91/281, train_loss: 0.4585, step time: 0.2493\n",
      "92/281, train_loss: 0.3565, step time: 0.2552\n",
      "93/281, train_loss: 0.3388, step time: 0.2493\n",
      "94/281, train_loss: 0.3497, step time: 0.2494\n",
      "95/281, train_loss: 0.2981, step time: 0.2440\n",
      "96/281, train_loss: 0.3302, step time: 0.2477\n",
      "97/281, train_loss: 0.3927, step time: 0.2586\n",
      "98/281, train_loss: 0.3669, step time: 0.2494\n",
      "99/281, train_loss: 0.3482, step time: 0.2503\n",
      "100/281, train_loss: 0.3299, step time: 0.2536\n",
      "101/281, train_loss: 0.3676, step time: 0.2536\n",
      "102/281, train_loss: 0.3440, step time: 0.2497\n",
      "103/281, train_loss: 0.3112, step time: 0.2455\n",
      "104/281, train_loss: 0.3541, step time: 0.2558\n",
      "105/281, train_loss: 0.3461, step time: 0.2486\n",
      "106/281, train_loss: 0.3899, step time: 0.2477\n",
      "107/281, train_loss: 0.3946, step time: 0.2481\n",
      "108/281, train_loss: 0.4151, step time: 0.2522\n",
      "109/281, train_loss: 0.3400, step time: 0.2480\n",
      "110/281, train_loss: 0.3726, step time: 0.2496\n",
      "111/281, train_loss: 0.4875, step time: 0.2496\n",
      "112/281, train_loss: 0.3514, step time: 0.2439\n",
      "113/281, train_loss: 0.4040, step time: 0.2496\n",
      "114/281, train_loss: 0.3443, step time: 0.2463\n",
      "115/281, train_loss: 0.3177, step time: 0.2479\n",
      "116/281, train_loss: 0.3750, step time: 0.2407\n",
      "117/281, train_loss: 0.3575, step time: 0.2819\n",
      "118/281, train_loss: 0.2740, step time: 0.2565\n",
      "119/281, train_loss: 0.3618, step time: 0.2538\n",
      "120/281, train_loss: 0.4924, step time: 0.2474\n",
      "121/281, train_loss: 0.4832, step time: 0.2511\n",
      "122/281, train_loss: 0.3701, step time: 0.2454\n",
      "123/281, train_loss: 0.3506, step time: 0.2490\n",
      "124/281, train_loss: 0.3677, step time: 0.2469\n",
      "125/281, train_loss: 0.3833, step time: 0.2456\n",
      "126/281, train_loss: 0.3765, step time: 0.2509\n",
      "127/281, train_loss: 0.4795, step time: 0.2484\n",
      "128/281, train_loss: 0.3585, step time: 0.2541\n",
      "129/281, train_loss: 0.3262, step time: 0.2430\n",
      "130/281, train_loss: 0.3130, step time: 0.2454\n",
      "131/281, train_loss: 0.5140, step time: 0.2418\n",
      "132/281, train_loss: 0.4871, step time: 0.2491\n",
      "133/281, train_loss: 0.3918, step time: 0.2508\n",
      "134/281, train_loss: 0.3570, step time: 0.2511\n",
      "135/281, train_loss: 0.2932, step time: 0.2529\n",
      "136/281, train_loss: 0.3698, step time: 0.2478\n",
      "137/281, train_loss: 0.5441, step time: 0.2477\n",
      "138/281, train_loss: 0.3349, step time: 0.2459\n",
      "139/281, train_loss: 0.3578, step time: 0.2455\n",
      "140/281, train_loss: 0.3447, step time: 0.2508\n",
      "141/281, train_loss: 0.4428, step time: 0.2485\n",
      "142/281, train_loss: 0.4729, step time: 0.2462\n",
      "143/281, train_loss: 0.3217, step time: 0.2429\n",
      "144/281, train_loss: 0.3992, step time: 0.2478\n",
      "145/281, train_loss: 0.3774, step time: 0.2451\n",
      "146/281, train_loss: 0.3730, step time: 0.2505\n",
      "147/281, train_loss: 0.3807, step time: 0.2447\n",
      "148/281, train_loss: 0.3809, step time: 0.2458\n",
      "149/281, train_loss: 0.3491, step time: 0.2490\n",
      "150/281, train_loss: 0.3961, step time: 0.2509\n",
      "151/281, train_loss: 0.4122, step time: 0.2516\n",
      "152/281, train_loss: 0.4590, step time: 0.2492\n",
      "153/281, train_loss: 0.3034, step time: 0.2449\n",
      "154/281, train_loss: 0.3449, step time: 0.2496\n",
      "155/281, train_loss: 0.3223, step time: 0.2532\n",
      "156/281, train_loss: 0.3206, step time: 0.2520\n",
      "157/281, train_loss: 0.4420, step time: 0.2431\n",
      "158/281, train_loss: 0.3103, step time: 0.2489\n",
      "159/281, train_loss: 0.4028, step time: 0.2492\n",
      "160/281, train_loss: 0.2975, step time: 0.2440\n",
      "161/281, train_loss: 0.3681, step time: 0.2449\n",
      "162/281, train_loss: 0.3304, step time: 0.2495\n",
      "163/281, train_loss: 0.3164, step time: 0.2483\n",
      "164/281, train_loss: 0.3789, step time: 0.2481\n",
      "165/281, train_loss: 0.4691, step time: 0.2467\n",
      "166/281, train_loss: 0.3250, step time: 0.2531\n",
      "167/281, train_loss: 0.3823, step time: 0.2512\n",
      "168/281, train_loss: 0.4150, step time: 0.2485\n",
      "169/281, train_loss: 0.3184, step time: 0.2488\n",
      "170/281, train_loss: 0.3310, step time: 0.2477\n",
      "171/281, train_loss: 0.3234, step time: 0.2509\n",
      "172/281, train_loss: 0.3313, step time: 0.2460\n",
      "173/281, train_loss: 0.3966, step time: 0.2434\n",
      "174/281, train_loss: 0.3488, step time: 0.2494\n",
      "175/281, train_loss: 0.3303, step time: 0.2532\n",
      "176/281, train_loss: 0.4930, step time: 0.2499\n",
      "177/281, train_loss: 0.4338, step time: 0.2555\n",
      "178/281, train_loss: 0.2722, step time: 0.2506\n",
      "179/281, train_loss: 0.3674, step time: 0.2548\n",
      "180/281, train_loss: 0.4826, step time: 0.2449\n",
      "181/281, train_loss: 0.3387, step time: 0.2530\n",
      "182/281, train_loss: 0.2762, step time: 0.2507\n",
      "183/281, train_loss: 0.3412, step time: 0.2473\n",
      "184/281, train_loss: 0.2857, step time: 0.2484\n",
      "185/281, train_loss: 0.3452, step time: 0.2614\n",
      "186/281, train_loss: 0.3067, step time: 0.2527\n",
      "187/281, train_loss: 0.4565, step time: 0.2525\n",
      "188/281, train_loss: 0.3382, step time: 0.2523\n",
      "189/281, train_loss: 0.3676, step time: 0.2488\n",
      "190/281, train_loss: 0.2725, step time: 0.2517\n",
      "191/281, train_loss: 0.3130, step time: 0.2546\n",
      "192/281, train_loss: 0.3079, step time: 0.2515\n",
      "193/281, train_loss: 0.4673, step time: 0.2543\n",
      "194/281, train_loss: 0.3779, step time: 0.2593\n",
      "195/281, train_loss: 0.3500, step time: 0.2538\n",
      "196/281, train_loss: 0.4218, step time: 0.2471\n",
      "197/281, train_loss: 0.3925, step time: 0.2468\n",
      "198/281, train_loss: 0.4367, step time: 0.2522\n",
      "199/281, train_loss: 0.3057, step time: 0.2488\n",
      "200/281, train_loss: 0.2976, step time: 0.2513\n",
      "201/281, train_loss: 0.2962, step time: 0.2429\n",
      "202/281, train_loss: 0.3023, step time: 0.2550\n",
      "203/281, train_loss: 0.3768, step time: 0.2550\n",
      "204/281, train_loss: 0.4678, step time: 0.2563\n",
      "205/281, train_loss: 0.3424, step time: 0.2513\n",
      "206/281, train_loss: 0.3911, step time: 0.2545\n",
      "207/281, train_loss: 0.2924, step time: 0.2526\n",
      "208/281, train_loss: 0.4011, step time: 0.2482\n",
      "209/281, train_loss: 0.3015, step time: 0.2508\n",
      "210/281, train_loss: 0.4623, step time: 0.2517\n",
      "211/281, train_loss: 0.3376, step time: 0.2525\n",
      "212/281, train_loss: 0.3780, step time: 0.2502\n",
      "213/281, train_loss: 0.2728, step time: 0.2539\n",
      "214/281, train_loss: 0.3754, step time: 0.2532\n",
      "215/281, train_loss: 0.3241, step time: 0.2521\n",
      "216/281, train_loss: 0.3506, step time: 0.2499\n",
      "217/281, train_loss: 0.2819, step time: 0.2537\n",
      "218/281, train_loss: 0.3220, step time: 0.2604\n",
      "219/281, train_loss: 0.2808, step time: 0.2595\n",
      "220/281, train_loss: 0.3240, step time: 0.2533\n",
      "221/281, train_loss: 0.3422, step time: 0.2480\n",
      "222/281, train_loss: 0.3215, step time: 0.2554\n",
      "223/281, train_loss: 0.3362, step time: 0.2588\n",
      "224/281, train_loss: 0.3721, step time: 0.2543\n",
      "225/281, train_loss: 0.4966, step time: 0.2525\n",
      "226/281, train_loss: 0.4329, step time: 0.2593\n",
      "227/281, train_loss: 0.3019, step time: 0.2563\n",
      "228/281, train_loss: 0.3272, step time: 0.2572\n",
      "229/281, train_loss: 0.3046, step time: 0.2566\n",
      "230/281, train_loss: 0.3148, step time: 0.2619\n",
      "231/281, train_loss: 0.3292, step time: 0.2576\n",
      "232/281, train_loss: 0.2920, step time: 0.2489\n",
      "233/281, train_loss: 0.2268, step time: 0.2499\n",
      "234/281, train_loss: 0.3779, step time: 0.2576\n",
      "235/281, train_loss: 0.3228, step time: 0.2591\n",
      "236/281, train_loss: 0.2629, step time: 0.2561\n",
      "237/281, train_loss: 0.3359, step time: 0.2556\n",
      "238/281, train_loss: 0.3376, step time: 0.2576\n",
      "239/281, train_loss: 0.2825, step time: 0.2564\n",
      "240/281, train_loss: 0.3529, step time: 0.2572\n",
      "241/281, train_loss: 0.3223, step time: 0.2596\n",
      "242/281, train_loss: 0.3508, step time: 0.2572\n",
      "243/281, train_loss: 0.2929, step time: 0.2551\n",
      "244/281, train_loss: 0.4382, step time: 0.2558\n",
      "245/281, train_loss: 0.3620, step time: 0.2561\n",
      "246/281, train_loss: 0.3354, step time: 0.2594\n",
      "247/281, train_loss: 0.4602, step time: 0.2593\n",
      "248/281, train_loss: 0.4620, step time: 0.2599\n",
      "249/281, train_loss: 0.2688, step time: 0.2579\n",
      "250/281, train_loss: 0.3033, step time: 0.2494\n",
      "251/281, train_loss: 0.2724, step time: 0.2558\n",
      "252/281, train_loss: 0.4166, step time: 0.2577\n",
      "253/281, train_loss: 0.3625, step time: 0.2563\n",
      "254/281, train_loss: 0.5671, step time: 0.2578\n",
      "255/281, train_loss: 0.3122, step time: 0.2591\n",
      "256/281, train_loss: 0.3222, step time: 0.2573\n",
      "257/281, train_loss: 0.2882, step time: 0.2544\n",
      "258/281, train_loss: 0.4048, step time: 0.2539\n",
      "259/281, train_loss: 0.2806, step time: 0.2547\n",
      "260/281, train_loss: 0.4067, step time: 0.2592\n",
      "261/281, train_loss: 0.2929, step time: 0.2590\n",
      "262/281, train_loss: 0.4187, step time: 0.2565\n",
      "263/281, train_loss: 0.3059, step time: 0.2543\n",
      "264/281, train_loss: 0.3051, step time: 0.2612\n",
      "265/281, train_loss: 0.3977, step time: 0.2531\n",
      "266/281, train_loss: 0.2560, step time: 0.2559\n",
      "267/281, train_loss: 0.3025, step time: 0.2546\n",
      "268/281, train_loss: 0.3583, step time: 0.2571\n",
      "269/281, train_loss: 0.2975, step time: 0.2502\n",
      "270/281, train_loss: 0.2391, step time: 0.2546\n",
      "271/281, train_loss: 0.2595, step time: 0.2556\n",
      "272/281, train_loss: 0.3094, step time: 0.2510\n",
      "273/281, train_loss: 0.2303, step time: 0.2525\n",
      "274/281, train_loss: 0.3831, step time: 0.2534\n",
      "275/281, train_loss: 0.2860, step time: 0.2550\n",
      "276/281, train_loss: 0.3637, step time: 0.2578\n",
      "277/281, train_loss: 0.3510, step time: 0.2568\n",
      "278/281, train_loss: 0.3260, step time: 0.2560\n",
      "279/281, train_loss: 0.3448, step time: 0.2552\n",
      "280/281, train_loss: 0.2757, step time: 0.2502\n",
      "281/281, train_loss: 0.4037, step time: 0.2532\n",
      "282/281, train_loss: 0.3514, step time: 0.1533\n",
      "epoch 30 average loss: 0.3754\n",
      "saved new best metric model\n",
      "current epoch: 30 current mean dice: 0.7651 tc: 0.5811 wt: 0.9022 et: 0.8214\n",
      "best mean dice: 0.7651 at epoch: 30\n",
      "time consuming of epoch 30 is: 453.1030\n",
      "----------\n",
      "epoch 31/200\n",
      "1/281, train_loss: 0.2180, step time: 0.2632\n",
      "2/281, train_loss: 0.2906, step time: 0.2498\n",
      "3/281, train_loss: 0.2227, step time: 0.2502\n",
      "4/281, train_loss: 0.2817, step time: 0.2482\n",
      "5/281, train_loss: 0.1968, step time: 0.2593\n",
      "6/281, train_loss: 0.4597, step time: 0.2576\n",
      "7/281, train_loss: 0.2185, step time: 0.2580\n",
      "8/281, train_loss: 0.4027, step time: 0.2699\n",
      "9/281, train_loss: 0.3106, step time: 0.2650\n",
      "10/281, train_loss: 0.4664, step time: 0.2575\n",
      "11/281, train_loss: 0.4408, step time: 0.2555\n",
      "12/281, train_loss: 0.3732, step time: 0.2529\n",
      "13/281, train_loss: 0.4748, step time: 0.2559\n",
      "14/281, train_loss: 0.3554, step time: 0.2555\n",
      "15/281, train_loss: 0.3028, step time: 0.2537\n",
      "16/281, train_loss: 0.2737, step time: 0.2592\n",
      "17/281, train_loss: 0.2502, step time: 0.2560\n",
      "18/281, train_loss: 0.2877, step time: 0.2598\n",
      "19/281, train_loss: 0.2762, step time: 0.2578\n",
      "20/281, train_loss: 0.2840, step time: 0.2567\n",
      "21/281, train_loss: 0.3370, step time: 0.2550\n",
      "22/281, train_loss: 0.3717, step time: 0.2537\n",
      "23/281, train_loss: 0.4058, step time: 0.2515\n",
      "24/281, train_loss: 0.2702, step time: 0.2539\n",
      "25/281, train_loss: 0.4175, step time: 0.2521\n",
      "26/281, train_loss: 0.2304, step time: 0.2819\n",
      "27/281, train_loss: 0.4012, step time: 0.2742\n",
      "28/281, train_loss: 0.3112, step time: 0.2560\n",
      "29/281, train_loss: 0.3041, step time: 0.2531\n",
      "30/281, train_loss: 0.1748, step time: 0.2723\n",
      "31/281, train_loss: 0.2969, step time: 0.2512\n",
      "32/281, train_loss: 0.2631, step time: 0.2578\n",
      "33/281, train_loss: 0.1666, step time: 0.2564\n",
      "34/281, train_loss: 0.2104, step time: 0.2555\n",
      "35/281, train_loss: 0.4021, step time: 0.2613\n",
      "36/281, train_loss: 0.2310, step time: 0.2579\n",
      "37/281, train_loss: 0.3302, step time: 0.2606\n",
      "38/281, train_loss: 0.3537, step time: 0.2602\n",
      "39/281, train_loss: 0.2236, step time: 0.2581\n",
      "40/281, train_loss: 0.3440, step time: 0.2571\n",
      "41/281, train_loss: 0.2697, step time: 0.2572\n",
      "42/281, train_loss: 0.2891, step time: 0.2593\n",
      "43/281, train_loss: 0.3648, step time: 0.2621\n",
      "44/281, train_loss: 0.2830, step time: 0.2643\n",
      "45/281, train_loss: 0.3913, step time: 0.2664\n",
      "46/281, train_loss: 0.2622, step time: 0.2590\n",
      "47/281, train_loss: 0.1743, step time: 0.2575\n",
      "48/281, train_loss: 0.3354, step time: 0.2586\n",
      "49/281, train_loss: 0.2248, step time: 0.2554\n",
      "50/281, train_loss: 0.4168, step time: 0.2605\n",
      "51/281, train_loss: 0.3414, step time: 0.2562\n",
      "52/281, train_loss: 0.3005, step time: 0.2581\n",
      "53/281, train_loss: 0.3277, step time: 0.2532\n",
      "54/281, train_loss: 0.2595, step time: 0.2578\n",
      "55/281, train_loss: 0.3112, step time: 0.2573\n",
      "56/281, train_loss: 0.3274, step time: 0.2540\n",
      "57/281, train_loss: 0.2922, step time: 0.2572\n",
      "58/281, train_loss: 0.2457, step time: 0.2629\n",
      "59/281, train_loss: 0.2135, step time: 0.2618\n",
      "60/281, train_loss: 0.4170, step time: 0.2657\n",
      "61/281, train_loss: 0.1826, step time: 0.2648\n",
      "62/281, train_loss: 0.2655, step time: 0.2597\n",
      "63/281, train_loss: 0.2165, step time: 0.2579\n",
      "64/281, train_loss: 0.3491, step time: 0.2593\n",
      "65/281, train_loss: 0.1677, step time: 0.2598\n",
      "66/281, train_loss: 0.2325, step time: 0.2548\n",
      "67/281, train_loss: 0.3939, step time: 0.2581\n",
      "68/281, train_loss: 0.1842, step time: 0.2534\n",
      "69/281, train_loss: 0.3071, step time: 0.2613\n",
      "70/281, train_loss: 0.2516, step time: 0.2561\n",
      "71/281, train_loss: 0.1857, step time: 0.2522\n",
      "72/281, train_loss: 0.2242, step time: 0.2523\n",
      "73/281, train_loss: 0.2498, step time: 0.2563\n",
      "74/281, train_loss: 0.2657, step time: 0.2607\n",
      "75/281, train_loss: 0.2038, step time: 0.2585\n",
      "76/281, train_loss: 0.2535, step time: 0.2514\n",
      "77/281, train_loss: 0.3914, step time: 0.2552\n",
      "78/281, train_loss: 0.1579, step time: 0.2667\n",
      "79/281, train_loss: 0.1974, step time: 0.2796\n",
      "80/281, train_loss: 0.2016, step time: 0.2597\n",
      "81/281, train_loss: 0.2174, step time: 0.2570\n",
      "82/281, train_loss: 0.2431, step time: 0.2582\n",
      "83/281, train_loss: 0.2722, step time: 0.2584\n",
      "84/281, train_loss: 0.2382, step time: 0.2539\n",
      "85/281, train_loss: 0.2649, step time: 0.2597\n",
      "86/281, train_loss: 0.2266, step time: 0.2572\n",
      "87/281, train_loss: 0.2190, step time: 0.2566\n",
      "88/281, train_loss: 0.4405, step time: 0.2505\n",
      "89/281, train_loss: 0.3883, step time: 0.2517\n",
      "90/281, train_loss: 0.1665, step time: 0.2550\n",
      "91/281, train_loss: 0.2407, step time: 0.2575\n",
      "92/281, train_loss: 0.1943, step time: 0.2580\n",
      "93/281, train_loss: 0.2699, step time: 0.2527\n",
      "94/281, train_loss: 0.3772, step time: 0.2569\n",
      "95/281, train_loss: 0.1437, step time: 0.2590\n",
      "96/281, train_loss: 0.2796, step time: 0.2530\n",
      "97/281, train_loss: 0.3537, step time: 0.2559\n",
      "98/281, train_loss: 0.1684, step time: 0.2538\n",
      "99/281, train_loss: 0.1490, step time: 0.2543\n",
      "100/281, train_loss: 0.2669, step time: 0.2586\n",
      "101/281, train_loss: 0.3001, step time: 0.2636\n",
      "102/281, train_loss: 0.4028, step time: 0.2600\n",
      "103/281, train_loss: 0.3607, step time: 0.2562\n",
      "104/281, train_loss: 0.4091, step time: 0.2566\n",
      "105/281, train_loss: 0.3859, step time: 0.2578\n",
      "106/281, train_loss: 0.2789, step time: 0.2563\n",
      "107/281, train_loss: 0.1386, step time: 0.2528\n",
      "108/281, train_loss: 0.1421, step time: 0.2566\n",
      "109/281, train_loss: 0.2662, step time: 0.2538\n",
      "110/281, train_loss: 0.2619, step time: 0.2620\n",
      "111/281, train_loss: 0.2876, step time: 0.2526\n",
      "112/281, train_loss: 0.1658, step time: 0.2557\n",
      "113/281, train_loss: 0.1623, step time: 0.2519\n",
      "114/281, train_loss: 0.3660, step time: 0.2539\n",
      "115/281, train_loss: 0.2862, step time: 0.2521\n",
      "116/281, train_loss: 0.3667, step time: 0.2543\n",
      "117/281, train_loss: 0.1503, step time: 0.2571\n",
      "118/281, train_loss: 0.3246, step time: 0.2544\n",
      "119/281, train_loss: 0.3972, step time: 0.2475\n",
      "120/281, train_loss: 0.1676, step time: 0.2516\n",
      "121/281, train_loss: 0.1900, step time: 0.2554\n",
      "122/281, train_loss: 0.1802, step time: 0.2577\n",
      "123/281, train_loss: 0.2758, step time: 0.2570\n",
      "124/281, train_loss: 0.3878, step time: 0.2528\n",
      "125/281, train_loss: 0.2035, step time: 0.2586\n",
      "126/281, train_loss: 0.4940, step time: 0.2556\n",
      "127/281, train_loss: 0.2937, step time: 0.2581\n",
      "128/281, train_loss: 0.2174, step time: 0.2575\n",
      "129/281, train_loss: 0.2731, step time: 0.2584\n",
      "130/281, train_loss: 0.1956, step time: 0.2559\n",
      "131/281, train_loss: 0.2436, step time: 0.2538\n",
      "132/281, train_loss: 0.3767, step time: 0.2562\n",
      "133/281, train_loss: 0.2717, step time: 0.2580\n",
      "134/281, train_loss: 0.2283, step time: 0.2597\n",
      "135/281, train_loss: 0.2688, step time: 0.2593\n",
      "136/281, train_loss: 0.2586, step time: 0.2600\n",
      "137/281, train_loss: 0.3526, step time: 0.2546\n",
      "138/281, train_loss: 0.3273, step time: 0.2531\n",
      "139/281, train_loss: 0.1975, step time: 0.2562\n",
      "140/281, train_loss: 0.1715, step time: 0.2524\n",
      "141/281, train_loss: 0.1879, step time: 0.2534\n",
      "142/281, train_loss: 0.3465, step time: 0.2519\n",
      "143/281, train_loss: 0.2790, step time: 0.2550\n",
      "144/281, train_loss: 0.2568, step time: 0.2561\n",
      "145/281, train_loss: 0.1439, step time: 0.2587\n",
      "146/281, train_loss: 0.2477, step time: 0.2544\n",
      "147/281, train_loss: 0.2342, step time: 0.2530\n",
      "148/281, train_loss: 0.1996, step time: 0.2539\n",
      "149/281, train_loss: 0.2255, step time: 0.2520\n",
      "150/281, train_loss: 0.4499, step time: 0.2567\n",
      "151/281, train_loss: 0.2706, step time: 0.2608\n",
      "152/281, train_loss: 0.2464, step time: 0.2603\n",
      "153/281, train_loss: 0.1216, step time: 0.2525\n",
      "154/281, train_loss: 0.2765, step time: 0.2590\n",
      "155/281, train_loss: 0.3674, step time: 0.2596\n",
      "156/281, train_loss: 0.1716, step time: 0.2625\n",
      "157/281, train_loss: 0.1474, step time: 0.2531\n",
      "158/281, train_loss: 0.3389, step time: 0.2586\n",
      "159/281, train_loss: 0.3864, step time: 0.2600\n",
      "160/281, train_loss: 0.2408, step time: 0.2575\n",
      "161/281, train_loss: 0.2253, step time: 0.2590\n",
      "162/281, train_loss: 0.2285, step time: 0.2516\n",
      "163/281, train_loss: 0.1646, step time: 0.2503\n",
      "164/281, train_loss: 0.3451, step time: 0.2509\n",
      "165/281, train_loss: 0.1324, step time: 0.2685\n",
      "166/281, train_loss: 0.1636, step time: 0.2602\n",
      "167/281, train_loss: 0.1567, step time: 0.2557\n",
      "168/281, train_loss: 0.2410, step time: 0.2568\n",
      "169/281, train_loss: 0.1894, step time: 0.2560\n",
      "170/281, train_loss: 0.1364, step time: 0.2542\n",
      "171/281, train_loss: 0.1160, step time: 0.2570\n",
      "172/281, train_loss: 0.3677, step time: 0.2534\n",
      "173/281, train_loss: 0.1981, step time: 0.2514\n",
      "174/281, train_loss: 0.2163, step time: 0.2630\n",
      "175/281, train_loss: 0.2077, step time: 0.2585\n",
      "176/281, train_loss: 0.2921, step time: 0.2581\n",
      "177/281, train_loss: 0.1927, step time: 0.2557\n",
      "178/281, train_loss: 0.3793, step time: 0.2531\n",
      "179/281, train_loss: 0.2151, step time: 0.2536\n",
      "180/281, train_loss: 0.2502, step time: 0.2584\n",
      "181/281, train_loss: 0.1248, step time: 0.2567\n",
      "182/281, train_loss: 0.2024, step time: 0.2599\n",
      "183/281, train_loss: 0.1664, step time: 0.2574\n",
      "184/281, train_loss: 0.1443, step time: 0.2584\n",
      "185/281, train_loss: 0.2395, step time: 0.2558\n",
      "186/281, train_loss: 0.1471, step time: 0.2570\n",
      "187/281, train_loss: 0.2630, step time: 0.2601\n",
      "188/281, train_loss: 0.2149, step time: 0.2572\n",
      "189/281, train_loss: 0.1383, step time: 0.2590\n",
      "190/281, train_loss: 0.1251, step time: 0.2574\n",
      "191/281, train_loss: 0.1817, step time: 0.2560\n",
      "192/281, train_loss: 0.1993, step time: 0.2496\n",
      "193/281, train_loss: 0.3760, step time: 0.2563\n",
      "194/281, train_loss: 0.1768, step time: 0.2540\n",
      "195/281, train_loss: 0.2329, step time: 0.2514\n",
      "196/281, train_loss: 0.1690, step time: 0.2499\n",
      "197/281, train_loss: 0.3038, step time: 0.2557\n",
      "198/281, train_loss: 0.1737, step time: 0.2553\n",
      "199/281, train_loss: 0.1397, step time: 0.2481\n",
      "200/281, train_loss: 0.2703, step time: 0.2505\n",
      "201/281, train_loss: 0.1867, step time: 0.2514\n",
      "202/281, train_loss: 0.2418, step time: 0.2489\n",
      "203/281, train_loss: 0.2820, step time: 0.2481\n",
      "204/281, train_loss: 0.2394, step time: 0.2516\n",
      "205/281, train_loss: 0.1250, step time: 0.2525\n",
      "206/281, train_loss: 0.2222, step time: 0.2538\n",
      "207/281, train_loss: 0.1348, step time: 0.2561\n",
      "208/281, train_loss: 0.1816, step time: 0.2488\n",
      "209/281, train_loss: 0.1456, step time: 0.2544\n",
      "210/281, train_loss: 0.2556, step time: 0.2548\n",
      "211/281, train_loss: 0.2745, step time: 0.2511\n",
      "212/281, train_loss: 0.2989, step time: 0.2490\n",
      "213/281, train_loss: 0.3134, step time: 0.2586\n",
      "214/281, train_loss: 0.3991, step time: 0.2539\n",
      "215/281, train_loss: 0.2368, step time: 0.2590\n",
      "216/281, train_loss: 0.2266, step time: 0.2517\n",
      "217/281, train_loss: 0.2186, step time: 0.2606\n",
      "218/281, train_loss: 0.2190, step time: 0.2534\n",
      "219/281, train_loss: 0.2069, step time: 0.2443\n",
      "220/281, train_loss: 0.2080, step time: 0.2515\n",
      "221/281, train_loss: 0.1300, step time: 0.2530\n",
      "222/281, train_loss: 0.1733, step time: 0.2526\n",
      "223/281, train_loss: 0.1572, step time: 0.2546\n",
      "224/281, train_loss: 0.1447, step time: 0.2523\n",
      "225/281, train_loss: 0.1807, step time: 0.2554\n",
      "226/281, train_loss: 0.1728, step time: 0.2544\n",
      "227/281, train_loss: 0.2543, step time: 0.2517\n",
      "228/281, train_loss: 0.1400, step time: 0.2577\n",
      "229/281, train_loss: 0.2133, step time: 0.2443\n",
      "230/281, train_loss: 0.2237, step time: 0.2481\n",
      "231/281, train_loss: 0.1690, step time: 0.2482\n",
      "232/281, train_loss: 0.2831, step time: 0.2588\n",
      "233/281, train_loss: 0.3629, step time: 0.2504\n",
      "234/281, train_loss: 0.2904, step time: 0.2518\n",
      "235/281, train_loss: 0.2453, step time: 0.2539\n",
      "236/281, train_loss: 0.1637, step time: 0.2507\n",
      "237/281, train_loss: 0.1819, step time: 0.2480\n",
      "238/281, train_loss: 0.1588, step time: 0.2505\n",
      "239/281, train_loss: 0.3001, step time: 0.2481\n",
      "240/281, train_loss: 0.2085, step time: 0.2502\n",
      "241/281, train_loss: 0.1427, step time: 0.2536\n",
      "242/281, train_loss: 0.1523, step time: 0.2576\n",
      "243/281, train_loss: 0.1735, step time: 0.2527\n",
      "244/281, train_loss: 0.1559, step time: 0.2533\n",
      "245/281, train_loss: 0.1591, step time: 0.2578\n",
      "246/281, train_loss: 0.1846, step time: 0.2551\n",
      "247/281, train_loss: 0.1932, step time: 0.2540\n",
      "248/281, train_loss: 0.1875, step time: 0.2555\n",
      "249/281, train_loss: 0.1555, step time: 0.2554\n",
      "250/281, train_loss: 0.2272, step time: 0.2535\n",
      "251/281, train_loss: 0.1973, step time: 0.2510\n",
      "252/281, train_loss: 0.1134, step time: 0.2506\n",
      "253/281, train_loss: 0.1221, step time: 0.2527\n",
      "254/281, train_loss: 0.1837, step time: 0.2583\n",
      "255/281, train_loss: 0.3280, step time: 0.2523\n",
      "256/281, train_loss: 0.2893, step time: 0.2570\n",
      "257/281, train_loss: 0.2925, step time: 0.2522\n",
      "258/281, train_loss: 0.1989, step time: 0.2551\n",
      "259/281, train_loss: 0.1890, step time: 0.2501\n",
      "260/281, train_loss: 0.1774, step time: 0.2609\n",
      "261/281, train_loss: 0.4690, step time: 0.2546\n",
      "262/281, train_loss: 0.3248, step time: 0.2509\n",
      "263/281, train_loss: 0.1756, step time: 0.2517\n",
      "264/281, train_loss: 0.4210, step time: 0.2519\n",
      "265/281, train_loss: 0.1247, step time: 0.2550\n",
      "266/281, train_loss: 0.1704, step time: 0.2572\n",
      "267/281, train_loss: 0.4050, step time: 0.2538\n",
      "268/281, train_loss: 0.3569, step time: 0.2529\n",
      "269/281, train_loss: 0.2242, step time: 0.2552\n",
      "270/281, train_loss: 0.2195, step time: 0.2485\n",
      "271/281, train_loss: 0.3885, step time: 0.2536\n",
      "272/281, train_loss: 0.2108, step time: 0.2550\n",
      "273/281, train_loss: 0.2920, step time: 0.2526\n",
      "274/281, train_loss: 0.1252, step time: 0.2550\n",
      "275/281, train_loss: 0.2531, step time: 0.2809\n",
      "276/281, train_loss: 0.2961, step time: 0.2594\n",
      "277/281, train_loss: 0.1126, step time: 0.2520\n",
      "278/281, train_loss: 0.1385, step time: 0.2530\n",
      "279/281, train_loss: 0.2127, step time: 0.2484\n",
      "280/281, train_loss: 0.3094, step time: 0.2544\n",
      "281/281, train_loss: 0.2397, step time: 0.2520\n",
      "282/281, train_loss: 0.1226, step time: 0.1501\n",
      "epoch 31 average loss: 0.2526\n",
      "saved new best metric model\n",
      "current epoch: 31 current mean dice: 0.8210 tc: 0.7704 wt: 0.8983 et: 0.8087\n",
      "best mean dice: 0.8210 at epoch: 31\n",
      "time consuming of epoch 31 is: 367.4154\n",
      "----------\n",
      "epoch 32/200\n",
      "1/281, train_loss: 0.1314, step time: 0.2594\n",
      "2/281, train_loss: 0.1184, step time: 0.2548\n",
      "3/281, train_loss: 0.1305, step time: 0.2506\n",
      "4/281, train_loss: 0.1453, step time: 0.2746\n",
      "5/281, train_loss: 0.1517, step time: 0.2521\n",
      "6/281, train_loss: 0.1817, step time: 0.2479\n",
      "7/281, train_loss: 0.2835, step time: 0.2524\n",
      "8/281, train_loss: 0.1618, step time: 0.2462\n",
      "9/281, train_loss: 0.2416, step time: 0.2470\n",
      "10/281, train_loss: 0.3003, step time: 0.2491\n",
      "11/281, train_loss: 0.1342, step time: 0.2509\n",
      "12/281, train_loss: 0.1844, step time: 0.2481\n",
      "13/281, train_loss: 0.1721, step time: 0.2499\n",
      "14/281, train_loss: 0.1111, step time: 0.2491\n",
      "15/281, train_loss: 0.1850, step time: 0.2455\n",
      "16/281, train_loss: 0.1156, step time: 0.2490\n",
      "17/281, train_loss: 0.1227, step time: 0.2507\n",
      "18/281, train_loss: 0.1921, step time: 0.2417\n",
      "19/281, train_loss: 0.3627, step time: 0.2487\n",
      "20/281, train_loss: 0.1436, step time: 0.2514\n",
      "21/281, train_loss: 0.1082, step time: 0.2701\n",
      "22/281, train_loss: 0.2610, step time: 0.2490\n",
      "23/281, train_loss: 0.1221, step time: 0.2480\n",
      "24/281, train_loss: 0.1291, step time: 0.2818\n",
      "25/281, train_loss: 0.3484, step time: 0.2511\n",
      "26/281, train_loss: 0.1569, step time: 0.2528\n",
      "27/281, train_loss: 0.1323, step time: 0.2599\n",
      "28/281, train_loss: 0.1122, step time: 0.2518\n",
      "29/281, train_loss: 0.1215, step time: 0.2477\n",
      "30/281, train_loss: 0.2275, step time: 0.2453\n",
      "31/281, train_loss: 0.1549, step time: 0.2507\n",
      "32/281, train_loss: 0.1389, step time: 0.2466\n",
      "33/281, train_loss: 0.1913, step time: 0.2515\n",
      "34/281, train_loss: 0.1923, step time: 0.2498\n",
      "35/281, train_loss: 0.3651, step time: 0.2476\n",
      "36/281, train_loss: 0.4801, step time: 0.2441\n",
      "37/281, train_loss: 0.1630, step time: 0.2575\n",
      "38/281, train_loss: 0.1756, step time: 0.2582\n",
      "39/281, train_loss: 0.3065, step time: 0.2484\n",
      "40/281, train_loss: 0.1516, step time: 0.2454\n",
      "41/281, train_loss: 0.2331, step time: 0.2568\n",
      "42/281, train_loss: 0.1037, step time: 0.2567\n",
      "43/281, train_loss: 0.1290, step time: 0.2492\n",
      "44/281, train_loss: 0.4524, step time: 0.2591\n",
      "45/281, train_loss: 0.2529, step time: 0.2515\n",
      "46/281, train_loss: 0.2545, step time: 0.2529\n",
      "47/281, train_loss: 0.1618, step time: 0.2546\n",
      "48/281, train_loss: 0.3248, step time: 0.2483\n",
      "49/281, train_loss: 0.1208, step time: 0.2463\n",
      "50/281, train_loss: 0.1569, step time: 0.2474\n",
      "51/281, train_loss: 0.2877, step time: 0.2791\n",
      "52/281, train_loss: 0.1702, step time: 0.2529\n",
      "53/281, train_loss: 0.1137, step time: 0.2480\n",
      "54/281, train_loss: 0.2527, step time: 0.2489\n",
      "55/281, train_loss: 0.1581, step time: 0.2568\n",
      "56/281, train_loss: 0.1578, step time: 0.2548\n",
      "57/281, train_loss: 0.3653, step time: 0.2538\n",
      "58/281, train_loss: 0.1180, step time: 0.2524\n",
      "59/281, train_loss: 0.1553, step time: 0.2523\n",
      "60/281, train_loss: 0.1950, step time: 0.2474\n",
      "61/281, train_loss: 0.1311, step time: 0.2512\n",
      "62/281, train_loss: 0.1424, step time: 0.2478\n",
      "63/281, train_loss: 0.3911, step time: 0.2503\n",
      "64/281, train_loss: 0.2933, step time: 0.2535\n",
      "65/281, train_loss: 0.1059, step time: 0.2565\n",
      "66/281, train_loss: 0.2145, step time: 0.2501\n",
      "67/281, train_loss: 0.1025, step time: 0.2511\n",
      "68/281, train_loss: 0.1888, step time: 0.2495\n",
      "69/281, train_loss: 0.1867, step time: 0.2520\n",
      "70/281, train_loss: 0.1946, step time: 0.2485\n",
      "71/281, train_loss: 0.1509, step time: 0.2541\n",
      "72/281, train_loss: 0.1845, step time: 0.2548\n",
      "73/281, train_loss: 0.2055, step time: 0.2518\n",
      "74/281, train_loss: 0.1051, step time: 0.2530\n",
      "75/281, train_loss: 0.1699, step time: 0.2531\n",
      "76/281, train_loss: 0.1959, step time: 0.2531\n",
      "77/281, train_loss: 0.1643, step time: 0.2541\n",
      "78/281, train_loss: 0.2888, step time: 0.2566\n",
      "79/281, train_loss: 0.1576, step time: 0.2513\n",
      "80/281, train_loss: 0.1876, step time: 0.2579\n",
      "81/281, train_loss: 0.1021, step time: 0.2520\n",
      "82/281, train_loss: 0.3714, step time: 0.2512\n",
      "83/281, train_loss: 0.3306, step time: 0.2571\n",
      "84/281, train_loss: 0.1261, step time: 0.2531\n",
      "85/281, train_loss: 0.4577, step time: 0.2500\n",
      "86/281, train_loss: 0.1223, step time: 0.2533\n",
      "87/281, train_loss: 0.2033, step time: 0.2500\n",
      "88/281, train_loss: 0.2720, step time: 0.2539\n",
      "89/281, train_loss: 0.1608, step time: 0.2500\n",
      "90/281, train_loss: 0.2459, step time: 0.2523\n",
      "91/281, train_loss: 0.3225, step time: 0.2509\n",
      "92/281, train_loss: 0.2647, step time: 0.2540\n",
      "93/281, train_loss: 0.0813, step time: 0.2496\n",
      "94/281, train_loss: 0.1098, step time: 0.2530\n",
      "95/281, train_loss: 0.2468, step time: 0.2535\n",
      "96/281, train_loss: 0.1324, step time: 0.2548\n",
      "97/281, train_loss: 0.2012, step time: 0.2535\n",
      "98/281, train_loss: 0.2881, step time: 0.2546\n",
      "99/281, train_loss: 0.1512, step time: 0.2520\n",
      "100/281, train_loss: 0.1400, step time: 0.2558\n",
      "101/281, train_loss: 0.1529, step time: 0.2567\n",
      "102/281, train_loss: 0.1533, step time: 0.2530\n",
      "103/281, train_loss: 0.1531, step time: 0.2530\n",
      "104/281, train_loss: 0.1209, step time: 0.2521\n",
      "105/281, train_loss: 0.3157, step time: 0.2519\n",
      "106/281, train_loss: 0.1350, step time: 0.2494\n",
      "107/281, train_loss: 0.4424, step time: 0.2523\n",
      "108/281, train_loss: 0.1705, step time: 0.2523\n",
      "109/281, train_loss: 0.0878, step time: 0.2526\n",
      "110/281, train_loss: 0.1283, step time: 0.2493\n",
      "111/281, train_loss: 0.2131, step time: 0.2547\n",
      "112/281, train_loss: 0.1307, step time: 0.2457\n",
      "113/281, train_loss: 0.2322, step time: 0.2520\n",
      "114/281, train_loss: 0.1342, step time: 0.2488\n",
      "115/281, train_loss: 0.1511, step time: 0.2532\n",
      "116/281, train_loss: 0.1550, step time: 0.2495\n",
      "117/281, train_loss: 0.1737, step time: 0.2498\n",
      "118/281, train_loss: 0.1489, step time: 0.2475\n",
      "119/281, train_loss: 0.1715, step time: 0.2493\n",
      "120/281, train_loss: 0.1513, step time: 0.2476\n",
      "121/281, train_loss: 0.1148, step time: 0.2453\n",
      "122/281, train_loss: 0.1027, step time: 0.2482\n",
      "123/281, train_loss: 0.1412, step time: 0.2531\n",
      "124/281, train_loss: 0.1798, step time: 0.2489\n",
      "125/281, train_loss: 0.1596, step time: 0.2512\n",
      "126/281, train_loss: 0.1017, step time: 0.2537\n",
      "127/281, train_loss: 0.1658, step time: 0.2496\n",
      "128/281, train_loss: 0.3426, step time: 0.2566\n",
      "129/281, train_loss: 0.1704, step time: 0.2475\n",
      "130/281, train_loss: 0.1183, step time: 0.2506\n",
      "131/281, train_loss: 0.1831, step time: 0.2518\n",
      "132/281, train_loss: 0.1746, step time: 0.2523\n",
      "133/281, train_loss: 0.1359, step time: 0.2521\n",
      "134/281, train_loss: 0.2050, step time: 0.2515\n",
      "135/281, train_loss: 0.2427, step time: 0.2480\n",
      "136/281, train_loss: 0.2929, step time: 0.2531\n",
      "137/281, train_loss: 0.2334, step time: 0.2536\n",
      "138/281, train_loss: 0.1292, step time: 0.2586\n",
      "139/281, train_loss: 0.2921, step time: 0.2530\n",
      "140/281, train_loss: 0.2566, step time: 0.2525\n",
      "141/281, train_loss: 0.1906, step time: 0.2715\n",
      "142/281, train_loss: 0.2141, step time: 0.2929\n",
      "143/281, train_loss: 0.1298, step time: 0.2800\n",
      "144/281, train_loss: 0.2969, step time: 0.2617\n",
      "145/281, train_loss: 0.2241, step time: 0.2546\n",
      "146/281, train_loss: 0.1821, step time: 0.2545\n",
      "147/281, train_loss: 0.2540, step time: 0.2522\n",
      "148/281, train_loss: 0.1671, step time: 0.2570\n",
      "149/281, train_loss: 0.1226, step time: 0.2496\n",
      "150/281, train_loss: 0.2853, step time: 0.2487\n",
      "151/281, train_loss: 0.4021, step time: 0.2507\n",
      "152/281, train_loss: 0.1303, step time: 0.2513\n",
      "153/281, train_loss: 0.1139, step time: 0.2491\n",
      "154/281, train_loss: 0.2150, step time: 0.2508\n",
      "155/281, train_loss: 0.1139, step time: 0.2534\n",
      "156/281, train_loss: 0.1915, step time: 0.2567\n",
      "157/281, train_loss: 0.1292, step time: 0.2546\n",
      "158/281, train_loss: 0.3559, step time: 0.2625\n",
      "159/281, train_loss: 0.2314, step time: 0.2563\n",
      "160/281, train_loss: 0.1964, step time: 0.2567\n",
      "161/281, train_loss: 0.1216, step time: 0.2562\n",
      "162/281, train_loss: 0.1233, step time: 0.2488\n",
      "163/281, train_loss: 0.3243, step time: 0.2487\n",
      "164/281, train_loss: 0.2583, step time: 0.2551\n",
      "165/281, train_loss: 0.1276, step time: 0.2546\n",
      "166/281, train_loss: 0.2984, step time: 0.2494\n",
      "167/281, train_loss: 0.3007, step time: 0.2483\n",
      "168/281, train_loss: 0.1535, step time: 0.2508\n",
      "169/281, train_loss: 0.1654, step time: 0.2490\n",
      "170/281, train_loss: 0.3551, step time: 0.2526\n",
      "171/281, train_loss: 0.1354, step time: 0.2499\n",
      "172/281, train_loss: 0.1640, step time: 0.2530\n",
      "173/281, train_loss: 0.1629, step time: 0.2518\n",
      "174/281, train_loss: 0.3350, step time: 0.2557\n",
      "175/281, train_loss: 0.2265, step time: 0.2572\n",
      "176/281, train_loss: 0.3797, step time: 0.2517\n",
      "177/281, train_loss: 0.1416, step time: 0.2555\n",
      "178/281, train_loss: 0.2791, step time: 0.2555\n",
      "179/281, train_loss: 0.1453, step time: 0.2497\n",
      "180/281, train_loss: 0.2725, step time: 0.2468\n",
      "181/281, train_loss: 0.3048, step time: 0.2507\n",
      "182/281, train_loss: 0.1628, step time: 0.2476\n",
      "183/281, train_loss: 0.1854, step time: 0.2496\n",
      "184/281, train_loss: 0.4658, step time: 0.2502\n",
      "185/281, train_loss: 0.2805, step time: 0.2506\n",
      "186/281, train_loss: 0.1304, step time: 0.2556\n",
      "187/281, train_loss: 0.1399, step time: 0.2492\n",
      "188/281, train_loss: 0.1665, step time: 0.2515\n",
      "189/281, train_loss: 0.1597, step time: 0.2476\n",
      "190/281, train_loss: 0.2431, step time: 0.2455\n",
      "191/281, train_loss: 0.1786, step time: 0.2451\n",
      "192/281, train_loss: 0.1411, step time: 0.2469\n",
      "193/281, train_loss: 0.1953, step time: 0.2469\n",
      "194/281, train_loss: 0.2303, step time: 0.2476\n",
      "195/281, train_loss: 0.1139, step time: 0.2507\n",
      "196/281, train_loss: 0.1543, step time: 0.2455\n",
      "197/281, train_loss: 0.2609, step time: 0.2415\n",
      "198/281, train_loss: 0.3210, step time: 0.2461\n",
      "199/281, train_loss: 0.1322, step time: 0.2541\n",
      "200/281, train_loss: 0.3102, step time: 0.2527\n",
      "201/281, train_loss: 0.3827, step time: 0.2501\n",
      "202/281, train_loss: 0.2255, step time: 0.2527\n",
      "203/281, train_loss: 0.3595, step time: 0.2487\n",
      "204/281, train_loss: 0.1395, step time: 0.2496\n",
      "205/281, train_loss: 0.1915, step time: 0.2503\n",
      "206/281, train_loss: 0.1327, step time: 0.2509\n",
      "207/281, train_loss: 0.1425, step time: 0.2495\n",
      "208/281, train_loss: 0.2052, step time: 0.2504\n",
      "209/281, train_loss: 0.1904, step time: 0.2495\n",
      "210/281, train_loss: 0.1440, step time: 0.2469\n",
      "211/281, train_loss: 0.0858, step time: 0.2510\n",
      "212/281, train_loss: 0.2202, step time: 0.2485\n",
      "213/281, train_loss: 0.1036, step time: 0.2457\n",
      "214/281, train_loss: 0.0781, step time: 0.2500\n",
      "215/281, train_loss: 0.0883, step time: 0.2561\n",
      "216/281, train_loss: 0.3418, step time: 0.2570\n",
      "217/281, train_loss: 0.1152, step time: 0.2452\n",
      "218/281, train_loss: 0.2815, step time: 0.2487\n",
      "219/281, train_loss: 0.2917, step time: 0.2534\n",
      "220/281, train_loss: 0.1135, step time: 0.2492\n",
      "221/281, train_loss: 0.1519, step time: 0.2508\n",
      "222/281, train_loss: 0.1023, step time: 0.2527\n",
      "223/281, train_loss: 0.3185, step time: 0.2486\n",
      "224/281, train_loss: 0.2717, step time: 0.2532\n",
      "225/281, train_loss: 0.3082, step time: 0.2502\n",
      "226/281, train_loss: 0.1331, step time: 0.2485\n",
      "227/281, train_loss: 0.1220, step time: 0.2478\n",
      "228/281, train_loss: 0.1856, step time: 0.2480\n",
      "229/281, train_loss: 0.1160, step time: 0.2517\n",
      "230/281, train_loss: 0.1260, step time: 0.2482\n",
      "231/281, train_loss: 0.1952, step time: 0.2481\n",
      "232/281, train_loss: 0.1272, step time: 0.2500\n",
      "233/281, train_loss: 0.1529, step time: 0.2475\n",
      "234/281, train_loss: 0.1306, step time: 0.2478\n",
      "235/281, train_loss: 0.4819, step time: 0.2479\n",
      "236/281, train_loss: 0.0771, step time: 0.2498\n",
      "237/281, train_loss: 0.2258, step time: 0.2498\n",
      "238/281, train_loss: 0.1426, step time: 0.2554\n",
      "239/281, train_loss: 0.2348, step time: 0.2504\n",
      "240/281, train_loss: 0.2714, step time: 0.2501\n",
      "241/281, train_loss: 0.2030, step time: 0.2463\n",
      "242/281, train_loss: 0.2258, step time: 0.2561\n",
      "243/281, train_loss: 0.1600, step time: 0.2617\n",
      "244/281, train_loss: 0.1620, step time: 0.2462\n",
      "245/281, train_loss: 0.2644, step time: 0.2489\n",
      "246/281, train_loss: 0.1624, step time: 0.2492\n",
      "247/281, train_loss: 0.1477, step time: 0.2504\n",
      "248/281, train_loss: 0.1422, step time: 0.2475\n",
      "249/281, train_loss: 0.1042, step time: 0.2494\n",
      "250/281, train_loss: 0.1235, step time: 0.2569\n",
      "251/281, train_loss: 0.0860, step time: 0.2530\n",
      "252/281, train_loss: 0.1080, step time: 0.2519\n",
      "253/281, train_loss: 0.1719, step time: 0.2475\n",
      "254/281, train_loss: 0.2363, step time: 0.2487\n",
      "255/281, train_loss: 0.1454, step time: 0.2502\n",
      "256/281, train_loss: 0.2333, step time: 0.2520\n",
      "257/281, train_loss: 0.1467, step time: 0.2445\n",
      "258/281, train_loss: 0.1686, step time: 0.2473\n",
      "259/281, train_loss: 0.1374, step time: 0.2514\n",
      "260/281, train_loss: 0.1105, step time: 0.2545\n",
      "261/281, train_loss: 0.1600, step time: 0.2557\n",
      "262/281, train_loss: 0.3438, step time: 0.2224\n",
      "263/281, train_loss: 0.1363, step time: 0.2523\n",
      "264/281, train_loss: 0.2335, step time: 0.2532\n",
      "265/281, train_loss: 0.1912, step time: 0.2522\n",
      "266/281, train_loss: 0.1161, step time: 0.2584\n",
      "267/281, train_loss: 0.1624, step time: 0.2629\n",
      "268/281, train_loss: 0.3298, step time: 0.2464\n",
      "269/281, train_loss: 0.0929, step time: 0.2510\n",
      "270/281, train_loss: 0.1433, step time: 0.2486\n",
      "271/281, train_loss: 0.2482, step time: 0.2566\n",
      "272/281, train_loss: 0.1704, step time: 0.2503\n",
      "273/281, train_loss: 0.1313, step time: 0.2543\n",
      "274/281, train_loss: 0.2622, step time: 0.2525\n",
      "275/281, train_loss: 0.1458, step time: 0.2495\n",
      "276/281, train_loss: 0.1451, step time: 0.2456\n",
      "277/281, train_loss: 0.2718, step time: 0.2578\n",
      "278/281, train_loss: 0.3259, step time: 0.2621\n",
      "279/281, train_loss: 0.1952, step time: 0.2523\n",
      "280/281, train_loss: 0.1606, step time: 0.2537\n",
      "281/281, train_loss: 0.1753, step time: 0.2482\n",
      "282/281, train_loss: 0.1940, step time: 0.1483\n",
      "epoch 32 average loss: 0.1961\n",
      "saved new best metric model\n",
      "current epoch: 32 current mean dice: 0.8441 tc: 0.8152 wt: 0.9047 et: 0.8248\n",
      "best mean dice: 0.8441 at epoch: 32\n",
      "time consuming of epoch 32 is: 415.6417\n",
      "----------\n",
      "epoch 33/200\n",
      "1/281, train_loss: 0.1196, step time: 0.2569\n",
      "2/281, train_loss: 0.0888, step time: 0.2549\n",
      "3/281, train_loss: 0.1693, step time: 0.2524\n",
      "4/281, train_loss: 0.2435, step time: 0.2503\n",
      "5/281, train_loss: 0.1864, step time: 0.2553\n",
      "6/281, train_loss: 0.1174, step time: 0.2570\n",
      "7/281, train_loss: 0.1346, step time: 0.2531\n",
      "8/281, train_loss: 0.1814, step time: 0.2515\n",
      "9/281, train_loss: 0.1656, step time: 0.2556\n",
      "10/281, train_loss: 0.3400, step time: 0.2476\n",
      "11/281, train_loss: 0.2525, step time: 0.2496\n",
      "12/281, train_loss: 0.1674, step time: 0.2502\n",
      "13/281, train_loss: 0.1854, step time: 0.2557\n",
      "14/281, train_loss: 0.3451, step time: 0.2544\n",
      "15/281, train_loss: 0.1407, step time: 0.2493\n",
      "16/281, train_loss: 0.2807, step time: 0.2547\n",
      "17/281, train_loss: 0.0825, step time: 0.2517\n",
      "18/281, train_loss: 0.1313, step time: 0.2608\n",
      "19/281, train_loss: 0.1282, step time: 0.2576\n",
      "20/281, train_loss: 0.1064, step time: 0.2676\n",
      "21/281, train_loss: 0.0822, step time: 0.2635\n",
      "22/281, train_loss: 0.2304, step time: 0.2621\n",
      "23/281, train_loss: 0.1307, step time: 0.2544\n",
      "24/281, train_loss: 0.2787, step time: 0.2754\n",
      "25/281, train_loss: 0.1576, step time: 0.2571\n",
      "26/281, train_loss: 0.2336, step time: 0.2470\n",
      "27/281, train_loss: 0.1530, step time: 0.2523\n",
      "28/281, train_loss: 0.1154, step time: 0.2511\n",
      "29/281, train_loss: 0.1234, step time: 0.2492\n",
      "30/281, train_loss: 0.0681, step time: 0.2511\n",
      "31/281, train_loss: 0.3727, step time: 0.2524\n",
      "32/281, train_loss: 0.1838, step time: 0.2509\n",
      "33/281, train_loss: 0.1584, step time: 0.2515\n",
      "34/281, train_loss: 0.1420, step time: 0.2463\n",
      "35/281, train_loss: 0.2121, step time: 0.2511\n",
      "36/281, train_loss: 0.1396, step time: 0.2492\n",
      "37/281, train_loss: 0.2140, step time: 0.2493\n",
      "38/281, train_loss: 0.1180, step time: 0.2491\n",
      "39/281, train_loss: 0.1357, step time: 0.2519\n",
      "40/281, train_loss: 0.0828, step time: 0.2585\n",
      "41/281, train_loss: 0.1446, step time: 0.2548\n",
      "42/281, train_loss: 0.2667, step time: 0.2551\n",
      "43/281, train_loss: 0.2385, step time: 0.2530\n",
      "44/281, train_loss: 0.2671, step time: 0.2602\n",
      "45/281, train_loss: 0.1276, step time: 0.2576\n",
      "46/281, train_loss: 0.1384, step time: 0.2620\n",
      "47/281, train_loss: 0.2078, step time: 0.2531\n",
      "48/281, train_loss: 0.2482, step time: 0.2554\n",
      "49/281, train_loss: 0.1162, step time: 0.2556\n",
      "50/281, train_loss: 0.1255, step time: 0.2556\n",
      "51/281, train_loss: 0.3353, step time: 0.2593\n",
      "52/281, train_loss: 0.1656, step time: 0.2501\n",
      "53/281, train_loss: 0.0998, step time: 0.2566\n",
      "54/281, train_loss: 0.2872, step time: 0.2566\n",
      "55/281, train_loss: 0.1219, step time: 0.2509\n",
      "56/281, train_loss: 0.3309, step time: 0.2535\n",
      "57/281, train_loss: 0.1051, step time: 0.2576\n",
      "58/281, train_loss: 0.1049, step time: 0.2584\n",
      "59/281, train_loss: 0.3870, step time: 0.2580\n",
      "60/281, train_loss: 0.2630, step time: 0.2561\n",
      "61/281, train_loss: 0.1453, step time: 0.2617\n",
      "62/281, train_loss: 0.1112, step time: 0.2558\n",
      "63/281, train_loss: 0.2814, step time: 0.2542\n",
      "64/281, train_loss: 0.2764, step time: 0.2699\n",
      "65/281, train_loss: 0.2979, step time: 0.2638\n",
      "66/281, train_loss: 0.1217, step time: 0.2569\n",
      "67/281, train_loss: 0.1037, step time: 0.2558\n",
      "68/281, train_loss: 0.3757, step time: 0.2532\n",
      "69/281, train_loss: 0.3056, step time: 0.2561\n",
      "70/281, train_loss: 0.2573, step time: 0.2526\n",
      "71/281, train_loss: 0.2871, step time: 0.2533\n",
      "72/281, train_loss: 0.1565, step time: 0.2564\n",
      "73/281, train_loss: 0.1480, step time: 0.2547\n",
      "74/281, train_loss: 0.1356, step time: 0.2559\n",
      "75/281, train_loss: 0.1479, step time: 0.2543\n",
      "76/281, train_loss: 0.1148, step time: 0.2540\n",
      "77/281, train_loss: 0.3045, step time: 0.2570\n",
      "78/281, train_loss: 0.1123, step time: 0.2572\n",
      "79/281, train_loss: 0.3371, step time: 0.2569\n",
      "80/281, train_loss: 0.2837, step time: 0.2527\n",
      "81/281, train_loss: 0.2839, step time: 0.2576\n",
      "82/281, train_loss: 0.1871, step time: 0.2587\n",
      "83/281, train_loss: 0.1340, step time: 0.2551\n",
      "84/281, train_loss: 0.1043, step time: 0.2505\n",
      "85/281, train_loss: 0.0999, step time: 0.2533\n",
      "86/281, train_loss: 0.2875, step time: 0.2526\n",
      "87/281, train_loss: 0.0733, step time: 0.2555\n",
      "88/281, train_loss: 0.1480, step time: 0.2542\n",
      "89/281, train_loss: 0.1126, step time: 0.2565\n",
      "90/281, train_loss: 0.3150, step time: 0.2567\n",
      "91/281, train_loss: 0.1608, step time: 0.2597\n",
      "92/281, train_loss: 0.1222, step time: 0.2575\n",
      "93/281, train_loss: 0.1477, step time: 0.2640\n",
      "94/281, train_loss: 0.2827, step time: 0.2619\n",
      "95/281, train_loss: 0.2777, step time: 0.2520\n",
      "96/281, train_loss: 0.1034, step time: 0.2576\n",
      "97/281, train_loss: 0.3084, step time: 0.2528\n",
      "98/281, train_loss: 0.1223, step time: 0.2712\n",
      "99/281, train_loss: 0.1204, step time: 0.2547\n",
      "100/281, train_loss: 0.1754, step time: 0.2521\n",
      "101/281, train_loss: 0.1390, step time: 0.2550\n",
      "102/281, train_loss: 0.2022, step time: 0.2508\n",
      "103/281, train_loss: 0.1551, step time: 0.2553\n",
      "104/281, train_loss: 0.3213, step time: 0.2540\n",
      "105/281, train_loss: 0.0807, step time: 0.2525\n",
      "106/281, train_loss: 0.0728, step time: 0.2557\n",
      "107/281, train_loss: 0.0978, step time: 0.2547\n",
      "108/281, train_loss: 0.1402, step time: 0.2569\n",
      "109/281, train_loss: 0.2546, step time: 0.2548\n",
      "110/281, train_loss: 0.2884, step time: 0.2564\n",
      "111/281, train_loss: 0.0986, step time: 0.2562\n",
      "112/281, train_loss: 0.1667, step time: 0.2601\n",
      "113/281, train_loss: 0.1128, step time: 0.2577\n",
      "114/281, train_loss: 0.1134, step time: 0.2577\n",
      "115/281, train_loss: 0.1594, step time: 0.2538\n",
      "116/281, train_loss: 0.1115, step time: 0.2563\n",
      "117/281, train_loss: 0.1249, step time: 0.2570\n",
      "118/281, train_loss: 0.1749, step time: 0.2593\n",
      "119/281, train_loss: 0.1478, step time: 0.2586\n",
      "120/281, train_loss: 0.1539, step time: 0.2579\n",
      "121/281, train_loss: 0.1536, step time: 0.2556\n",
      "122/281, train_loss: 0.5793, step time: 0.2580\n",
      "123/281, train_loss: 0.1216, step time: 0.2558\n",
      "124/281, train_loss: 0.0879, step time: 0.2580\n",
      "125/281, train_loss: 0.1498, step time: 0.2590\n",
      "126/281, train_loss: 0.2603, step time: 0.2557\n",
      "127/281, train_loss: 0.1758, step time: 0.2581\n",
      "128/281, train_loss: 0.1167, step time: 0.2533\n",
      "129/281, train_loss: 0.1204, step time: 0.2528\n",
      "130/281, train_loss: 0.2440, step time: 0.2516\n",
      "131/281, train_loss: 0.1072, step time: 0.2505\n",
      "132/281, train_loss: 0.1500, step time: 0.2565\n",
      "133/281, train_loss: 0.1196, step time: 0.2561\n",
      "134/281, train_loss: 0.1146, step time: 0.2574\n",
      "135/281, train_loss: 0.1015, step time: 0.2544\n",
      "136/281, train_loss: 0.3684, step time: 0.2559\n",
      "137/281, train_loss: 0.1079, step time: 0.2567\n",
      "138/281, train_loss: 0.1541, step time: 0.2591\n",
      "139/281, train_loss: 0.4429, step time: 0.2588\n",
      "140/281, train_loss: 0.1307, step time: 0.2512\n",
      "141/281, train_loss: 0.1368, step time: 0.2557\n",
      "142/281, train_loss: 0.1141, step time: 0.2529\n",
      "143/281, train_loss: 0.0992, step time: 0.2547\n",
      "144/281, train_loss: 0.1145, step time: 0.2510\n",
      "145/281, train_loss: 0.0980, step time: 0.2550\n",
      "146/281, train_loss: 0.1441, step time: 0.2557\n",
      "147/281, train_loss: 0.1699, step time: 0.2560\n",
      "148/281, train_loss: 0.1931, step time: 0.2550\n",
      "149/281, train_loss: 0.1745, step time: 0.2517\n",
      "150/281, train_loss: 0.1481, step time: 0.2537\n",
      "151/281, train_loss: 0.1069, step time: 0.2564\n",
      "152/281, train_loss: 0.1284, step time: 0.2508\n",
      "153/281, train_loss: 0.0976, step time: 0.2494\n",
      "154/281, train_loss: 0.1768, step time: 0.2509\n",
      "155/281, train_loss: 0.1688, step time: 0.2517\n",
      "156/281, train_loss: 0.1717, step time: 0.2554\n",
      "157/281, train_loss: 0.3146, step time: 0.2551\n",
      "158/281, train_loss: 0.2205, step time: 0.2571\n",
      "159/281, train_loss: 0.1280, step time: 0.2520\n",
      "160/281, train_loss: 0.1946, step time: 0.2538\n",
      "161/281, train_loss: 0.1130, step time: 0.2618\n",
      "162/281, train_loss: 0.2134, step time: 0.2624\n",
      "163/281, train_loss: 0.2873, step time: 0.2605\n",
      "164/281, train_loss: 0.1529, step time: 0.2613\n",
      "165/281, train_loss: 0.1736, step time: 0.2601\n",
      "166/281, train_loss: 0.1167, step time: 0.2539\n",
      "167/281, train_loss: 0.1548, step time: 0.2501\n",
      "168/281, train_loss: 0.1526, step time: 0.2536\n",
      "169/281, train_loss: 0.1402, step time: 0.2539\n",
      "170/281, train_loss: 0.1687, step time: 0.2570\n",
      "171/281, train_loss: 0.1006, step time: 0.2623\n",
      "172/281, train_loss: 0.1200, step time: 0.2586\n",
      "173/281, train_loss: 0.3189, step time: 0.2562\n",
      "174/281, train_loss: 0.2584, step time: 0.2561\n",
      "175/281, train_loss: 0.2244, step time: 0.2568\n",
      "176/281, train_loss: 0.1027, step time: 0.2570\n",
      "177/281, train_loss: 0.1529, step time: 0.2576\n",
      "178/281, train_loss: 0.1224, step time: 0.2581\n",
      "179/281, train_loss: 0.1279, step time: 0.2553\n",
      "180/281, train_loss: 0.1444, step time: 0.2540\n",
      "181/281, train_loss: 0.2008, step time: 0.2522\n",
      "182/281, train_loss: 0.0869, step time: 0.2579\n",
      "183/281, train_loss: 0.1632, step time: 0.2574\n",
      "184/281, train_loss: 0.2765, step time: 0.2592\n",
      "185/281, train_loss: 0.4836, step time: 0.2535\n",
      "186/281, train_loss: 0.3874, step time: 0.2569\n",
      "187/281, train_loss: 0.1862, step time: 0.2597\n",
      "188/281, train_loss: 0.0696, step time: 0.2559\n",
      "189/281, train_loss: 0.1620, step time: 0.2517\n",
      "190/281, train_loss: 0.2651, step time: 0.2532\n",
      "191/281, train_loss: 0.1654, step time: 0.2555\n",
      "192/281, train_loss: 0.1587, step time: 0.2615\n",
      "193/281, train_loss: 0.1994, step time: 0.2590\n",
      "194/281, train_loss: 0.1354, step time: 0.2574\n",
      "195/281, train_loss: 0.2288, step time: 0.2528\n",
      "196/281, train_loss: 0.1877, step time: 0.2543\n",
      "197/281, train_loss: 0.1697, step time: 0.2524\n",
      "198/281, train_loss: 0.2762, step time: 0.2518\n",
      "199/281, train_loss: 0.2020, step time: 0.2507\n",
      "200/281, train_loss: 0.1317, step time: 0.2534\n",
      "201/281, train_loss: 0.1346, step time: 0.2543\n",
      "202/281, train_loss: 0.0754, step time: 0.2549\n",
      "203/281, train_loss: 0.1141, step time: 0.2534\n",
      "204/281, train_loss: 0.3335, step time: 0.2543\n",
      "205/281, train_loss: 0.1941, step time: 0.2515\n",
      "206/281, train_loss: 0.1057, step time: 0.2544\n",
      "207/281, train_loss: 0.2089, step time: 0.2532\n",
      "208/281, train_loss: 0.1353, step time: 0.2519\n",
      "209/281, train_loss: 0.1214, step time: 0.2548\n",
      "210/281, train_loss: 0.1054, step time: 0.2568\n",
      "211/281, train_loss: 0.1032, step time: 0.2570\n",
      "212/281, train_loss: 0.1409, step time: 0.2580\n",
      "213/281, train_loss: 0.1321, step time: 0.2481\n",
      "214/281, train_loss: 0.1588, step time: 0.2463\n",
      "215/281, train_loss: 0.1354, step time: 0.2478\n",
      "216/281, train_loss: 0.0933, step time: 0.2529\n",
      "217/281, train_loss: 0.2796, step time: 0.2507\n",
      "218/281, train_loss: 0.1639, step time: 0.2483\n",
      "219/281, train_loss: 0.1578, step time: 0.2549\n",
      "220/281, train_loss: 0.0806, step time: 0.2513\n",
      "221/281, train_loss: 0.2349, step time: 0.2549\n",
      "222/281, train_loss: 0.1197, step time: 0.2502\n",
      "223/281, train_loss: 0.1882, step time: 0.2553\n",
      "224/281, train_loss: 0.3330, step time: 0.2571\n",
      "225/281, train_loss: 0.1631, step time: 0.2521\n",
      "226/281, train_loss: 0.3173, step time: 0.2514\n",
      "227/281, train_loss: 0.3174, step time: 0.2576\n",
      "228/281, train_loss: 0.1654, step time: 0.2524\n",
      "229/281, train_loss: 0.1682, step time: 0.2540\n",
      "230/281, train_loss: 0.0871, step time: 0.2567\n",
      "231/281, train_loss: 0.1297, step time: 0.2527\n",
      "232/281, train_loss: 0.1092, step time: 0.2491\n",
      "233/281, train_loss: 0.3531, step time: 0.2553\n",
      "234/281, train_loss: 0.1076, step time: 0.2552\n",
      "235/281, train_loss: 0.2384, step time: 0.2600\n",
      "236/281, train_loss: 0.1368, step time: 0.2523\n",
      "237/281, train_loss: 0.2772, step time: 0.2611\n",
      "238/281, train_loss: 0.0995, step time: 0.2574\n",
      "239/281, train_loss: 0.2204, step time: 0.2560\n",
      "240/281, train_loss: 0.0990, step time: 0.2498\n",
      "241/281, train_loss: 0.3179, step time: 0.2490\n",
      "242/281, train_loss: 0.1752, step time: 0.2524\n",
      "243/281, train_loss: 0.1055, step time: 0.2544\n",
      "244/281, train_loss: 0.1038, step time: 0.2483\n",
      "245/281, train_loss: 0.2614, step time: 0.2520\n",
      "246/281, train_loss: 0.3076, step time: 0.2562\n",
      "247/281, train_loss: 0.1028, step time: 0.2511\n",
      "248/281, train_loss: 0.1722, step time: 0.2544\n",
      "249/281, train_loss: 0.1608, step time: 0.2461\n",
      "250/281, train_loss: 0.2145, step time: 0.2517\n",
      "251/281, train_loss: 0.1522, step time: 0.2561\n",
      "252/281, train_loss: 0.2944, step time: 0.2478\n",
      "253/281, train_loss: 0.3007, step time: 0.2486\n",
      "254/281, train_loss: 0.1412, step time: 0.2540\n",
      "255/281, train_loss: 0.1248, step time: 0.2511\n",
      "256/281, train_loss: 0.0962, step time: 0.2489\n",
      "257/281, train_loss: 0.1949, step time: 0.2524\n",
      "258/281, train_loss: 0.1897, step time: 0.2680\n",
      "259/281, train_loss: 0.2592, step time: 0.2518\n",
      "260/281, train_loss: 0.1618, step time: 0.2550\n",
      "261/281, train_loss: 0.1192, step time: 0.2484\n",
      "262/281, train_loss: 0.2933, step time: 0.2555\n",
      "263/281, train_loss: 0.2361, step time: 0.2544\n",
      "264/281, train_loss: 0.2392, step time: 0.2511\n",
      "265/281, train_loss: 0.2536, step time: 0.2479\n",
      "266/281, train_loss: 0.3685, step time: 0.2526\n",
      "267/281, train_loss: 0.2419, step time: 0.2505\n",
      "268/281, train_loss: 0.1378, step time: 0.2514\n",
      "269/281, train_loss: 0.2260, step time: 0.2490\n",
      "270/281, train_loss: 0.0993, step time: 0.2466\n",
      "271/281, train_loss: 0.2111, step time: 0.2491\n",
      "272/281, train_loss: 0.1444, step time: 0.2485\n",
      "273/281, train_loss: 0.2343, step time: 0.2501\n",
      "274/281, train_loss: 0.1604, step time: 0.2602\n",
      "275/281, train_loss: 0.1771, step time: 0.2500\n",
      "276/281, train_loss: 0.1229, step time: 0.2549\n",
      "277/281, train_loss: 0.2732, step time: 0.2589\n",
      "278/281, train_loss: 0.1567, step time: 0.2518\n",
      "279/281, train_loss: 0.0880, step time: 0.2542\n",
      "280/281, train_loss: 0.1327, step time: 0.2545\n",
      "281/281, train_loss: 0.1663, step time: 0.2597\n",
      "282/281, train_loss: 0.1121, step time: 0.1539\n",
      "epoch 33 average loss: 0.1817\n",
      "saved new best metric model\n",
      "current epoch: 33 current mean dice: 0.8474 tc: 0.8250 wt: 0.9002 et: 0.8300\n",
      "best mean dice: 0.8474 at epoch: 33\n",
      "time consuming of epoch 33 is: 381.6580\n",
      "----------\n",
      "epoch 34/200\n",
      "1/281, train_loss: 0.0895, step time: 0.2561\n",
      "2/281, train_loss: 0.1090, step time: 0.2544\n",
      "3/281, train_loss: 0.1340, step time: 0.2559\n",
      "4/281, train_loss: 0.1589, step time: 0.2493\n",
      "5/281, train_loss: 0.1354, step time: 0.2511\n",
      "6/281, train_loss: 0.1377, step time: 0.2565\n",
      "7/281, train_loss: 0.1075, step time: 0.2559\n",
      "8/281, train_loss: 0.1131, step time: 0.2478\n",
      "9/281, train_loss: 0.1419, step time: 0.2659\n",
      "10/281, train_loss: 0.4689, step time: 0.2593\n",
      "11/281, train_loss: 0.1043, step time: 0.2576\n",
      "12/281, train_loss: 0.1910, step time: 0.2555\n",
      "13/281, train_loss: 0.0695, step time: 0.2487\n",
      "14/281, train_loss: 0.1676, step time: 0.2519\n",
      "15/281, train_loss: 0.2774, step time: 0.2541\n",
      "16/281, train_loss: 0.1825, step time: 0.2523\n",
      "17/281, train_loss: 0.2703, step time: 0.2590\n",
      "18/281, train_loss: 0.1193, step time: 0.2555\n",
      "19/281, train_loss: 0.1593, step time: 0.2531\n",
      "20/281, train_loss: 0.1919, step time: 0.2537\n",
      "21/281, train_loss: 0.1956, step time: 0.2507\n",
      "22/281, train_loss: 0.1612, step time: 0.2540\n",
      "23/281, train_loss: 0.0827, step time: 0.2552\n",
      "24/281, train_loss: 0.1233, step time: 0.2476\n",
      "25/281, train_loss: 0.2124, step time: 0.2498\n",
      "26/281, train_loss: 0.1537, step time: 0.2524\n",
      "27/281, train_loss: 0.1044, step time: 0.2540\n",
      "28/281, train_loss: 0.1246, step time: 0.2565\n",
      "29/281, train_loss: 0.2641, step time: 0.2436\n",
      "30/281, train_loss: 0.1363, step time: 0.2528\n",
      "31/281, train_loss: 0.1238, step time: 0.2551\n",
      "32/281, train_loss: 0.2248, step time: 0.2519\n",
      "33/281, train_loss: 0.2853, step time: 0.2569\n",
      "34/281, train_loss: 0.1281, step time: 0.2527\n",
      "35/281, train_loss: 0.1911, step time: 0.2508\n",
      "36/281, train_loss: 0.3239, step time: 0.2536\n",
      "37/281, train_loss: 0.1255, step time: 0.2431\n",
      "38/281, train_loss: 0.1480, step time: 0.2504\n",
      "39/281, train_loss: 0.0879, step time: 0.2479\n",
      "40/281, train_loss: 0.1125, step time: 0.2525\n",
      "41/281, train_loss: 0.2168, step time: 0.2532\n",
      "42/281, train_loss: 0.1355, step time: 0.2531\n",
      "43/281, train_loss: 0.1093, step time: 0.2489\n",
      "44/281, train_loss: 0.1080, step time: 0.2535\n",
      "45/281, train_loss: 0.2108, step time: 0.2479\n",
      "46/281, train_loss: 0.1153, step time: 0.2464\n",
      "47/281, train_loss: 0.1500, step time: 0.2552\n",
      "48/281, train_loss: 0.1704, step time: 0.2533\n",
      "49/281, train_loss: 0.1578, step time: 0.2468\n",
      "50/281, train_loss: 0.2778, step time: 0.2480\n",
      "51/281, train_loss: 0.1047, step time: 0.2563\n",
      "52/281, train_loss: 0.2815, step time: 0.2563\n",
      "53/281, train_loss: 0.0994, step time: 0.2490\n",
      "54/281, train_loss: 0.2865, step time: 0.2501\n",
      "55/281, train_loss: 0.1621, step time: 0.2478\n",
      "56/281, train_loss: 0.4232, step time: 0.2560\n",
      "57/281, train_loss: 0.1037, step time: 0.2571\n",
      "58/281, train_loss: 0.2025, step time: 0.2489\n",
      "59/281, train_loss: 0.1700, step time: 0.2543\n",
      "60/281, train_loss: 0.2688, step time: 0.2527\n",
      "61/281, train_loss: 0.1211, step time: 0.2490\n",
      "62/281, train_loss: 0.0742, step time: 0.2528\n",
      "63/281, train_loss: 0.0886, step time: 0.2533\n",
      "64/281, train_loss: 0.2719, step time: 0.2539\n",
      "65/281, train_loss: 0.0827, step time: 0.2488\n",
      "66/281, train_loss: 0.2809, step time: 0.2500\n",
      "67/281, train_loss: 0.1208, step time: 0.2522\n",
      "68/281, train_loss: 0.2392, step time: 0.2569\n",
      "69/281, train_loss: 0.1618, step time: 0.2531\n",
      "70/281, train_loss: 0.2481, step time: 0.2509\n",
      "71/281, train_loss: 0.3198, step time: 0.2526\n",
      "72/281, train_loss: 0.1324, step time: 0.2528\n",
      "73/281, train_loss: 0.1392, step time: 0.2586\n",
      "74/281, train_loss: 0.1087, step time: 0.2532\n",
      "75/281, train_loss: 0.1288, step time: 0.2564\n",
      "76/281, train_loss: 0.1224, step time: 0.2508\n",
      "77/281, train_loss: 0.2645, step time: 0.2524\n",
      "78/281, train_loss: 0.1607, step time: 0.2479\n",
      "79/281, train_loss: 0.1432, step time: 0.2580\n",
      "80/281, train_loss: 0.1500, step time: 0.2564\n",
      "81/281, train_loss: 0.1030, step time: 0.2546\n",
      "82/281, train_loss: 0.2227, step time: 0.2587\n",
      "83/281, train_loss: 0.1324, step time: 0.2595\n",
      "84/281, train_loss: 0.1586, step time: 0.2587\n",
      "85/281, train_loss: 0.1776, step time: 0.2485\n",
      "86/281, train_loss: 0.0943, step time: 0.2496\n",
      "87/281, train_loss: 0.1043, step time: 0.2516\n",
      "88/281, train_loss: 0.2642, step time: 0.2521\n",
      "89/281, train_loss: 0.1295, step time: 0.2539\n",
      "90/281, train_loss: 0.1595, step time: 0.2519\n",
      "91/281, train_loss: 0.1326, step time: 0.2531\n",
      "92/281, train_loss: 0.1899, step time: 0.2540\n",
      "93/281, train_loss: 0.0754, step time: 0.2536\n",
      "94/281, train_loss: 0.1287, step time: 0.2543\n",
      "95/281, train_loss: 0.0897, step time: 0.2524\n",
      "96/281, train_loss: 0.1183, step time: 0.2509\n",
      "97/281, train_loss: 0.1802, step time: 0.2483\n",
      "98/281, train_loss: 0.1429, step time: 0.2528\n",
      "99/281, train_loss: 0.0882, step time: 0.2585\n",
      "100/281, train_loss: 0.3064, step time: 0.2560\n",
      "101/281, train_loss: 0.2861, step time: 0.2521\n",
      "102/281, train_loss: 0.1009, step time: 0.2460\n",
      "103/281, train_loss: 0.1963, step time: 0.2566\n",
      "104/281, train_loss: 0.0759, step time: 0.2565\n",
      "105/281, train_loss: 0.1751, step time: 0.2473\n",
      "106/281, train_loss: 0.1804, step time: 0.2497\n",
      "107/281, train_loss: 0.1251, step time: 0.2619\n",
      "108/281, train_loss: 0.0874, step time: 0.2520\n",
      "109/281, train_loss: 0.3641, step time: 0.2540\n",
      "110/281, train_loss: 0.0805, step time: 0.2550\n",
      "111/281, train_loss: 0.0941, step time: 0.2583\n",
      "112/281, train_loss: 0.2095, step time: 0.2531\n",
      "113/281, train_loss: 0.2497, step time: 0.2474\n",
      "114/281, train_loss: 0.1299, step time: 0.2522\n",
      "115/281, train_loss: 0.3200, step time: 0.2536\n",
      "116/281, train_loss: 0.1057, step time: 0.2558\n",
      "117/281, train_loss: 0.1779, step time: 0.2544\n",
      "118/281, train_loss: 0.1838, step time: 0.2534\n",
      "119/281, train_loss: 0.1761, step time: 0.2519\n",
      "120/281, train_loss: 0.1103, step time: 0.2529\n",
      "121/281, train_loss: 0.3192, step time: 0.2544\n",
      "122/281, train_loss: 0.1327, step time: 0.2560\n",
      "123/281, train_loss: 0.0718, step time: 0.2557\n",
      "124/281, train_loss: 0.1559, step time: 0.2569\n",
      "125/281, train_loss: 0.2054, step time: 0.2522\n",
      "126/281, train_loss: 0.1063, step time: 0.2508\n",
      "127/281, train_loss: 0.1076, step time: 0.2597\n",
      "128/281, train_loss: 0.1493, step time: 0.2488\n",
      "129/281, train_loss: 0.1039, step time: 0.2541\n",
      "130/281, train_loss: 0.1109, step time: 0.2498\n",
      "131/281, train_loss: 0.1094, step time: 0.2499\n",
      "132/281, train_loss: 0.1469, step time: 0.2538\n",
      "133/281, train_loss: 0.1009, step time: 0.2539\n",
      "134/281, train_loss: 0.3567, step time: 0.2545\n",
      "135/281, train_loss: 0.1801, step time: 0.2534\n",
      "136/281, train_loss: 0.2973, step time: 0.2539\n",
      "137/281, train_loss: 0.1899, step time: 0.2458\n",
      "138/281, train_loss: 0.1053, step time: 0.2482\n",
      "139/281, train_loss: 0.2928, step time: 0.2499\n",
      "140/281, train_loss: 0.1920, step time: 0.2514\n",
      "141/281, train_loss: 0.2568, step time: 0.2528\n",
      "142/281, train_loss: 0.1071, step time: 0.2523\n",
      "143/281, train_loss: 0.1150, step time: 0.2506\n",
      "144/281, train_loss: 0.4478, step time: 0.2524\n",
      "145/281, train_loss: 0.2544, step time: 0.2537\n",
      "146/281, train_loss: 0.1363, step time: 0.2563\n",
      "147/281, train_loss: 0.3472, step time: 0.2520\n",
      "148/281, train_loss: 0.1146, step time: 0.2476\n",
      "149/281, train_loss: 0.0802, step time: 0.2490\n",
      "150/281, train_loss: 0.1099, step time: 0.2496\n",
      "151/281, train_loss: 0.0666, step time: 0.2563\n",
      "152/281, train_loss: 0.1325, step time: 0.2546\n",
      "153/281, train_loss: 0.1143, step time: 0.2533\n",
      "154/281, train_loss: 0.1718, step time: 0.2504\n",
      "155/281, train_loss: 0.1535, step time: 0.2505\n",
      "156/281, train_loss: 0.0817, step time: 0.2521\n",
      "157/281, train_loss: 0.0987, step time: 0.2578\n",
      "158/281, train_loss: 0.0858, step time: 0.2507\n",
      "159/281, train_loss: 0.1537, step time: 0.2530\n",
      "160/281, train_loss: 0.1943, step time: 0.2524\n",
      "161/281, train_loss: 0.0742, step time: 0.2511\n",
      "162/281, train_loss: 0.3597, step time: 0.2488\n",
      "163/281, train_loss: 0.1652, step time: 0.2457\n",
      "164/281, train_loss: 0.2830, step time: 0.2466\n",
      "165/281, train_loss: 0.2077, step time: 0.2532\n",
      "166/281, train_loss: 0.1605, step time: 0.2451\n",
      "167/281, train_loss: 0.0676, step time: 0.2474\n",
      "168/281, train_loss: 0.3708, step time: 0.2462\n",
      "169/281, train_loss: 0.3080, step time: 0.2491\n",
      "170/281, train_loss: 0.1370, step time: 0.2503\n",
      "171/281, train_loss: 0.1503, step time: 0.2494\n",
      "172/281, train_loss: 0.0727, step time: 0.2541\n",
      "173/281, train_loss: 0.1587, step time: 0.2477\n",
      "174/281, train_loss: 0.2028, step time: 0.2497\n",
      "175/281, train_loss: 0.2075, step time: 0.2551\n",
      "176/281, train_loss: 0.1160, step time: 0.2466\n",
      "177/281, train_loss: 0.1157, step time: 0.2525\n",
      "178/281, train_loss: 0.1765, step time: 0.2481\n",
      "179/281, train_loss: 0.3389, step time: 0.2506\n",
      "180/281, train_loss: 0.1120, step time: 0.2556\n",
      "181/281, train_loss: 0.2017, step time: 0.2562\n",
      "182/281, train_loss: 0.1880, step time: 0.2525\n",
      "183/281, train_loss: 0.0920, step time: 0.2506\n",
      "184/281, train_loss: 0.1202, step time: 0.2530\n",
      "185/281, train_loss: 0.0879, step time: 0.2510\n",
      "186/281, train_loss: 0.2752, step time: 0.2505\n",
      "187/281, train_loss: 0.1661, step time: 0.2529\n",
      "188/281, train_loss: 0.0928, step time: 0.2543\n",
      "189/281, train_loss: 0.0822, step time: 0.2476\n",
      "190/281, train_loss: 0.1008, step time: 0.2474\n",
      "191/281, train_loss: 0.2206, step time: 0.2504\n",
      "192/281, train_loss: 0.1555, step time: 0.2505\n",
      "193/281, train_loss: 0.2589, step time: 0.2516\n",
      "194/281, train_loss: 0.1879, step time: 0.2544\n",
      "195/281, train_loss: 0.0953, step time: 0.2587\n",
      "196/281, train_loss: 0.3256, step time: 0.2496\n",
      "197/281, train_loss: 0.1458, step time: 0.2532\n",
      "198/281, train_loss: 0.1312, step time: 0.2563\n",
      "199/281, train_loss: 0.1231, step time: 0.2503\n",
      "200/281, train_loss: 0.1606, step time: 0.2496\n",
      "201/281, train_loss: 0.1945, step time: 0.2544\n",
      "202/281, train_loss: 0.1043, step time: 0.2463\n",
      "203/281, train_loss: 0.0833, step time: 0.2481\n",
      "204/281, train_loss: 0.0809, step time: 0.2507\n",
      "205/281, train_loss: 0.1602, step time: 0.2451\n",
      "206/281, train_loss: 0.0880, step time: 0.2444\n",
      "207/281, train_loss: 0.1515, step time: 0.2498\n",
      "208/281, train_loss: 0.1566, step time: 0.2558\n",
      "209/281, train_loss: 0.3236, step time: 0.2471\n",
      "210/281, train_loss: 0.1453, step time: 0.2495\n",
      "211/281, train_loss: 0.2592, step time: 0.2475\n",
      "212/281, train_loss: 0.2334, step time: 0.2439\n",
      "213/281, train_loss: 0.1125, step time: 0.2599\n",
      "214/281, train_loss: 0.0839, step time: 0.2504\n",
      "215/281, train_loss: 0.1235, step time: 0.2484\n",
      "216/281, train_loss: 0.1218, step time: 0.2498\n",
      "217/281, train_loss: 0.2563, step time: 0.2506\n",
      "218/281, train_loss: 0.2435, step time: 0.2487\n",
      "219/281, train_loss: 0.2523, step time: 0.2472\n",
      "220/281, train_loss: 0.1015, step time: 0.2485\n",
      "221/281, train_loss: 0.1683, step time: 0.2486\n",
      "222/281, train_loss: 0.1024, step time: 0.2478\n",
      "223/281, train_loss: 0.1618, step time: 0.2485\n",
      "224/281, train_loss: 0.1398, step time: 0.2487\n",
      "225/281, train_loss: 0.1081, step time: 0.2557\n",
      "226/281, train_loss: 0.1790, step time: 0.2512\n",
      "227/281, train_loss: 0.1664, step time: 0.2491\n",
      "228/281, train_loss: 0.1349, step time: 0.2454\n",
      "229/281, train_loss: 0.2075, step time: 0.2524\n",
      "230/281, train_loss: 0.2751, step time: 0.2527\n",
      "231/281, train_loss: 0.1181, step time: 0.2456\n",
      "232/281, train_loss: 0.3012, step time: 0.2426\n",
      "233/281, train_loss: 0.1208, step time: 0.2512\n",
      "234/281, train_loss: 0.1321, step time: 0.2481\n",
      "235/281, train_loss: 0.3904, step time: 0.2444\n",
      "236/281, train_loss: 0.2930, step time: 0.2448\n",
      "237/281, train_loss: 0.1755, step time: 0.2525\n",
      "238/281, train_loss: 0.2269, step time: 0.2678\n",
      "239/281, train_loss: 0.1373, step time: 0.2590\n",
      "240/281, train_loss: 0.1077, step time: 0.2501\n",
      "241/281, train_loss: 0.1545, step time: 0.2551\n",
      "242/281, train_loss: 0.1254, step time: 0.2517\n",
      "243/281, train_loss: 0.1198, step time: 0.2498\n",
      "244/281, train_loss: 0.1641, step time: 0.2502\n",
      "245/281, train_loss: 0.1429, step time: 0.2559\n",
      "246/281, train_loss: 0.0902, step time: 0.2436\n",
      "247/281, train_loss: 0.0985, step time: 0.2444\n",
      "248/281, train_loss: 0.0809, step time: 0.2475\n",
      "249/281, train_loss: 0.1013, step time: 0.2510\n",
      "250/281, train_loss: 0.1072, step time: 0.2530\n",
      "251/281, train_loss: 0.1274, step time: 0.2517\n",
      "252/281, train_loss: 0.1940, step time: 0.2519\n",
      "253/281, train_loss: 0.2178, step time: 0.2512\n",
      "254/281, train_loss: 0.1333, step time: 0.2481\n",
      "255/281, train_loss: 0.1448, step time: 0.2505\n",
      "256/281, train_loss: 0.0976, step time: 0.2466\n",
      "257/281, train_loss: 0.1205, step time: 0.2546\n",
      "258/281, train_loss: 0.1368, step time: 0.2536\n",
      "259/281, train_loss: 0.0887, step time: 0.2530\n",
      "260/281, train_loss: 0.1250, step time: 0.2541\n",
      "261/281, train_loss: 0.0841, step time: 0.2589\n",
      "262/281, train_loss: 0.2939, step time: 0.2531\n",
      "263/281, train_loss: 0.1215, step time: 0.2454\n",
      "264/281, train_loss: 0.1053, step time: 0.2415\n",
      "265/281, train_loss: 0.0835, step time: 0.2491\n",
      "266/281, train_loss: 0.1547, step time: 0.2486\n",
      "267/281, train_loss: 0.1122, step time: 0.2522\n",
      "268/281, train_loss: 0.3296, step time: 0.2513\n",
      "269/281, train_loss: 0.1049, step time: 0.2510\n",
      "270/281, train_loss: 0.3244, step time: 0.2479\n",
      "271/281, train_loss: 0.1808, step time: 0.2521\n",
      "272/281, train_loss: 0.1328, step time: 0.2493\n",
      "273/281, train_loss: 0.1832, step time: 0.2550\n",
      "274/281, train_loss: 0.2972, step time: 0.2511\n",
      "275/281, train_loss: 0.0822, step time: 0.2483\n",
      "276/281, train_loss: 0.1987, step time: 0.2464\n",
      "277/281, train_loss: 0.1012, step time: 0.2475\n",
      "278/281, train_loss: 0.1395, step time: 0.2458\n",
      "279/281, train_loss: 0.1329, step time: 0.2477\n",
      "280/281, train_loss: 0.1334, step time: 0.2515\n",
      "281/281, train_loss: 0.5358, step time: 0.2484\n",
      "282/281, train_loss: 0.4215, step time: 0.1490\n",
      "epoch 34 average loss: 0.1687\n",
      "saved new best metric model\n",
      "current epoch: 34 current mean dice: 0.8507 tc: 0.8317 wt: 0.9059 et: 0.8231\n",
      "best mean dice: 0.8507 at epoch: 34\n",
      "time consuming of epoch 34 is: 425.6969\n",
      "----------\n",
      "epoch 35/200\n",
      "1/281, train_loss: 0.0957, step time: 0.2753\n",
      "2/281, train_loss: 0.0957, step time: 0.2565\n",
      "3/281, train_loss: 0.1231, step time: 0.2644\n",
      "4/281, train_loss: 0.2635, step time: 0.2614\n",
      "5/281, train_loss: 0.0901, step time: 0.2519\n",
      "6/281, train_loss: 0.1145, step time: 0.2603\n",
      "7/281, train_loss: 0.0725, step time: 0.2595\n",
      "8/281, train_loss: 0.0704, step time: 0.2580\n",
      "9/281, train_loss: 0.3404, step time: 0.2566\n",
      "10/281, train_loss: 0.2000, step time: 0.2654\n",
      "11/281, train_loss: 0.2145, step time: 0.2612\n",
      "12/281, train_loss: 0.1061, step time: 0.2630\n",
      "13/281, train_loss: 0.1285, step time: 0.2602\n",
      "14/281, train_loss: 0.1880, step time: 0.2490\n",
      "15/281, train_loss: 0.0973, step time: 0.2510\n",
      "16/281, train_loss: 0.1202, step time: 0.2530\n",
      "17/281, train_loss: 0.1527, step time: 0.2499\n",
      "18/281, train_loss: 0.1062, step time: 0.2515\n",
      "19/281, train_loss: 0.1142, step time: 0.2589\n",
      "20/281, train_loss: 0.1150, step time: 0.2575\n",
      "21/281, train_loss: 0.1096, step time: 0.2562\n",
      "22/281, train_loss: 0.4532, step time: 0.2500\n",
      "23/281, train_loss: 0.3028, step time: 0.2489\n",
      "24/281, train_loss: 0.1083, step time: 0.2512\n",
      "25/281, train_loss: 0.0785, step time: 0.2549\n",
      "26/281, train_loss: 0.0890, step time: 0.2495\n",
      "27/281, train_loss: 0.1823, step time: 0.2536\n",
      "28/281, train_loss: 0.1808, step time: 0.2482\n",
      "29/281, train_loss: 0.1515, step time: 0.2570\n",
      "30/281, train_loss: 0.0858, step time: 0.2519\n",
      "31/281, train_loss: 0.1005, step time: 0.2477\n",
      "32/281, train_loss: 0.0872, step time: 0.2567\n",
      "33/281, train_loss: 0.2582, step time: 0.2562\n",
      "34/281, train_loss: 0.0843, step time: 0.2566\n",
      "35/281, train_loss: 0.1316, step time: 0.2600\n",
      "36/281, train_loss: 0.2722, step time: 0.2558\n",
      "37/281, train_loss: 0.1372, step time: 0.2554\n",
      "38/281, train_loss: 0.1291, step time: 0.2578\n",
      "39/281, train_loss: 0.0886, step time: 0.2642\n",
      "40/281, train_loss: 0.1344, step time: 0.2551\n",
      "41/281, train_loss: 0.1081, step time: 0.2608\n",
      "42/281, train_loss: 0.3109, step time: 0.2523\n",
      "43/281, train_loss: 0.1890, step time: 0.2453\n",
      "44/281, train_loss: 0.0900, step time: 0.2487\n",
      "45/281, train_loss: 0.1466, step time: 0.2729\n",
      "46/281, train_loss: 0.1077, step time: 0.2574\n",
      "47/281, train_loss: 0.1054, step time: 0.2498\n",
      "48/281, train_loss: 0.1411, step time: 0.2504\n",
      "49/281, train_loss: 0.1131, step time: 0.2540\n",
      "50/281, train_loss: 0.1154, step time: 0.2544\n",
      "51/281, train_loss: 0.0781, step time: 0.2595\n",
      "52/281, train_loss: 0.1496, step time: 0.2780\n",
      "53/281, train_loss: 0.1336, step time: 0.2546\n",
      "54/281, train_loss: 0.1097, step time: 0.2539\n",
      "55/281, train_loss: 0.1114, step time: 0.2583\n",
      "56/281, train_loss: 0.1593, step time: 0.2600\n",
      "57/281, train_loss: 0.1159, step time: 0.2555\n",
      "58/281, train_loss: 0.1937, step time: 0.2504\n",
      "59/281, train_loss: 0.1607, step time: 0.2516\n",
      "60/281, train_loss: 0.0979, step time: 0.2506\n",
      "61/281, train_loss: 0.0735, step time: 0.2546\n",
      "62/281, train_loss: 0.1670, step time: 0.2707\n",
      "63/281, train_loss: 0.1374, step time: 0.2604\n",
      "64/281, train_loss: 0.2229, step time: 0.2610\n",
      "65/281, train_loss: 0.0938, step time: 0.2512\n",
      "66/281, train_loss: 0.0900, step time: 0.2536\n",
      "67/281, train_loss: 0.1234, step time: 0.2496\n",
      "68/281, train_loss: 0.2215, step time: 0.2523\n",
      "69/281, train_loss: 0.2906, step time: 0.2484\n",
      "70/281, train_loss: 0.0920, step time: 0.2506\n",
      "71/281, train_loss: 0.1611, step time: 0.2502\n",
      "72/281, train_loss: 0.1841, step time: 0.2550\n",
      "73/281, train_loss: 0.1073, step time: 0.2534\n",
      "74/281, train_loss: 0.2564, step time: 0.2599\n",
      "75/281, train_loss: 0.4495, step time: 0.2544\n",
      "76/281, train_loss: 0.1362, step time: 0.2549\n",
      "77/281, train_loss: 0.1644, step time: 0.2575\n",
      "78/281, train_loss: 0.1682, step time: 0.2582\n",
      "79/281, train_loss: 0.1483, step time: 0.2583\n",
      "80/281, train_loss: 0.4330, step time: 0.2603\n",
      "81/281, train_loss: 0.1020, step time: 0.2631\n",
      "82/281, train_loss: 0.3137, step time: 0.2667\n",
      "83/281, train_loss: 0.0875, step time: 0.2581\n",
      "84/281, train_loss: 0.1093, step time: 0.2580\n",
      "85/281, train_loss: 0.1356, step time: 0.2556\n",
      "86/281, train_loss: 0.1032, step time: 0.2583\n",
      "87/281, train_loss: 0.2456, step time: 0.2591\n",
      "88/281, train_loss: 0.1200, step time: 0.2580\n",
      "89/281, train_loss: 0.1440, step time: 0.2583\n",
      "90/281, train_loss: 0.1634, step time: 0.2574\n",
      "91/281, train_loss: 0.1623, step time: 0.2566\n",
      "92/281, train_loss: 0.1350, step time: 0.2562\n",
      "93/281, train_loss: 0.1712, step time: 0.2539\n",
      "94/281, train_loss: 0.2993, step time: 0.2585\n",
      "95/281, train_loss: 0.0910, step time: 0.2591\n",
      "96/281, train_loss: 0.1143, step time: 0.2612\n",
      "97/281, train_loss: 0.1260, step time: 0.2652\n",
      "98/281, train_loss: 0.1510, step time: 0.2597\n",
      "99/281, train_loss: 0.1934, step time: 0.2575\n",
      "100/281, train_loss: 0.2237, step time: 0.2585\n",
      "101/281, train_loss: 0.1478, step time: 0.2600\n",
      "102/281, train_loss: 0.0902, step time: 0.2599\n",
      "103/281, train_loss: 0.1744, step time: 0.2589\n",
      "104/281, train_loss: 0.1199, step time: 0.2614\n",
      "105/281, train_loss: 0.3031, step time: 0.2613\n",
      "106/281, train_loss: 0.1091, step time: 0.2584\n",
      "107/281, train_loss: 0.1147, step time: 0.2582\n",
      "108/281, train_loss: 0.1480, step time: 0.2575\n",
      "109/281, train_loss: 0.1491, step time: 0.2664\n",
      "110/281, train_loss: 0.3495, step time: 0.2613\n",
      "111/281, train_loss: 0.2973, step time: 0.2605\n",
      "112/281, train_loss: 0.1526, step time: 0.2627\n",
      "113/281, train_loss: 0.1455, step time: 0.2542\n",
      "114/281, train_loss: 0.1057, step time: 0.2558\n",
      "115/281, train_loss: 0.0905, step time: 0.2585\n",
      "116/281, train_loss: 0.1195, step time: 0.2629\n",
      "117/281, train_loss: 0.1014, step time: 0.2613\n",
      "118/281, train_loss: 0.2854, step time: 0.2590\n",
      "119/281, train_loss: 0.1147, step time: 0.2596\n",
      "120/281, train_loss: 0.1851, step time: 0.2563\n",
      "121/281, train_loss: 0.1065, step time: 0.2595\n",
      "122/281, train_loss: 0.1102, step time: 0.2584\n",
      "123/281, train_loss: 0.1069, step time: 0.2595\n",
      "124/281, train_loss: 0.1677, step time: 0.2589\n",
      "125/281, train_loss: 0.1234, step time: 0.2598\n",
      "126/281, train_loss: 0.1566, step time: 0.2599\n",
      "127/281, train_loss: 0.0982, step time: 0.2567\n",
      "128/281, train_loss: 0.2069, step time: 0.2596\n",
      "129/281, train_loss: 0.0743, step time: 0.2619\n",
      "130/281, train_loss: 0.1322, step time: 0.2555\n",
      "131/281, train_loss: 0.1611, step time: 0.2689\n",
      "132/281, train_loss: 0.1377, step time: 0.2548\n",
      "133/281, train_loss: 0.2006, step time: 0.2584\n",
      "134/281, train_loss: 0.0837, step time: 0.2562\n",
      "135/281, train_loss: 0.2623, step time: 0.2511\n",
      "136/281, train_loss: 0.0725, step time: 0.2724\n",
      "137/281, train_loss: 0.1054, step time: 0.2578\n",
      "138/281, train_loss: 0.0946, step time: 0.2559\n",
      "139/281, train_loss: 0.2111, step time: 0.2548\n",
      "140/281, train_loss: 0.1636, step time: 0.2670\n",
      "141/281, train_loss: 0.1512, step time: 0.2575\n",
      "142/281, train_loss: 0.2666, step time: 0.2533\n",
      "143/281, train_loss: 0.3358, step time: 0.2580\n",
      "144/281, train_loss: 0.0870, step time: 0.2581\n",
      "145/281, train_loss: 0.2506, step time: 0.2535\n",
      "146/281, train_loss: 0.1790, step time: 0.2573\n",
      "147/281, train_loss: 0.2646, step time: 0.2571\n",
      "148/281, train_loss: 0.0976, step time: 0.2538\n",
      "149/281, train_loss: 0.1186, step time: 0.2574\n",
      "150/281, train_loss: 0.0818, step time: 0.2566\n",
      "151/281, train_loss: 0.1049, step time: 0.2520\n",
      "152/281, train_loss: 0.1292, step time: 0.2563\n",
      "153/281, train_loss: 0.1168, step time: 0.2557\n",
      "154/281, train_loss: 0.2627, step time: 0.2549\n",
      "155/281, train_loss: 0.1018, step time: 0.2594\n",
      "156/281, train_loss: 0.1093, step time: 0.2607\n",
      "157/281, train_loss: 0.0952, step time: 0.2534\n",
      "158/281, train_loss: 0.1177, step time: 0.2556\n",
      "159/281, train_loss: 0.1726, step time: 0.2535\n",
      "160/281, train_loss: 0.2604, step time: 0.2592\n",
      "161/281, train_loss: 0.2565, step time: 0.2581\n",
      "162/281, train_loss: 0.1095, step time: 0.2601\n",
      "163/281, train_loss: 0.1488, step time: 0.2581\n",
      "164/281, train_loss: 0.1073, step time: 0.2589\n",
      "165/281, train_loss: 0.1828, step time: 0.2604\n",
      "166/281, train_loss: 0.1490, step time: 0.2578\n",
      "167/281, train_loss: 0.2792, step time: 0.2595\n",
      "168/281, train_loss: 0.3063, step time: 0.2553\n",
      "169/281, train_loss: 0.1284, step time: 0.2584\n",
      "170/281, train_loss: 0.1436, step time: 0.2585\n",
      "171/281, train_loss: 0.1305, step time: 0.2577\n",
      "172/281, train_loss: 0.1082, step time: 0.2588\n",
      "173/281, train_loss: 0.3012, step time: 0.2586\n",
      "174/281, train_loss: 0.1032, step time: 0.2584\n",
      "175/281, train_loss: 0.1243, step time: 0.2639\n",
      "176/281, train_loss: 0.0910, step time: 0.2569\n",
      "177/281, train_loss: 0.2276, step time: 0.2552\n",
      "178/281, train_loss: 0.1334, step time: 0.2565\n",
      "179/281, train_loss: 0.3338, step time: 0.2579\n",
      "180/281, train_loss: 0.1290, step time: 0.2525\n",
      "181/281, train_loss: 0.1046, step time: 0.2518\n",
      "182/281, train_loss: 0.0993, step time: 0.2560\n",
      "183/281, train_loss: 0.2090, step time: 0.2500\n",
      "184/281, train_loss: 0.1416, step time: 0.2549\n",
      "185/281, train_loss: 0.1003, step time: 0.2526\n",
      "186/281, train_loss: 0.0902, step time: 0.2517\n",
      "187/281, train_loss: 0.2198, step time: 0.2512\n",
      "188/281, train_loss: 0.2221, step time: 0.2543\n",
      "189/281, train_loss: 0.1301, step time: 0.2527\n",
      "190/281, train_loss: 0.0965, step time: 0.2493\n",
      "191/281, train_loss: 0.0946, step time: 0.2471\n",
      "192/281, train_loss: 0.1011, step time: 0.2502\n",
      "193/281, train_loss: 0.1377, step time: 0.2520\n",
      "194/281, train_loss: 0.0893, step time: 0.2515\n",
      "195/281, train_loss: 0.0854, step time: 0.2532\n",
      "196/281, train_loss: 0.1066, step time: 0.2575\n",
      "197/281, train_loss: 0.0807, step time: 0.2552\n",
      "198/281, train_loss: 0.3414, step time: 0.2529\n",
      "199/281, train_loss: 0.1669, step time: 0.2501\n",
      "200/281, train_loss: 0.0976, step time: 0.2570\n",
      "201/281, train_loss: 0.3094, step time: 0.2540\n",
      "202/281, train_loss: 0.2556, step time: 0.2542\n",
      "203/281, train_loss: 0.1259, step time: 0.2544\n",
      "204/281, train_loss: 0.1221, step time: 0.2621\n",
      "205/281, train_loss: 0.1066, step time: 0.2561\n",
      "206/281, train_loss: 0.1363, step time: 0.2559\n",
      "207/281, train_loss: 0.1550, step time: 0.2530\n",
      "208/281, train_loss: 0.1852, step time: 0.2586\n",
      "209/281, train_loss: 0.1122, step time: 0.2582\n",
      "210/281, train_loss: 0.0966, step time: 0.2596\n",
      "211/281, train_loss: 0.1224, step time: 0.2572\n",
      "212/281, train_loss: 0.1266, step time: 0.2599\n",
      "213/281, train_loss: 0.2154, step time: 0.2577\n",
      "214/281, train_loss: 0.0741, step time: 0.2508\n",
      "215/281, train_loss: 0.1855, step time: 0.2572\n",
      "216/281, train_loss: 0.2564, step time: 0.2535\n",
      "217/281, train_loss: 0.1200, step time: 0.2553\n",
      "218/281, train_loss: 0.1233, step time: 0.2540\n",
      "219/281, train_loss: 0.2377, step time: 0.2548\n",
      "220/281, train_loss: 0.2525, step time: 0.2561\n",
      "221/281, train_loss: 0.1103, step time: 0.2536\n",
      "222/281, train_loss: 0.1924, step time: 0.2547\n",
      "223/281, train_loss: 0.3829, step time: 0.2515\n",
      "224/281, train_loss: 0.1166, step time: 0.2518\n",
      "225/281, train_loss: 0.1139, step time: 0.2548\n",
      "226/281, train_loss: 0.3532, step time: 0.2561\n",
      "227/281, train_loss: 0.1055, step time: 0.2598\n",
      "228/281, train_loss: 0.2910, step time: 0.2837\n",
      "229/281, train_loss: 0.1176, step time: 0.2574\n",
      "230/281, train_loss: 0.2754, step time: 0.2562\n",
      "231/281, train_loss: 0.1375, step time: 0.2604\n",
      "232/281, train_loss: 0.1372, step time: 0.2561\n",
      "233/281, train_loss: 0.1091, step time: 0.2527\n",
      "234/281, train_loss: 0.1643, step time: 0.2525\n",
      "235/281, train_loss: 0.2984, step time: 0.2564\n",
      "236/281, train_loss: 0.0770, step time: 0.2565\n",
      "237/281, train_loss: 0.2854, step time: 0.2553\n",
      "238/281, train_loss: 0.1119, step time: 0.2547\n",
      "239/281, train_loss: 0.1435, step time: 0.2543\n",
      "240/281, train_loss: 0.1010, step time: 0.2569\n",
      "241/281, train_loss: 0.1544, step time: 0.2514\n",
      "242/281, train_loss: 0.1792, step time: 0.2510\n",
      "243/281, train_loss: 0.2703, step time: 0.2514\n",
      "244/281, train_loss: 0.0876, step time: 0.2544\n",
      "245/281, train_loss: 0.2622, step time: 0.2764\n",
      "246/281, train_loss: 0.2028, step time: 0.2647\n",
      "247/281, train_loss: 0.0881, step time: 0.2553\n",
      "248/281, train_loss: 0.1124, step time: 0.2550\n",
      "249/281, train_loss: 0.1767, step time: 0.2547\n",
      "250/281, train_loss: 0.1507, step time: 0.2537\n",
      "251/281, train_loss: 0.1279, step time: 0.2508\n",
      "252/281, train_loss: 0.1484, step time: 0.2511\n",
      "253/281, train_loss: 0.1186, step time: 0.2862\n",
      "254/281, train_loss: 0.2728, step time: 0.2687\n",
      "255/281, train_loss: 0.1200, step time: 0.2559\n",
      "256/281, train_loss: 0.2953, step time: 0.2506\n",
      "257/281, train_loss: 0.1398, step time: 0.2546\n",
      "258/281, train_loss: 0.1167, step time: 0.2500\n",
      "259/281, train_loss: 0.1079, step time: 0.2555\n",
      "260/281, train_loss: 0.1119, step time: 0.2513\n",
      "261/281, train_loss: 0.1146, step time: 0.2625\n",
      "262/281, train_loss: 0.4764, step time: 0.2536\n",
      "263/281, train_loss: 0.1129, step time: 0.2538\n",
      "264/281, train_loss: 0.1174, step time: 0.2490\n",
      "265/281, train_loss: 0.0923, step time: 0.2563\n",
      "266/281, train_loss: 0.3825, step time: 0.2551\n",
      "267/281, train_loss: 0.1035, step time: 0.2520\n",
      "268/281, train_loss: 0.0895, step time: 0.2479\n",
      "269/281, train_loss: 0.1382, step time: 0.2537\n",
      "270/281, train_loss: 0.1049, step time: 0.2507\n",
      "271/281, train_loss: 0.1349, step time: 0.2510\n",
      "272/281, train_loss: 0.1341, step time: 0.2496\n",
      "273/281, train_loss: 0.0923, step time: 0.2579\n",
      "274/281, train_loss: 0.1371, step time: 0.2530\n",
      "275/281, train_loss: 0.1019, step time: 0.2483\n",
      "276/281, train_loss: 0.1497, step time: 0.2506\n",
      "277/281, train_loss: 0.1919, step time: 0.2552\n",
      "278/281, train_loss: 0.0791, step time: 0.2493\n",
      "279/281, train_loss: 0.1423, step time: 0.2539\n",
      "280/281, train_loss: 0.2981, step time: 0.2505\n",
      "281/281, train_loss: 0.0736, step time: 0.2491\n",
      "282/281, train_loss: 0.4273, step time: 0.1519\n",
      "epoch 35 average loss: 0.1599\n",
      "saved new best metric model\n",
      "current epoch: 35 current mean dice: 0.8546 tc: 0.8399 wt: 0.9056 et: 0.8283\n",
      "best mean dice: 0.8546 at epoch: 35\n",
      "time consuming of epoch 35 is: 368.9939\n",
      "----------\n",
      "epoch 36/200\n",
      "1/281, train_loss: 0.0774, step time: 0.2596\n",
      "2/281, train_loss: 0.0937, step time: 0.2557\n",
      "3/281, train_loss: 0.0758, step time: 0.2570\n",
      "4/281, train_loss: 0.2707, step time: 0.2565\n",
      "5/281, train_loss: 0.0761, step time: 0.2545\n",
      "6/281, train_loss: 0.0737, step time: 0.2524\n",
      "7/281, train_loss: 0.0824, step time: 0.2516\n",
      "8/281, train_loss: 0.2671, step time: 0.2595\n",
      "9/281, train_loss: 0.1372, step time: 0.2562\n",
      "10/281, train_loss: 0.1619, step time: 0.2563\n",
      "11/281, train_loss: 0.1482, step time: 0.2592\n",
      "12/281, train_loss: 0.0579, step time: 0.2540\n",
      "13/281, train_loss: 0.1640, step time: 0.2547\n",
      "14/281, train_loss: 0.1362, step time: 0.2633\n",
      "15/281, train_loss: 0.1721, step time: 0.2624\n",
      "16/281, train_loss: 0.2823, step time: 0.2511\n",
      "17/281, train_loss: 0.2575, step time: 0.2546\n",
      "18/281, train_loss: 0.0875, step time: 0.2540\n",
      "19/281, train_loss: 0.2967, step time: 0.2608\n",
      "20/281, train_loss: 0.0995, step time: 0.2626\n",
      "21/281, train_loss: 0.1343, step time: 0.2531\n",
      "22/281, train_loss: 0.1073, step time: 0.2530\n",
      "23/281, train_loss: 0.0939, step time: 0.2537\n",
      "24/281, train_loss: 0.1396, step time: 0.2529\n",
      "25/281, train_loss: 0.1020, step time: 0.2586\n",
      "26/281, train_loss: 0.1693, step time: 0.2524\n",
      "27/281, train_loss: 0.0920, step time: 0.2503\n",
      "28/281, train_loss: 0.1056, step time: 0.2527\n",
      "29/281, train_loss: 0.1117, step time: 0.2538\n",
      "30/281, train_loss: 0.1092, step time: 0.2544\n",
      "31/281, train_loss: 0.1608, step time: 0.2521\n",
      "32/281, train_loss: 0.0640, step time: 0.2506\n",
      "33/281, train_loss: 0.1335, step time: 0.2546\n",
      "34/281, train_loss: 0.1259, step time: 0.2666\n",
      "35/281, train_loss: 0.3270, step time: 0.2576\n",
      "36/281, train_loss: 0.2470, step time: 0.2533\n",
      "37/281, train_loss: 0.1015, step time: 0.2554\n",
      "38/281, train_loss: 0.1458, step time: 0.2589\n",
      "39/281, train_loss: 0.2450, step time: 0.2502\n",
      "40/281, train_loss: 0.1098, step time: 0.2530\n",
      "41/281, train_loss: 0.0672, step time: 0.2546\n",
      "42/281, train_loss: 0.2665, step time: 0.2502\n",
      "43/281, train_loss: 0.0711, step time: 0.2526\n",
      "44/281, train_loss: 0.1196, step time: 0.2539\n",
      "45/281, train_loss: 0.2625, step time: 0.2546\n",
      "46/281, train_loss: 0.2324, step time: 0.2588\n",
      "47/281, train_loss: 0.1170, step time: 0.2652\n",
      "48/281, train_loss: 0.1674, step time: 0.2530\n",
      "49/281, train_loss: 0.1730, step time: 0.2520\n",
      "50/281, train_loss: 0.1542, step time: 0.2464\n",
      "51/281, train_loss: 0.0671, step time: 0.2496\n",
      "52/281, train_loss: 0.1725, step time: 0.2517\n",
      "53/281, train_loss: 0.1120, step time: 0.2520\n",
      "54/281, train_loss: 0.0877, step time: 0.2493\n",
      "55/281, train_loss: 0.1803, step time: 0.2498\n",
      "56/281, train_loss: 0.0620, step time: 0.2492\n",
      "57/281, train_loss: 0.1341, step time: 0.2525\n",
      "58/281, train_loss: 0.1005, step time: 0.2530\n",
      "59/281, train_loss: 0.1045, step time: 0.2521\n",
      "60/281, train_loss: 0.1812, step time: 0.2514\n",
      "61/281, train_loss: 0.1485, step time: 0.2539\n",
      "62/281, train_loss: 0.1267, step time: 0.2573\n",
      "63/281, train_loss: 0.1023, step time: 0.2517\n",
      "64/281, train_loss: 0.0699, step time: 0.2511\n",
      "65/281, train_loss: 0.1349, step time: 0.2540\n",
      "66/281, train_loss: 0.1380, step time: 0.2554\n",
      "67/281, train_loss: 0.1546, step time: 0.2527\n",
      "68/281, train_loss: 0.1282, step time: 0.2621\n",
      "69/281, train_loss: 0.2606, step time: 0.2575\n",
      "70/281, train_loss: 0.1160, step time: 0.2593\n",
      "71/281, train_loss: 0.1057, step time: 0.2575\n",
      "72/281, train_loss: 0.1099, step time: 0.2543\n",
      "73/281, train_loss: 0.1125, step time: 0.2496\n",
      "74/281, train_loss: 0.2096, step time: 0.2484\n",
      "75/281, train_loss: 0.1320, step time: 0.2551\n",
      "76/281, train_loss: 0.0671, step time: 0.2680\n",
      "77/281, train_loss: 0.2064, step time: 0.2558\n",
      "78/281, train_loss: 0.1209, step time: 0.2612\n",
      "79/281, train_loss: 0.3101, step time: 0.2532\n",
      "80/281, train_loss: 0.1273, step time: 0.2548\n",
      "81/281, train_loss: 0.0736, step time: 0.2514\n",
      "82/281, train_loss: 0.1617, step time: 0.2523\n",
      "83/281, train_loss: 0.1728, step time: 0.2529\n",
      "84/281, train_loss: 0.0987, step time: 0.2589\n",
      "85/281, train_loss: 0.1369, step time: 0.2554\n",
      "86/281, train_loss: 0.2888, step time: 0.2512\n",
      "87/281, train_loss: 0.1262, step time: 0.2558\n",
      "88/281, train_loss: 0.0912, step time: 0.2549\n",
      "89/281, train_loss: 0.1480, step time: 0.2575\n",
      "90/281, train_loss: 0.1495, step time: 0.2536\n",
      "91/281, train_loss: 0.2523, step time: 0.2452\n",
      "92/281, train_loss: 0.0970, step time: 0.2549\n",
      "93/281, train_loss: 0.1135, step time: 0.2524\n",
      "94/281, train_loss: 0.0888, step time: 0.2560\n",
      "95/281, train_loss: 0.1195, step time: 0.2592\n",
      "96/281, train_loss: 0.2223, step time: 0.2543\n",
      "97/281, train_loss: 0.1238, step time: 0.2473\n",
      "98/281, train_loss: 0.1263, step time: 0.2464\n",
      "99/281, train_loss: 0.1061, step time: 0.2507\n",
      "100/281, train_loss: 0.2037, step time: 0.2518\n",
      "101/281, train_loss: 0.0661, step time: 0.2533\n",
      "102/281, train_loss: 0.1808, step time: 0.2543\n",
      "103/281, train_loss: 0.1727, step time: 0.2485\n",
      "104/281, train_loss: 0.1398, step time: 0.2594\n",
      "105/281, train_loss: 0.2956, step time: 0.2553\n",
      "106/281, train_loss: 0.0872, step time: 0.2569\n",
      "107/281, train_loss: 0.3404, step time: 0.2547\n",
      "108/281, train_loss: 0.2720, step time: 0.2557\n",
      "109/281, train_loss: 0.0630, step time: 0.2523\n",
      "110/281, train_loss: 0.1626, step time: 0.2542\n",
      "111/281, train_loss: 0.1177, step time: 0.2523\n",
      "112/281, train_loss: 0.2395, step time: 0.2539\n",
      "113/281, train_loss: 0.2578, step time: 0.2542\n",
      "114/281, train_loss: 0.1772, step time: 0.2535\n",
      "115/281, train_loss: 0.1601, step time: 0.2504\n",
      "116/281, train_loss: 0.2689, step time: 0.2480\n",
      "117/281, train_loss: 0.0850, step time: 0.2468\n",
      "118/281, train_loss: 0.1423, step time: 0.2501\n",
      "119/281, train_loss: 0.1065, step time: 0.2466\n",
      "120/281, train_loss: 0.1453, step time: 0.2460\n",
      "121/281, train_loss: 0.1430, step time: 0.2458\n",
      "122/281, train_loss: 0.0836, step time: 0.2490\n",
      "123/281, train_loss: 0.1188, step time: 0.2512\n",
      "124/281, train_loss: 0.0961, step time: 0.2587\n",
      "125/281, train_loss: 0.1539, step time: 0.2556\n",
      "126/281, train_loss: 0.1670, step time: 0.2559\n",
      "127/281, train_loss: 0.1376, step time: 0.2475\n",
      "128/281, train_loss: 0.1108, step time: 0.2508\n",
      "129/281, train_loss: 0.0756, step time: 0.2542\n",
      "130/281, train_loss: 0.1304, step time: 0.2520\n",
      "131/281, train_loss: 0.1417, step time: 0.2536\n",
      "132/281, train_loss: 0.2778, step time: 0.2543\n",
      "133/281, train_loss: 0.0813, step time: 0.2538\n",
      "134/281, train_loss: 0.1003, step time: 0.2592\n",
      "135/281, train_loss: 0.1436, step time: 0.2577\n",
      "136/281, train_loss: 0.1343, step time: 0.2548\n",
      "137/281, train_loss: 0.1125, step time: 0.2520\n",
      "138/281, train_loss: 0.1707, step time: 0.2494\n",
      "139/281, train_loss: 0.1975, step time: 0.2528\n",
      "140/281, train_loss: 0.3159, step time: 0.2553\n",
      "141/281, train_loss: 0.1760, step time: 0.2584\n",
      "142/281, train_loss: 0.0836, step time: 0.2542\n",
      "143/281, train_loss: 0.1724, step time: 0.2484\n",
      "144/281, train_loss: 0.1300, step time: 0.2529\n",
      "145/281, train_loss: 0.1469, step time: 0.2536\n",
      "146/281, train_loss: 0.1280, step time: 0.2566\n",
      "147/281, train_loss: 0.1000, step time: 0.2512\n",
      "148/281, train_loss: 0.1218, step time: 0.2563\n",
      "149/281, train_loss: 0.1773, step time: 0.2608\n",
      "150/281, train_loss: 0.1096, step time: 0.2514\n",
      "151/281, train_loss: 0.2919, step time: 0.2569\n",
      "152/281, train_loss: 0.1184, step time: 0.2572\n",
      "153/281, train_loss: 0.1365, step time: 0.2546\n",
      "154/281, train_loss: 0.2722, step time: 0.2577\n",
      "155/281, train_loss: 0.1281, step time: 0.2552\n",
      "156/281, train_loss: 0.0873, step time: 0.2508\n",
      "157/281, train_loss: 0.1453, step time: 0.2577\n",
      "158/281, train_loss: 0.2642, step time: 0.2558\n",
      "159/281, train_loss: 0.1660, step time: 0.2523\n",
      "160/281, train_loss: 0.1253, step time: 0.2556\n",
      "161/281, train_loss: 0.1163, step time: 0.2516\n",
      "162/281, train_loss: 0.1432, step time: 0.2557\n",
      "163/281, train_loss: 0.1154, step time: 0.2553\n",
      "164/281, train_loss: 0.1129, step time: 0.2580\n",
      "165/281, train_loss: 0.2701, step time: 0.2586\n",
      "166/281, train_loss: 0.2441, step time: 0.2537\n",
      "167/281, train_loss: 0.1016, step time: 0.2551\n",
      "168/281, train_loss: 0.1596, step time: 0.2537\n",
      "169/281, train_loss: 0.1401, step time: 0.2519\n",
      "170/281, train_loss: 0.1739, step time: 0.2513\n",
      "171/281, train_loss: 0.1303, step time: 0.2591\n",
      "172/281, train_loss: 0.2363, step time: 0.2627\n",
      "173/281, train_loss: 0.0928, step time: 0.2522\n",
      "174/281, train_loss: 0.2823, step time: 0.2540\n",
      "175/281, train_loss: 0.1406, step time: 0.2536\n",
      "176/281, train_loss: 0.1344, step time: 0.2588\n",
      "177/281, train_loss: 0.0895, step time: 0.2532\n",
      "178/281, train_loss: 0.2790, step time: 0.2538\n",
      "179/281, train_loss: 0.0917, step time: 0.2547\n",
      "180/281, train_loss: 0.0529, step time: 0.2521\n",
      "181/281, train_loss: 0.1060, step time: 0.2532\n",
      "182/281, train_loss: 0.1703, step time: 0.2491\n",
      "183/281, train_loss: 0.2952, step time: 0.2471\n",
      "184/281, train_loss: 0.1017, step time: 0.2514\n",
      "185/281, train_loss: 0.1051, step time: 0.2480\n",
      "186/281, train_loss: 0.0929, step time: 0.2481\n",
      "187/281, train_loss: 0.1034, step time: 0.2464\n",
      "188/281, train_loss: 0.0969, step time: 0.2593\n",
      "189/281, train_loss: 0.0755, step time: 0.2560\n",
      "190/281, train_loss: 0.3046, step time: 0.2522\n",
      "191/281, train_loss: 0.1255, step time: 0.2499\n",
      "192/281, train_loss: 0.3403, step time: 0.2580\n",
      "193/281, train_loss: 0.0636, step time: 0.2535\n",
      "194/281, train_loss: 0.0899, step time: 0.2545\n",
      "195/281, train_loss: 0.2481, step time: 0.2502\n",
      "196/281, train_loss: 0.2719, step time: 0.2519\n",
      "197/281, train_loss: 0.1310, step time: 0.2530\n",
      "198/281, train_loss: 0.1431, step time: 0.2507\n",
      "199/281, train_loss: 0.1253, step time: 0.2516\n",
      "200/281, train_loss: 0.3282, step time: 0.2552\n",
      "201/281, train_loss: 0.1251, step time: 0.2529\n",
      "202/281, train_loss: 0.0621, step time: 0.2492\n",
      "203/281, train_loss: 0.1339, step time: 0.2509\n",
      "204/281, train_loss: 0.1474, step time: 0.2505\n",
      "205/281, train_loss: 0.1145, step time: 0.2443\n",
      "206/281, train_loss: 0.1561, step time: 0.2453\n",
      "207/281, train_loss: 0.1642, step time: 0.2450\n",
      "208/281, train_loss: 0.1077, step time: 0.2480\n",
      "209/281, train_loss: 0.1356, step time: 0.2441\n",
      "210/281, train_loss: 0.1617, step time: 0.2439\n",
      "211/281, train_loss: 0.0694, step time: 0.2410\n",
      "212/281, train_loss: 0.2889, step time: 0.2454\n",
      "213/281, train_loss: 0.2886, step time: 0.2482\n",
      "214/281, train_loss: 0.1052, step time: 0.2500\n",
      "215/281, train_loss: 0.1677, step time: 0.2483\n",
      "216/281, train_loss: 0.2477, step time: 0.2500\n",
      "217/281, train_loss: 0.0945, step time: 0.2489\n",
      "218/281, train_loss: 0.1159, step time: 0.2513\n",
      "219/281, train_loss: 0.1202, step time: 0.2565\n",
      "220/281, train_loss: 0.1271, step time: 0.2525\n",
      "221/281, train_loss: 0.1060, step time: 0.2502\n",
      "222/281, train_loss: 0.1886, step time: 0.2522\n",
      "223/281, train_loss: 0.1009, step time: 0.2513\n",
      "224/281, train_loss: 0.2299, step time: 0.2576\n",
      "225/281, train_loss: 0.1046, step time: 0.2549\n",
      "226/281, train_loss: 0.1200, step time: 0.2518\n",
      "227/281, train_loss: 0.2565, step time: 0.2518\n",
      "228/281, train_loss: 0.2978, step time: 0.2508\n",
      "229/281, train_loss: 0.4601, step time: 0.2463\n",
      "230/281, train_loss: 0.1058, step time: 0.2469\n",
      "231/281, train_loss: 0.0758, step time: 0.2472\n",
      "232/281, train_loss: 0.1562, step time: 0.2580\n",
      "233/281, train_loss: 0.1399, step time: 0.2542\n",
      "234/281, train_loss: 0.2694, step time: 0.2470\n",
      "235/281, train_loss: 0.1253, step time: 0.2445\n",
      "236/281, train_loss: 0.1149, step time: 0.2465\n",
      "237/281, train_loss: 0.1199, step time: 0.2474\n",
      "238/281, train_loss: 0.2444, step time: 0.2484\n",
      "239/281, train_loss: 0.1024, step time: 0.2470\n",
      "240/281, train_loss: 0.1039, step time: 0.2511\n",
      "241/281, train_loss: 0.1408, step time: 0.2500\n",
      "242/281, train_loss: 0.1204, step time: 0.2513\n",
      "243/281, train_loss: 0.1780, step time: 0.2493\n",
      "244/281, train_loss: 0.0821, step time: 0.2473\n",
      "245/281, train_loss: 0.1194, step time: 0.2506\n",
      "246/281, train_loss: 0.1177, step time: 0.2528\n",
      "247/281, train_loss: 0.1002, step time: 0.2459\n",
      "248/281, train_loss: 0.0920, step time: 0.2516\n",
      "249/281, train_loss: 0.2933, step time: 0.2468\n",
      "250/281, train_loss: 0.0873, step time: 0.2467\n",
      "251/281, train_loss: 0.1935, step time: 0.2476\n",
      "252/281, train_loss: 0.1054, step time: 0.2574\n",
      "253/281, train_loss: 0.2354, step time: 0.2451\n",
      "254/281, train_loss: 0.0892, step time: 0.2457\n",
      "255/281, train_loss: 0.0753, step time: 0.2442\n",
      "256/281, train_loss: 0.0955, step time: 0.2480\n",
      "257/281, train_loss: 0.1182, step time: 0.2448\n",
      "258/281, train_loss: 0.1256, step time: 0.2522\n",
      "259/281, train_loss: 0.0589, step time: 0.2462\n",
      "260/281, train_loss: 0.1087, step time: 0.2444\n",
      "261/281, train_loss: 0.2886, step time: 0.2522\n",
      "262/281, train_loss: 0.1244, step time: 0.2597\n",
      "263/281, train_loss: 0.3373, step time: 0.2465\n",
      "264/281, train_loss: 0.3248, step time: 0.2462\n",
      "265/281, train_loss: 0.1031, step time: 0.2495\n",
      "266/281, train_loss: 0.1212, step time: 0.2531\n",
      "267/281, train_loss: 0.1436, step time: 0.2509\n",
      "268/281, train_loss: 0.1549, step time: 0.2517\n",
      "269/281, train_loss: 0.1990, step time: 0.2481\n",
      "270/281, train_loss: 0.2586, step time: 0.2411\n",
      "271/281, train_loss: 0.2471, step time: 0.2419\n",
      "272/281, train_loss: 0.2274, step time: 0.2471\n",
      "273/281, train_loss: 0.2389, step time: 0.2441\n",
      "274/281, train_loss: 0.1433, step time: 0.2523\n",
      "275/281, train_loss: 0.1414, step time: 0.2495\n",
      "276/281, train_loss: 0.0998, step time: 0.2519\n",
      "277/281, train_loss: 0.2863, step time: 0.2520\n",
      "278/281, train_loss: 0.1174, step time: 0.2451\n",
      "279/281, train_loss: 0.0998, step time: 0.2489\n",
      "280/281, train_loss: 0.3244, step time: 0.2419\n",
      "281/281, train_loss: 0.0891, step time: 0.2423\n",
      "282/281, train_loss: 0.4537, step time: 0.1445\n",
      "epoch 36 average loss: 0.1552\n",
      "current epoch: 36 current mean dice: 0.8489 tc: 0.8387 wt: 0.9082 et: 0.8063\n",
      "best mean dice: 0.8546 at epoch: 35\n",
      "time consuming of epoch 36 is: 387.1522\n",
      "----------\n",
      "epoch 37/200\n",
      "1/281, train_loss: 0.1612, step time: 0.2605\n",
      "2/281, train_loss: 0.0907, step time: 0.2519\n",
      "3/281, train_loss: 0.0617, step time: 0.2510\n",
      "4/281, train_loss: 0.2284, step time: 0.2491\n",
      "5/281, train_loss: 0.1858, step time: 0.2519\n",
      "6/281, train_loss: 0.0803, step time: 0.2495\n",
      "7/281, train_loss: 0.1171, step time: 0.2482\n",
      "8/281, train_loss: 0.0863, step time: 0.2488\n",
      "9/281, train_loss: 0.1415, step time: 0.2551\n",
      "10/281, train_loss: 0.0668, step time: 0.2545\n",
      "11/281, train_loss: 0.1053, step time: 0.2570\n",
      "12/281, train_loss: 0.1986, step time: 0.2591\n",
      "13/281, train_loss: 0.1398, step time: 0.2537\n",
      "14/281, train_loss: 0.0717, step time: 0.2499\n",
      "15/281, train_loss: 0.1259, step time: 0.2488\n",
      "16/281, train_loss: 0.1184, step time: 0.2470\n",
      "17/281, train_loss: 0.3985, step time: 0.2466\n",
      "18/281, train_loss: 0.1396, step time: 0.2483\n",
      "19/281, train_loss: 0.4305, step time: 0.2504\n",
      "20/281, train_loss: 0.0918, step time: 0.2514\n",
      "21/281, train_loss: 0.3424, step time: 0.2554\n",
      "22/281, train_loss: 0.0964, step time: 0.2529\n",
      "23/281, train_loss: 0.2832, step time: 0.2525\n",
      "24/281, train_loss: 0.2065, step time: 0.2536\n",
      "25/281, train_loss: 0.0732, step time: 0.2541\n",
      "26/281, train_loss: 0.1820, step time: 0.2565\n",
      "27/281, train_loss: 0.1385, step time: 0.2533\n",
      "28/281, train_loss: 0.0816, step time: 0.2570\n",
      "29/281, train_loss: 0.1339, step time: 0.2562\n",
      "30/281, train_loss: 0.1738, step time: 0.2507\n",
      "31/281, train_loss: 0.3017, step time: 0.2440\n",
      "32/281, train_loss: 0.2552, step time: 0.2477\n",
      "33/281, train_loss: 0.1288, step time: 0.2544\n",
      "34/281, train_loss: 0.3166, step time: 0.2541\n",
      "35/281, train_loss: 0.2750, step time: 0.2533\n",
      "36/281, train_loss: 0.1108, step time: 0.2596\n",
      "37/281, train_loss: 0.1154, step time: 0.2477\n",
      "38/281, train_loss: 0.1164, step time: 0.2509\n",
      "39/281, train_loss: 0.1056, step time: 0.2510\n",
      "40/281, train_loss: 0.1451, step time: 0.2531\n",
      "41/281, train_loss: 0.1398, step time: 0.2534\n",
      "42/281, train_loss: 0.0836, step time: 0.2514\n",
      "43/281, train_loss: 0.1339, step time: 0.2573\n",
      "44/281, train_loss: 0.1078, step time: 0.2525\n",
      "45/281, train_loss: 0.1099, step time: 0.2508\n",
      "46/281, train_loss: 0.2686, step time: 0.2454\n",
      "47/281, train_loss: 0.1243, step time: 0.2519\n",
      "48/281, train_loss: 0.2209, step time: 0.2508\n",
      "49/281, train_loss: 0.1685, step time: 0.2554\n",
      "50/281, train_loss: 0.1402, step time: 0.2614\n",
      "51/281, train_loss: 0.2566, step time: 0.2591\n",
      "52/281, train_loss: 0.0887, step time: 0.2559\n",
      "53/281, train_loss: 0.1130, step time: 0.2536\n",
      "54/281, train_loss: 0.1869, step time: 0.2564\n",
      "55/281, train_loss: 0.1144, step time: 0.2571\n",
      "56/281, train_loss: 0.3127, step time: 0.2477\n",
      "57/281, train_loss: 0.1381, step time: 0.2540\n",
      "58/281, train_loss: 0.1126, step time: 0.2535\n",
      "59/281, train_loss: 0.0776, step time: 0.2506\n",
      "60/281, train_loss: 0.2024, step time: 0.2563\n",
      "61/281, train_loss: 0.3166, step time: 0.2474\n",
      "62/281, train_loss: 0.2736, step time: 0.2454\n",
      "63/281, train_loss: 0.1308, step time: 0.2456\n",
      "64/281, train_loss: 0.1218, step time: 0.2486\n",
      "65/281, train_loss: 0.2657, step time: 0.2550\n",
      "66/281, train_loss: 0.1460, step time: 0.2468\n",
      "67/281, train_loss: 0.1433, step time: 0.2482\n",
      "68/281, train_loss: 0.1250, step time: 0.2488\n",
      "69/281, train_loss: 0.2787, step time: 0.2494\n",
      "70/281, train_loss: 0.1138, step time: 0.2542\n",
      "71/281, train_loss: 0.0812, step time: 0.2586\n",
      "72/281, train_loss: 0.0898, step time: 0.2519\n",
      "73/281, train_loss: 0.0867, step time: 0.2593\n",
      "74/281, train_loss: 0.0856, step time: 0.2569\n",
      "75/281, train_loss: 0.1239, step time: 0.2576\n",
      "76/281, train_loss: 0.1068, step time: 0.2654\n",
      "77/281, train_loss: 0.1067, step time: 0.2550\n",
      "78/281, train_loss: 0.1077, step time: 0.2538\n",
      "79/281, train_loss: 0.0893, step time: 0.2547\n",
      "80/281, train_loss: 0.1063, step time: 0.2580\n",
      "81/281, train_loss: 0.1275, step time: 0.2528\n",
      "82/281, train_loss: 0.1031, step time: 0.2542\n",
      "83/281, train_loss: 0.0875, step time: 0.2562\n",
      "84/281, train_loss: 0.0858, step time: 0.2556\n",
      "85/281, train_loss: 0.1935, step time: 0.2553\n",
      "86/281, train_loss: 0.0639, step time: 0.2541\n",
      "87/281, train_loss: 0.1157, step time: 0.2468\n",
      "88/281, train_loss: 0.2078, step time: 0.2489\n",
      "89/281, train_loss: 0.1992, step time: 0.2550\n",
      "90/281, train_loss: 0.0711, step time: 0.2549\n",
      "91/281, train_loss: 0.1612, step time: 0.2564\n",
      "92/281, train_loss: 0.1843, step time: 0.2539\n",
      "93/281, train_loss: 0.1007, step time: 0.2509\n",
      "94/281, train_loss: 0.1494, step time: 0.2577\n",
      "95/281, train_loss: 0.0877, step time: 0.2593\n",
      "96/281, train_loss: 0.3259, step time: 0.2522\n",
      "97/281, train_loss: 0.0885, step time: 0.2532\n",
      "98/281, train_loss: 0.1036, step time: 0.2541\n",
      "99/281, train_loss: 0.1054, step time: 0.2533\n",
      "100/281, train_loss: 0.1157, step time: 0.2518\n",
      "101/281, train_loss: 0.1444, step time: 0.2525\n",
      "102/281, train_loss: 0.1089, step time: 0.2490\n",
      "103/281, train_loss: 0.1089, step time: 0.2518\n",
      "104/281, train_loss: 0.1696, step time: 0.2507\n",
      "105/281, train_loss: 0.1768, step time: 0.2524\n",
      "106/281, train_loss: 0.1246, step time: 0.2527\n",
      "107/281, train_loss: 0.1383, step time: 0.2585\n",
      "108/281, train_loss: 0.0797, step time: 0.2552\n",
      "109/281, train_loss: 0.1704, step time: 0.2553\n",
      "110/281, train_loss: 0.3379, step time: 0.2567\n",
      "111/281, train_loss: 0.1322, step time: 0.2562\n",
      "112/281, train_loss: 0.1173, step time: 0.2575\n",
      "113/281, train_loss: 0.3091, step time: 0.2602\n",
      "114/281, train_loss: 0.0904, step time: 0.2615\n",
      "115/281, train_loss: 0.3277, step time: 0.2574\n",
      "116/281, train_loss: 0.2619, step time: 0.2561\n",
      "117/281, train_loss: 0.2065, step time: 0.2516\n",
      "118/281, train_loss: 0.1203, step time: 0.2534\n",
      "119/281, train_loss: 0.0890, step time: 0.2536\n",
      "120/281, train_loss: 0.2671, step time: 0.2567\n",
      "121/281, train_loss: 0.0702, step time: 0.2579\n",
      "122/281, train_loss: 0.1249, step time: 0.2588\n",
      "123/281, train_loss: 0.1265, step time: 0.2575\n",
      "124/281, train_loss: 0.0982, step time: 0.2547\n",
      "125/281, train_loss: 0.2273, step time: 0.2494\n",
      "126/281, train_loss: 0.1251, step time: 0.2530\n",
      "127/281, train_loss: 0.1196, step time: 0.2534\n",
      "128/281, train_loss: 0.0645, step time: 0.2514\n",
      "129/281, train_loss: 0.1286, step time: 0.2464\n",
      "130/281, train_loss: 0.1608, step time: 0.2520\n",
      "131/281, train_loss: 0.0905, step time: 0.2534\n",
      "132/281, train_loss: 0.0745, step time: 0.2495\n",
      "133/281, train_loss: 0.2069, step time: 0.2493\n",
      "134/281, train_loss: 0.0819, step time: 0.2484\n",
      "135/281, train_loss: 0.0782, step time: 0.2567\n",
      "136/281, train_loss: 0.1283, step time: 0.2490\n",
      "137/281, train_loss: 0.1002, step time: 0.2588\n",
      "138/281, train_loss: 0.0850, step time: 0.2581\n",
      "139/281, train_loss: 0.1698, step time: 0.2547\n",
      "140/281, train_loss: 0.1106, step time: 0.2597\n",
      "141/281, train_loss: 0.1358, step time: 0.2581\n",
      "142/281, train_loss: 0.1022, step time: 0.2488\n",
      "143/281, train_loss: 0.2636, step time: 0.2484\n",
      "144/281, train_loss: 0.1742, step time: 0.2501\n",
      "145/281, train_loss: 0.2570, step time: 0.2507\n",
      "146/281, train_loss: 0.2672, step time: 0.2499\n",
      "147/281, train_loss: 0.1132, step time: 0.2503\n",
      "148/281, train_loss: 0.1423, step time: 0.2524\n",
      "149/281, train_loss: 0.2662, step time: 0.2589\n",
      "150/281, train_loss: 0.3572, step time: 0.2593\n",
      "151/281, train_loss: 0.1381, step time: 0.2593\n",
      "152/281, train_loss: 0.2659, step time: 0.2540\n",
      "153/281, train_loss: 0.1007, step time: 0.2570\n",
      "154/281, train_loss: 0.2825, step time: 0.2540\n",
      "155/281, train_loss: 0.2573, step time: 0.2499\n",
      "156/281, train_loss: 0.1772, step time: 0.2534\n",
      "157/281, train_loss: 0.1900, step time: 0.2586\n",
      "158/281, train_loss: 0.0959, step time: 0.2547\n",
      "159/281, train_loss: 0.1291, step time: 0.2552\n",
      "160/281, train_loss: 0.0980, step time: 0.2503\n",
      "161/281, train_loss: 0.2476, step time: 0.2499\n",
      "162/281, train_loss: 0.1926, step time: 0.2502\n",
      "163/281, train_loss: 0.1009, step time: 0.2541\n",
      "164/281, train_loss: 0.4203, step time: 0.2455\n",
      "165/281, train_loss: 0.1014, step time: 0.2484\n",
      "166/281, train_loss: 0.1074, step time: 0.2543\n",
      "167/281, train_loss: 0.2330, step time: 0.2550\n",
      "168/281, train_loss: 0.1681, step time: 0.2555\n",
      "169/281, train_loss: 0.1099, step time: 0.2553\n",
      "170/281, train_loss: 0.2479, step time: 0.2514\n",
      "171/281, train_loss: 0.0928, step time: 0.2529\n",
      "172/281, train_loss: 0.0913, step time: 0.2516\n",
      "173/281, train_loss: 0.1349, step time: 0.2531\n",
      "174/281, train_loss: 0.1385, step time: 0.2495\n",
      "175/281, train_loss: 0.0811, step time: 0.2501\n",
      "176/281, train_loss: 0.0659, step time: 0.2527\n",
      "177/281, train_loss: 0.1083, step time: 0.2582\n",
      "178/281, train_loss: 0.0883, step time: 0.2540\n",
      "179/281, train_loss: 0.1066, step time: 0.2557\n",
      "180/281, train_loss: 0.1717, step time: 0.2503\n",
      "181/281, train_loss: 0.2824, step time: 0.2539\n",
      "182/281, train_loss: 0.1237, step time: 0.2533\n",
      "183/281, train_loss: 0.0923, step time: 0.2511\n",
      "184/281, train_loss: 0.2887, step time: 0.2521\n",
      "185/281, train_loss: 0.0980, step time: 0.2492\n",
      "186/281, train_loss: 0.1752, step time: 0.2517\n",
      "187/281, train_loss: 0.1558, step time: 0.2546\n",
      "188/281, train_loss: 0.1104, step time: 0.2585\n",
      "189/281, train_loss: 0.1426, step time: 0.2556\n",
      "190/281, train_loss: 0.1087, step time: 0.2547\n",
      "191/281, train_loss: 0.1344, step time: 0.2535\n",
      "192/281, train_loss: 0.1157, step time: 0.2523\n",
      "193/281, train_loss: 0.1361, step time: 0.2482\n",
      "194/281, train_loss: 0.3751, step time: 0.2512\n",
      "195/281, train_loss: 0.0896, step time: 0.2526\n",
      "196/281, train_loss: 0.2410, step time: 0.2455\n",
      "197/281, train_loss: 0.1387, step time: 0.2486\n",
      "198/281, train_loss: 0.0907, step time: 0.2514\n",
      "199/281, train_loss: 0.2325, step time: 0.2554\n",
      "200/281, train_loss: 0.1126, step time: 0.2587\n",
      "201/281, train_loss: 0.1856, step time: 0.2563\n",
      "202/281, train_loss: 0.1162, step time: 0.2542\n",
      "203/281, train_loss: 0.0928, step time: 0.2530\n",
      "204/281, train_loss: 0.0910, step time: 0.2514\n",
      "205/281, train_loss: 0.0809, step time: 0.2513\n",
      "206/281, train_loss: 0.1086, step time: 0.2502\n",
      "207/281, train_loss: 0.1675, step time: 0.2637\n",
      "208/281, train_loss: 0.1216, step time: 0.2617\n",
      "209/281, train_loss: 0.0980, step time: 0.2606\n",
      "210/281, train_loss: 0.2998, step time: 0.2559\n",
      "211/281, train_loss: 0.4205, step time: 0.2582\n",
      "212/281, train_loss: 0.1104, step time: 0.2554\n",
      "213/281, train_loss: 0.0951, step time: 0.2579\n",
      "214/281, train_loss: 0.1008, step time: 0.2586\n",
      "215/281, train_loss: 0.0983, step time: 0.2554\n",
      "216/281, train_loss: 0.0851, step time: 0.2564\n",
      "217/281, train_loss: 0.1762, step time: 0.2535\n",
      "218/281, train_loss: 0.0661, step time: 0.2468\n",
      "219/281, train_loss: 0.3421, step time: 0.2560\n",
      "220/281, train_loss: 0.1868, step time: 0.2540\n",
      "221/281, train_loss: 0.1212, step time: 0.2522\n",
      "222/281, train_loss: 0.1498, step time: 0.2564\n",
      "223/281, train_loss: 0.0679, step time: 0.2563\n",
      "224/281, train_loss: 0.1134, step time: 0.2552\n",
      "225/281, train_loss: 0.0879, step time: 0.2533\n",
      "226/281, train_loss: 0.2428, step time: 0.2483\n",
      "227/281, train_loss: 0.1233, step time: 0.2570\n",
      "228/281, train_loss: 0.0855, step time: 0.2558\n",
      "229/281, train_loss: 0.1446, step time: 0.2584\n",
      "230/281, train_loss: 0.1311, step time: 0.2582\n",
      "231/281, train_loss: 0.0872, step time: 0.2601\n",
      "232/281, train_loss: 0.1045, step time: 0.2517\n",
      "233/281, train_loss: 0.1522, step time: 0.2511\n",
      "234/281, train_loss: 0.1823, step time: 0.2538\n",
      "235/281, train_loss: 0.1084, step time: 0.2521\n",
      "236/281, train_loss: 0.1188, step time: 0.2495\n",
      "237/281, train_loss: 0.1325, step time: 0.2550\n",
      "238/281, train_loss: 0.1862, step time: 0.2528\n",
      "239/281, train_loss: 0.1568, step time: 0.2576\n",
      "240/281, train_loss: 0.2071, step time: 0.2528\n",
      "241/281, train_loss: 0.2383, step time: 0.2533\n",
      "242/281, train_loss: 0.1074, step time: 0.2526\n",
      "243/281, train_loss: 0.1096, step time: 0.2548\n",
      "244/281, train_loss: 0.2918, step time: 0.2517\n",
      "245/281, train_loss: 0.2878, step time: 0.2517\n",
      "246/281, train_loss: 0.0936, step time: 0.2521\n",
      "247/281, train_loss: 0.0822, step time: 0.2524\n",
      "248/281, train_loss: 0.1575, step time: 0.2551\n",
      "249/281, train_loss: 0.0892, step time: 0.2568\n",
      "250/281, train_loss: 0.1580, step time: 0.2529\n",
      "251/281, train_loss: 0.1479, step time: 0.2529\n",
      "252/281, train_loss: 0.1611, step time: 0.2509\n",
      "253/281, train_loss: 0.0849, step time: 0.2513\n",
      "254/281, train_loss: 0.1423, step time: 0.2600\n",
      "255/281, train_loss: 0.1668, step time: 0.2654\n",
      "256/281, train_loss: 0.0880, step time: 0.2524\n",
      "257/281, train_loss: 0.1271, step time: 0.2529\n",
      "258/281, train_loss: 0.0761, step time: 0.2512\n",
      "259/281, train_loss: 0.1051, step time: 0.2512\n",
      "260/281, train_loss: 0.2688, step time: 0.2535\n",
      "261/281, train_loss: 0.1264, step time: 0.2527\n",
      "262/281, train_loss: 0.1168, step time: 0.2571\n",
      "263/281, train_loss: 0.1745, step time: 0.2522\n",
      "264/281, train_loss: 0.0891, step time: 0.2497\n",
      "265/281, train_loss: 0.2337, step time: 0.2539\n",
      "266/281, train_loss: 0.1755, step time: 0.2542\n",
      "267/281, train_loss: 0.0809, step time: 0.2471\n",
      "268/281, train_loss: 0.1221, step time: 0.2474\n",
      "269/281, train_loss: 0.1089, step time: 0.2529\n",
      "270/281, train_loss: 0.0959, step time: 0.2538\n",
      "271/281, train_loss: 0.1711, step time: 0.2450\n",
      "272/281, train_loss: 0.1151, step time: 0.2479\n",
      "273/281, train_loss: 0.1157, step time: 0.2465\n",
      "274/281, train_loss: 0.1698, step time: 0.2477\n",
      "275/281, train_loss: 0.1232, step time: 0.2522\n",
      "276/281, train_loss: 0.1322, step time: 0.2558\n",
      "277/281, train_loss: 0.1566, step time: 0.2552\n",
      "278/281, train_loss: 0.2964, step time: 0.2506\n",
      "279/281, train_loss: 0.1326, step time: 0.2538\n",
      "280/281, train_loss: 0.1204, step time: 0.2579\n",
      "281/281, train_loss: 0.1253, step time: 0.2556\n",
      "282/281, train_loss: 0.0827, step time: 0.1504\n",
      "epoch 37 average loss: 0.1525\n",
      "current epoch: 37 current mean dice: 0.8541 tc: 0.8337 wt: 0.9065 et: 0.8314\n",
      "best mean dice: 0.8546 at epoch: 35\n",
      "time consuming of epoch 37 is: 358.8091\n",
      "----------\n",
      "epoch 38/200\n",
      "1/281, train_loss: 0.1056, step time: 0.2669\n",
      "2/281, train_loss: 0.0960, step time: 0.2559\n",
      "3/281, train_loss: 0.1554, step time: 0.2540\n",
      "4/281, train_loss: 0.1287, step time: 0.2555\n",
      "5/281, train_loss: 0.0935, step time: 0.2646\n",
      "6/281, train_loss: 0.1089, step time: 0.2580\n",
      "7/281, train_loss: 0.1148, step time: 0.2564\n",
      "8/281, train_loss: 0.0945, step time: 0.2648\n",
      "9/281, train_loss: 0.0668, step time: 0.2571\n",
      "10/281, train_loss: 0.1355, step time: 0.2564\n",
      "11/281, train_loss: 0.1596, step time: 0.2568\n",
      "12/281, train_loss: 0.0728, step time: 0.2587\n",
      "13/281, train_loss: 0.1075, step time: 0.2502\n",
      "14/281, train_loss: 0.0936, step time: 0.2544\n",
      "15/281, train_loss: 0.1987, step time: 0.2570\n",
      "16/281, train_loss: 0.1495, step time: 0.2544\n",
      "17/281, train_loss: 0.1024, step time: 0.2530\n",
      "18/281, train_loss: 0.0871, step time: 0.2529\n",
      "19/281, train_loss: 0.1125, step time: 0.2545\n",
      "20/281, train_loss: 0.0668, step time: 0.2490\n",
      "21/281, train_loss: 0.1441, step time: 0.2547\n",
      "22/281, train_loss: 0.0806, step time: 0.2586\n",
      "23/281, train_loss: 0.1348, step time: 0.2549\n",
      "24/281, train_loss: 0.1376, step time: 0.2515\n",
      "25/281, train_loss: 0.1012, step time: 0.2537\n",
      "26/281, train_loss: 0.0977, step time: 0.2516\n",
      "27/281, train_loss: 0.0940, step time: 0.2604\n",
      "28/281, train_loss: 0.1009, step time: 0.2568\n",
      "29/281, train_loss: 0.1021, step time: 0.2595\n",
      "30/281, train_loss: 0.1306, step time: 0.2523\n",
      "31/281, train_loss: 0.0725, step time: 0.2589\n",
      "32/281, train_loss: 0.0994, step time: 0.2606\n",
      "33/281, train_loss: 0.0616, step time: 0.2637\n",
      "34/281, train_loss: 0.1142, step time: 0.2541\n",
      "35/281, train_loss: 0.0760, step time: 0.2596\n",
      "36/281, train_loss: 0.1472, step time: 0.2666\n",
      "37/281, train_loss: 0.1366, step time: 0.2551\n",
      "38/281, train_loss: 0.2124, step time: 0.2564\n",
      "39/281, train_loss: 0.0822, step time: 0.2572\n",
      "40/281, train_loss: 0.0728, step time: 0.2596\n",
      "41/281, train_loss: 0.0925, step time: 0.2601\n",
      "42/281, train_loss: 0.0613, step time: 0.2600\n",
      "43/281, train_loss: 0.4206, step time: 0.2842\n",
      "44/281, train_loss: 0.0876, step time: 0.2585\n",
      "45/281, train_loss: 0.0703, step time: 0.2546\n",
      "46/281, train_loss: 0.1225, step time: 0.2579\n",
      "47/281, train_loss: 0.1982, step time: 0.2597\n",
      "48/281, train_loss: 0.2429, step time: 0.2562\n",
      "49/281, train_loss: 0.1834, step time: 0.2583\n",
      "50/281, train_loss: 0.1322, step time: 0.2553\n",
      "51/281, train_loss: 0.1343, step time: 0.2623\n",
      "52/281, train_loss: 0.0949, step time: 0.2595\n",
      "53/281, train_loss: 0.2595, step time: 0.2516\n",
      "54/281, train_loss: 0.1156, step time: 0.2610\n",
      "55/281, train_loss: 0.2638, step time: 0.2568\n",
      "56/281, train_loss: 0.1115, step time: 0.2545\n",
      "57/281, train_loss: 0.1102, step time: 0.2530\n",
      "58/281, train_loss: 0.1094, step time: 0.2549\n",
      "59/281, train_loss: 0.2552, step time: 0.2574\n",
      "60/281, train_loss: 0.1520, step time: 0.2591\n",
      "61/281, train_loss: 0.1229, step time: 0.2576\n",
      "62/281, train_loss: 0.0936, step time: 0.2574\n",
      "63/281, train_loss: 0.1461, step time: 0.2576\n",
      "64/281, train_loss: 0.0874, step time: 0.2579\n",
      "65/281, train_loss: 0.1825, step time: 0.2570\n",
      "66/281, train_loss: 0.0910, step time: 0.2502\n",
      "67/281, train_loss: 0.0749, step time: 0.2519\n",
      "68/281, train_loss: 0.2316, step time: 0.2599\n",
      "69/281, train_loss: 0.1568, step time: 0.2574\n",
      "70/281, train_loss: 0.1769, step time: 0.2572\n",
      "71/281, train_loss: 0.0803, step time: 0.2594\n",
      "72/281, train_loss: 0.1010, step time: 0.2576\n",
      "73/281, train_loss: 0.1420, step time: 0.2545\n",
      "74/281, train_loss: 0.0909, step time: 0.2528\n",
      "75/281, train_loss: 0.0747, step time: 0.2561\n",
      "76/281, train_loss: 0.0589, step time: 0.2564\n",
      "77/281, train_loss: 0.2247, step time: 0.2491\n",
      "78/281, train_loss: 0.3837, step time: 0.2558\n",
      "79/281, train_loss: 0.2403, step time: 0.2578\n",
      "80/281, train_loss: 0.1385, step time: 0.2601\n",
      "81/281, train_loss: 0.1171, step time: 0.2609\n",
      "82/281, train_loss: 0.1533, step time: 0.2547\n",
      "83/281, train_loss: 0.1001, step time: 0.2578\n",
      "84/281, train_loss: 0.2600, step time: 0.2638\n",
      "85/281, train_loss: 0.1262, step time: 0.2609\n",
      "86/281, train_loss: 0.2575, step time: 0.2633\n",
      "87/281, train_loss: 0.2679, step time: 0.2566\n",
      "88/281, train_loss: 0.1104, step time: 0.2661\n",
      "89/281, train_loss: 0.1592, step time: 0.2669\n",
      "90/281, train_loss: 0.2730, step time: 0.2564\n",
      "91/281, train_loss: 0.1176, step time: 0.2558\n",
      "92/281, train_loss: 0.0853, step time: 0.2584\n",
      "93/281, train_loss: 0.0716, step time: 0.2585\n",
      "94/281, train_loss: 0.0982, step time: 0.2538\n",
      "95/281, train_loss: 0.1323, step time: 0.2675\n",
      "96/281, train_loss: 0.0750, step time: 0.2575\n",
      "97/281, train_loss: 0.2237, step time: 0.2569\n",
      "98/281, train_loss: 0.1230, step time: 0.2536\n",
      "99/281, train_loss: 0.1119, step time: 0.2636\n",
      "100/281, train_loss: 0.1050, step time: 0.2572\n",
      "101/281, train_loss: 0.1039, step time: 0.2586\n",
      "102/281, train_loss: 0.1433, step time: 0.2578\n",
      "103/281, train_loss: 0.1687, step time: 0.2512\n",
      "104/281, train_loss: 0.0888, step time: 0.2501\n",
      "105/281, train_loss: 0.0868, step time: 0.2478\n",
      "106/281, train_loss: 0.1083, step time: 0.2543\n",
      "107/281, train_loss: 0.3064, step time: 0.2540\n",
      "108/281, train_loss: 0.3002, step time: 0.2572\n",
      "109/281, train_loss: 0.1288, step time: 0.2583\n",
      "110/281, train_loss: 0.2433, step time: 0.2579\n",
      "111/281, train_loss: 0.2600, step time: 0.2592\n",
      "112/281, train_loss: 0.0746, step time: 0.2572\n",
      "113/281, train_loss: 0.1061, step time: 0.2535\n",
      "114/281, train_loss: 0.0811, step time: 0.2551\n",
      "115/281, train_loss: 0.1335, step time: 0.2584\n",
      "116/281, train_loss: 0.2518, step time: 0.2566\n",
      "117/281, train_loss: 0.1392, step time: 0.2566\n",
      "118/281, train_loss: 0.1150, step time: 0.2566\n",
      "119/281, train_loss: 0.0888, step time: 0.2600\n",
      "120/281, train_loss: 0.1136, step time: 0.2606\n",
      "121/281, train_loss: 0.1573, step time: 0.2623\n",
      "122/281, train_loss: 0.1146, step time: 0.2595\n",
      "123/281, train_loss: 0.2263, step time: 0.2511\n",
      "124/281, train_loss: 0.1733, step time: 0.2586\n",
      "125/281, train_loss: 0.1292, step time: 0.2558\n",
      "126/281, train_loss: 0.3466, step time: 0.2564\n",
      "127/281, train_loss: 0.1192, step time: 0.2526\n",
      "128/281, train_loss: 0.2620, step time: 0.2564\n",
      "129/281, train_loss: 0.1765, step time: 0.2554\n",
      "130/281, train_loss: 0.1832, step time: 0.2580\n",
      "131/281, train_loss: 0.1214, step time: 0.2544\n",
      "132/281, train_loss: 0.1316, step time: 0.2598\n",
      "133/281, train_loss: 0.2478, step time: 0.2569\n",
      "134/281, train_loss: 0.1526, step time: 0.2574\n",
      "135/281, train_loss: 0.1055, step time: 0.2564\n",
      "136/281, train_loss: 0.0930, step time: 0.2584\n",
      "137/281, train_loss: 0.1187, step time: 0.2572\n",
      "138/281, train_loss: 0.1407, step time: 0.2551\n",
      "139/281, train_loss: 0.1014, step time: 0.2526\n",
      "140/281, train_loss: 0.0693, step time: 0.2568\n",
      "141/281, train_loss: 0.0938, step time: 0.2562\n",
      "142/281, train_loss: 0.2350, step time: 0.2590\n",
      "143/281, train_loss: 0.0738, step time: 0.2548\n",
      "144/281, train_loss: 0.0826, step time: 0.2598\n",
      "145/281, train_loss: 0.2556, step time: 0.2588\n",
      "146/281, train_loss: 0.1137, step time: 0.2533\n",
      "147/281, train_loss: 0.1305, step time: 0.2536\n",
      "148/281, train_loss: 0.0854, step time: 0.2554\n",
      "149/281, train_loss: 0.0988, step time: 0.2566\n",
      "150/281, train_loss: 0.2702, step time: 0.2505\n",
      "151/281, train_loss: 0.0827, step time: 0.2639\n",
      "152/281, train_loss: 0.1257, step time: 0.2567\n",
      "153/281, train_loss: 0.0769, step time: 0.2581\n",
      "154/281, train_loss: 0.1151, step time: 0.2571\n",
      "155/281, train_loss: 0.1535, step time: 0.2682\n",
      "156/281, train_loss: 0.0676, step time: 0.2553\n",
      "157/281, train_loss: 0.1176, step time: 0.2532\n",
      "158/281, train_loss: 0.2823, step time: 0.2590\n",
      "159/281, train_loss: 0.1530, step time: 0.2512\n",
      "160/281, train_loss: 0.1004, step time: 0.2505\n",
      "161/281, train_loss: 0.0924, step time: 0.2512\n",
      "162/281, train_loss: 0.0695, step time: 0.2509\n",
      "163/281, train_loss: 0.1222, step time: 0.2548\n",
      "164/281, train_loss: 0.2683, step time: 0.2526\n",
      "165/281, train_loss: 0.2605, step time: 0.2542\n",
      "166/281, train_loss: 0.0805, step time: 0.2533\n",
      "167/281, train_loss: 0.0793, step time: 0.2531\n",
      "168/281, train_loss: 0.1068, step time: 0.2525\n",
      "169/281, train_loss: 0.3250, step time: 0.2503\n",
      "170/281, train_loss: 0.1526, step time: 0.2533\n",
      "171/281, train_loss: 0.0815, step time: 0.2516\n",
      "172/281, train_loss: 0.1755, step time: 0.2518\n",
      "173/281, train_loss: 0.2123, step time: 0.2563\n",
      "174/281, train_loss: 0.2931, step time: 0.2564\n",
      "175/281, train_loss: 0.3178, step time: 0.2497\n",
      "176/281, train_loss: 0.0852, step time: 0.2539\n",
      "177/281, train_loss: 0.1188, step time: 0.2506\n",
      "178/281, train_loss: 0.1048, step time: 0.2524\n",
      "179/281, train_loss: 0.1319, step time: 0.2562\n",
      "180/281, train_loss: 0.1430, step time: 0.2570\n",
      "181/281, train_loss: 0.0953, step time: 0.2453\n",
      "182/281, train_loss: 0.1685, step time: 0.2451\n",
      "183/281, train_loss: 0.2568, step time: 0.2578\n",
      "184/281, train_loss: 0.3313, step time: 0.2530\n",
      "185/281, train_loss: 0.0688, step time: 0.2728\n",
      "186/281, train_loss: 0.1345, step time: 0.2506\n",
      "187/281, train_loss: 0.0775, step time: 0.2614\n",
      "188/281, train_loss: 0.1699, step time: 0.2549\n",
      "189/281, train_loss: 0.3485, step time: 0.2568\n",
      "190/281, train_loss: 0.1291, step time: 0.2495\n",
      "191/281, train_loss: 0.2968, step time: 0.2551\n",
      "192/281, train_loss: 0.1163, step time: 0.2487\n",
      "193/281, train_loss: 0.1219, step time: 0.2523\n",
      "194/281, train_loss: 0.1298, step time: 0.2483\n",
      "195/281, train_loss: 0.1666, step time: 0.2637\n",
      "196/281, train_loss: 0.2667, step time: 0.2596\n",
      "197/281, train_loss: 0.2355, step time: 0.2595\n",
      "198/281, train_loss: 0.1308, step time: 0.2584\n",
      "199/281, train_loss: 0.0807, step time: 0.2548\n",
      "200/281, train_loss: 0.0925, step time: 0.2573\n",
      "201/281, train_loss: 0.0782, step time: 0.2573\n",
      "202/281, train_loss: 0.0862, step time: 0.2523\n",
      "203/281, train_loss: 0.1213, step time: 0.2531\n",
      "204/281, train_loss: 0.1499, step time: 0.2589\n",
      "205/281, train_loss: 0.1250, step time: 0.2547\n",
      "206/281, train_loss: 0.0739, step time: 0.2546\n",
      "207/281, train_loss: 0.1986, step time: 0.2565\n",
      "208/281, train_loss: 0.0991, step time: 0.2588\n",
      "209/281, train_loss: 0.0934, step time: 0.2523\n",
      "210/281, train_loss: 0.1506, step time: 0.2523\n",
      "211/281, train_loss: 0.1105, step time: 0.2542\n",
      "212/281, train_loss: 0.0742, step time: 0.2611\n",
      "213/281, train_loss: 0.0614, step time: 0.2601\n",
      "214/281, train_loss: 0.4199, step time: 0.2592\n",
      "215/281, train_loss: 0.1301, step time: 0.2562\n",
      "216/281, train_loss: 0.1733, step time: 0.2571\n",
      "217/281, train_loss: 0.1046, step time: 0.2557\n",
      "218/281, train_loss: 0.1130, step time: 0.2532\n",
      "219/281, train_loss: 0.1176, step time: 0.2556\n",
      "220/281, train_loss: 0.1526, step time: 0.2603\n",
      "221/281, train_loss: 0.0964, step time: 0.2568\n",
      "222/281, train_loss: 0.0970, step time: 0.2550\n",
      "223/281, train_loss: 0.1194, step time: 0.2525\n",
      "224/281, train_loss: 0.0750, step time: 0.2531\n",
      "225/281, train_loss: 0.0799, step time: 0.2502\n",
      "226/281, train_loss: 0.1092, step time: 0.2518\n",
      "227/281, train_loss: 0.3295, step time: 0.2509\n",
      "228/281, train_loss: 0.1308, step time: 0.2551\n",
      "229/281, train_loss: 0.1541, step time: 0.2512\n",
      "230/281, train_loss: 0.2359, step time: 0.2547\n",
      "231/281, train_loss: 0.1551, step time: 0.2562\n",
      "232/281, train_loss: 0.1901, step time: 0.2482\n",
      "233/281, train_loss: 0.0688, step time: 0.2481\n",
      "234/281, train_loss: 0.1183, step time: 0.2481\n",
      "235/281, train_loss: 0.1182, step time: 0.2481\n",
      "236/281, train_loss: 0.3168, step time: 0.2567\n",
      "237/281, train_loss: 0.0931, step time: 0.2508\n",
      "238/281, train_loss: 0.0768, step time: 0.2494\n",
      "239/281, train_loss: 0.2732, step time: 0.2549\n",
      "240/281, train_loss: 0.2730, step time: 0.2536\n",
      "241/281, train_loss: 0.1474, step time: 0.2523\n",
      "242/281, train_loss: 0.2745, step time: 0.2485\n",
      "243/281, train_loss: 0.1502, step time: 0.2516\n",
      "244/281, train_loss: 0.1374, step time: 0.2538\n",
      "245/281, train_loss: 0.0823, step time: 0.2502\n",
      "246/281, train_loss: 0.1224, step time: 0.2542\n",
      "247/281, train_loss: 0.1612, step time: 0.2562\n",
      "248/281, train_loss: 0.1659, step time: 0.2560\n",
      "249/281, train_loss: 0.1416, step time: 0.2683\n",
      "250/281, train_loss: 0.0659, step time: 0.2577\n",
      "251/281, train_loss: 0.0971, step time: 0.2524\n",
      "252/281, train_loss: 0.0939, step time: 0.2527\n",
      "253/281, train_loss: 0.1091, step time: 0.2556\n",
      "254/281, train_loss: 0.0857, step time: 0.2583\n",
      "255/281, train_loss: 0.0978, step time: 0.2573\n",
      "256/281, train_loss: 0.2502, step time: 0.2581\n",
      "257/281, train_loss: 0.1109, step time: 0.2574\n",
      "258/281, train_loss: 0.1211, step time: 0.2574\n",
      "259/281, train_loss: 0.1472, step time: 0.2527\n",
      "260/281, train_loss: 0.1634, step time: 0.2615\n",
      "261/281, train_loss: 0.0949, step time: 0.2582\n",
      "262/281, train_loss: 0.0900, step time: 0.2533\n",
      "263/281, train_loss: 0.1940, step time: 0.2515\n",
      "264/281, train_loss: 0.1100, step time: 0.2481\n",
      "265/281, train_loss: 0.1544, step time: 0.2511\n",
      "266/281, train_loss: 0.1260, step time: 0.2503\n",
      "267/281, train_loss: 0.0791, step time: 0.2475\n",
      "268/281, train_loss: 0.1051, step time: 0.2496\n",
      "269/281, train_loss: 0.2047, step time: 0.2591\n",
      "270/281, train_loss: 0.1336, step time: 0.2564\n",
      "271/281, train_loss: 0.2093, step time: 0.2561\n",
      "272/281, train_loss: 0.0919, step time: 0.2546\n",
      "273/281, train_loss: 0.1170, step time: 0.2522\n",
      "274/281, train_loss: 0.3491, step time: 0.2500\n",
      "275/281, train_loss: 0.1110, step time: 0.2473\n",
      "276/281, train_loss: 0.3124, step time: 0.2496\n",
      "277/281, train_loss: 0.2814, step time: 0.2521\n",
      "278/281, train_loss: 0.1703, step time: 0.2541\n",
      "279/281, train_loss: 0.1392, step time: 0.2503\n",
      "280/281, train_loss: 0.0903, step time: 0.2487\n",
      "281/281, train_loss: 0.1219, step time: 0.2425\n",
      "282/281, train_loss: 0.0584, step time: 0.1472\n",
      "epoch 38 average loss: 0.1451\n",
      "saved new best metric model\n",
      "current epoch: 38 current mean dice: 0.8573 tc: 0.8466 wt: 0.9052 et: 0.8309\n",
      "best mean dice: 0.8573 at epoch: 38\n",
      "time consuming of epoch 38 is: 392.7053\n",
      "----------\n",
      "epoch 39/200\n",
      "1/281, train_loss: 0.2162, step time: 0.2527\n",
      "2/281, train_loss: 0.2616, step time: 0.2658\n",
      "3/281, train_loss: 0.3069, step time: 0.2548\n",
      "4/281, train_loss: 0.1651, step time: 0.2488\n",
      "5/281, train_loss: 0.1309, step time: 0.2610\n",
      "6/281, train_loss: 0.1359, step time: 0.2536\n",
      "7/281, train_loss: 0.2530, step time: 0.2519\n",
      "8/281, train_loss: 0.1041, step time: 0.2517\n",
      "9/281, train_loss: 0.1114, step time: 0.2589\n",
      "10/281, train_loss: 0.0734, step time: 0.2496\n",
      "11/281, train_loss: 0.2959, step time: 0.2538\n",
      "12/281, train_loss: 0.1035, step time: 0.2554\n",
      "13/281, train_loss: 0.1490, step time: 0.2515\n",
      "14/281, train_loss: 0.2888, step time: 0.2558\n",
      "15/281, train_loss: 0.1517, step time: 0.2522\n",
      "16/281, train_loss: 0.1350, step time: 0.2484\n",
      "17/281, train_loss: 0.0847, step time: 0.2514\n",
      "18/281, train_loss: 0.0787, step time: 0.2506\n",
      "19/281, train_loss: 0.1205, step time: 0.2490\n",
      "20/281, train_loss: 0.2149, step time: 0.2547\n",
      "21/281, train_loss: 0.0952, step time: 0.2519\n",
      "22/281, train_loss: 0.1736, step time: 0.2464\n",
      "23/281, train_loss: 0.3236, step time: 0.2479\n",
      "24/281, train_loss: 0.1394, step time: 0.2519\n",
      "25/281, train_loss: 0.0879, step time: 0.2615\n",
      "26/281, train_loss: 0.0971, step time: 0.2495\n",
      "27/281, train_loss: 0.1013, step time: 0.2514\n",
      "28/281, train_loss: 0.2548, step time: 0.2492\n",
      "29/281, train_loss: 0.1065, step time: 0.2579\n",
      "30/281, train_loss: 0.1225, step time: 0.2587\n",
      "31/281, train_loss: 0.1310, step time: 0.2529\n",
      "32/281, train_loss: 0.0782, step time: 0.2488\n",
      "33/281, train_loss: 0.1016, step time: 0.2525\n",
      "34/281, train_loss: 0.1271, step time: 0.2499\n",
      "35/281, train_loss: 0.1318, step time: 0.2551\n",
      "36/281, train_loss: 0.0989, step time: 0.2535\n",
      "37/281, train_loss: 0.2076, step time: 0.2529\n",
      "38/281, train_loss: 0.3958, step time: 0.2571\n",
      "39/281, train_loss: 0.1046, step time: 0.2522\n",
      "40/281, train_loss: 0.0822, step time: 0.2510\n",
      "41/281, train_loss: 0.0769, step time: 0.2536\n",
      "42/281, train_loss: 0.1087, step time: 0.2510\n",
      "43/281, train_loss: 0.1343, step time: 0.2542\n",
      "44/281, train_loss: 0.2400, step time: 0.2584\n",
      "45/281, train_loss: 0.0669, step time: 0.2550\n",
      "46/281, train_loss: 0.1008, step time: 0.2544\n",
      "47/281, train_loss: 0.0764, step time: 0.2493\n",
      "48/281, train_loss: 0.0928, step time: 0.2480\n",
      "49/281, train_loss: 0.2756, step time: 0.2578\n",
      "50/281, train_loss: 0.2435, step time: 0.2570\n",
      "51/281, train_loss: 0.4166, step time: 0.2482\n",
      "52/281, train_loss: 0.0785, step time: 0.2513\n",
      "53/281, train_loss: 0.1150, step time: 0.2590\n",
      "54/281, train_loss: 0.2667, step time: 0.2523\n",
      "55/281, train_loss: 0.1271, step time: 0.2501\n",
      "56/281, train_loss: 0.1264, step time: 0.2464\n",
      "57/281, train_loss: 0.1480, step time: 0.2557\n",
      "58/281, train_loss: 0.1058, step time: 0.2538\n",
      "59/281, train_loss: 0.1175, step time: 0.2483\n",
      "60/281, train_loss: 0.1308, step time: 0.2627\n",
      "61/281, train_loss: 0.2556, step time: 0.2530\n",
      "62/281, train_loss: 0.0910, step time: 0.2484\n",
      "63/281, train_loss: 0.1667, step time: 0.2553\n",
      "64/281, train_loss: 0.1013, step time: 0.2555\n",
      "65/281, train_loss: 0.1749, step time: 0.2539\n",
      "66/281, train_loss: 0.1310, step time: 0.2524\n",
      "67/281, train_loss: 0.0892, step time: 0.2549\n",
      "68/281, train_loss: 0.0701, step time: 0.2546\n",
      "69/281, train_loss: 0.1210, step time: 0.2567\n",
      "70/281, train_loss: 0.0899, step time: 0.2559\n",
      "71/281, train_loss: 0.3079, step time: 0.2566\n",
      "72/281, train_loss: 0.2768, step time: 0.2506\n",
      "73/281, train_loss: 0.2371, step time: 0.2465\n",
      "74/281, train_loss: 0.1489, step time: 0.2506\n",
      "75/281, train_loss: 0.0992, step time: 0.2437\n",
      "76/281, train_loss: 0.1376, step time: 0.2474\n",
      "77/281, train_loss: 0.0669, step time: 0.2510\n",
      "78/281, train_loss: 0.1042, step time: 0.2551\n",
      "79/281, train_loss: 0.0968, step time: 0.2519\n",
      "80/281, train_loss: 0.0779, step time: 0.2540\n",
      "81/281, train_loss: 0.1091, step time: 0.2577\n",
      "82/281, train_loss: 0.0797, step time: 0.2579\n",
      "83/281, train_loss: 0.0463, step time: 0.2542\n",
      "84/281, train_loss: 0.1514, step time: 0.2495\n",
      "85/281, train_loss: 0.1048, step time: 0.2478\n",
      "86/281, train_loss: 0.1342, step time: 0.2519\n",
      "87/281, train_loss: 0.1681, step time: 0.2556\n",
      "88/281, train_loss: 0.1328, step time: 0.2521\n",
      "89/281, train_loss: 0.0921, step time: 0.2538\n",
      "90/281, train_loss: 0.1363, step time: 0.2441\n",
      "91/281, train_loss: 0.1442, step time: 0.2464\n",
      "92/281, train_loss: 0.1080, step time: 0.2531\n",
      "93/281, train_loss: 0.1950, step time: 0.2487\n",
      "94/281, train_loss: 0.1411, step time: 0.2500\n",
      "95/281, train_loss: 0.3685, step time: 0.2546\n",
      "96/281, train_loss: 0.1559, step time: 0.2529\n",
      "97/281, train_loss: 0.1303, step time: 0.2481\n",
      "98/281, train_loss: 0.0772, step time: 0.2525\n",
      "99/281, train_loss: 0.0748, step time: 0.2557\n",
      "100/281, train_loss: 0.0767, step time: 0.2521\n",
      "101/281, train_loss: 0.0948, step time: 0.2530\n",
      "102/281, train_loss: 0.2772, step time: 0.2503\n",
      "103/281, train_loss: 0.3021, step time: 0.2470\n",
      "104/281, train_loss: 0.3018, step time: 0.2460\n",
      "105/281, train_loss: 0.1066, step time: 0.2494\n",
      "106/281, train_loss: 0.1182, step time: 0.2569\n",
      "107/281, train_loss: 0.0897, step time: 0.2519\n",
      "108/281, train_loss: 0.2247, step time: 0.2530\n",
      "109/281, train_loss: 0.1502, step time: 0.2530\n",
      "110/281, train_loss: 0.0877, step time: 0.2567\n",
      "111/281, train_loss: 0.3703, step time: 0.2493\n",
      "112/281, train_loss: 0.1035, step time: 0.2545\n",
      "113/281, train_loss: 0.2731, step time: 0.2555\n",
      "114/281, train_loss: 0.1271, step time: 0.2447\n",
      "115/281, train_loss: 0.1347, step time: 0.2520\n",
      "116/281, train_loss: 0.1750, step time: 0.2567\n",
      "117/281, train_loss: 0.1508, step time: 0.2536\n",
      "118/281, train_loss: 0.0684, step time: 0.2467\n",
      "119/281, train_loss: 0.1051, step time: 0.2524\n",
      "120/281, train_loss: 0.1832, step time: 0.2531\n",
      "121/281, train_loss: 0.1000, step time: 0.2485\n",
      "122/281, train_loss: 0.0974, step time: 0.2472\n",
      "123/281, train_loss: 0.1288, step time: 0.2488\n",
      "124/281, train_loss: 0.0919, step time: 0.2426\n",
      "125/281, train_loss: 0.0879, step time: 0.2489\n",
      "126/281, train_loss: 0.0857, step time: 0.2475\n",
      "127/281, train_loss: 0.0587, step time: 0.2606\n",
      "128/281, train_loss: 0.0811, step time: 0.2631\n",
      "129/281, train_loss: 0.1225, step time: 0.2530\n",
      "130/281, train_loss: 0.1094, step time: 0.2476\n",
      "131/281, train_loss: 0.1123, step time: 0.2553\n",
      "132/281, train_loss: 0.1935, step time: 0.2548\n",
      "133/281, train_loss: 0.1116, step time: 0.2559\n",
      "134/281, train_loss: 0.1041, step time: 0.2427\n",
      "135/281, train_loss: 0.1125, step time: 0.2562\n",
      "136/281, train_loss: 0.0925, step time: 0.2506\n",
      "137/281, train_loss: 0.1205, step time: 0.2483\n",
      "138/281, train_loss: 0.0931, step time: 0.2511\n",
      "139/281, train_loss: 0.0990, step time: 0.2510\n",
      "140/281, train_loss: 0.1163, step time: 0.2499\n",
      "141/281, train_loss: 0.0765, step time: 0.2527\n",
      "142/281, train_loss: 0.1198, step time: 0.2525\n",
      "143/281, train_loss: 0.0746, step time: 0.2500\n",
      "144/281, train_loss: 0.0826, step time: 0.2497\n",
      "145/281, train_loss: 0.1161, step time: 0.2474\n",
      "146/281, train_loss: 0.2735, step time: 0.2494\n",
      "147/281, train_loss: 0.1295, step time: 0.2463\n",
      "148/281, train_loss: 0.1663, step time: 0.2475\n",
      "149/281, train_loss: 0.3186, step time: 0.2498\n",
      "150/281, train_loss: 0.0707, step time: 0.2503\n",
      "151/281, train_loss: 0.1061, step time: 0.2466\n",
      "152/281, train_loss: 0.2723, step time: 0.2465\n",
      "153/281, train_loss: 0.1048, step time: 0.2473\n",
      "154/281, train_loss: 0.0803, step time: 0.2516\n",
      "155/281, train_loss: 0.1798, step time: 0.2500\n",
      "156/281, train_loss: 0.0854, step time: 0.2450\n",
      "157/281, train_loss: 0.0925, step time: 0.2431\n",
      "158/281, train_loss: 0.1881, step time: 0.2438\n",
      "159/281, train_loss: 0.1018, step time: 0.2817\n",
      "160/281, train_loss: 0.0809, step time: 0.2474\n",
      "161/281, train_loss: 0.3469, step time: 0.2455\n",
      "162/281, train_loss: 0.1342, step time: 0.2484\n",
      "163/281, train_loss: 0.1577, step time: 0.2501\n",
      "164/281, train_loss: 0.1468, step time: 0.2485\n",
      "165/281, train_loss: 0.0836, step time: 0.2465\n",
      "166/281, train_loss: 0.1120, step time: 0.2414\n",
      "167/281, train_loss: 0.1556, step time: 0.2498\n",
      "168/281, train_loss: 0.0787, step time: 0.2430\n",
      "169/281, train_loss: 0.1089, step time: 0.2439\n",
      "170/281, train_loss: 0.2567, step time: 0.2481\n",
      "171/281, train_loss: 0.1120, step time: 0.2579\n",
      "172/281, train_loss: 0.1048, step time: 0.2462\n",
      "173/281, train_loss: 0.0864, step time: 0.2477\n",
      "174/281, train_loss: 0.0873, step time: 0.2506\n",
      "175/281, train_loss: 0.1055, step time: 0.2529\n",
      "176/281, train_loss: 0.0912, step time: 0.2550\n",
      "177/281, train_loss: 0.0855, step time: 0.2513\n",
      "178/281, train_loss: 0.1902, step time: 0.2523\n",
      "179/281, train_loss: 0.1670, step time: 0.2421\n",
      "180/281, train_loss: 0.2787, step time: 0.2440\n",
      "181/281, train_loss: 0.2587, step time: 0.2453\n",
      "182/281, train_loss: 0.1200, step time: 0.2496\n",
      "183/281, train_loss: 0.0863, step time: 0.2478\n",
      "184/281, train_loss: 0.1474, step time: 0.2486\n",
      "185/281, train_loss: 0.1605, step time: 0.2460\n",
      "186/281, train_loss: 0.1386, step time: 0.2473\n",
      "187/281, train_loss: 0.1316, step time: 0.2465\n",
      "188/281, train_loss: 0.2595, step time: 0.2458\n",
      "189/281, train_loss: 0.0892, step time: 0.2526\n",
      "190/281, train_loss: 0.0975, step time: 0.2480\n",
      "191/281, train_loss: 0.1001, step time: 0.2506\n",
      "192/281, train_loss: 0.1870, step time: 0.2501\n",
      "193/281, train_loss: 0.1419, step time: 0.2468\n",
      "194/281, train_loss: 0.1389, step time: 0.2441\n",
      "195/281, train_loss: 0.1555, step time: 0.2570\n",
      "196/281, train_loss: 0.1565, step time: 0.2538\n",
      "197/281, train_loss: 0.0969, step time: 0.2506\n",
      "198/281, train_loss: 0.3352, step time: 0.2530\n",
      "199/281, train_loss: 0.1285, step time: 0.2488\n",
      "200/281, train_loss: 0.1655, step time: 0.2507\n",
      "201/281, train_loss: 0.1582, step time: 0.2460\n",
      "202/281, train_loss: 0.1159, step time: 0.2474\n",
      "203/281, train_loss: 0.1743, step time: 0.2479\n",
      "204/281, train_loss: 0.0864, step time: 0.2547\n",
      "205/281, train_loss: 0.2633, step time: 0.2517\n",
      "206/281, train_loss: 0.2731, step time: 0.2555\n",
      "207/281, train_loss: 0.0890, step time: 0.2504\n",
      "208/281, train_loss: 0.1182, step time: 0.2462\n",
      "209/281, train_loss: 0.0705, step time: 0.2509\n",
      "210/281, train_loss: 0.0780, step time: 0.2465\n",
      "211/281, train_loss: 0.0600, step time: 0.2505\n",
      "212/281, train_loss: 0.1353, step time: 0.2537\n",
      "213/281, train_loss: 0.0865, step time: 0.2533\n",
      "214/281, train_loss: 0.1187, step time: 0.2516\n",
      "215/281, train_loss: 0.1097, step time: 0.2409\n",
      "216/281, train_loss: 0.0761, step time: 0.2507\n",
      "217/281, train_loss: 0.1343, step time: 0.2497\n",
      "218/281, train_loss: 0.0745, step time: 0.2411\n",
      "219/281, train_loss: 0.0885, step time: 0.2448\n",
      "220/281, train_loss: 0.2522, step time: 0.2502\n",
      "221/281, train_loss: 0.1296, step time: 0.2515\n",
      "222/281, train_loss: 0.1211, step time: 0.2525\n",
      "223/281, train_loss: 0.2700, step time: 0.2542\n",
      "224/281, train_loss: 0.0826, step time: 0.2558\n",
      "225/281, train_loss: 0.0969, step time: 0.2562\n",
      "226/281, train_loss: 0.1118, step time: 0.2525\n",
      "227/281, train_loss: 0.0839, step time: 0.2526\n",
      "228/281, train_loss: 0.1563, step time: 0.2517\n",
      "229/281, train_loss: 0.2362, step time: 0.2522\n",
      "230/281, train_loss: 0.1018, step time: 0.2535\n",
      "231/281, train_loss: 0.1182, step time: 0.2556\n",
      "232/281, train_loss: 0.1166, step time: 0.2507\n",
      "233/281, train_loss: 0.1990, step time: 0.2551\n",
      "234/281, train_loss: 0.2640, step time: 0.2547\n",
      "235/281, train_loss: 0.1114, step time: 0.2541\n",
      "236/281, train_loss: 0.0739, step time: 0.2507\n",
      "237/281, train_loss: 0.1046, step time: 0.2496\n",
      "238/281, train_loss: 0.0900, step time: 0.2447\n",
      "239/281, train_loss: 0.1086, step time: 0.2451\n",
      "240/281, train_loss: 0.0805, step time: 0.2479\n",
      "241/281, train_loss: 0.1379, step time: 0.2500\n",
      "242/281, train_loss: 0.1509, step time: 0.2468\n",
      "243/281, train_loss: 0.1329, step time: 0.2573\n",
      "244/281, train_loss: 0.1327, step time: 0.2649\n",
      "245/281, train_loss: 0.1401, step time: 0.2531\n",
      "246/281, train_loss: 0.1070, step time: 0.2570\n",
      "247/281, train_loss: 0.1616, step time: 0.2506\n",
      "248/281, train_loss: 0.1027, step time: 0.2494\n",
      "249/281, train_loss: 0.1144, step time: 0.2516\n",
      "250/281, train_loss: 0.3535, step time: 0.2523\n",
      "251/281, train_loss: 0.1123, step time: 0.2518\n",
      "252/281, train_loss: 0.0928, step time: 0.2511\n",
      "253/281, train_loss: 0.1057, step time: 0.2517\n",
      "254/281, train_loss: 0.0977, step time: 0.2482\n",
      "255/281, train_loss: 0.0849, step time: 0.2481\n",
      "256/281, train_loss: 0.1438, step time: 0.2516\n",
      "257/281, train_loss: 0.3481, step time: 0.2501\n",
      "258/281, train_loss: 0.4261, step time: 0.2536\n",
      "259/281, train_loss: 0.1381, step time: 0.2565\n",
      "260/281, train_loss: 0.0744, step time: 0.2503\n",
      "261/281, train_loss: 0.1229, step time: 0.2517\n",
      "262/281, train_loss: 0.1037, step time: 0.2505\n",
      "263/281, train_loss: 0.1481, step time: 0.2489\n",
      "264/281, train_loss: 0.0853, step time: 0.2494\n",
      "265/281, train_loss: 0.0889, step time: 0.2482\n",
      "266/281, train_loss: 0.1158, step time: 0.2480\n",
      "267/281, train_loss: 0.3074, step time: 0.2488\n",
      "268/281, train_loss: 0.0788, step time: 0.2499\n",
      "269/281, train_loss: 0.1369, step time: 0.2456\n",
      "270/281, train_loss: 0.0957, step time: 0.2518\n",
      "271/281, train_loss: 0.1081, step time: 0.2538\n",
      "272/281, train_loss: 0.0810, step time: 0.2525\n",
      "273/281, train_loss: 0.0671, step time: 0.2511\n",
      "274/281, train_loss: 0.2762, step time: 0.2479\n",
      "275/281, train_loss: 0.1020, step time: 0.2588\n",
      "276/281, train_loss: 0.1166, step time: 0.2494\n",
      "277/281, train_loss: 0.1232, step time: 0.2524\n",
      "278/281, train_loss: 0.2694, step time: 0.2519\n",
      "279/281, train_loss: 0.1355, step time: 0.2475\n",
      "280/281, train_loss: 0.1330, step time: 0.2455\n",
      "281/281, train_loss: 0.2348, step time: 0.2460\n",
      "282/281, train_loss: 0.0446, step time: 0.1493\n",
      "epoch 39 average loss: 0.1435\n",
      "current epoch: 39 current mean dice: 0.8510 tc: 0.8429 wt: 0.9114 et: 0.8074\n",
      "best mean dice: 0.8573 at epoch: 38\n",
      "time consuming of epoch 39 is: 411.8471\n",
      "----------\n",
      "epoch 40/200\n",
      "1/281, train_loss: 0.3199, step time: 0.2644\n",
      "2/281, train_loss: 0.2544, step time: 0.2584\n",
      "3/281, train_loss: 0.1287, step time: 0.2560\n",
      "4/281, train_loss: 0.2530, step time: 0.2583\n",
      "5/281, train_loss: 0.0687, step time: 0.2770\n",
      "6/281, train_loss: 0.1672, step time: 0.2596\n",
      "7/281, train_loss: 0.2231, step time: 0.2575\n",
      "8/281, train_loss: 0.1157, step time: 0.2559\n",
      "9/281, train_loss: 0.0889, step time: 0.2511\n",
      "10/281, train_loss: 0.0968, step time: 0.2616\n",
      "11/281, train_loss: 0.0840, step time: 0.2530\n",
      "12/281, train_loss: 0.2667, step time: 0.2470\n",
      "13/281, train_loss: 0.0997, step time: 0.2538\n",
      "14/281, train_loss: 0.1029, step time: 0.2645\n",
      "15/281, train_loss: 0.1022, step time: 0.2592\n",
      "16/281, train_loss: 0.0626, step time: 0.2566\n",
      "17/281, train_loss: 0.0556, step time: 0.2530\n",
      "18/281, train_loss: 0.0860, step time: 0.2531\n",
      "19/281, train_loss: 0.2670, step time: 0.2533\n",
      "20/281, train_loss: 0.1144, step time: 0.2501\n",
      "21/281, train_loss: 0.1378, step time: 0.2568\n",
      "22/281, train_loss: 0.1256, step time: 0.2553\n",
      "23/281, train_loss: 0.1586, step time: 0.2564\n",
      "24/281, train_loss: 0.0689, step time: 0.2599\n",
      "25/281, train_loss: 0.1010, step time: 0.2587\n",
      "26/281, train_loss: 0.0964, step time: 0.2588\n",
      "27/281, train_loss: 0.1292, step time: 0.2560\n",
      "28/281, train_loss: 0.0934, step time: 0.2585\n",
      "29/281, train_loss: 0.2578, step time: 0.2501\n",
      "30/281, train_loss: 0.1809, step time: 0.2546\n",
      "31/281, train_loss: 0.0914, step time: 0.2559\n",
      "32/281, train_loss: 0.4225, step time: 0.2596\n",
      "33/281, train_loss: 0.0691, step time: 0.2605\n",
      "34/281, train_loss: 0.2264, step time: 0.2584\n",
      "35/281, train_loss: 0.0926, step time: 0.2550\n",
      "36/281, train_loss: 0.1005, step time: 0.2526\n",
      "37/281, train_loss: 0.1060, step time: 0.2632\n",
      "38/281, train_loss: 0.3185, step time: 0.2596\n",
      "39/281, train_loss: 0.0818, step time: 0.2552\n",
      "40/281, train_loss: 0.1483, step time: 0.2579\n",
      "41/281, train_loss: 0.2516, step time: 0.2580\n",
      "42/281, train_loss: 0.2399, step time: 0.2607\n",
      "43/281, train_loss: 0.2417, step time: 0.2649\n",
      "44/281, train_loss: 0.0999, step time: 0.2566\n",
      "45/281, train_loss: 0.0744, step time: 0.2566\n",
      "46/281, train_loss: 0.1438, step time: 0.2544\n",
      "47/281, train_loss: 0.2567, step time: 0.2500\n",
      "48/281, train_loss: 0.0792, step time: 0.2513\n",
      "49/281, train_loss: 0.0868, step time: 0.2656\n",
      "50/281, train_loss: 0.1075, step time: 0.2602\n",
      "51/281, train_loss: 0.3515, step time: 0.2589\n",
      "52/281, train_loss: 0.0823, step time: 0.2616\n",
      "53/281, train_loss: 0.1491, step time: 0.2633\n",
      "54/281, train_loss: 0.2962, step time: 0.2596\n",
      "55/281, train_loss: 0.3034, step time: 0.2825\n",
      "56/281, train_loss: 0.1164, step time: 0.2703\n",
      "57/281, train_loss: 0.3028, step time: 0.2584\n",
      "58/281, train_loss: 0.2565, step time: 0.2595\n",
      "59/281, train_loss: 0.1239, step time: 0.2598\n",
      "60/281, train_loss: 0.2684, step time: 0.2588\n",
      "61/281, train_loss: 0.2609, step time: 0.2630\n",
      "62/281, train_loss: 0.1032, step time: 0.2617\n",
      "63/281, train_loss: 0.1117, step time: 0.2612\n",
      "64/281, train_loss: 0.1036, step time: 0.2511\n",
      "65/281, train_loss: 0.1033, step time: 0.2552\n",
      "66/281, train_loss: 0.1654, step time: 0.2571\n",
      "67/281, train_loss: 0.1218, step time: 0.2554\n",
      "68/281, train_loss: 0.1230, step time: 0.2646\n",
      "69/281, train_loss: 0.2052, step time: 0.2582\n",
      "70/281, train_loss: 0.1818, step time: 0.2555\n",
      "71/281, train_loss: 0.2885, step time: 0.2523\n",
      "72/281, train_loss: 0.1461, step time: 0.2571\n",
      "73/281, train_loss: 0.1282, step time: 0.2523\n",
      "74/281, train_loss: 0.0987, step time: 0.2545\n",
      "75/281, train_loss: 0.0855, step time: 0.2598\n",
      "76/281, train_loss: 0.0646, step time: 0.2595\n",
      "77/281, train_loss: 0.1901, step time: 0.2580\n",
      "78/281, train_loss: 0.0844, step time: 0.2557\n",
      "79/281, train_loss: 0.0845, step time: 0.2495\n",
      "80/281, train_loss: 0.2106, step time: 0.2285\n",
      "81/281, train_loss: 0.0563, step time: 0.2674\n",
      "82/281, train_loss: 0.1333, step time: 0.2529\n",
      "83/281, train_loss: 0.1442, step time: 0.2488\n",
      "84/281, train_loss: 0.2808, step time: 0.2499\n",
      "85/281, train_loss: 0.2667, step time: 0.2574\n",
      "86/281, train_loss: 0.1727, step time: 0.2538\n",
      "87/281, train_loss: 0.2785, step time: 0.2496\n",
      "88/281, train_loss: 0.1256, step time: 0.2504\n",
      "89/281, train_loss: 0.0760, step time: 0.2556\n",
      "90/281, train_loss: 0.2464, step time: 0.2553\n",
      "91/281, train_loss: 0.1203, step time: 0.2575\n",
      "92/281, train_loss: 0.2710, step time: 0.2550\n",
      "93/281, train_loss: 0.1532, step time: 0.2626\n",
      "94/281, train_loss: 0.1454, step time: 0.2579\n",
      "95/281, train_loss: 0.1016, step time: 0.2551\n",
      "96/281, train_loss: 0.0991, step time: 0.2549\n",
      "97/281, train_loss: 0.0754, step time: 0.2503\n",
      "98/281, train_loss: 0.2030, step time: 0.2532\n",
      "99/281, train_loss: 0.1145, step time: 0.2570\n",
      "100/281, train_loss: 0.1312, step time: 0.2530\n",
      "101/281, train_loss: 0.1427, step time: 0.2568\n",
      "102/281, train_loss: 0.1362, step time: 0.2598\n",
      "103/281, train_loss: 0.0866, step time: 0.2543\n",
      "104/281, train_loss: 0.1305, step time: 0.2524\n",
      "105/281, train_loss: 0.1145, step time: 0.2655\n",
      "106/281, train_loss: 0.0938, step time: 0.2529\n",
      "107/281, train_loss: 0.1424, step time: 0.2466\n",
      "108/281, train_loss: 0.1433, step time: 0.2508\n",
      "109/281, train_loss: 0.2699, step time: 0.2579\n",
      "110/281, train_loss: 0.0913, step time: 0.2530\n",
      "111/281, train_loss: 0.1470, step time: 0.2528\n",
      "112/281, train_loss: 0.0929, step time: 0.2560\n",
      "113/281, train_loss: 0.0665, step time: 0.2544\n",
      "114/281, train_loss: 0.0763, step time: 0.2558\n",
      "115/281, train_loss: 0.0941, step time: 0.2524\n",
      "116/281, train_loss: 0.0767, step time: 0.2490\n",
      "117/281, train_loss: 0.1030, step time: 0.2576\n",
      "118/281, train_loss: 0.0779, step time: 0.2577\n",
      "119/281, train_loss: 0.1304, step time: 0.2557\n",
      "120/281, train_loss: 0.1208, step time: 0.2585\n",
      "121/281, train_loss: 0.0956, step time: 0.2559\n",
      "122/281, train_loss: 0.0805, step time: 0.2579\n",
      "123/281, train_loss: 0.2723, step time: 0.2681\n",
      "124/281, train_loss: 0.1202, step time: 0.2635\n",
      "125/281, train_loss: 0.1187, step time: 0.2539\n",
      "126/281, train_loss: 0.2716, step time: 0.2576\n",
      "127/281, train_loss: 0.1410, step time: 0.2544\n",
      "128/281, train_loss: 0.0737, step time: 0.2547\n",
      "129/281, train_loss: 0.2262, step time: 0.2584\n",
      "130/281, train_loss: 0.1074, step time: 0.2560\n",
      "131/281, train_loss: 0.3114, step time: 0.2513\n",
      "132/281, train_loss: 0.0779, step time: 0.2534\n",
      "133/281, train_loss: 0.1189, step time: 0.2603\n",
      "134/281, train_loss: 0.1591, step time: 0.2543\n",
      "135/281, train_loss: 0.1706, step time: 0.2553\n",
      "136/281, train_loss: 0.0825, step time: 0.2580\n",
      "137/281, train_loss: 0.1567, step time: 0.2586\n",
      "138/281, train_loss: 0.2778, step time: 0.2526\n",
      "139/281, train_loss: 0.1932, step time: 0.2555\n",
      "140/281, train_loss: 0.1200, step time: 0.2503\n",
      "141/281, train_loss: 0.0742, step time: 0.2590\n",
      "142/281, train_loss: 0.1137, step time: 0.2580\n",
      "143/281, train_loss: 0.1089, step time: 0.2590\n",
      "144/281, train_loss: 0.0750, step time: 0.2568\n",
      "145/281, train_loss: 0.0727, step time: 0.2547\n",
      "146/281, train_loss: 0.0919, step time: 0.2590\n",
      "147/281, train_loss: 0.1328, step time: 0.2608\n",
      "148/281, train_loss: 0.0559, step time: 0.2592\n",
      "149/281, train_loss: 0.0804, step time: 0.2601\n",
      "150/281, train_loss: 0.1140, step time: 0.2488\n",
      "151/281, train_loss: 0.1006, step time: 0.2548\n",
      "152/281, train_loss: 0.2933, step time: 0.2501\n",
      "153/281, train_loss: 0.1365, step time: 0.2552\n",
      "154/281, train_loss: 0.1549, step time: 0.2522\n",
      "155/281, train_loss: 0.1349, step time: 0.2553\n",
      "156/281, train_loss: 0.2344, step time: 0.2611\n",
      "157/281, train_loss: 0.1539, step time: 0.2581\n",
      "158/281, train_loss: 0.1603, step time: 0.2542\n",
      "159/281, train_loss: 0.1046, step time: 0.2562\n",
      "160/281, train_loss: 0.2334, step time: 0.2537\n",
      "161/281, train_loss: 0.1404, step time: 0.2551\n",
      "162/281, train_loss: 0.1778, step time: 0.2554\n",
      "163/281, train_loss: 0.1477, step time: 0.2581\n",
      "164/281, train_loss: 0.1678, step time: 0.2564\n",
      "165/281, train_loss: 0.1166, step time: 0.2587\n",
      "166/281, train_loss: 0.0814, step time: 0.2567\n",
      "167/281, train_loss: 0.0872, step time: 0.2565\n",
      "168/281, train_loss: 0.0756, step time: 0.2514\n",
      "169/281, train_loss: 0.2767, step time: 0.2552\n",
      "170/281, train_loss: 0.1186, step time: 0.2571\n",
      "171/281, train_loss: 0.0978, step time: 0.2578\n",
      "172/281, train_loss: 0.1202, step time: 0.2582\n",
      "173/281, train_loss: 0.1191, step time: 0.2562\n",
      "174/281, train_loss: 0.1165, step time: 0.2542\n",
      "175/281, train_loss: 0.1487, step time: 0.2532\n",
      "176/281, train_loss: 0.2392, step time: 0.2524\n",
      "177/281, train_loss: 0.1314, step time: 0.2620\n",
      "178/281, train_loss: 0.2725, step time: 0.2574\n",
      "179/281, train_loss: 0.1283, step time: 0.2603\n",
      "180/281, train_loss: 0.1181, step time: 0.2592\n",
      "181/281, train_loss: 0.0907, step time: 0.2581\n",
      "182/281, train_loss: 0.1426, step time: 0.2553\n",
      "183/281, train_loss: 0.2663, step time: 0.2658\n",
      "184/281, train_loss: 0.1418, step time: 0.2566\n",
      "185/281, train_loss: 0.1089, step time: 0.2552\n",
      "186/281, train_loss: 0.1390, step time: 0.2550\n",
      "187/281, train_loss: 0.1750, step time: 0.2541\n",
      "188/281, train_loss: 0.1533, step time: 0.2557\n",
      "189/281, train_loss: 0.1442, step time: 0.2587\n",
      "190/281, train_loss: 0.0909, step time: 0.2538\n",
      "191/281, train_loss: 0.2024, step time: 0.2545\n",
      "192/281, train_loss: 0.2597, step time: 0.2533\n",
      "193/281, train_loss: 0.1639, step time: 0.2521\n",
      "194/281, train_loss: 0.1107, step time: 0.2541\n",
      "195/281, train_loss: 0.1418, step time: 0.2502\n",
      "196/281, train_loss: 0.1147, step time: 0.2506\n",
      "197/281, train_loss: 0.0863, step time: 0.2559\n",
      "198/281, train_loss: 0.1420, step time: 0.2524\n",
      "199/281, train_loss: 0.2008, step time: 0.2554\n",
      "200/281, train_loss: 0.1858, step time: 0.2483\n",
      "201/281, train_loss: 0.1058, step time: 0.2498\n",
      "202/281, train_loss: 0.1240, step time: 0.2562\n",
      "203/281, train_loss: 0.1518, step time: 0.2580\n",
      "204/281, train_loss: 0.0957, step time: 0.2541\n",
      "205/281, train_loss: 0.2416, step time: 0.2531\n",
      "206/281, train_loss: 0.1729, step time: 0.2564\n",
      "207/281, train_loss: 0.1223, step time: 0.2584\n",
      "208/281, train_loss: 0.1327, step time: 0.2563\n",
      "209/281, train_loss: 0.1298, step time: 0.2565\n",
      "210/281, train_loss: 0.0700, step time: 0.2493\n",
      "211/281, train_loss: 0.1159, step time: 0.2474\n",
      "212/281, train_loss: 0.2390, step time: 0.2485\n",
      "213/281, train_loss: 0.1137, step time: 0.2495\n",
      "214/281, train_loss: 0.1692, step time: 0.2510\n",
      "215/281, train_loss: 0.1333, step time: 0.2671\n",
      "216/281, train_loss: 0.1155, step time: 0.2541\n",
      "217/281, train_loss: 0.0975, step time: 0.2469\n",
      "218/281, train_loss: 0.1051, step time: 0.2552\n",
      "219/281, train_loss: 0.0839, step time: 0.2532\n",
      "220/281, train_loss: 0.0712, step time: 0.2542\n",
      "221/281, train_loss: 0.1183, step time: 0.2576\n",
      "222/281, train_loss: 0.1321, step time: 0.2530\n",
      "223/281, train_loss: 0.1445, step time: 0.2577\n",
      "224/281, train_loss: 0.1496, step time: 0.2565\n",
      "225/281, train_loss: 0.3061, step time: 0.2549\n",
      "226/281, train_loss: 0.1360, step time: 0.2581\n",
      "227/281, train_loss: 0.1254, step time: 0.2597\n",
      "228/281, train_loss: 0.0714, step time: 0.2565\n",
      "229/281, train_loss: 0.1666, step time: 0.2552\n",
      "230/281, train_loss: 0.0751, step time: 0.2558\n",
      "231/281, train_loss: 0.0926, step time: 0.2577\n",
      "232/281, train_loss: 0.0801, step time: 0.2576\n",
      "233/281, train_loss: 0.1696, step time: 0.2550\n",
      "234/281, train_loss: 0.1423, step time: 0.2542\n",
      "235/281, train_loss: 0.0928, step time: 0.2556\n",
      "236/281, train_loss: 0.0865, step time: 0.2505\n",
      "237/281, train_loss: 0.0721, step time: 0.2547\n",
      "238/281, train_loss: 0.1162, step time: 0.2572\n",
      "239/281, train_loss: 0.0989, step time: 0.2522\n",
      "240/281, train_loss: 0.0888, step time: 0.2557\n",
      "241/281, train_loss: 0.0818, step time: 0.2547\n",
      "242/281, train_loss: 0.0822, step time: 0.2543\n",
      "243/281, train_loss: 0.1223, step time: 0.2576\n",
      "244/281, train_loss: 0.1083, step time: 0.2526\n",
      "245/281, train_loss: 0.1319, step time: 0.2558\n",
      "246/281, train_loss: 0.0672, step time: 0.2579\n",
      "247/281, train_loss: 0.3473, step time: 0.2573\n",
      "248/281, train_loss: 0.1219, step time: 0.2517\n",
      "249/281, train_loss: 0.1033, step time: 0.2528\n",
      "250/281, train_loss: 0.1442, step time: 0.2544\n",
      "251/281, train_loss: 0.0589, step time: 0.2535\n",
      "252/281, train_loss: 0.1518, step time: 0.2532\n",
      "253/281, train_loss: 0.0802, step time: 0.2577\n",
      "254/281, train_loss: 0.1181, step time: 0.2607\n",
      "255/281, train_loss: 0.1360, step time: 0.2573\n",
      "256/281, train_loss: 0.1023, step time: 0.2518\n",
      "257/281, train_loss: 0.1426, step time: 0.2506\n",
      "258/281, train_loss: 0.0587, step time: 0.2565\n",
      "259/281, train_loss: 0.1540, step time: 0.2549\n",
      "260/281, train_loss: 0.2152, step time: 0.2524\n",
      "261/281, train_loss: 0.0879, step time: 0.2534\n",
      "262/281, train_loss: 0.3255, step time: 0.2506\n",
      "263/281, train_loss: 0.0983, step time: 0.2547\n",
      "264/281, train_loss: 0.2588, step time: 0.2528\n",
      "265/281, train_loss: 0.0974, step time: 0.2479\n",
      "266/281, train_loss: 0.0816, step time: 0.2560\n",
      "267/281, train_loss: 0.1082, step time: 0.2567\n",
      "268/281, train_loss: 0.1081, step time: 0.2511\n",
      "269/281, train_loss: 0.1577, step time: 0.2513\n",
      "270/281, train_loss: 0.1094, step time: 0.2561\n",
      "271/281, train_loss: 0.0959, step time: 0.2590\n",
      "272/281, train_loss: 0.0838, step time: 0.2589\n",
      "273/281, train_loss: 0.1070, step time: 0.2546\n",
      "274/281, train_loss: 0.1015, step time: 0.2545\n",
      "275/281, train_loss: 0.4207, step time: 0.2582\n",
      "276/281, train_loss: 0.1531, step time: 0.2521\n",
      "277/281, train_loss: 0.1304, step time: 0.2552\n",
      "278/281, train_loss: 0.1379, step time: 0.2590\n",
      "279/281, train_loss: 0.1257, step time: 0.2505\n",
      "280/281, train_loss: 0.2588, step time: 0.2507\n",
      "281/281, train_loss: 0.1996, step time: 0.2562\n",
      "282/281, train_loss: 0.2067, step time: 0.1551\n",
      "epoch 40 average loss: 0.1453\n",
      "saved new best metric model\n",
      "current epoch: 40 current mean dice: 0.8614 tc: 0.8551 wt: 0.8957 et: 0.8443\n",
      "best mean dice: 0.8614 at epoch: 40\n",
      "time consuming of epoch 40 is: 390.1034\n",
      "----------\n",
      "epoch 41/200\n",
      "1/281, train_loss: 0.0940, step time: 0.2641\n",
      "2/281, train_loss: 0.1837, step time: 0.2883\n",
      "3/281, train_loss: 0.1061, step time: 0.2595\n",
      "4/281, train_loss: 0.1177, step time: 0.2537\n",
      "5/281, train_loss: 0.1091, step time: 0.2509\n",
      "6/281, train_loss: 0.0690, step time: 0.2524\n",
      "7/281, train_loss: 0.0592, step time: 0.2546\n",
      "8/281, train_loss: 0.1050, step time: 0.2550\n",
      "9/281, train_loss: 0.1133, step time: 0.2534\n",
      "10/281, train_loss: 0.1480, step time: 0.2505\n",
      "11/281, train_loss: 0.2239, step time: 0.2537\n",
      "12/281, train_loss: 0.1314, step time: 0.2505\n",
      "13/281, train_loss: 0.1060, step time: 0.2537\n",
      "14/281, train_loss: 0.1248, step time: 0.2523\n",
      "15/281, train_loss: 0.1108, step time: 0.2533\n",
      "16/281, train_loss: 0.3202, step time: 0.2481\n",
      "17/281, train_loss: 0.2831, step time: 0.2553\n",
      "18/281, train_loss: 0.1092, step time: 0.2569\n",
      "19/281, train_loss: 0.0954, step time: 0.2543\n",
      "20/281, train_loss: 0.1384, step time: 0.2476\n",
      "21/281, train_loss: 0.1404, step time: 0.2552\n",
      "22/281, train_loss: 0.1870, step time: 0.2547\n",
      "23/281, train_loss: 0.2099, step time: 0.2567\n",
      "24/281, train_loss: 0.1137, step time: 0.2571\n",
      "25/281, train_loss: 0.2470, step time: 0.2526\n",
      "26/281, train_loss: 0.1314, step time: 0.2571\n",
      "27/281, train_loss: 0.1231, step time: 0.2539\n",
      "28/281, train_loss: 0.1392, step time: 0.2557\n",
      "29/281, train_loss: 0.1280, step time: 0.2542\n",
      "30/281, train_loss: 0.1038, step time: 0.2532\n",
      "31/281, train_loss: 0.0983, step time: 0.2534\n",
      "32/281, train_loss: 0.1655, step time: 0.2544\n",
      "33/281, train_loss: 0.1378, step time: 0.2547\n",
      "34/281, train_loss: 0.1930, step time: 0.2545\n",
      "35/281, train_loss: 0.1506, step time: 0.2538\n",
      "36/281, train_loss: 0.0980, step time: 0.2540\n",
      "37/281, train_loss: 0.0894, step time: 0.2520\n",
      "38/281, train_loss: 0.1859, step time: 0.2490\n",
      "39/281, train_loss: 0.0950, step time: 0.2574\n",
      "40/281, train_loss: 0.0963, step time: 0.2585\n",
      "41/281, train_loss: 0.3265, step time: 0.2534\n",
      "42/281, train_loss: 0.0901, step time: 0.2504\n",
      "43/281, train_loss: 0.1055, step time: 0.2541\n",
      "44/281, train_loss: 0.1001, step time: 0.2500\n",
      "45/281, train_loss: 0.1747, step time: 0.2560\n",
      "46/281, train_loss: 0.1025, step time: 0.2481\n",
      "47/281, train_loss: 0.1319, step time: 0.2485\n",
      "48/281, train_loss: 0.1182, step time: 0.2515\n",
      "49/281, train_loss: 0.1366, step time: 0.2514\n",
      "50/281, train_loss: 0.2820, step time: 0.2500\n",
      "51/281, train_loss: 0.2913, step time: 0.2553\n",
      "52/281, train_loss: 0.1138, step time: 0.2630\n",
      "53/281, train_loss: 0.1099, step time: 0.2530\n",
      "54/281, train_loss: 0.3104, step time: 0.2510\n",
      "55/281, train_loss: 0.1057, step time: 0.2524\n",
      "56/281, train_loss: 0.1226, step time: 0.2506\n",
      "57/281, train_loss: 0.0989, step time: 0.2548\n",
      "58/281, train_loss: 0.1011, step time: 0.2494\n",
      "59/281, train_loss: 0.1026, step time: 0.2468\n",
      "60/281, train_loss: 0.1618, step time: 0.2471\n",
      "61/281, train_loss: 0.1064, step time: 0.2509\n",
      "62/281, train_loss: 0.1021, step time: 0.2542\n",
      "63/281, train_loss: 0.1070, step time: 0.2511\n",
      "64/281, train_loss: 0.1391, step time: 0.2506\n",
      "65/281, train_loss: 0.2260, step time: 0.2596\n",
      "66/281, train_loss: 0.0883, step time: 0.2567\n",
      "67/281, train_loss: 0.2846, step time: 0.2569\n",
      "68/281, train_loss: 0.0869, step time: 0.2502\n",
      "69/281, train_loss: 0.1024, step time: 0.2588\n",
      "70/281, train_loss: 0.0773, step time: 0.2729\n",
      "71/281, train_loss: 0.0594, step time: 0.2582\n",
      "72/281, train_loss: 0.2510, step time: 0.2575\n",
      "73/281, train_loss: 0.1983, step time: 0.2549\n",
      "74/281, train_loss: 0.0784, step time: 0.2566\n",
      "75/281, train_loss: 0.0960, step time: 0.2592\n",
      "76/281, train_loss: 0.2311, step time: 0.2528\n",
      "77/281, train_loss: 0.0747, step time: 0.2598\n",
      "78/281, train_loss: 0.0913, step time: 0.2524\n",
      "79/281, train_loss: 0.0795, step time: 0.2510\n",
      "80/281, train_loss: 0.1201, step time: 0.2523\n",
      "81/281, train_loss: 0.0932, step time: 0.2560\n",
      "82/281, train_loss: 0.0893, step time: 0.2560\n",
      "83/281, train_loss: 0.2546, step time: 0.2541\n",
      "84/281, train_loss: 0.1323, step time: 0.2560\n",
      "85/281, train_loss: 0.1262, step time: 0.2596\n",
      "86/281, train_loss: 0.1046, step time: 0.2517\n",
      "87/281, train_loss: 0.0980, step time: 0.2497\n",
      "88/281, train_loss: 0.0763, step time: 0.2547\n",
      "89/281, train_loss: 0.1155, step time: 0.2556\n",
      "90/281, train_loss: 0.2135, step time: 0.2517\n",
      "91/281, train_loss: 0.1301, step time: 0.2508\n",
      "92/281, train_loss: 0.1176, step time: 0.2529\n",
      "93/281, train_loss: 0.1321, step time: 0.2510\n",
      "94/281, train_loss: 0.1486, step time: 0.2488\n",
      "95/281, train_loss: 0.2549, step time: 0.2568\n",
      "96/281, train_loss: 0.0946, step time: 0.2496\n",
      "97/281, train_loss: 0.0664, step time: 0.2541\n",
      "98/281, train_loss: 0.1209, step time: 0.2541\n",
      "99/281, train_loss: 0.1215, step time: 0.2498\n",
      "100/281, train_loss: 0.2138, step time: 0.2525\n",
      "101/281, train_loss: 0.0834, step time: 0.2588\n",
      "102/281, train_loss: 0.0938, step time: 0.2552\n",
      "103/281, train_loss: 0.1261, step time: 0.2544\n",
      "104/281, train_loss: 0.1760, step time: 0.2554\n",
      "105/281, train_loss: 0.1465, step time: 0.2524\n",
      "106/281, train_loss: 0.0862, step time: 0.2520\n",
      "107/281, train_loss: 0.1085, step time: 0.2709\n",
      "108/281, train_loss: 0.1553, step time: 0.2692\n",
      "109/281, train_loss: 0.0964, step time: 0.2507\n",
      "110/281, train_loss: 0.1213, step time: 0.2581\n",
      "111/281, train_loss: 0.1445, step time: 0.2545\n",
      "112/281, train_loss: 0.1040, step time: 0.2527\n",
      "113/281, train_loss: 0.1024, step time: 0.2515\n",
      "114/281, train_loss: 0.0948, step time: 0.2520\n",
      "115/281, train_loss: 0.1263, step time: 0.2511\n",
      "116/281, train_loss: 0.1506, step time: 0.2534\n",
      "117/281, train_loss: 0.2633, step time: 0.2528\n",
      "118/281, train_loss: 0.1572, step time: 0.2543\n",
      "119/281, train_loss: 0.1311, step time: 0.2522\n",
      "120/281, train_loss: 0.0778, step time: 0.2601\n",
      "121/281, train_loss: 0.1211, step time: 0.2545\n",
      "122/281, train_loss: 0.1589, step time: 0.2492\n",
      "123/281, train_loss: 0.1071, step time: 0.2549\n",
      "124/281, train_loss: 0.2192, step time: 0.2526\n",
      "125/281, train_loss: 0.2776, step time: 0.2542\n",
      "126/281, train_loss: 0.0843, step time: 0.2548\n",
      "127/281, train_loss: 0.1753, step time: 0.2525\n",
      "128/281, train_loss: 0.0784, step time: 0.2543\n",
      "129/281, train_loss: 0.1763, step time: 0.2450\n",
      "130/281, train_loss: 0.0596, step time: 0.2446\n",
      "131/281, train_loss: 0.1078, step time: 0.2542\n",
      "132/281, train_loss: 0.2962, step time: 0.2495\n",
      "133/281, train_loss: 0.0852, step time: 0.2535\n",
      "134/281, train_loss: 0.0906, step time: 0.2495\n",
      "135/281, train_loss: 0.2304, step time: 0.2532\n",
      "136/281, train_loss: 0.0916, step time: 0.2531\n",
      "137/281, train_loss: 0.2553, step time: 0.2502\n",
      "138/281, train_loss: 0.2374, step time: 0.2565\n",
      "139/281, train_loss: 0.0765, step time: 0.2536\n",
      "140/281, train_loss: 0.1165, step time: 0.2565\n",
      "141/281, train_loss: 0.3202, step time: 0.2484\n",
      "142/281, train_loss: 0.1007, step time: 0.2495\n",
      "143/281, train_loss: 0.1977, step time: 0.2548\n",
      "144/281, train_loss: 0.0633, step time: 0.2471\n",
      "145/281, train_loss: 0.0558, step time: 0.2573\n",
      "146/281, train_loss: 0.0998, step time: 0.2564\n",
      "147/281, train_loss: 0.1165, step time: 0.2514\n",
      "148/281, train_loss: 0.1503, step time: 0.2529\n",
      "149/281, train_loss: 0.0411, step time: 0.2511\n",
      "150/281, train_loss: 0.1033, step time: 0.2518\n",
      "151/281, train_loss: 0.2786, step time: 0.2579\n",
      "152/281, train_loss: 0.0918, step time: 0.2538\n",
      "153/281, train_loss: 0.0789, step time: 0.2520\n",
      "154/281, train_loss: 0.0697, step time: 0.2548\n",
      "155/281, train_loss: 0.0973, step time: 0.2536\n",
      "156/281, train_loss: 0.1862, step time: 0.2523\n",
      "157/281, train_loss: 0.0879, step time: 0.2526\n",
      "158/281, train_loss: 0.0784, step time: 0.2496\n",
      "159/281, train_loss: 0.1111, step time: 0.2551\n",
      "160/281, train_loss: 0.1697, step time: 0.2573\n",
      "161/281, train_loss: 0.0974, step time: 0.2514\n",
      "162/281, train_loss: 0.1322, step time: 0.2517\n",
      "163/281, train_loss: 0.0957, step time: 0.2421\n",
      "164/281, train_loss: 0.0968, step time: 0.2430\n",
      "165/281, train_loss: 0.1242, step time: 0.2475\n",
      "166/281, train_loss: 0.1498, step time: 0.2447\n",
      "167/281, train_loss: 0.4933, step time: 0.2460\n",
      "168/281, train_loss: 0.0719, step time: 0.2515\n",
      "169/281, train_loss: 0.0613, step time: 0.2544\n",
      "170/281, train_loss: 0.1212, step time: 0.2555\n",
      "171/281, train_loss: 0.1203, step time: 0.2523\n",
      "172/281, train_loss: 0.0789, step time: 0.2485\n",
      "173/281, train_loss: 0.1633, step time: 0.2515\n",
      "174/281, train_loss: 0.0833, step time: 0.2493\n",
      "175/281, train_loss: 0.1037, step time: 0.2503\n",
      "176/281, train_loss: 0.1102, step time: 0.2494\n",
      "177/281, train_loss: 0.1120, step time: 0.2488\n",
      "178/281, train_loss: 0.1164, step time: 0.2541\n",
      "179/281, train_loss: 0.2962, step time: 0.2457\n",
      "180/281, train_loss: 0.1185, step time: 0.2494\n",
      "181/281, train_loss: 0.0872, step time: 0.2501\n",
      "182/281, train_loss: 0.0815, step time: 0.2535\n",
      "183/281, train_loss: 0.0910, step time: 0.2526\n",
      "184/281, train_loss: 0.1347, step time: 0.2509\n",
      "185/281, train_loss: 0.1046, step time: 0.2475\n",
      "186/281, train_loss: 0.0625, step time: 0.2516\n",
      "187/281, train_loss: 0.2522, step time: 0.2503\n",
      "188/281, train_loss: 0.0928, step time: 0.2523\n",
      "189/281, train_loss: 0.1405, step time: 0.2520\n",
      "190/281, train_loss: 0.1052, step time: 0.2466\n",
      "191/281, train_loss: 0.1021, step time: 0.2515\n",
      "192/281, train_loss: 0.2852, step time: 0.2477\n",
      "193/281, train_loss: 0.1653, step time: 0.2470\n",
      "194/281, train_loss: 0.2557, step time: 0.2450\n",
      "195/281, train_loss: 0.0572, step time: 0.2453\n",
      "196/281, train_loss: 0.0839, step time: 0.2476\n",
      "197/281, train_loss: 0.0680, step time: 0.2515\n",
      "198/281, train_loss: 0.1204, step time: 0.2528\n",
      "199/281, train_loss: 0.0686, step time: 0.2530\n",
      "200/281, train_loss: 0.0622, step time: 0.2497\n",
      "201/281, train_loss: 0.0827, step time: 0.2475\n",
      "202/281, train_loss: 0.2843, step time: 0.2486\n",
      "203/281, train_loss: 0.0868, step time: 0.2469\n",
      "204/281, train_loss: 0.0626, step time: 0.2443\n",
      "205/281, train_loss: 0.3026, step time: 0.2496\n",
      "206/281, train_loss: 0.1354, step time: 0.2538\n",
      "207/281, train_loss: 0.1306, step time: 0.2525\n",
      "208/281, train_loss: 0.1192, step time: 0.2508\n",
      "209/281, train_loss: 0.1000, step time: 0.2476\n",
      "210/281, train_loss: 0.1873, step time: 0.2457\n",
      "211/281, train_loss: 0.1240, step time: 0.2488\n",
      "212/281, train_loss: 0.2700, step time: 0.2467\n",
      "213/281, train_loss: 0.0738, step time: 0.2520\n",
      "214/281, train_loss: 0.1180, step time: 0.2479\n",
      "215/281, train_loss: 0.1025, step time: 0.2510\n",
      "216/281, train_loss: 0.1022, step time: 0.2464\n",
      "217/281, train_loss: 0.1162, step time: 0.2508\n",
      "218/281, train_loss: 0.2553, step time: 0.2465\n",
      "219/281, train_loss: 0.1780, step time: 0.2460\n",
      "220/281, train_loss: 0.0672, step time: 0.2432\n",
      "221/281, train_loss: 0.0783, step time: 0.2516\n",
      "222/281, train_loss: 0.1548, step time: 0.2472\n",
      "223/281, train_loss: 0.0769, step time: 0.2495\n",
      "224/281, train_loss: 0.0992, step time: 0.2499\n",
      "225/281, train_loss: 0.1288, step time: 0.2511\n",
      "226/281, train_loss: 0.1007, step time: 0.2533\n",
      "227/281, train_loss: 0.1285, step time: 0.2506\n",
      "228/281, train_loss: 0.0635, step time: 0.2499\n",
      "229/281, train_loss: 0.1511, step time: 0.2508\n",
      "230/281, train_loss: 0.0843, step time: 0.2499\n",
      "231/281, train_loss: 0.1219, step time: 0.2500\n",
      "232/281, train_loss: 0.1326, step time: 0.2512\n",
      "233/281, train_loss: 0.2479, step time: 0.2518\n",
      "234/281, train_loss: 0.0641, step time: 0.2525\n",
      "235/281, train_loss: 0.1542, step time: 0.2546\n",
      "236/281, train_loss: 0.0941, step time: 0.2476\n",
      "237/281, train_loss: 0.0872, step time: 0.2473\n",
      "238/281, train_loss: 0.1001, step time: 0.2510\n",
      "239/281, train_loss: 0.1063, step time: 0.2485\n",
      "240/281, train_loss: 0.1583, step time: 0.2540\n",
      "241/281, train_loss: 0.0828, step time: 0.2547\n",
      "242/281, train_loss: 0.1048, step time: 0.2462\n",
      "243/281, train_loss: 0.1066, step time: 0.2438\n",
      "244/281, train_loss: 0.1538, step time: 0.2436\n",
      "245/281, train_loss: 0.1130, step time: 0.2481\n",
      "246/281, train_loss: 0.2471, step time: 0.2457\n",
      "247/281, train_loss: 0.2455, step time: 0.2547\n",
      "248/281, train_loss: 0.1184, step time: 0.2525\n",
      "249/281, train_loss: 0.4395, step time: 0.2498\n",
      "250/281, train_loss: 0.1094, step time: 0.2500\n",
      "251/281, train_loss: 0.1277, step time: 0.2500\n",
      "252/281, train_loss: 0.2786, step time: 0.2540\n",
      "253/281, train_loss: 0.0986, step time: 0.2479\n",
      "254/281, train_loss: 0.2629, step time: 0.2487\n",
      "255/281, train_loss: 0.0844, step time: 0.2520\n",
      "256/281, train_loss: 0.3028, step time: 0.2527\n",
      "257/281, train_loss: 0.1220, step time: 0.2550\n",
      "258/281, train_loss: 0.1483, step time: 0.2462\n",
      "259/281, train_loss: 0.0435, step time: 0.2531\n",
      "260/281, train_loss: 0.1046, step time: 0.2479\n",
      "261/281, train_loss: 0.4029, step time: 0.2434\n",
      "262/281, train_loss: 0.0914, step time: 0.2441\n",
      "263/281, train_loss: 0.2652, step time: 0.2515\n",
      "264/281, train_loss: 0.0746, step time: 0.2519\n",
      "265/281, train_loss: 0.2003, step time: 0.2464\n",
      "266/281, train_loss: 0.0708, step time: 0.2435\n",
      "267/281, train_loss: 0.2909, step time: 0.2525\n",
      "268/281, train_loss: 0.0885, step time: 0.2545\n",
      "269/281, train_loss: 0.0882, step time: 0.2574\n",
      "270/281, train_loss: 0.1613, step time: 0.2541\n",
      "271/281, train_loss: 0.2477, step time: 0.2569\n",
      "272/281, train_loss: 0.1174, step time: 0.2540\n",
      "273/281, train_loss: 0.1077, step time: 0.2475\n",
      "274/281, train_loss: 0.1570, step time: 0.2447\n",
      "275/281, train_loss: 0.2827, step time: 0.2533\n",
      "276/281, train_loss: 0.0771, step time: 0.2498\n",
      "277/281, train_loss: 0.0698, step time: 0.2509\n",
      "278/281, train_loss: 0.1821, step time: 0.2490\n",
      "279/281, train_loss: 0.0651, step time: 0.2459\n",
      "280/281, train_loss: 0.0732, step time: 0.2500\n",
      "281/281, train_loss: 0.1307, step time: 0.2499\n",
      "282/281, train_loss: 0.0640, step time: 0.1500\n",
      "epoch 41 average loss: 0.1385\n",
      "saved new best metric model\n",
      "current epoch: 41 current mean dice: 0.8652 tc: 0.8527 wt: 0.9098 et: 0.8457\n",
      "best mean dice: 0.8652 at epoch: 41\n",
      "time consuming of epoch 41 is: 438.8298\n",
      "----------\n",
      "epoch 42/200\n",
      "1/281, train_loss: 0.2569, step time: 0.2563\n",
      "2/281, train_loss: 0.2654, step time: 0.2574\n",
      "3/281, train_loss: 0.1223, step time: 0.2604\n",
      "4/281, train_loss: 0.2643, step time: 0.2591\n",
      "5/281, train_loss: 0.0971, step time: 0.2600\n",
      "6/281, train_loss: 0.0931, step time: 0.2617\n",
      "7/281, train_loss: 0.1016, step time: 0.2527\n",
      "8/281, train_loss: 0.2553, step time: 0.2560\n",
      "9/281, train_loss: 0.1033, step time: 0.2677\n",
      "10/281, train_loss: 0.0790, step time: 0.2577\n",
      "11/281, train_loss: 0.0698, step time: 0.2576\n",
      "12/281, train_loss: 0.1002, step time: 0.2555\n",
      "13/281, train_loss: 0.1105, step time: 0.2517\n",
      "14/281, train_loss: 0.0595, step time: 0.2537\n",
      "15/281, train_loss: 0.0642, step time: 0.2530\n",
      "16/281, train_loss: 0.1638, step time: 0.2543\n",
      "17/281, train_loss: 0.1189, step time: 0.2539\n",
      "18/281, train_loss: 0.1309, step time: 0.2567\n",
      "19/281, train_loss: 0.1190, step time: 0.2605\n",
      "20/281, train_loss: 0.1603, step time: 0.2568\n",
      "21/281, train_loss: 0.2445, step time: 0.2584\n",
      "22/281, train_loss: 0.0712, step time: 0.2615\n",
      "23/281, train_loss: 0.1044, step time: 0.2569\n",
      "24/281, train_loss: 0.1657, step time: 0.2571\n",
      "25/281, train_loss: 0.2453, step time: 0.2547\n",
      "26/281, train_loss: 0.1211, step time: 0.2556\n",
      "27/281, train_loss: 0.2644, step time: 0.2558\n",
      "28/281, train_loss: 0.0698, step time: 0.2521\n",
      "29/281, train_loss: 0.0946, step time: 0.2512\n",
      "30/281, train_loss: 0.2517, step time: 0.2550\n",
      "31/281, train_loss: 0.1059, step time: 0.2584\n",
      "32/281, train_loss: 0.0797, step time: 0.2568\n",
      "33/281, train_loss: 0.1783, step time: 0.2500\n",
      "34/281, train_loss: 0.2743, step time: 0.2525\n",
      "35/281, train_loss: 0.1208, step time: 0.2569\n",
      "36/281, train_loss: 0.0640, step time: 0.2566\n",
      "37/281, train_loss: 0.0748, step time: 0.2580\n",
      "38/281, train_loss: 0.1375, step time: 0.2527\n",
      "39/281, train_loss: 0.1056, step time: 0.2636\n",
      "40/281, train_loss: 0.2274, step time: 0.2559\n",
      "41/281, train_loss: 0.1202, step time: 0.2548\n",
      "42/281, train_loss: 0.1368, step time: 0.2579\n",
      "43/281, train_loss: 0.1025, step time: 0.2544\n",
      "44/281, train_loss: 0.0931, step time: 0.2584\n",
      "45/281, train_loss: 0.0940, step time: 0.2568\n",
      "46/281, train_loss: 0.0696, step time: 0.2694\n",
      "47/281, train_loss: 0.1561, step time: 0.2593\n",
      "48/281, train_loss: 0.1075, step time: 0.2591\n",
      "49/281, train_loss: 0.0734, step time: 0.2572\n",
      "50/281, train_loss: 0.1118, step time: 0.2515\n",
      "51/281, train_loss: 0.0756, step time: 0.2626\n",
      "52/281, train_loss: 0.0556, step time: 0.2638\n",
      "53/281, train_loss: 0.0784, step time: 0.2664\n",
      "54/281, train_loss: 0.1395, step time: 0.2586\n",
      "55/281, train_loss: 0.1155, step time: 0.2570\n",
      "56/281, train_loss: 0.1182, step time: 0.2559\n",
      "57/281, train_loss: 0.0690, step time: 0.2515\n",
      "58/281, train_loss: 0.0619, step time: 0.2535\n",
      "59/281, train_loss: 0.2368, step time: 0.2643\n",
      "60/281, train_loss: 0.0937, step time: 0.2594\n",
      "61/281, train_loss: 0.0763, step time: 0.2590\n",
      "62/281, train_loss: 0.3724, step time: 0.2584\n",
      "63/281, train_loss: 0.1054, step time: 0.2571\n",
      "64/281, train_loss: 0.0732, step time: 0.2509\n",
      "65/281, train_loss: 0.1072, step time: 0.2513\n",
      "66/281, train_loss: 0.1135, step time: 0.2594\n",
      "67/281, train_loss: 0.0770, step time: 0.2502\n",
      "68/281, train_loss: 0.1237, step time: 0.2521\n",
      "69/281, train_loss: 0.0656, step time: 0.2520\n",
      "70/281, train_loss: 0.0753, step time: 0.2559\n",
      "71/281, train_loss: 0.1497, step time: 0.2616\n",
      "72/281, train_loss: 0.1051, step time: 0.2605\n",
      "73/281, train_loss: 0.3050, step time: 0.2608\n",
      "74/281, train_loss: 0.1363, step time: 0.2594\n",
      "75/281, train_loss: 0.0762, step time: 0.2541\n",
      "76/281, train_loss: 0.1308, step time: 0.2536\n",
      "77/281, train_loss: 0.3098, step time: 0.2549\n",
      "78/281, train_loss: 0.0929, step time: 0.2557\n",
      "79/281, train_loss: 0.0855, step time: 0.2607\n",
      "80/281, train_loss: 0.1050, step time: 0.2594\n",
      "81/281, train_loss: 0.0964, step time: 0.2584\n",
      "82/281, train_loss: 0.0743, step time: 0.2578\n",
      "83/281, train_loss: 0.0790, step time: 0.2625\n",
      "84/281, train_loss: 0.0720, step time: 0.2580\n",
      "85/281, train_loss: 0.0794, step time: 0.2584\n",
      "86/281, train_loss: 0.0895, step time: 0.2571\n",
      "87/281, train_loss: 0.1506, step time: 0.2580\n",
      "88/281, train_loss: 0.0904, step time: 0.2571\n",
      "89/281, train_loss: 0.0666, step time: 0.2563\n",
      "90/281, train_loss: 0.1131, step time: 0.2510\n",
      "91/281, train_loss: 0.1320, step time: 0.2637\n",
      "92/281, train_loss: 0.2633, step time: 0.2574\n",
      "93/281, train_loss: 0.1582, step time: 0.2569\n",
      "94/281, train_loss: 0.1114, step time: 0.2581\n",
      "95/281, train_loss: 0.1291, step time: 0.2555\n",
      "96/281, train_loss: 0.2315, step time: 0.2505\n",
      "97/281, train_loss: 0.0711, step time: 0.2483\n",
      "98/281, train_loss: 0.0926, step time: 0.2511\n",
      "99/281, train_loss: 0.1662, step time: 0.2535\n",
      "100/281, train_loss: 0.1317, step time: 0.2583\n",
      "101/281, train_loss: 0.1110, step time: 0.2517\n",
      "102/281, train_loss: 0.0981, step time: 0.2529\n",
      "103/281, train_loss: 0.2781, step time: 0.2542\n",
      "104/281, train_loss: 0.1838, step time: 0.2517\n",
      "105/281, train_loss: 0.1008, step time: 0.2528\n",
      "106/281, train_loss: 0.2496, step time: 0.2524\n",
      "107/281, train_loss: 0.2252, step time: 0.2565\n",
      "108/281, train_loss: 0.1033, step time: 0.2556\n",
      "109/281, train_loss: 0.0553, step time: 0.2585\n",
      "110/281, train_loss: 0.1325, step time: 0.2588\n",
      "111/281, train_loss: 0.0809, step time: 0.2524\n",
      "112/281, train_loss: 0.1509, step time: 0.2583\n",
      "113/281, train_loss: 0.1090, step time: 0.2544\n",
      "114/281, train_loss: 0.1215, step time: 0.2569\n",
      "115/281, train_loss: 0.0833, step time: 0.2562\n",
      "116/281, train_loss: 0.1727, step time: 0.2543\n",
      "117/281, train_loss: 0.0861, step time: 0.2565\n",
      "118/281, train_loss: 0.2944, step time: 0.2550\n",
      "119/281, train_loss: 0.1356, step time: 0.2535\n",
      "120/281, train_loss: 0.1134, step time: 0.2531\n",
      "121/281, train_loss: 0.2219, step time: 0.2513\n",
      "122/281, train_loss: 0.1011, step time: 0.2506\n",
      "123/281, train_loss: 0.0461, step time: 0.2579\n",
      "124/281, train_loss: 0.0952, step time: 0.2576\n",
      "125/281, train_loss: 0.2847, step time: 0.2612\n",
      "126/281, train_loss: 0.0835, step time: 0.2670\n",
      "127/281, train_loss: 0.1573, step time: 0.2553\n",
      "128/281, train_loss: 0.0812, step time: 0.2584\n",
      "129/281, train_loss: 0.1468, step time: 0.2596\n",
      "130/281, train_loss: 0.1374, step time: 0.2572\n",
      "131/281, train_loss: 0.1039, step time: 0.2526\n",
      "132/281, train_loss: 0.0884, step time: 0.2499\n",
      "133/281, train_loss: 0.1149, step time: 0.2530\n",
      "134/281, train_loss: 0.0976, step time: 0.2550\n",
      "135/281, train_loss: 0.0852, step time: 0.2619\n",
      "136/281, train_loss: 0.1694, step time: 0.2506\n",
      "137/281, train_loss: 0.1102, step time: 0.2578\n",
      "138/281, train_loss: 0.1134, step time: 0.2566\n",
      "139/281, train_loss: 0.1082, step time: 0.2615\n",
      "140/281, train_loss: 0.1547, step time: 0.2522\n",
      "141/281, train_loss: 0.3348, step time: 0.2529\n",
      "142/281, train_loss: 0.0861, step time: 0.2575\n",
      "143/281, train_loss: 0.1183, step time: 0.2580\n",
      "144/281, train_loss: 0.1251, step time: 0.2564\n",
      "145/281, train_loss: 0.1003, step time: 0.2536\n",
      "146/281, train_loss: 0.1007, step time: 0.2556\n",
      "147/281, train_loss: 0.2469, step time: 0.2550\n",
      "148/281, train_loss: 0.1114, step time: 0.2534\n",
      "149/281, train_loss: 0.1221, step time: 0.2546\n",
      "150/281, train_loss: 0.1960, step time: 0.2526\n",
      "151/281, train_loss: 0.1374, step time: 0.2535\n",
      "152/281, train_loss: 0.1102, step time: 0.2505\n",
      "153/281, train_loss: 0.1498, step time: 0.2486\n",
      "154/281, train_loss: 0.0595, step time: 0.2542\n",
      "155/281, train_loss: 0.1584, step time: 0.2583\n",
      "156/281, train_loss: 0.2379, step time: 0.2546\n",
      "157/281, train_loss: 0.1645, step time: 0.2536\n",
      "158/281, train_loss: 0.2451, step time: 0.2492\n",
      "159/281, train_loss: 0.1093, step time: 0.2558\n",
      "160/281, train_loss: 0.0933, step time: 0.2566\n",
      "161/281, train_loss: 0.1314, step time: 0.2534\n",
      "162/281, train_loss: 0.0933, step time: 0.2570\n",
      "163/281, train_loss: 0.0893, step time: 0.2538\n",
      "164/281, train_loss: 0.1194, step time: 0.2525\n",
      "165/281, train_loss: 0.1563, step time: 0.2495\n",
      "166/281, train_loss: 0.1253, step time: 0.2519\n",
      "167/281, train_loss: 0.1381, step time: 0.2564\n",
      "168/281, train_loss: 0.0758, step time: 0.2540\n",
      "169/281, train_loss: 0.0674, step time: 0.2576\n",
      "170/281, train_loss: 0.1252, step time: 0.2555\n",
      "171/281, train_loss: 0.0991, step time: 0.2553\n",
      "172/281, train_loss: 0.4510, step time: 0.2586\n",
      "173/281, train_loss: 0.1212, step time: 0.2505\n",
      "174/281, train_loss: 0.1350, step time: 0.2506\n",
      "175/281, train_loss: 0.1002, step time: 0.2535\n",
      "176/281, train_loss: 0.0942, step time: 0.2513\n",
      "177/281, train_loss: 0.1036, step time: 0.2469\n",
      "178/281, train_loss: 0.1013, step time: 0.2498\n",
      "179/281, train_loss: 0.0586, step time: 0.2520\n",
      "180/281, train_loss: 0.0783, step time: 0.2550\n",
      "181/281, train_loss: 0.0946, step time: 0.2558\n",
      "182/281, train_loss: 0.1266, step time: 0.2522\n",
      "183/281, train_loss: 0.2991, step time: 0.2477\n",
      "184/281, train_loss: 0.1131, step time: 0.2476\n",
      "185/281, train_loss: 0.2397, step time: 0.2523\n",
      "186/281, train_loss: 0.1149, step time: 0.2510\n",
      "187/281, train_loss: 0.1414, step time: 0.2509\n",
      "188/281, train_loss: 0.1044, step time: 0.2565\n",
      "189/281, train_loss: 0.0791, step time: 0.2522\n",
      "190/281, train_loss: 0.0537, step time: 0.2519\n",
      "191/281, train_loss: 0.1025, step time: 0.2496\n",
      "192/281, train_loss: 0.0927, step time: 0.2483\n",
      "193/281, train_loss: 0.1252, step time: 0.2525\n",
      "194/281, train_loss: 0.1928, step time: 0.2516\n",
      "195/281, train_loss: 0.1088, step time: 0.2509\n",
      "196/281, train_loss: 0.1129, step time: 0.2557\n",
      "197/281, train_loss: 0.2440, step time: 0.2548\n",
      "198/281, train_loss: 0.2799, step time: 0.2516\n",
      "199/281, train_loss: 0.2394, step time: 0.2556\n",
      "200/281, train_loss: 0.1336, step time: 0.2541\n",
      "201/281, train_loss: 0.0553, step time: 0.2507\n",
      "202/281, train_loss: 0.0913, step time: 0.2512\n",
      "203/281, train_loss: 0.2134, step time: 0.2577\n",
      "204/281, train_loss: 0.2637, step time: 0.2708\n",
      "205/281, train_loss: 0.1079, step time: 0.2561\n",
      "206/281, train_loss: 0.1178, step time: 0.2529\n",
      "207/281, train_loss: 0.0963, step time: 0.2500\n",
      "208/281, train_loss: 0.1643, step time: 0.2525\n",
      "209/281, train_loss: 0.1009, step time: 0.2531\n",
      "210/281, train_loss: 0.1236, step time: 0.2528\n",
      "211/281, train_loss: 0.0855, step time: 0.2542\n",
      "212/281, train_loss: 0.1596, step time: 0.2582\n",
      "213/281, train_loss: 0.2847, step time: 0.2579\n",
      "214/281, train_loss: 0.1832, step time: 0.2609\n",
      "215/281, train_loss: 0.0755, step time: 0.2529\n",
      "216/281, train_loss: 0.2547, step time: 0.2520\n",
      "217/281, train_loss: 0.1117, step time: 0.2510\n",
      "218/281, train_loss: 0.1138, step time: 0.2520\n",
      "219/281, train_loss: 0.1602, step time: 0.2599\n",
      "220/281, train_loss: 0.1368, step time: 0.2568\n",
      "221/281, train_loss: 0.0774, step time: 0.2489\n",
      "222/281, train_loss: 0.2748, step time: 0.2596\n",
      "223/281, train_loss: 0.0715, step time: 0.2529\n",
      "224/281, train_loss: 0.2331, step time: 0.2550\n",
      "225/281, train_loss: 0.0580, step time: 0.2564\n",
      "226/281, train_loss: 0.1075, step time: 0.2585\n",
      "227/281, train_loss: 0.1832, step time: 0.2586\n",
      "228/281, train_loss: 0.0723, step time: 0.2560\n",
      "229/281, train_loss: 0.1689, step time: 0.2544\n",
      "230/281, train_loss: 0.1012, step time: 0.2552\n",
      "231/281, train_loss: 0.0859, step time: 0.2555\n",
      "232/281, train_loss: 0.0886, step time: 0.2525\n",
      "233/281, train_loss: 0.2520, step time: 0.2548\n",
      "234/281, train_loss: 0.2518, step time: 0.2567\n",
      "235/281, train_loss: 0.0765, step time: 0.2579\n",
      "236/281, train_loss: 0.1211, step time: 0.2540\n",
      "237/281, train_loss: 0.1025, step time: 0.2551\n",
      "238/281, train_loss: 0.0757, step time: 0.2508\n",
      "239/281, train_loss: 0.1288, step time: 0.2576\n",
      "240/281, train_loss: 0.1194, step time: 0.2512\n",
      "241/281, train_loss: 0.1771, step time: 0.2547\n",
      "242/281, train_loss: 0.1446, step time: 0.2555\n",
      "243/281, train_loss: 0.0995, step time: 0.2579\n",
      "244/281, train_loss: 0.1152, step time: 0.2580\n",
      "245/281, train_loss: 0.1051, step time: 0.2510\n",
      "246/281, train_loss: 0.1023, step time: 0.2525\n",
      "247/281, train_loss: 0.0569, step time: 0.2559\n",
      "248/281, train_loss: 0.1681, step time: 0.2545\n",
      "249/281, train_loss: 0.1445, step time: 0.2557\n",
      "250/281, train_loss: 0.1128, step time: 0.2619\n",
      "251/281, train_loss: 0.1055, step time: 0.2605\n",
      "252/281, train_loss: 0.2268, step time: 0.2568\n",
      "253/281, train_loss: 0.0774, step time: 0.2575\n",
      "254/281, train_loss: 0.0630, step time: 0.2527\n",
      "255/281, train_loss: 0.1042, step time: 0.2622\n",
      "256/281, train_loss: 0.1811, step time: 0.2572\n",
      "257/281, train_loss: 0.1078, step time: 0.2562\n",
      "258/281, train_loss: 0.2340, step time: 0.2518\n",
      "259/281, train_loss: 0.1395, step time: 0.2583\n",
      "260/281, train_loss: 0.0620, step time: 0.2556\n",
      "261/281, train_loss: 0.2287, step time: 0.2529\n",
      "262/281, train_loss: 0.1038, step time: 0.2532\n",
      "263/281, train_loss: 0.0928, step time: 0.2561\n",
      "264/281, train_loss: 0.2327, step time: 0.2591\n",
      "265/281, train_loss: 0.2730, step time: 0.2583\n",
      "266/281, train_loss: 0.1134, step time: 0.2525\n",
      "267/281, train_loss: 0.0688, step time: 0.2574\n",
      "268/281, train_loss: 0.0980, step time: 0.2572\n",
      "269/281, train_loss: 0.1081, step time: 0.2509\n",
      "270/281, train_loss: 0.3707, step time: 0.2537\n",
      "271/281, train_loss: 0.1627, step time: 0.2565\n",
      "272/281, train_loss: 0.1008, step time: 0.2577\n",
      "273/281, train_loss: 0.0992, step time: 0.2492\n",
      "274/281, train_loss: 0.0783, step time: 0.2663\n",
      "275/281, train_loss: 0.1450, step time: 0.2524\n",
      "276/281, train_loss: 0.4287, step time: 0.2534\n",
      "277/281, train_loss: 0.0783, step time: 0.2567\n",
      "278/281, train_loss: 0.1356, step time: 0.2538\n",
      "279/281, train_loss: 0.3063, step time: 0.2533\n",
      "280/281, train_loss: 0.1176, step time: 0.2565\n",
      "281/281, train_loss: 0.0890, step time: 0.2555\n",
      "282/281, train_loss: 0.1508, step time: 0.1519\n",
      "epoch 42 average loss: 0.1363\n",
      "current epoch: 42 current mean dice: 0.8650 tc: 0.8613 wt: 0.9028 et: 0.8419\n",
      "best mean dice: 0.8652 at epoch: 41\n",
      "time consuming of epoch 42 is: 398.2541\n",
      "----------\n",
      "epoch 43/200\n",
      "1/281, train_loss: 0.1697, step time: 0.2560\n",
      "2/281, train_loss: 0.0703, step time: 0.2528\n",
      "3/281, train_loss: 0.1210, step time: 0.2542\n",
      "4/281, train_loss: 0.1491, step time: 0.2557\n",
      "5/281, train_loss: 0.2226, step time: 0.2526\n",
      "6/281, train_loss: 0.1078, step time: 0.2555\n",
      "7/281, train_loss: 0.1063, step time: 0.2457\n",
      "8/281, train_loss: 0.0858, step time: 0.2465\n",
      "9/281, train_loss: 0.2599, step time: 0.2488\n",
      "10/281, train_loss: 0.0773, step time: 0.2503\n",
      "11/281, train_loss: 0.1742, step time: 0.2475\n",
      "12/281, train_loss: 0.0686, step time: 0.2527\n",
      "13/281, train_loss: 0.1288, step time: 0.2660\n",
      "14/281, train_loss: 0.1120, step time: 0.2568\n",
      "15/281, train_loss: 0.0857, step time: 0.2533\n",
      "16/281, train_loss: 0.3073, step time: 0.2535\n",
      "17/281, train_loss: 0.0666, step time: 0.2699\n",
      "18/281, train_loss: 0.1165, step time: 0.2510\n",
      "19/281, train_loss: 0.1250, step time: 0.2522\n",
      "20/281, train_loss: 0.1400, step time: 0.2554\n",
      "21/281, train_loss: 0.1017, step time: 0.2500\n",
      "22/281, train_loss: 0.1424, step time: 0.2474\n",
      "23/281, train_loss: 0.2554, step time: 0.2533\n",
      "24/281, train_loss: 0.0617, step time: 0.2589\n",
      "25/281, train_loss: 0.1166, step time: 0.2597\n",
      "26/281, train_loss: 0.2510, step time: 0.2518\n",
      "27/281, train_loss: 0.0883, step time: 0.2519\n",
      "28/281, train_loss: 0.1609, step time: 0.2528\n",
      "29/281, train_loss: 0.1014, step time: 0.2550\n",
      "30/281, train_loss: 0.0900, step time: 0.2559\n",
      "31/281, train_loss: 0.0749, step time: 0.2537\n",
      "32/281, train_loss: 0.0742, step time: 0.2577\n",
      "33/281, train_loss: 0.1227, step time: 0.2571\n",
      "34/281, train_loss: 0.1179, step time: 0.2528\n",
      "35/281, train_loss: 0.0969, step time: 0.2507\n",
      "36/281, train_loss: 0.0924, step time: 0.2544\n",
      "37/281, train_loss: 0.1342, step time: 0.2532\n",
      "38/281, train_loss: 0.0895, step time: 0.2596\n",
      "39/281, train_loss: 0.1128, step time: 0.2690\n",
      "40/281, train_loss: 0.0939, step time: 0.2637\n",
      "41/281, train_loss: 0.1413, step time: 0.2553\n",
      "42/281, train_loss: 0.1282, step time: 0.2545\n",
      "43/281, train_loss: 0.2542, step time: 0.2486\n",
      "44/281, train_loss: 0.1269, step time: 0.2493\n",
      "45/281, train_loss: 0.0497, step time: 0.2570\n",
      "46/281, train_loss: 0.1480, step time: 0.2556\n",
      "47/281, train_loss: 0.1004, step time: 0.2557\n",
      "48/281, train_loss: 0.0682, step time: 0.2572\n",
      "49/281, train_loss: 0.1041, step time: 0.2526\n",
      "50/281, train_loss: 0.1328, step time: 0.2585\n",
      "51/281, train_loss: 0.1802, step time: 0.2578\n",
      "52/281, train_loss: 0.0967, step time: 0.2532\n",
      "53/281, train_loss: 0.0657, step time: 0.2504\n",
      "54/281, train_loss: 0.1553, step time: 0.2503\n",
      "55/281, train_loss: 0.0803, step time: 0.2495\n",
      "56/281, train_loss: 0.0873, step time: 0.2486\n",
      "57/281, train_loss: 0.1505, step time: 0.2549\n",
      "58/281, train_loss: 0.1112, step time: 0.2558\n",
      "59/281, train_loss: 0.0671, step time: 0.2516\n",
      "60/281, train_loss: 0.1057, step time: 0.2538\n",
      "61/281, train_loss: 0.1671, step time: 0.2532\n",
      "62/281, train_loss: 0.1102, step time: 0.2552\n",
      "63/281, train_loss: 0.1010, step time: 0.2474\n",
      "64/281, train_loss: 0.1493, step time: 0.2477\n",
      "65/281, train_loss: 0.0873, step time: 0.2550\n",
      "66/281, train_loss: 0.1550, step time: 0.2520\n",
      "67/281, train_loss: 0.1256, step time: 0.2550\n",
      "68/281, train_loss: 0.1087, step time: 0.2517\n",
      "69/281, train_loss: 0.0735, step time: 0.2510\n",
      "70/281, train_loss: 0.0572, step time: 0.2502\n",
      "71/281, train_loss: 0.1425, step time: 0.2484\n",
      "72/281, train_loss: 0.1211, step time: 0.2559\n",
      "73/281, train_loss: 0.1075, step time: 0.2512\n",
      "74/281, train_loss: 0.0745, step time: 0.2551\n",
      "75/281, train_loss: 0.0649, step time: 0.2562\n",
      "76/281, train_loss: 0.3174, step time: 0.2523\n",
      "77/281, train_loss: 0.1555, step time: 0.2541\n",
      "78/281, train_loss: 0.1216, step time: 0.2550\n",
      "79/281, train_loss: 0.1333, step time: 0.2569\n",
      "80/281, train_loss: 0.1587, step time: 0.2530\n",
      "81/281, train_loss: 0.2521, step time: 0.2504\n",
      "82/281, train_loss: 0.0796, step time: 0.2520\n",
      "83/281, train_loss: 0.1072, step time: 0.2476\n",
      "84/281, train_loss: 0.0592, step time: 0.2503\n",
      "85/281, train_loss: 0.0943, step time: 0.2501\n",
      "86/281, train_loss: 0.1036, step time: 0.2521\n",
      "87/281, train_loss: 0.1713, step time: 0.2496\n",
      "88/281, train_loss: 0.1477, step time: 0.2472\n",
      "89/281, train_loss: 0.1034, step time: 0.2478\n",
      "90/281, train_loss: 0.1173, step time: 0.2545\n",
      "91/281, train_loss: 0.0510, step time: 0.2573\n",
      "92/281, train_loss: 0.0705, step time: 0.2607\n",
      "93/281, train_loss: 0.0930, step time: 0.2518\n",
      "94/281, train_loss: 0.1651, step time: 0.2521\n",
      "95/281, train_loss: 0.1906, step time: 0.2555\n",
      "96/281, train_loss: 0.0827, step time: 0.2500\n",
      "97/281, train_loss: 0.1194, step time: 0.2561\n",
      "98/281, train_loss: 0.0775, step time: 0.2499\n",
      "99/281, train_loss: 0.0836, step time: 0.2512\n",
      "100/281, train_loss: 0.0805, step time: 0.2546\n",
      "101/281, train_loss: 0.0931, step time: 0.2598\n",
      "102/281, train_loss: 0.1179, step time: 0.2497\n",
      "103/281, train_loss: 0.0745, step time: 0.2503\n",
      "104/281, train_loss: 0.0933, step time: 0.2554\n",
      "105/281, train_loss: 0.0908, step time: 0.2563\n",
      "106/281, train_loss: 0.1493, step time: 0.2539\n",
      "107/281, train_loss: 0.1152, step time: 0.2512\n",
      "108/281, train_loss: 0.1164, step time: 0.2521\n",
      "109/281, train_loss: 0.0750, step time: 0.2540\n",
      "110/281, train_loss: 0.0821, step time: 0.2517\n",
      "111/281, train_loss: 0.2724, step time: 0.2576\n",
      "112/281, train_loss: 0.1059, step time: 0.2534\n",
      "113/281, train_loss: 0.4220, step time: 0.2505\n",
      "114/281, train_loss: 0.1250, step time: 0.2501\n",
      "115/281, train_loss: 0.0987, step time: 0.2442\n",
      "116/281, train_loss: 0.0986, step time: 0.2478\n",
      "117/281, train_loss: 0.2524, step time: 0.2502\n",
      "118/281, train_loss: 0.1297, step time: 0.2477\n",
      "119/281, train_loss: 0.2595, step time: 0.2446\n",
      "120/281, train_loss: 0.0763, step time: 0.2512\n",
      "121/281, train_loss: 0.2304, step time: 0.2502\n",
      "122/281, train_loss: 0.2492, step time: 0.2582\n",
      "123/281, train_loss: 0.0619, step time: 0.2657\n",
      "124/281, train_loss: 0.2679, step time: 0.2505\n",
      "125/281, train_loss: 0.0760, step time: 0.2532\n",
      "126/281, train_loss: 0.1390, step time: 0.2498\n",
      "127/281, train_loss: 0.0915, step time: 0.2488\n",
      "128/281, train_loss: 0.0809, step time: 0.2476\n",
      "129/281, train_loss: 0.2987, step time: 0.2532\n",
      "130/281, train_loss: 0.1166, step time: 0.2498\n",
      "131/281, train_loss: 0.1097, step time: 0.2503\n",
      "132/281, train_loss: 0.2370, step time: 0.2499\n",
      "133/281, train_loss: 0.0953, step time: 0.2473\n",
      "134/281, train_loss: 0.0818, step time: 0.2510\n",
      "135/281, train_loss: 0.1855, step time: 0.2457\n",
      "136/281, train_loss: 0.0895, step time: 0.2445\n",
      "137/281, train_loss: 0.1597, step time: 0.2534\n",
      "138/281, train_loss: 0.0619, step time: 0.2509\n",
      "139/281, train_loss: 0.0967, step time: 0.2475\n",
      "140/281, train_loss: 0.2358, step time: 0.2500\n",
      "141/281, train_loss: 0.0864, step time: 0.2609\n",
      "142/281, train_loss: 0.2305, step time: 0.2488\n",
      "143/281, train_loss: 0.1896, step time: 0.2515\n",
      "144/281, train_loss: 0.0644, step time: 0.2492\n",
      "145/281, train_loss: 0.1370, step time: 0.2520\n",
      "146/281, train_loss: 0.0673, step time: 0.2509\n",
      "147/281, train_loss: 0.0826, step time: 0.2470\n",
      "148/281, train_loss: 0.0743, step time: 0.2468\n",
      "149/281, train_loss: 0.1213, step time: 0.2524\n",
      "150/281, train_loss: 0.0903, step time: 0.2498\n",
      "151/281, train_loss: 0.1657, step time: 0.2479\n",
      "152/281, train_loss: 0.1017, step time: 0.2462\n",
      "153/281, train_loss: 0.2243, step time: 0.2511\n",
      "154/281, train_loss: 0.1041, step time: 0.2468\n",
      "155/281, train_loss: 0.1338, step time: 0.2564\n",
      "156/281, train_loss: 0.2222, step time: 0.2485\n",
      "157/281, train_loss: 0.1000, step time: 0.2527\n",
      "158/281, train_loss: 0.1034, step time: 0.2496\n",
      "159/281, train_loss: 0.0518, step time: 0.2515\n",
      "160/281, train_loss: 0.1023, step time: 0.2498\n",
      "161/281, train_loss: 0.2737, step time: 0.2509\n",
      "162/281, train_loss: 0.1116, step time: 0.2491\n",
      "163/281, train_loss: 0.0915, step time: 0.2509\n",
      "164/281, train_loss: 0.0748, step time: 0.2490\n",
      "165/281, train_loss: 0.0766, step time: 0.2529\n",
      "166/281, train_loss: 0.1440, step time: 0.2469\n",
      "167/281, train_loss: 0.1022, step time: 0.2568\n",
      "168/281, train_loss: 0.0893, step time: 0.2573\n",
      "169/281, train_loss: 0.3415, step time: 0.2541\n",
      "170/281, train_loss: 0.0634, step time: 0.2575\n",
      "171/281, train_loss: 0.0763, step time: 0.2471\n",
      "172/281, train_loss: 0.1339, step time: 0.2484\n",
      "173/281, train_loss: 0.1307, step time: 0.2492\n",
      "174/281, train_loss: 0.1107, step time: 0.2427\n",
      "175/281, train_loss: 0.0876, step time: 0.2501\n",
      "176/281, train_loss: 0.1051, step time: 0.2462\n",
      "177/281, train_loss: 0.0579, step time: 0.2589\n",
      "178/281, train_loss: 0.2138, step time: 0.2506\n",
      "179/281, train_loss: 0.2524, step time: 0.2517\n",
      "180/281, train_loss: 0.0840, step time: 0.2525\n",
      "181/281, train_loss: 0.1294, step time: 0.2536\n",
      "182/281, train_loss: 0.0967, step time: 0.2576\n",
      "183/281, train_loss: 0.2814, step time: 0.2561\n",
      "184/281, train_loss: 0.2774, step time: 0.2489\n",
      "185/281, train_loss: 0.3798, step time: 0.2475\n",
      "186/281, train_loss: 0.1018, step time: 0.2442\n",
      "187/281, train_loss: 0.1234, step time: 0.2503\n",
      "188/281, train_loss: 0.0833, step time: 0.2521\n",
      "189/281, train_loss: 0.1661, step time: 0.2461\n",
      "190/281, train_loss: 0.1399, step time: 0.2492\n",
      "191/281, train_loss: 0.1714, step time: 0.2553\n",
      "192/281, train_loss: 0.1135, step time: 0.2537\n",
      "193/281, train_loss: 0.1204, step time: 0.2460\n",
      "194/281, train_loss: 0.1802, step time: 0.2434\n",
      "195/281, train_loss: 0.0619, step time: 0.2502\n",
      "196/281, train_loss: 0.3522, step time: 0.2469\n",
      "197/281, train_loss: 0.1392, step time: 0.2508\n",
      "198/281, train_loss: 0.0711, step time: 0.2464\n",
      "199/281, train_loss: 0.0884, step time: 0.2479\n",
      "200/281, train_loss: 0.2812, step time: 0.2447\n",
      "201/281, train_loss: 0.0794, step time: 0.2426\n",
      "202/281, train_loss: 0.0531, step time: 0.2421\n",
      "203/281, train_loss: 0.0812, step time: 0.2438\n",
      "204/281, train_loss: 0.1342, step time: 0.2473\n",
      "205/281, train_loss: 0.2400, step time: 0.2499\n",
      "206/281, train_loss: 0.0818, step time: 0.2455\n",
      "207/281, train_loss: 0.1079, step time: 0.2599\n",
      "208/281, train_loss: 0.1185, step time: 0.2400\n",
      "209/281, train_loss: 0.1204, step time: 0.2447\n",
      "210/281, train_loss: 0.0903, step time: 0.2436\n",
      "211/281, train_loss: 0.1059, step time: 0.2578\n",
      "212/281, train_loss: 0.1501, step time: 0.2543\n",
      "213/281, train_loss: 0.0979, step time: 0.2453\n",
      "214/281, train_loss: 0.0905, step time: 0.2475\n",
      "215/281, train_loss: 0.1022, step time: 0.2588\n",
      "216/281, train_loss: 0.1035, step time: 0.2542\n",
      "217/281, train_loss: 0.0713, step time: 0.2493\n",
      "218/281, train_loss: 0.2408, step time: 0.2487\n",
      "219/281, train_loss: 0.3311, step time: 0.2441\n",
      "220/281, train_loss: 0.3069, step time: 0.2456\n",
      "221/281, train_loss: 0.1037, step time: 0.2411\n",
      "222/281, train_loss: 0.1073, step time: 0.2409\n",
      "223/281, train_loss: 0.0901, step time: 0.2474\n",
      "224/281, train_loss: 0.1300, step time: 0.2472\n",
      "225/281, train_loss: 0.1484, step time: 0.2496\n",
      "226/281, train_loss: 0.1006, step time: 0.2463\n",
      "227/281, train_loss: 0.0688, step time: 0.2468\n",
      "228/281, train_loss: 0.0528, step time: 0.2515\n",
      "229/281, train_loss: 0.1123, step time: 0.2479\n",
      "230/281, train_loss: 0.2755, step time: 0.2476\n",
      "231/281, train_loss: 0.0879, step time: 0.2499\n",
      "232/281, train_loss: 0.2838, step time: 0.2483\n",
      "233/281, train_loss: 0.0892, step time: 0.2470\n",
      "234/281, train_loss: 0.1012, step time: 0.2481\n",
      "235/281, train_loss: 0.0768, step time: 0.2521\n",
      "236/281, train_loss: 0.0839, step time: 0.2519\n",
      "237/281, train_loss: 0.1245, step time: 0.2466\n",
      "238/281, train_loss: 0.0845, step time: 0.2454\n",
      "239/281, train_loss: 0.1079, step time: 0.2524\n",
      "240/281, train_loss: 0.2970, step time: 0.2489\n",
      "241/281, train_loss: 0.2740, step time: 0.2478\n",
      "242/281, train_loss: 0.1108, step time: 0.2493\n",
      "243/281, train_loss: 0.1761, step time: 0.2521\n",
      "244/281, train_loss: 0.1188, step time: 0.2516\n",
      "245/281, train_loss: 0.2473, step time: 0.2476\n",
      "246/281, train_loss: 0.3180, step time: 0.2463\n",
      "247/281, train_loss: 0.0967, step time: 0.2488\n",
      "248/281, train_loss: 0.0623, step time: 0.2532\n",
      "249/281, train_loss: 0.1371, step time: 0.2488\n",
      "250/281, train_loss: 0.1309, step time: 0.2497\n",
      "251/281, train_loss: 0.1241, step time: 0.2526\n",
      "252/281, train_loss: 0.0805, step time: 0.2598\n",
      "253/281, train_loss: 0.1280, step time: 0.2534\n",
      "254/281, train_loss: 0.1215, step time: 0.2549\n",
      "255/281, train_loss: 0.0979, step time: 0.2467\n",
      "256/281, train_loss: 0.0861, step time: 0.2489\n",
      "257/281, train_loss: 0.0850, step time: 0.2481\n",
      "258/281, train_loss: 0.2728, step time: 0.2460\n",
      "259/281, train_loss: 0.2108, step time: 0.2490\n",
      "260/281, train_loss: 0.2398, step time: 0.2509\n",
      "261/281, train_loss: 0.0710, step time: 0.2475\n",
      "262/281, train_loss: 0.0549, step time: 0.2448\n",
      "263/281, train_loss: 0.1225, step time: 0.2532\n",
      "264/281, train_loss: 0.2760, step time: 0.2512\n",
      "265/281, train_loss: 0.1169, step time: 0.2520\n",
      "266/281, train_loss: 0.1339, step time: 0.2481\n",
      "267/281, train_loss: 0.0995, step time: 0.2542\n",
      "268/281, train_loss: 0.1194, step time: 0.2556\n",
      "269/281, train_loss: 0.1068, step time: 0.2524\n",
      "270/281, train_loss: 0.0874, step time: 0.2998\n",
      "271/281, train_loss: 0.1406, step time: 0.2517\n",
      "272/281, train_loss: 0.1213, step time: 0.2538\n",
      "273/281, train_loss: 0.0976, step time: 0.2523\n",
      "274/281, train_loss: 0.0837, step time: 0.2529\n",
      "275/281, train_loss: 0.2663, step time: 0.2595\n",
      "276/281, train_loss: 0.0830, step time: 0.2687\n",
      "277/281, train_loss: 0.3186, step time: 0.2480\n",
      "278/281, train_loss: 0.1183, step time: 0.2562\n",
      "279/281, train_loss: 0.0987, step time: 0.2489\n",
      "280/281, train_loss: 0.1218, step time: 0.2501\n",
      "281/281, train_loss: 0.1143, step time: 0.2511\n",
      "282/281, train_loss: 0.1557, step time: 0.1513\n",
      "epoch 43 average loss: 0.1337\n",
      "current epoch: 43 current mean dice: 0.8622 tc: 0.8539 wt: 0.9086 et: 0.8339\n",
      "best mean dice: 0.8652 at epoch: 41\n",
      "time consuming of epoch 43 is: 396.1854\n",
      "----------\n",
      "epoch 44/200\n",
      "1/281, train_loss: 0.0663, step time: 0.2628\n",
      "2/281, train_loss: 0.0913, step time: 0.2514\n",
      "3/281, train_loss: 0.0691, step time: 0.2544\n",
      "4/281, train_loss: 0.1297, step time: 0.2552\n",
      "5/281, train_loss: 0.1851, step time: 0.2614\n",
      "6/281, train_loss: 0.0688, step time: 0.2579\n",
      "7/281, train_loss: 0.1069, step time: 0.2593\n",
      "8/281, train_loss: 0.1024, step time: 0.2577\n",
      "9/281, train_loss: 0.1453, step time: 0.2571\n",
      "10/281, train_loss: 0.0760, step time: 0.2557\n",
      "11/281, train_loss: 0.0641, step time: 0.2794\n",
      "12/281, train_loss: 0.2488, step time: 0.2886\n",
      "13/281, train_loss: 0.0941, step time: 0.2585\n",
      "14/281, train_loss: 0.1241, step time: 0.2618\n",
      "15/281, train_loss: 0.1025, step time: 0.2571\n",
      "16/281, train_loss: 0.1015, step time: 0.2559\n",
      "17/281, train_loss: 0.2593, step time: 0.2555\n",
      "18/281, train_loss: 0.2669, step time: 0.2595\n",
      "19/281, train_loss: 0.1309, step time: 0.2657\n",
      "20/281, train_loss: 0.2544, step time: 0.2603\n",
      "21/281, train_loss: 0.0911, step time: 0.2598\n",
      "22/281, train_loss: 0.0743, step time: 0.2599\n",
      "23/281, train_loss: 0.0659, step time: 0.2572\n",
      "24/281, train_loss: 0.1367, step time: 0.2599\n",
      "25/281, train_loss: 0.1301, step time: 0.2498\n",
      "26/281, train_loss: 0.1446, step time: 0.2555\n",
      "27/281, train_loss: 0.1405, step time: 0.2503\n",
      "28/281, train_loss: 0.2502, step time: 0.2612\n",
      "29/281, train_loss: 0.1424, step time: 0.2524\n",
      "30/281, train_loss: 0.1061, step time: 0.2584\n",
      "31/281, train_loss: 0.2454, step time: 0.2576\n",
      "32/281, train_loss: 0.3016, step time: 0.2594\n",
      "33/281, train_loss: 0.1371, step time: 0.2523\n",
      "34/281, train_loss: 0.1286, step time: 0.2498\n",
      "35/281, train_loss: 0.0829, step time: 0.2475\n",
      "36/281, train_loss: 0.1107, step time: 0.2534\n",
      "37/281, train_loss: 0.1225, step time: 0.2643\n",
      "38/281, train_loss: 0.0716, step time: 0.2568\n",
      "39/281, train_loss: 0.0628, step time: 0.2533\n",
      "40/281, train_loss: 0.1743, step time: 0.2573\n",
      "41/281, train_loss: 0.1713, step time: 0.2575\n",
      "42/281, train_loss: 0.1929, step time: 0.2569\n",
      "43/281, train_loss: 0.0785, step time: 0.2584\n",
      "44/281, train_loss: 0.0566, step time: 0.2594\n",
      "45/281, train_loss: 0.1157, step time: 0.2563\n",
      "46/281, train_loss: 0.1284, step time: 0.2586\n",
      "47/281, train_loss: 0.0896, step time: 0.2558\n",
      "48/281, train_loss: 0.0996, step time: 0.2552\n",
      "49/281, train_loss: 0.3654, step time: 0.2535\n",
      "50/281, train_loss: 0.3586, step time: 0.2542\n",
      "51/281, train_loss: 0.2743, step time: 0.2571\n",
      "52/281, train_loss: 0.0560, step time: 0.2515\n",
      "53/281, train_loss: 0.1129, step time: 0.2598\n",
      "54/281, train_loss: 0.0965, step time: 0.2585\n",
      "55/281, train_loss: 0.0998, step time: 0.2586\n",
      "56/281, train_loss: 0.1181, step time: 0.2585\n",
      "57/281, train_loss: 0.0907, step time: 0.2597\n",
      "58/281, train_loss: 0.1018, step time: 0.2600\n",
      "59/281, train_loss: 0.1511, step time: 0.2605\n",
      "60/281, train_loss: 0.0694, step time: 0.2645\n",
      "61/281, train_loss: 0.1255, step time: 0.2575\n",
      "62/281, train_loss: 0.1127, step time: 0.2523\n",
      "63/281, train_loss: 0.1672, step time: 0.2526\n",
      "64/281, train_loss: 0.2124, step time: 0.2571\n",
      "65/281, train_loss: 0.2273, step time: 0.2577\n",
      "66/281, train_loss: 0.2466, step time: 0.2568\n",
      "67/281, train_loss: 0.2428, step time: 0.2592\n",
      "68/281, train_loss: 0.0929, step time: 0.2596\n",
      "69/281, train_loss: 0.0976, step time: 0.2548\n",
      "70/281, train_loss: 0.0846, step time: 0.2580\n",
      "71/281, train_loss: 0.1663, step time: 0.2607\n",
      "72/281, train_loss: 0.1161, step time: 0.2558\n",
      "73/281, train_loss: 0.1575, step time: 0.2563\n",
      "74/281, train_loss: 0.0806, step time: 0.2554\n",
      "75/281, train_loss: 0.1148, step time: 0.2567\n",
      "76/281, train_loss: 0.1002, step time: 0.2584\n",
      "77/281, train_loss: 0.1106, step time: 0.2519\n",
      "78/281, train_loss: 0.1290, step time: 0.2545\n",
      "79/281, train_loss: 0.0637, step time: 0.2545\n",
      "80/281, train_loss: 0.0953, step time: 0.2507\n",
      "81/281, train_loss: 0.0616, step time: 0.2571\n",
      "82/281, train_loss: 0.0931, step time: 0.2528\n",
      "83/281, train_loss: 0.0933, step time: 0.2513\n",
      "84/281, train_loss: 0.1075, step time: 0.2488\n",
      "85/281, train_loss: 0.1219, step time: 0.2592\n",
      "86/281, train_loss: 0.1889, step time: 0.2585\n",
      "87/281, train_loss: 0.1097, step time: 0.2589\n",
      "88/281, train_loss: 0.2605, step time: 0.2580\n",
      "89/281, train_loss: 0.1312, step time: 0.2686\n",
      "90/281, train_loss: 0.0746, step time: 0.2581\n",
      "91/281, train_loss: 0.0719, step time: 0.2581\n",
      "92/281, train_loss: 0.1792, step time: 0.2509\n",
      "93/281, train_loss: 0.0937, step time: 0.2535\n",
      "94/281, train_loss: 0.1064, step time: 0.2542\n",
      "95/281, train_loss: 0.1105, step time: 0.2538\n",
      "96/281, train_loss: 0.2633, step time: 0.2484\n",
      "97/281, train_loss: 0.0991, step time: 0.2572\n",
      "98/281, train_loss: 0.1624, step time: 0.2515\n",
      "99/281, train_loss: 0.2799, step time: 0.2502\n",
      "100/281, train_loss: 0.0763, step time: 0.2470\n",
      "101/281, train_loss: 0.1194, step time: 0.2554\n",
      "102/281, train_loss: 0.0872, step time: 0.2526\n",
      "103/281, train_loss: 0.1402, step time: 0.2579\n",
      "104/281, train_loss: 0.1003, step time: 0.2522\n",
      "105/281, train_loss: 0.1168, step time: 0.2547\n",
      "106/281, train_loss: 0.0822, step time: 0.2554\n",
      "107/281, train_loss: 0.1479, step time: 0.2528\n",
      "108/281, train_loss: 0.1654, step time: 0.2557\n",
      "109/281, train_loss: 0.2653, step time: 0.2552\n",
      "110/281, train_loss: 0.1232, step time: 0.2481\n",
      "111/281, train_loss: 0.1187, step time: 0.2518\n",
      "112/281, train_loss: 0.0816, step time: 0.2533\n",
      "113/281, train_loss: 0.2812, step time: 0.2591\n",
      "114/281, train_loss: 0.1034, step time: 0.2538\n",
      "115/281, train_loss: 0.1677, step time: 0.2540\n",
      "116/281, train_loss: 0.2788, step time: 0.2517\n",
      "117/281, train_loss: 0.0833, step time: 0.2571\n",
      "118/281, train_loss: 0.1235, step time: 0.2529\n",
      "119/281, train_loss: 0.1320, step time: 0.2492\n",
      "120/281, train_loss: 0.0731, step time: 0.2502\n",
      "121/281, train_loss: 0.1724, step time: 0.2536\n",
      "122/281, train_loss: 0.0983, step time: 0.2492\n",
      "123/281, train_loss: 0.2338, step time: 0.2521\n",
      "124/281, train_loss: 0.1277, step time: 0.2546\n",
      "125/281, train_loss: 0.1070, step time: 0.2541\n",
      "126/281, train_loss: 0.1077, step time: 0.2540\n",
      "127/281, train_loss: 0.0899, step time: 0.2509\n",
      "128/281, train_loss: 0.0866, step time: 0.2576\n",
      "129/281, train_loss: 0.1265, step time: 0.2513\n",
      "130/281, train_loss: 0.1018, step time: 0.2548\n",
      "131/281, train_loss: 0.1045, step time: 0.2558\n",
      "132/281, train_loss: 0.1621, step time: 0.2611\n",
      "133/281, train_loss: 0.1425, step time: 0.2557\n",
      "134/281, train_loss: 0.0863, step time: 0.2510\n",
      "135/281, train_loss: 0.2605, step time: 0.2498\n",
      "136/281, train_loss: 0.1203, step time: 0.2486\n",
      "137/281, train_loss: 0.0820, step time: 0.2584\n",
      "138/281, train_loss: 0.1039, step time: 0.2546\n",
      "139/281, train_loss: 0.0854, step time: 0.2519\n",
      "140/281, train_loss: 0.2650, step time: 0.2512\n",
      "141/281, train_loss: 0.2650, step time: 0.2578\n",
      "142/281, train_loss: 0.1129, step time: 0.2563\n",
      "143/281, train_loss: 0.0614, step time: 0.2484\n",
      "144/281, train_loss: 0.2878, step time: 0.2536\n",
      "145/281, train_loss: 0.2728, step time: 0.2525\n",
      "146/281, train_loss: 0.1034, step time: 0.2518\n",
      "147/281, train_loss: 0.1584, step time: 0.2494\n",
      "148/281, train_loss: 0.1288, step time: 0.2513\n",
      "149/281, train_loss: 0.0875, step time: 0.2518\n",
      "150/281, train_loss: 0.1113, step time: 0.2512\n",
      "151/281, train_loss: 0.4570, step time: 0.2574\n",
      "152/281, train_loss: 0.0964, step time: 0.2523\n",
      "153/281, train_loss: 0.1396, step time: 0.2556\n",
      "154/281, train_loss: 0.0737, step time: 0.2570\n",
      "155/281, train_loss: 0.1424, step time: 0.2597\n",
      "156/281, train_loss: 0.0942, step time: 0.2539\n",
      "157/281, train_loss: 0.1293, step time: 0.2604\n",
      "158/281, train_loss: 0.0790, step time: 0.2570\n",
      "159/281, train_loss: 0.1116, step time: 0.2535\n",
      "160/281, train_loss: 0.0743, step time: 0.2499\n",
      "161/281, train_loss: 0.2399, step time: 0.2545\n",
      "162/281, train_loss: 0.1003, step time: 0.2533\n",
      "163/281, train_loss: 0.3148, step time: 0.2579\n",
      "164/281, train_loss: 0.0957, step time: 0.2582\n",
      "165/281, train_loss: 0.1186, step time: 0.2579\n",
      "166/281, train_loss: 0.3317, step time: 0.2556\n",
      "167/281, train_loss: 0.2564, step time: 0.2530\n",
      "168/281, train_loss: 0.1528, step time: 0.2586\n",
      "169/281, train_loss: 0.1341, step time: 0.2576\n",
      "170/281, train_loss: 0.1287, step time: 0.2583\n",
      "171/281, train_loss: 0.1118, step time: 0.2563\n",
      "172/281, train_loss: 0.2373, step time: 0.2551\n",
      "173/281, train_loss: 0.2338, step time: 0.2550\n",
      "174/281, train_loss: 0.0659, step time: 0.2597\n",
      "175/281, train_loss: 0.1019, step time: 0.2566\n",
      "176/281, train_loss: 0.1139, step time: 0.2557\n",
      "177/281, train_loss: 0.1283, step time: 0.2583\n",
      "178/281, train_loss: 0.2539, step time: 0.2531\n",
      "179/281, train_loss: 0.1395, step time: 0.2562\n",
      "180/281, train_loss: 0.1221, step time: 0.2601\n",
      "181/281, train_loss: 0.0666, step time: 0.2558\n",
      "182/281, train_loss: 0.0768, step time: 0.2497\n",
      "183/281, train_loss: 0.0978, step time: 0.2511\n",
      "184/281, train_loss: 0.3153, step time: 0.2576\n",
      "185/281, train_loss: 0.1923, step time: 0.2546\n",
      "186/281, train_loss: 0.0953, step time: 0.2521\n",
      "187/281, train_loss: 0.0901, step time: 0.2473\n",
      "188/281, train_loss: 0.1364, step time: 0.2540\n",
      "189/281, train_loss: 0.1428, step time: 0.2515\n",
      "190/281, train_loss: 0.0833, step time: 0.2585\n",
      "191/281, train_loss: 0.0854, step time: 0.2535\n",
      "192/281, train_loss: 0.2611, step time: 0.2588\n",
      "193/281, train_loss: 0.0595, step time: 0.2558\n",
      "194/281, train_loss: 0.0939, step time: 0.2587\n",
      "195/281, train_loss: 0.0925, step time: 0.2529\n",
      "196/281, train_loss: 0.1134, step time: 0.2655\n",
      "197/281, train_loss: 0.3151, step time: 0.2574\n",
      "198/281, train_loss: 0.0726, step time: 0.2512\n",
      "199/281, train_loss: 0.0680, step time: 0.2539\n",
      "200/281, train_loss: 0.0884, step time: 0.2591\n",
      "201/281, train_loss: 0.1168, step time: 0.2542\n",
      "202/281, train_loss: 0.1180, step time: 0.2485\n",
      "203/281, train_loss: 0.1521, step time: 0.2473\n",
      "204/281, train_loss: 0.1145, step time: 0.2521\n",
      "205/281, train_loss: 0.0729, step time: 0.2534\n",
      "206/281, train_loss: 0.0570, step time: 0.2543\n",
      "207/281, train_loss: 0.0873, step time: 0.2568\n",
      "208/281, train_loss: 0.0845, step time: 0.2611\n",
      "209/281, train_loss: 0.1217, step time: 0.2571\n",
      "210/281, train_loss: 0.1708, step time: 0.2587\n",
      "211/281, train_loss: 0.0927, step time: 0.2553\n",
      "212/281, train_loss: 0.1192, step time: 0.2532\n",
      "213/281, train_loss: 0.1310, step time: 0.2504\n",
      "214/281, train_loss: 0.0633, step time: 0.2528\n",
      "215/281, train_loss: 0.1197, step time: 0.2543\n",
      "216/281, train_loss: 0.1543, step time: 0.2533\n",
      "217/281, train_loss: 0.0840, step time: 0.2528\n",
      "218/281, train_loss: 0.1033, step time: 0.2531\n",
      "219/281, train_loss: 0.0826, step time: 0.2457\n",
      "220/281, train_loss: 0.1605, step time: 0.2480\n",
      "221/281, train_loss: 0.1514, step time: 0.2497\n",
      "222/281, train_loss: 0.0851, step time: 0.2524\n",
      "223/281, train_loss: 0.1115, step time: 0.2498\n",
      "224/281, train_loss: 0.1163, step time: 0.2521\n",
      "225/281, train_loss: 0.0979, step time: 0.2540\n",
      "226/281, train_loss: 0.1049, step time: 0.2520\n",
      "227/281, train_loss: 0.0952, step time: 0.2506\n",
      "228/281, train_loss: 0.0891, step time: 0.2528\n",
      "229/281, train_loss: 0.1188, step time: 0.2568\n",
      "230/281, train_loss: 0.1207, step time: 0.2535\n",
      "231/281, train_loss: 0.1597, step time: 0.2513\n",
      "232/281, train_loss: 0.1957, step time: 0.2551\n",
      "233/281, train_loss: 0.1234, step time: 0.2518\n",
      "234/281, train_loss: 0.1062, step time: 0.2591\n",
      "235/281, train_loss: 0.1794, step time: 0.2570\n",
      "236/281, train_loss: 0.1287, step time: 0.2553\n",
      "237/281, train_loss: 0.0576, step time: 0.2573\n",
      "238/281, train_loss: 0.1505, step time: 0.2505\n",
      "239/281, train_loss: 0.0884, step time: 0.2533\n",
      "240/281, train_loss: 0.0913, step time: 0.2539\n",
      "241/281, train_loss: 0.1264, step time: 0.2564\n",
      "242/281, train_loss: 0.1046, step time: 0.2501\n",
      "243/281, train_loss: 0.1821, step time: 0.2505\n",
      "244/281, train_loss: 0.0828, step time: 0.2532\n",
      "245/281, train_loss: 0.2784, step time: 0.2567\n",
      "246/281, train_loss: 0.2682, step time: 0.2612\n",
      "247/281, train_loss: 0.2591, step time: 0.2557\n",
      "248/281, train_loss: 0.0959, step time: 0.2588\n",
      "249/281, train_loss: 0.1403, step time: 0.2546\n",
      "250/281, train_loss: 0.0847, step time: 0.2554\n",
      "251/281, train_loss: 0.0921, step time: 0.2521\n",
      "252/281, train_loss: 0.0662, step time: 0.2530\n",
      "253/281, train_loss: 0.2595, step time: 0.2509\n",
      "254/281, train_loss: 0.0698, step time: 0.2484\n",
      "255/281, train_loss: 0.4307, step time: 0.2514\n",
      "256/281, train_loss: 0.0800, step time: 0.2564\n",
      "257/281, train_loss: 0.1020, step time: 0.2578\n",
      "258/281, train_loss: 0.1136, step time: 0.2567\n",
      "259/281, train_loss: 0.1169, step time: 0.2527\n",
      "260/281, train_loss: 0.2070, step time: 0.2572\n",
      "261/281, train_loss: 0.1039, step time: 0.2597\n",
      "262/281, train_loss: 0.1063, step time: 0.2520\n",
      "263/281, train_loss: 0.1130, step time: 0.2597\n",
      "264/281, train_loss: 0.0920, step time: 0.2536\n",
      "265/281, train_loss: 0.2764, step time: 0.2551\n",
      "266/281, train_loss: 0.1235, step time: 0.2505\n",
      "267/281, train_loss: 0.0900, step time: 0.2534\n",
      "268/281, train_loss: 0.0742, step time: 0.2512\n",
      "269/281, train_loss: 0.0880, step time: 0.2469\n",
      "270/281, train_loss: 0.0990, step time: 0.2487\n",
      "271/281, train_loss: 0.1237, step time: 0.2511\n",
      "272/281, train_loss: 0.1025, step time: 0.2483\n",
      "273/281, train_loss: 0.1022, step time: 0.2506\n",
      "274/281, train_loss: 0.0556, step time: 0.2481\n",
      "275/281, train_loss: 0.0956, step time: 0.2494\n",
      "276/281, train_loss: 0.1346, step time: 0.2464\n",
      "277/281, train_loss: 0.1277, step time: 0.2498\n",
      "278/281, train_loss: 0.1435, step time: 0.2533\n",
      "279/281, train_loss: 0.3338, step time: 0.2519\n",
      "280/281, train_loss: 0.0740, step time: 0.2500\n",
      "281/281, train_loss: 0.1089, step time: 0.2472\n",
      "282/281, train_loss: 0.1276, step time: 0.1482\n",
      "epoch 44 average loss: 0.1364\n",
      "current epoch: 44 current mean dice: 0.8644 tc: 0.8490 wt: 0.9112 et: 0.8454\n",
      "best mean dice: 0.8652 at epoch: 41\n",
      "time consuming of epoch 44 is: 405.1681\n",
      "----------\n",
      "epoch 45/200\n",
      "1/281, train_loss: 0.2429, step time: 0.2845\n",
      "2/281, train_loss: 0.0855, step time: 0.2588\n",
      "3/281, train_loss: 0.1294, step time: 0.2586\n",
      "4/281, train_loss: 0.1134, step time: 0.2599\n",
      "5/281, train_loss: 0.0909, step time: 0.2500\n",
      "6/281, train_loss: 0.2953, step time: 0.2518\n",
      "7/281, train_loss: 0.0667, step time: 0.2585\n",
      "8/281, train_loss: 0.0988, step time: 0.2832\n",
      "9/281, train_loss: 0.0943, step time: 0.2491\n",
      "10/281, train_loss: 0.0812, step time: 0.2551\n",
      "11/281, train_loss: 0.0856, step time: 0.2512\n",
      "12/281, train_loss: 0.1012, step time: 0.2727\n",
      "13/281, train_loss: 0.0920, step time: 0.2496\n",
      "14/281, train_loss: 0.1288, step time: 0.2492\n",
      "15/281, train_loss: 0.0564, step time: 0.2521\n",
      "16/281, train_loss: 0.1187, step time: 0.2539\n",
      "17/281, train_loss: 0.2607, step time: 0.2520\n",
      "18/281, train_loss: 0.1405, step time: 0.2509\n",
      "19/281, train_loss: 0.2171, step time: 0.2562\n",
      "20/281, train_loss: 0.1511, step time: 0.2480\n",
      "21/281, train_loss: 0.1075, step time: 0.2515\n",
      "22/281, train_loss: 0.1101, step time: 0.2495\n",
      "23/281, train_loss: 0.1513, step time: 0.2505\n",
      "24/281, train_loss: 0.0966, step time: 0.2550\n",
      "25/281, train_loss: 0.0799, step time: 0.2480\n",
      "26/281, train_loss: 0.0939, step time: 0.2564\n",
      "27/281, train_loss: 0.2690, step time: 0.2472\n",
      "28/281, train_loss: 0.1136, step time: 0.2506\n",
      "29/281, train_loss: 0.1443, step time: 0.2708\n",
      "30/281, train_loss: 0.0545, step time: 0.2566\n",
      "31/281, train_loss: 0.0656, step time: 0.2539\n",
      "32/281, train_loss: 0.1305, step time: 0.2570\n",
      "33/281, train_loss: 0.1710, step time: 0.2535\n",
      "34/281, train_loss: 0.1385, step time: 0.2537\n",
      "35/281, train_loss: 0.0925, step time: 0.2488\n",
      "36/281, train_loss: 0.1189, step time: 0.2453\n",
      "37/281, train_loss: 0.0563, step time: 0.2564\n",
      "38/281, train_loss: 0.0858, step time: 0.2586\n",
      "39/281, train_loss: 0.0763, step time: 0.2533\n",
      "40/281, train_loss: 0.1756, step time: 0.2552\n",
      "41/281, train_loss: 0.1639, step time: 0.2579\n",
      "42/281, train_loss: 0.1422, step time: 0.2566\n",
      "43/281, train_loss: 0.1252, step time: 0.2556\n",
      "44/281, train_loss: 0.1455, step time: 0.2640\n",
      "45/281, train_loss: 0.2528, step time: 0.2538\n",
      "46/281, train_loss: 0.0694, step time: 0.2594\n",
      "47/281, train_loss: 0.2941, step time: 0.2608\n",
      "48/281, train_loss: 0.0906, step time: 0.2597\n",
      "49/281, train_loss: 0.2487, step time: 0.2557\n",
      "50/281, train_loss: 0.0796, step time: 0.2520\n",
      "51/281, train_loss: 0.3012, step time: 0.2477\n",
      "52/281, train_loss: 0.1248, step time: 0.2586\n",
      "53/281, train_loss: 0.0681, step time: 0.2589\n",
      "54/281, train_loss: 0.1114, step time: 0.2638\n",
      "55/281, train_loss: 0.0856, step time: 0.2560\n",
      "56/281, train_loss: 0.0958, step time: 0.2556\n",
      "57/281, train_loss: 0.1059, step time: 0.2572\n",
      "58/281, train_loss: 0.1698, step time: 0.2625\n",
      "59/281, train_loss: 0.1505, step time: 0.2636\n",
      "60/281, train_loss: 0.2294, step time: 0.2594\n",
      "61/281, train_loss: 0.1480, step time: 0.2534\n",
      "62/281, train_loss: 0.0767, step time: 0.2503\n",
      "63/281, train_loss: 0.1209, step time: 0.2533\n",
      "64/281, train_loss: 0.0582, step time: 0.2496\n",
      "65/281, train_loss: 0.1454, step time: 0.2546\n",
      "66/281, train_loss: 0.0847, step time: 0.2527\n",
      "67/281, train_loss: 0.0620, step time: 0.2532\n",
      "68/281, train_loss: 0.0902, step time: 0.2570\n",
      "69/281, train_loss: 0.0587, step time: 0.2565\n",
      "70/281, train_loss: 0.3079, step time: 0.2552\n",
      "71/281, train_loss: 0.1771, step time: 0.2521\n",
      "72/281, train_loss: 0.2976, step time: 0.2571\n",
      "73/281, train_loss: 0.0728, step time: 0.2602\n",
      "74/281, train_loss: 0.0793, step time: 0.2526\n",
      "75/281, train_loss: 0.1630, step time: 0.2526\n",
      "76/281, train_loss: 0.1230, step time: 0.2632\n",
      "77/281, train_loss: 0.0797, step time: 0.2550\n",
      "78/281, train_loss: 0.0756, step time: 0.2555\n",
      "79/281, train_loss: 0.0620, step time: 0.2506\n",
      "80/281, train_loss: 0.0967, step time: 0.2515\n",
      "81/281, train_loss: 0.0952, step time: 0.2509\n",
      "82/281, train_loss: 0.0815, step time: 0.2549\n",
      "83/281, train_loss: 0.0766, step time: 0.2509\n",
      "84/281, train_loss: 0.0823, step time: 0.2556\n",
      "85/281, train_loss: 0.0635, step time: 0.2558\n",
      "86/281, train_loss: 0.2410, step time: 0.2532\n",
      "87/281, train_loss: 0.0869, step time: 0.2578\n",
      "88/281, train_loss: 0.2504, step time: 0.2626\n",
      "89/281, train_loss: 0.2688, step time: 0.2602\n",
      "90/281, train_loss: 0.0751, step time: 0.2581\n",
      "91/281, train_loss: 0.3483, step time: 0.2587\n",
      "92/281, train_loss: 0.1267, step time: 0.2503\n",
      "93/281, train_loss: 0.0837, step time: 0.2534\n",
      "94/281, train_loss: 0.0976, step time: 0.2533\n",
      "95/281, train_loss: 0.0860, step time: 0.2512\n",
      "96/281, train_loss: 0.1650, step time: 0.2556\n",
      "97/281, train_loss: 0.0940, step time: 0.2504\n",
      "98/281, train_loss: 0.2498, step time: 0.2543\n",
      "99/281, train_loss: 0.2489, step time: 0.2527\n",
      "100/281, train_loss: 0.1565, step time: 0.2515\n",
      "101/281, train_loss: 0.1966, step time: 0.2513\n",
      "102/281, train_loss: 0.0685, step time: 0.2589\n",
      "103/281, train_loss: 0.1247, step time: 0.2538\n",
      "104/281, train_loss: 0.2810, step time: 0.2518\n",
      "105/281, train_loss: 0.2547, step time: 0.2496\n",
      "106/281, train_loss: 0.1652, step time: 0.2566\n",
      "107/281, train_loss: 0.0866, step time: 0.2472\n",
      "108/281, train_loss: 0.1608, step time: 0.2568\n",
      "109/281, train_loss: 0.0835, step time: 0.2573\n",
      "110/281, train_loss: 0.1495, step time: 0.2538\n",
      "111/281, train_loss: 0.0973, step time: 0.2485\n",
      "112/281, train_loss: 0.1403, step time: 0.2497\n",
      "113/281, train_loss: 0.0824, step time: 0.2489\n",
      "114/281, train_loss: 0.2528, step time: 0.2518\n",
      "115/281, train_loss: 0.0958, step time: 0.2520\n",
      "116/281, train_loss: 0.2415, step time: 0.2553\n",
      "117/281, train_loss: 0.2531, step time: 0.2481\n",
      "118/281, train_loss: 0.1394, step time: 0.2485\n",
      "119/281, train_loss: 0.0688, step time: 0.2525\n",
      "120/281, train_loss: 0.1270, step time: 0.2480\n",
      "121/281, train_loss: 0.1936, step time: 0.2498\n",
      "122/281, train_loss: 0.0859, step time: 0.2553\n",
      "123/281, train_loss: 0.1047, step time: 0.2513\n",
      "124/281, train_loss: 0.2060, step time: 0.2502\n",
      "125/281, train_loss: 0.4065, step time: 0.2524\n",
      "126/281, train_loss: 0.1268, step time: 0.2524\n",
      "127/281, train_loss: 0.1477, step time: 0.2522\n",
      "128/281, train_loss: 0.1201, step time: 0.2555\n",
      "129/281, train_loss: 0.1095, step time: 0.2521\n",
      "130/281, train_loss: 0.1006, step time: 0.2543\n",
      "131/281, train_loss: 0.2556, step time: 0.2534\n",
      "132/281, train_loss: 0.0600, step time: 0.2473\n",
      "133/281, train_loss: 0.1648, step time: 0.2498\n",
      "134/281, train_loss: 0.1061, step time: 0.2708\n",
      "135/281, train_loss: 0.3006, step time: 0.2596\n",
      "136/281, train_loss: 0.0769, step time: 0.2503\n",
      "137/281, train_loss: 0.0997, step time: 0.2459\n",
      "138/281, train_loss: 0.0774, step time: 0.2533\n",
      "139/281, train_loss: 0.0858, step time: 0.2522\n",
      "140/281, train_loss: 0.0973, step time: 0.2537\n",
      "141/281, train_loss: 0.0685, step time: 0.2544\n",
      "142/281, train_loss: 0.1433, step time: 0.2525\n",
      "143/281, train_loss: 0.1291, step time: 0.2541\n",
      "144/281, train_loss: 0.1577, step time: 0.2486\n",
      "145/281, train_loss: 0.1328, step time: 0.2503\n",
      "146/281, train_loss: 0.1092, step time: 0.2511\n",
      "147/281, train_loss: 0.1437, step time: 0.2497\n",
      "148/281, train_loss: 0.0914, step time: 0.2497\n",
      "149/281, train_loss: 0.2781, step time: 0.2477\n",
      "150/281, train_loss: 0.1396, step time: 0.2527\n",
      "151/281, train_loss: 0.0997, step time: 0.2479\n",
      "152/281, train_loss: 0.1771, step time: 0.2498\n",
      "153/281, train_loss: 0.0963, step time: 0.2462\n",
      "154/281, train_loss: 0.1156, step time: 0.2494\n",
      "155/281, train_loss: 0.1457, step time: 0.2475\n",
      "156/281, train_loss: 0.0762, step time: 0.2478\n",
      "157/281, train_loss: 0.1037, step time: 0.2436\n",
      "158/281, train_loss: 0.0816, step time: 0.2485\n",
      "159/281, train_loss: 0.0767, step time: 0.2430\n",
      "160/281, train_loss: 0.1402, step time: 0.2488\n",
      "161/281, train_loss: 0.2690, step time: 0.2566\n",
      "162/281, train_loss: 0.1294, step time: 0.2558\n",
      "163/281, train_loss: 0.1657, step time: 0.2550\n",
      "164/281, train_loss: 0.1042, step time: 0.2513\n",
      "165/281, train_loss: 0.0957, step time: 0.2448\n",
      "166/281, train_loss: 0.1166, step time: 0.2511\n",
      "167/281, train_loss: 0.0563, step time: 0.2483\n",
      "168/281, train_loss: 0.1520, step time: 0.2528\n",
      "169/281, train_loss: 0.1128, step time: 0.2539\n",
      "170/281, train_loss: 0.2815, step time: 0.2498\n",
      "171/281, train_loss: 0.1156, step time: 0.2500\n",
      "172/281, train_loss: 0.0949, step time: 0.2537\n",
      "173/281, train_loss: 0.0843, step time: 0.2546\n",
      "174/281, train_loss: 0.0704, step time: 0.2464\n",
      "175/281, train_loss: 0.1792, step time: 0.2630\n",
      "176/281, train_loss: 0.0508, step time: 0.2517\n",
      "177/281, train_loss: 0.1093, step time: 0.2533\n",
      "178/281, train_loss: 0.1370, step time: 0.2516\n",
      "179/281, train_loss: 0.0775, step time: 0.2503\n",
      "180/281, train_loss: 0.1107, step time: 0.2571\n",
      "181/281, train_loss: 0.2327, step time: 0.2546\n",
      "182/281, train_loss: 0.0776, step time: 0.2536\n",
      "183/281, train_loss: 0.1374, step time: 0.2468\n",
      "184/281, train_loss: 0.0911, step time: 0.2521\n",
      "185/281, train_loss: 0.1468, step time: 0.2508\n",
      "186/281, train_loss: 0.2301, step time: 0.2475\n",
      "187/281, train_loss: 0.1887, step time: 0.2477\n",
      "188/281, train_loss: 0.1574, step time: 0.2557\n",
      "189/281, train_loss: 0.2634, step time: 0.2588\n",
      "190/281, train_loss: 0.4471, step time: 0.2470\n",
      "191/281, train_loss: 0.0787, step time: 0.2452\n",
      "192/281, train_loss: 0.1059, step time: 0.2450\n",
      "193/281, train_loss: 0.0879, step time: 0.2426\n",
      "194/281, train_loss: 0.0970, step time: 0.2763\n",
      "195/281, train_loss: 0.0863, step time: 0.2662\n",
      "196/281, train_loss: 0.0830, step time: 0.2427\n",
      "197/281, train_loss: 0.1283, step time: 0.2448\n",
      "198/281, train_loss: 0.1115, step time: 0.2490\n",
      "199/281, train_loss: 0.1717, step time: 0.2441\n",
      "200/281, train_loss: 0.1512, step time: 0.2523\n",
      "201/281, train_loss: 0.1233, step time: 0.2486\n",
      "202/281, train_loss: 0.1238, step time: 0.2537\n",
      "203/281, train_loss: 0.0909, step time: 0.2551\n",
      "204/281, train_loss: 0.1694, step time: 0.2510\n",
      "205/281, train_loss: 0.0888, step time: 0.2517\n",
      "206/281, train_loss: 0.1245, step time: 0.2500\n",
      "207/281, train_loss: 0.2954, step time: 0.2470\n",
      "208/281, train_loss: 0.0984, step time: 0.2529\n",
      "209/281, train_loss: 0.1333, step time: 0.2514\n",
      "210/281, train_loss: 0.0856, step time: 0.2506\n",
      "211/281, train_loss: 0.0948, step time: 0.2494\n",
      "212/281, train_loss: 0.1107, step time: 0.2472\n",
      "213/281, train_loss: 0.0683, step time: 0.2440\n",
      "214/281, train_loss: 0.2657, step time: 0.2522\n",
      "215/281, train_loss: 0.0699, step time: 0.2522\n",
      "216/281, train_loss: 0.1895, step time: 0.2457\n",
      "217/281, train_loss: 0.1536, step time: 0.2480\n",
      "218/281, train_loss: 0.0686, step time: 0.2477\n",
      "219/281, train_loss: 0.1189, step time: 0.2502\n",
      "220/281, train_loss: 0.1085, step time: 0.2479\n",
      "221/281, train_loss: 0.1283, step time: 0.2415\n",
      "222/281, train_loss: 0.1337, step time: 0.2498\n",
      "223/281, train_loss: 0.0754, step time: 0.2501\n",
      "224/281, train_loss: 0.1828, step time: 0.2541\n",
      "225/281, train_loss: 0.2804, step time: 0.2496\n",
      "226/281, train_loss: 0.1465, step time: 0.2513\n",
      "227/281, train_loss: 0.0997, step time: 0.2487\n",
      "228/281, train_loss: 0.1167, step time: 0.2480\n",
      "229/281, train_loss: 0.2176, step time: 0.2483\n",
      "230/281, train_loss: 0.0555, step time: 0.2471\n",
      "231/281, train_loss: 0.1483, step time: 0.2494\n",
      "232/281, train_loss: 0.3252, step time: 0.2533\n",
      "233/281, train_loss: 0.1079, step time: 0.2551\n",
      "234/281, train_loss: 0.1513, step time: 0.2562\n",
      "235/281, train_loss: 0.2497, step time: 0.2472\n",
      "236/281, train_loss: 0.1477, step time: 0.2467\n",
      "237/281, train_loss: 0.1341, step time: 0.2500\n",
      "238/281, train_loss: 0.0595, step time: 0.2575\n",
      "239/281, train_loss: 0.1445, step time: 0.2450\n",
      "240/281, train_loss: 0.2562, step time: 0.2451\n",
      "241/281, train_loss: 0.1225, step time: 0.2446\n",
      "242/281, train_loss: 0.2409, step time: 0.2553\n",
      "243/281, train_loss: 0.1710, step time: 0.2508\n",
      "244/281, train_loss: 0.2482, step time: 0.2482\n",
      "245/281, train_loss: 0.0926, step time: 0.2470\n",
      "246/281, train_loss: 0.0716, step time: 0.2497\n",
      "247/281, train_loss: 0.1347, step time: 0.2467\n",
      "248/281, train_loss: 0.1212, step time: 0.2464\n",
      "249/281, train_loss: 0.1072, step time: 0.2486\n",
      "250/281, train_loss: 0.0872, step time: 0.2494\n",
      "251/281, train_loss: 0.0894, step time: 0.2493\n",
      "252/281, train_loss: 0.0710, step time: 0.2544\n",
      "253/281, train_loss: 0.0964, step time: 0.2486\n",
      "254/281, train_loss: 0.2378, step time: 0.2481\n",
      "255/281, train_loss: 0.1233, step time: 0.2457\n",
      "256/281, train_loss: 0.0598, step time: 0.2516\n",
      "257/281, train_loss: 0.1133, step time: 0.2477\n",
      "258/281, train_loss: 0.0821, step time: 0.2462\n",
      "259/281, train_loss: 0.0822, step time: 0.2521\n",
      "260/281, train_loss: 0.0745, step time: 0.2473\n",
      "261/281, train_loss: 0.0866, step time: 0.2427\n",
      "262/281, train_loss: 0.1166, step time: 0.2545\n",
      "263/281, train_loss: 0.0788, step time: 0.2518\n",
      "264/281, train_loss: 0.1274, step time: 0.2518\n",
      "265/281, train_loss: 0.0545, step time: 0.2498\n",
      "266/281, train_loss: 0.2249, step time: 0.2494\n",
      "267/281, train_loss: 0.1204, step time: 0.2521\n",
      "268/281, train_loss: 0.1452, step time: 0.2551\n",
      "269/281, train_loss: 0.1076, step time: 0.2526\n",
      "270/281, train_loss: 0.2600, step time: 0.2554\n",
      "271/281, train_loss: 0.1076, step time: 0.2509\n",
      "272/281, train_loss: 0.2319, step time: 0.2566\n",
      "273/281, train_loss: 0.0688, step time: 0.2554\n",
      "274/281, train_loss: 0.1043, step time: 0.2491\n",
      "275/281, train_loss: 0.0593, step time: 0.2492\n",
      "276/281, train_loss: 0.0822, step time: 0.2521\n",
      "277/281, train_loss: 0.1356, step time: 0.2449\n",
      "278/281, train_loss: 0.1288, step time: 0.2541\n",
      "279/281, train_loss: 0.0774, step time: 0.2511\n",
      "280/281, train_loss: 0.1074, step time: 0.2533\n",
      "281/281, train_loss: 0.0876, step time: 0.2498\n",
      "282/281, train_loss: 0.1158, step time: 0.1498\n",
      "epoch 45 average loss: 0.1357\n",
      "saved new best metric model\n",
      "current epoch: 45 current mean dice: 0.8670 tc: 0.8582 wt: 0.9100 et: 0.8429\n",
      "best mean dice: 0.8670 at epoch: 45\n",
      "time consuming of epoch 45 is: 391.7699\n",
      "----------\n",
      "epoch 46/200\n",
      "1/281, train_loss: 0.1002, step time: 0.2564\n",
      "2/281, train_loss: 0.0820, step time: 0.2548\n",
      "3/281, train_loss: 0.0635, step time: 0.2570\n",
      "4/281, train_loss: 0.0725, step time: 0.2617\n",
      "5/281, train_loss: 0.0838, step time: 0.2483\n",
      "6/281, train_loss: 0.0984, step time: 0.2463\n",
      "7/281, train_loss: 0.1077, step time: 0.2542\n",
      "8/281, train_loss: 0.1000, step time: 0.2564\n",
      "9/281, train_loss: 0.0954, step time: 0.2552\n",
      "10/281, train_loss: 0.1144, step time: 0.2539\n",
      "11/281, train_loss: 0.1106, step time: 0.2510\n",
      "12/281, train_loss: 0.1088, step time: 0.2521\n",
      "13/281, train_loss: 0.0841, step time: 0.2554\n",
      "14/281, train_loss: 0.1033, step time: 0.2584\n",
      "15/281, train_loss: 0.2475, step time: 0.2546\n",
      "16/281, train_loss: 0.1353, step time: 0.2489\n",
      "17/281, train_loss: 0.0971, step time: 0.2532\n",
      "18/281, train_loss: 0.0762, step time: 0.2523\n",
      "19/281, train_loss: 0.0988, step time: 0.2516\n",
      "20/281, train_loss: 0.0899, step time: 0.2548\n",
      "21/281, train_loss: 0.1083, step time: 0.2565\n",
      "22/281, train_loss: 0.1013, step time: 0.2535\n",
      "23/281, train_loss: 0.2543, step time: 0.2496\n",
      "24/281, train_loss: 0.2658, step time: 0.2594\n",
      "25/281, train_loss: 0.1074, step time: 0.2554\n",
      "26/281, train_loss: 0.3177, step time: 0.2555\n",
      "27/281, train_loss: 0.2772, step time: 0.2538\n",
      "28/281, train_loss: 0.1781, step time: 0.2513\n",
      "29/281, train_loss: 0.1162, step time: 0.2559\n",
      "30/281, train_loss: 0.0743, step time: 0.2592\n",
      "31/281, train_loss: 0.0942, step time: 0.2546\n",
      "32/281, train_loss: 0.0640, step time: 0.2573\n",
      "33/281, train_loss: 0.0725, step time: 0.2592\n",
      "34/281, train_loss: 0.0893, step time: 0.2620\n",
      "35/281, train_loss: 0.1129, step time: 0.2711\n",
      "36/281, train_loss: 0.1054, step time: 0.2550\n",
      "37/281, train_loss: 0.0883, step time: 0.2601\n",
      "38/281, train_loss: 0.1853, step time: 0.2711\n",
      "39/281, train_loss: 0.1157, step time: 0.2558\n",
      "40/281, train_loss: 0.0716, step time: 0.2623\n",
      "41/281, train_loss: 0.2746, step time: 0.2519\n",
      "42/281, train_loss: 0.0966, step time: 0.2644\n",
      "43/281, train_loss: 0.1118, step time: 0.2704\n",
      "44/281, train_loss: 0.1204, step time: 0.2556\n",
      "45/281, train_loss: 0.0614, step time: 0.2549\n",
      "46/281, train_loss: 0.1522, step time: 0.2577\n",
      "47/281, train_loss: 0.0592, step time: 0.2503\n",
      "48/281, train_loss: 0.1297, step time: 0.2520\n",
      "49/281, train_loss: 0.1757, step time: 0.2581\n",
      "50/281, train_loss: 0.1114, step time: 0.2594\n",
      "51/281, train_loss: 0.1147, step time: 0.2559\n",
      "52/281, train_loss: 0.1021, step time: 0.2572\n",
      "53/281, train_loss: 0.1188, step time: 0.2558\n",
      "54/281, train_loss: 0.1941, step time: 0.2581\n",
      "55/281, train_loss: 0.1527, step time: 0.2558\n",
      "56/281, train_loss: 0.1040, step time: 0.2592\n",
      "57/281, train_loss: 0.1020, step time: 0.2564\n",
      "58/281, train_loss: 0.0829, step time: 0.2570\n",
      "59/281, train_loss: 0.1290, step time: 0.2537\n",
      "60/281, train_loss: 0.1066, step time: 0.2579\n",
      "61/281, train_loss: 0.0730, step time: 0.2580\n",
      "62/281, train_loss: 0.0989, step time: 0.2586\n",
      "63/281, train_loss: 0.1638, step time: 0.2594\n",
      "64/281, train_loss: 0.0710, step time: 0.2609\n",
      "65/281, train_loss: 0.1078, step time: 0.2584\n",
      "66/281, train_loss: 0.0962, step time: 0.2579\n",
      "67/281, train_loss: 0.0874, step time: 0.2556\n",
      "68/281, train_loss: 0.0779, step time: 0.2585\n",
      "69/281, train_loss: 0.0831, step time: 0.2643\n",
      "70/281, train_loss: 0.1124, step time: 0.2560\n",
      "71/281, train_loss: 0.1291, step time: 0.2563\n",
      "72/281, train_loss: 0.0992, step time: 0.2537\n",
      "73/281, train_loss: 0.2152, step time: 0.2569\n",
      "74/281, train_loss: 0.1470, step time: 0.2630\n",
      "75/281, train_loss: 0.1233, step time: 0.2583\n",
      "76/281, train_loss: 0.0811, step time: 0.2578\n",
      "77/281, train_loss: 0.1448, step time: 0.2550\n",
      "78/281, train_loss: 0.0985, step time: 0.2537\n",
      "79/281, train_loss: 0.1142, step time: 0.2575\n",
      "80/281, train_loss: 0.0957, step time: 0.2592\n",
      "81/281, train_loss: 0.2450, step time: 0.2601\n",
      "82/281, train_loss: 0.0713, step time: 0.2563\n",
      "83/281, train_loss: 0.0967, step time: 0.2568\n",
      "84/281, train_loss: 0.4027, step time: 0.2616\n",
      "85/281, train_loss: 0.3108, step time: 0.2529\n",
      "86/281, train_loss: 0.1392, step time: 0.2522\n",
      "87/281, train_loss: 0.2915, step time: 0.2488\n",
      "88/281, train_loss: 0.1401, step time: 0.2568\n",
      "89/281, train_loss: 0.0903, step time: 0.2557\n",
      "90/281, train_loss: 0.2314, step time: 0.2582\n",
      "91/281, train_loss: 0.0988, step time: 0.2587\n",
      "92/281, train_loss: 0.0921, step time: 0.2575\n",
      "93/281, train_loss: 0.0840, step time: 0.2664\n",
      "94/281, train_loss: 0.0715, step time: 0.2667\n",
      "95/281, train_loss: 0.1324, step time: 0.2587\n",
      "96/281, train_loss: 0.0929, step time: 0.2503\n",
      "97/281, train_loss: 0.0950, step time: 0.2511\n",
      "98/281, train_loss: 0.0948, step time: 0.2533\n",
      "99/281, train_loss: 0.2805, step time: 0.2729\n",
      "100/281, train_loss: 0.0607, step time: 0.2530\n",
      "101/281, train_loss: 0.1568, step time: 0.2526\n",
      "102/281, train_loss: 0.0951, step time: 0.2569\n",
      "103/281, train_loss: 0.0989, step time: 0.2522\n",
      "104/281, train_loss: 0.0927, step time: 0.2494\n",
      "105/281, train_loss: 0.0801, step time: 0.2499\n",
      "106/281, train_loss: 0.1240, step time: 0.2520\n",
      "107/281, train_loss: 0.2421, step time: 0.2558\n",
      "108/281, train_loss: 0.1544, step time: 0.2513\n",
      "109/281, train_loss: 0.1498, step time: 0.2517\n",
      "110/281, train_loss: 0.0796, step time: 0.2582\n",
      "111/281, train_loss: 0.2377, step time: 0.2620\n",
      "112/281, train_loss: 0.0750, step time: 0.2569\n",
      "113/281, train_loss: 0.1586, step time: 0.2543\n",
      "114/281, train_loss: 0.2442, step time: 0.2577\n",
      "115/281, train_loss: 0.1204, step time: 0.2621\n",
      "116/281, train_loss: 0.3258, step time: 0.2572\n",
      "117/281, train_loss: 0.0803, step time: 0.2572\n",
      "118/281, train_loss: 0.1414, step time: 0.2569\n",
      "119/281, train_loss: 0.0775, step time: 0.2579\n",
      "120/281, train_loss: 0.0956, step time: 0.2593\n",
      "121/281, train_loss: 0.1385, step time: 0.2530\n",
      "122/281, train_loss: 0.1168, step time: 0.2502\n",
      "123/281, train_loss: 0.0977, step time: 0.2565\n",
      "124/281, train_loss: 0.1032, step time: 0.2508\n",
      "125/281, train_loss: 0.0753, step time: 0.2495\n",
      "126/281, train_loss: 0.1315, step time: 0.2528\n",
      "127/281, train_loss: 0.0724, step time: 0.2566\n",
      "128/281, train_loss: 0.0635, step time: 0.2588\n",
      "129/281, train_loss: 0.0919, step time: 0.2582\n",
      "130/281, train_loss: 0.2316, step time: 0.2595\n",
      "131/281, train_loss: 0.0953, step time: 0.2597\n",
      "132/281, train_loss: 0.0668, step time: 0.2580\n",
      "133/281, train_loss: 0.1289, step time: 0.2573\n",
      "134/281, train_loss: 0.2439, step time: 0.2588\n",
      "135/281, train_loss: 0.0991, step time: 0.2553\n",
      "136/281, train_loss: 0.0975, step time: 0.2544\n",
      "137/281, train_loss: 0.0951, step time: 0.2593\n",
      "138/281, train_loss: 0.1743, step time: 0.2579\n",
      "139/281, train_loss: 0.0702, step time: 0.2579\n",
      "140/281, train_loss: 0.0919, step time: 0.2502\n",
      "141/281, train_loss: 0.0781, step time: 0.2513\n",
      "142/281, train_loss: 0.2997, step time: 0.2528\n",
      "143/281, train_loss: 0.2343, step time: 0.2552\n",
      "144/281, train_loss: 0.0954, step time: 0.2539\n",
      "145/281, train_loss: 0.1442, step time: 0.2579\n",
      "146/281, train_loss: 0.1268, step time: 0.2581\n",
      "147/281, train_loss: 0.1167, step time: 0.2651\n",
      "148/281, train_loss: 0.1087, step time: 0.2522\n",
      "149/281, train_loss: 0.0757, step time: 0.2514\n",
      "150/281, train_loss: 0.1195, step time: 0.2514\n",
      "151/281, train_loss: 0.0991, step time: 0.2503\n",
      "152/281, train_loss: 0.0989, step time: 0.2551\n",
      "153/281, train_loss: 0.0951, step time: 0.2531\n",
      "154/281, train_loss: 0.0832, step time: 0.2539\n",
      "155/281, train_loss: 0.0818, step time: 0.2543\n",
      "156/281, train_loss: 0.0374, step time: 0.2505\n",
      "157/281, train_loss: 0.1018, step time: 0.2596\n",
      "158/281, train_loss: 0.0803, step time: 0.2524\n",
      "159/281, train_loss: 0.2711, step time: 0.2631\n",
      "160/281, train_loss: 0.1129, step time: 0.2546\n",
      "161/281, train_loss: 0.0916, step time: 0.2639\n",
      "162/281, train_loss: 0.0822, step time: 0.2522\n",
      "163/281, train_loss: 0.0795, step time: 0.2590\n",
      "164/281, train_loss: 0.2630, step time: 0.2490\n",
      "165/281, train_loss: 0.0888, step time: 0.2528\n",
      "166/281, train_loss: 0.1171, step time: 0.2619\n",
      "167/281, train_loss: 0.1326, step time: 0.2569\n",
      "168/281, train_loss: 0.1134, step time: 0.2542\n",
      "169/281, train_loss: 0.2166, step time: 0.2600\n",
      "170/281, train_loss: 0.0648, step time: 0.2569\n",
      "171/281, train_loss: 0.0908, step time: 0.2586\n",
      "172/281, train_loss: 0.2296, step time: 0.2591\n",
      "173/281, train_loss: 0.0632, step time: 0.2547\n",
      "174/281, train_loss: 0.2466, step time: 0.2538\n",
      "175/281, train_loss: 0.1609, step time: 0.2565\n",
      "176/281, train_loss: 0.1510, step time: 0.2576\n",
      "177/281, train_loss: 0.2556, step time: 0.2534\n",
      "178/281, train_loss: 0.0870, step time: 0.2559\n",
      "179/281, train_loss: 0.1010, step time: 0.2547\n",
      "180/281, train_loss: 0.1289, step time: 0.2547\n",
      "181/281, train_loss: 0.1448, step time: 0.2524\n",
      "182/281, train_loss: 0.1275, step time: 0.2593\n",
      "183/281, train_loss: 0.1620, step time: 0.2517\n",
      "184/281, train_loss: 0.1105, step time: 0.2508\n",
      "185/281, train_loss: 0.1095, step time: 0.2536\n",
      "186/281, train_loss: 0.0811, step time: 0.2549\n",
      "187/281, train_loss: 0.0948, step time: 0.2601\n",
      "188/281, train_loss: 0.1421, step time: 0.2510\n",
      "189/281, train_loss: 0.2541, step time: 0.2541\n",
      "190/281, train_loss: 0.1248, step time: 0.2612\n",
      "191/281, train_loss: 0.2532, step time: 0.2533\n",
      "192/281, train_loss: 0.0841, step time: 0.2583\n",
      "193/281, train_loss: 0.0965, step time: 0.2581\n",
      "194/281, train_loss: 0.0727, step time: 0.2556\n",
      "195/281, train_loss: 0.1117, step time: 0.2520\n",
      "196/281, train_loss: 0.0723, step time: 0.2535\n",
      "197/281, train_loss: 0.1473, step time: 0.2569\n",
      "198/281, train_loss: 0.1030, step time: 0.2576\n",
      "199/281, train_loss: 0.0629, step time: 0.2509\n",
      "200/281, train_loss: 0.0646, step time: 0.2644\n",
      "201/281, train_loss: 0.0919, step time: 0.2601\n",
      "202/281, train_loss: 0.0775, step time: 0.2610\n",
      "203/281, train_loss: 0.1842, step time: 0.2551\n",
      "204/281, train_loss: 0.1388, step time: 0.2525\n",
      "205/281, train_loss: 0.1004, step time: 0.2500\n",
      "206/281, train_loss: 0.1561, step time: 0.2574\n",
      "207/281, train_loss: 0.0906, step time: 0.2578\n",
      "208/281, train_loss: 0.2702, step time: 0.2556\n",
      "209/281, train_loss: 0.0786, step time: 0.2541\n",
      "210/281, train_loss: 0.1030, step time: 0.2556\n",
      "211/281, train_loss: 0.2231, step time: 0.2535\n",
      "212/281, train_loss: 0.4213, step time: 0.2719\n",
      "213/281, train_loss: 0.2778, step time: 0.2772\n",
      "214/281, train_loss: 0.1284, step time: 0.2533\n",
      "215/281, train_loss: 0.0754, step time: 0.2491\n",
      "216/281, train_loss: 0.0893, step time: 0.2542\n",
      "217/281, train_loss: 0.2128, step time: 0.2508\n",
      "218/281, train_loss: 0.1104, step time: 0.2539\n",
      "219/281, train_loss: 0.1275, step time: 0.2517\n",
      "220/281, train_loss: 0.1138, step time: 0.2493\n",
      "221/281, train_loss: 0.0490, step time: 0.2492\n",
      "222/281, train_loss: 0.0600, step time: 0.2537\n",
      "223/281, train_loss: 0.2966, step time: 0.2560\n",
      "224/281, train_loss: 0.1705, step time: 0.2574\n",
      "225/281, train_loss: 0.1046, step time: 0.2540\n",
      "226/281, train_loss: 0.0746, step time: 0.2730\n",
      "227/281, train_loss: 0.1395, step time: 0.2537\n",
      "228/281, train_loss: 0.1520, step time: 0.2560\n",
      "229/281, train_loss: 0.1511, step time: 0.2557\n",
      "230/281, train_loss: 0.2836, step time: 0.2532\n",
      "231/281, train_loss: 0.2873, step time: 0.2569\n",
      "232/281, train_loss: 0.0717, step time: 0.2537\n",
      "233/281, train_loss: 0.2522, step time: 0.2513\n",
      "234/281, train_loss: 0.0828, step time: 0.2572\n",
      "235/281, train_loss: 0.0712, step time: 0.2563\n",
      "236/281, train_loss: 0.0931, step time: 0.2495\n",
      "237/281, train_loss: 0.1316, step time: 0.2584\n",
      "238/281, train_loss: 0.2617, step time: 0.2535\n",
      "239/281, train_loss: 0.0958, step time: 0.2543\n",
      "240/281, train_loss: 0.1526, step time: 0.2513\n",
      "241/281, train_loss: 0.0974, step time: 0.2524\n",
      "242/281, train_loss: 0.1100, step time: 0.2586\n",
      "243/281, train_loss: 0.1012, step time: 0.2546\n",
      "244/281, train_loss: 0.0685, step time: 0.2542\n",
      "245/281, train_loss: 0.0700, step time: 0.2530\n",
      "246/281, train_loss: 0.0896, step time: 0.2550\n",
      "247/281, train_loss: 0.0753, step time: 0.2520\n",
      "248/281, train_loss: 0.2741, step time: 0.2507\n",
      "249/281, train_loss: 0.0936, step time: 0.2505\n",
      "250/281, train_loss: 0.0546, step time: 0.2519\n",
      "251/281, train_loss: 0.1215, step time: 0.2542\n",
      "252/281, train_loss: 0.2532, step time: 0.2574\n",
      "253/281, train_loss: 0.2482, step time: 0.2554\n",
      "254/281, train_loss: 0.0654, step time: 0.2531\n",
      "255/281, train_loss: 0.1376, step time: 0.2580\n",
      "256/281, train_loss: 0.1039, step time: 0.2506\n",
      "257/281, train_loss: 0.0558, step time: 0.2511\n",
      "258/281, train_loss: 0.1035, step time: 0.2505\n",
      "259/281, train_loss: 0.1095, step time: 0.2533\n",
      "260/281, train_loss: 0.0979, step time: 0.2699\n",
      "261/281, train_loss: 0.1187, step time: 0.2487\n",
      "262/281, train_loss: 0.1209, step time: 0.2563\n",
      "263/281, train_loss: 0.0929, step time: 0.2550\n",
      "264/281, train_loss: 0.1002, step time: 0.2525\n",
      "265/281, train_loss: 0.1075, step time: 0.2494\n",
      "266/281, train_loss: 0.1043, step time: 0.2499\n",
      "267/281, train_loss: 0.1534, step time: 0.2538\n",
      "268/281, train_loss: 0.3224, step time: 0.2463\n",
      "269/281, train_loss: 0.1104, step time: 0.2454\n",
      "270/281, train_loss: 0.2513, step time: 0.2518\n",
      "271/281, train_loss: 0.0671, step time: 0.2539\n",
      "272/281, train_loss: 0.0905, step time: 0.2561\n",
      "273/281, train_loss: 0.0573, step time: 0.2524\n",
      "274/281, train_loss: 0.1039, step time: 0.2542\n",
      "275/281, train_loss: 0.1253, step time: 0.2591\n",
      "276/281, train_loss: 0.0965, step time: 0.2480\n",
      "277/281, train_loss: 0.1399, step time: 0.2510\n",
      "278/281, train_loss: 0.0702, step time: 0.2503\n",
      "279/281, train_loss: 0.1174, step time: 0.2488\n",
      "280/281, train_loss: 0.0604, step time: 0.2519\n",
      "281/281, train_loss: 0.2852, step time: 0.2483\n",
      "282/281, train_loss: 0.1376, step time: 0.1480\n",
      "epoch 46 average loss: 0.1301\n",
      "saved new best metric model\n",
      "current epoch: 46 current mean dice: 0.8674 tc: 0.8533 wt: 0.9117 et: 0.8489\n",
      "best mean dice: 0.8674 at epoch: 46\n",
      "time consuming of epoch 46 is: 426.6245\n",
      "----------\n",
      "epoch 47/200\n",
      "1/281, train_loss: 0.2530, step time: 0.2524\n",
      "2/281, train_loss: 0.0633, step time: 0.2688\n",
      "3/281, train_loss: 0.1019, step time: 0.2575\n",
      "4/281, train_loss: 0.1091, step time: 0.3030\n",
      "5/281, train_loss: 0.2510, step time: 0.2513\n",
      "6/281, train_loss: 0.1191, step time: 0.2484\n",
      "7/281, train_loss: 0.0700, step time: 0.2524\n",
      "8/281, train_loss: 0.0893, step time: 0.2502\n",
      "9/281, train_loss: 0.1265, step time: 0.2548\n",
      "10/281, train_loss: 0.0783, step time: 0.2537\n",
      "11/281, train_loss: 0.0737, step time: 0.2536\n",
      "12/281, train_loss: 0.0821, step time: 0.2535\n",
      "13/281, train_loss: 0.0973, step time: 0.2550\n",
      "14/281, train_loss: 0.1657, step time: 0.2547\n",
      "15/281, train_loss: 0.2645, step time: 0.2594\n",
      "16/281, train_loss: 0.2452, step time: 0.2608\n",
      "17/281, train_loss: 0.1152, step time: 0.2529\n",
      "18/281, train_loss: 0.1070, step time: 0.2519\n",
      "19/281, train_loss: 0.1161, step time: 0.2546\n",
      "20/281, train_loss: 0.3148, step time: 0.2554\n",
      "21/281, train_loss: 0.0914, step time: 0.2510\n",
      "22/281, train_loss: 0.0911, step time: 0.2528\n",
      "23/281, train_loss: 0.1465, step time: 0.2559\n",
      "24/281, train_loss: 0.1085, step time: 0.2538\n",
      "25/281, train_loss: 0.1170, step time: 0.2676\n",
      "26/281, train_loss: 0.0836, step time: 0.2511\n",
      "27/281, train_loss: 0.0970, step time: 0.2562\n",
      "28/281, train_loss: 0.1035, step time: 0.2486\n",
      "29/281, train_loss: 0.2599, step time: 0.2603\n",
      "30/281, train_loss: 0.1737, step time: 0.2573\n",
      "31/281, train_loss: 0.0941, step time: 0.2581\n",
      "32/281, train_loss: 0.0904, step time: 0.2599\n",
      "33/281, train_loss: 0.0676, step time: 0.2548\n",
      "34/281, train_loss: 0.1262, step time: 0.2579\n",
      "35/281, train_loss: 0.0893, step time: 0.2563\n",
      "36/281, train_loss: 0.1285, step time: 0.2533\n",
      "37/281, train_loss: 0.4274, step time: 0.2569\n",
      "38/281, train_loss: 0.1327, step time: 0.2528\n",
      "39/281, train_loss: 0.2382, step time: 0.2496\n",
      "40/281, train_loss: 0.1175, step time: 0.2562\n",
      "41/281, train_loss: 0.1169, step time: 0.2536\n",
      "42/281, train_loss: 0.1067, step time: 0.2487\n",
      "43/281, train_loss: 0.0989, step time: 0.2534\n",
      "44/281, train_loss: 0.0778, step time: 0.2575\n",
      "45/281, train_loss: 0.1423, step time: 0.2475\n",
      "46/281, train_loss: 0.3366, step time: 0.2470\n",
      "47/281, train_loss: 0.0964, step time: 0.2469\n",
      "48/281, train_loss: 0.2543, step time: 0.2505\n",
      "49/281, train_loss: 0.2311, step time: 0.2519\n",
      "50/281, train_loss: 0.1433, step time: 0.2485\n",
      "51/281, train_loss: 0.1073, step time: 0.2595\n",
      "52/281, train_loss: 0.1564, step time: 0.2531\n",
      "53/281, train_loss: 0.0875, step time: 0.2501\n",
      "54/281, train_loss: 0.0792, step time: 0.2502\n",
      "55/281, train_loss: 0.0613, step time: 0.2596\n",
      "56/281, train_loss: 0.0903, step time: 0.2568\n",
      "57/281, train_loss: 0.1092, step time: 0.2543\n",
      "58/281, train_loss: 0.0678, step time: 0.2551\n",
      "59/281, train_loss: 0.0713, step time: 0.2571\n",
      "60/281, train_loss: 0.1304, step time: 0.2553\n",
      "61/281, train_loss: 0.1160, step time: 0.2493\n",
      "62/281, train_loss: 0.4284, step time: 0.2539\n",
      "63/281, train_loss: 0.1144, step time: 0.2678\n",
      "64/281, train_loss: 0.2475, step time: 0.2553\n",
      "65/281, train_loss: 0.2356, step time: 0.2531\n",
      "66/281, train_loss: 0.0937, step time: 0.2534\n",
      "67/281, train_loss: 0.2538, step time: 0.2507\n",
      "68/281, train_loss: 0.1405, step time: 0.2546\n",
      "69/281, train_loss: 0.0801, step time: 0.2468\n",
      "70/281, train_loss: 0.0769, step time: 0.2472\n",
      "71/281, train_loss: 0.0668, step time: 0.2568\n",
      "72/281, train_loss: 0.1128, step time: 0.2555\n",
      "73/281, train_loss: 0.1023, step time: 0.2507\n",
      "74/281, train_loss: 0.0772, step time: 0.2495\n",
      "75/281, train_loss: 0.0796, step time: 0.2486\n",
      "76/281, train_loss: 0.0521, step time: 0.2529\n",
      "77/281, train_loss: 0.0895, step time: 0.2529\n",
      "78/281, train_loss: 0.2879, step time: 0.2749\n",
      "79/281, train_loss: 0.0802, step time: 0.2555\n",
      "80/281, train_loss: 0.0824, step time: 0.2547\n",
      "81/281, train_loss: 0.0695, step time: 0.2538\n",
      "82/281, train_loss: 0.0867, step time: 0.2490\n",
      "83/281, train_loss: 0.1027, step time: 0.2511\n",
      "84/281, train_loss: 0.0604, step time: 0.2487\n",
      "85/281, train_loss: 0.1717, step time: 0.2514\n",
      "86/281, train_loss: 0.1772, step time: 0.2463\n",
      "87/281, train_loss: 0.1260, step time: 0.2546\n",
      "88/281, train_loss: 0.1201, step time: 0.2512\n",
      "89/281, train_loss: 0.1248, step time: 0.2465\n",
      "90/281, train_loss: 0.3214, step time: 0.2452\n",
      "91/281, train_loss: 0.0598, step time: 0.2499\n",
      "92/281, train_loss: 0.0879, step time: 0.2492\n",
      "93/281, train_loss: 0.1506, step time: 0.2474\n",
      "94/281, train_loss: 0.1029, step time: 0.2522\n",
      "95/281, train_loss: 0.0846, step time: 0.2522\n",
      "96/281, train_loss: 0.0844, step time: 0.2469\n",
      "97/281, train_loss: 0.0838, step time: 0.2480\n",
      "98/281, train_loss: 0.1593, step time: 0.2549\n",
      "99/281, train_loss: 0.0952, step time: 0.2486\n",
      "100/281, train_loss: 0.1494, step time: 0.2470\n",
      "101/281, train_loss: 0.1411, step time: 0.2463\n",
      "102/281, train_loss: 0.1070, step time: 0.2510\n",
      "103/281, train_loss: 0.0826, step time: 0.2509\n",
      "104/281, train_loss: 0.1022, step time: 0.2536\n",
      "105/281, train_loss: 0.2616, step time: 0.2464\n",
      "106/281, train_loss: 0.0849, step time: 0.2538\n",
      "107/281, train_loss: 0.2544, step time: 0.2475\n",
      "108/281, train_loss: 0.2496, step time: 0.2486\n",
      "109/281, train_loss: 0.0856, step time: 0.2527\n",
      "110/281, train_loss: 0.1380, step time: 0.2502\n",
      "111/281, train_loss: 0.2901, step time: 0.2485\n",
      "112/281, train_loss: 0.1232, step time: 0.2485\n",
      "113/281, train_loss: 0.1676, step time: 0.2512\n",
      "114/281, train_loss: 0.1189, step time: 0.2568\n",
      "115/281, train_loss: 0.0548, step time: 0.2578\n",
      "116/281, train_loss: 0.0736, step time: 0.2457\n",
      "117/281, train_loss: 0.0945, step time: 0.2481\n",
      "118/281, train_loss: 0.0882, step time: 0.2464\n",
      "119/281, train_loss: 0.3294, step time: 0.2497\n",
      "120/281, train_loss: 0.0789, step time: 0.2472\n",
      "121/281, train_loss: 0.2248, step time: 0.2546\n",
      "122/281, train_loss: 0.1333, step time: 0.2551\n",
      "123/281, train_loss: 0.0865, step time: 0.2710\n",
      "124/281, train_loss: 0.0760, step time: 0.2446\n",
      "125/281, train_loss: 0.0956, step time: 0.2556\n",
      "126/281, train_loss: 0.1476, step time: 0.2515\n",
      "127/281, train_loss: 0.0957, step time: 0.2514\n",
      "128/281, train_loss: 0.0939, step time: 0.2486\n",
      "129/281, train_loss: 0.1048, step time: 0.2430\n",
      "130/281, train_loss: 0.0959, step time: 0.2440\n",
      "131/281, train_loss: 0.2272, step time: 0.2467\n",
      "132/281, train_loss: 0.1205, step time: 0.2462\n",
      "133/281, train_loss: 0.1028, step time: 0.2453\n",
      "134/281, train_loss: 0.0897, step time: 0.2469\n",
      "135/281, train_loss: 0.2538, step time: 0.2499\n",
      "136/281, train_loss: 0.1770, step time: 0.2500\n",
      "137/281, train_loss: 0.1403, step time: 0.2508\n",
      "138/281, train_loss: 0.1506, step time: 0.2467\n",
      "139/281, train_loss: 0.1071, step time: 0.2463\n",
      "140/281, train_loss: 0.1513, step time: 0.2464\n",
      "141/281, train_loss: 0.1241, step time: 0.2502\n",
      "142/281, train_loss: 0.0676, step time: 0.2432\n",
      "143/281, train_loss: 0.0909, step time: 0.2447\n",
      "144/281, train_loss: 0.0975, step time: 0.2499\n",
      "145/281, train_loss: 0.0677, step time: 0.2466\n",
      "146/281, train_loss: 0.0810, step time: 0.2541\n",
      "147/281, train_loss: 0.2171, step time: 0.2482\n",
      "148/281, train_loss: 0.0498, step time: 0.2426\n",
      "149/281, train_loss: 0.1200, step time: 0.2426\n",
      "150/281, train_loss: 0.2070, step time: 0.2567\n",
      "151/281, train_loss: 0.1541, step time: 0.2571\n",
      "152/281, train_loss: 0.1399, step time: 0.2485\n",
      "153/281, train_loss: 0.1139, step time: 0.2511\n",
      "154/281, train_loss: 0.0996, step time: 0.2471\n",
      "155/281, train_loss: 0.1076, step time: 0.2506\n",
      "156/281, train_loss: 0.0529, step time: 0.2526\n",
      "157/281, train_loss: 0.1738, step time: 0.2481\n",
      "158/281, train_loss: 0.1018, step time: 0.2489\n",
      "159/281, train_loss: 0.1027, step time: 0.2477\n",
      "160/281, train_loss: 0.1008, step time: 0.2459\n",
      "161/281, train_loss: 0.2507, step time: 0.2450\n",
      "162/281, train_loss: 0.0551, step time: 0.2458\n",
      "163/281, train_loss: 0.1498, step time: 0.2456\n",
      "164/281, train_loss: 0.1166, step time: 0.2470\n",
      "165/281, train_loss: 0.2133, step time: 0.2500\n",
      "166/281, train_loss: 0.1057, step time: 0.2494\n",
      "167/281, train_loss: 0.3112, step time: 0.2461\n",
      "168/281, train_loss: 0.0941, step time: 0.2445\n",
      "169/281, train_loss: 0.1127, step time: 0.2654\n",
      "170/281, train_loss: 0.0622, step time: 0.2782\n",
      "171/281, train_loss: 0.1677, step time: 0.2638\n",
      "172/281, train_loss: 0.1499, step time: 0.2549\n",
      "173/281, train_loss: 0.0883, step time: 0.2504\n",
      "174/281, train_loss: 0.1271, step time: 0.2491\n",
      "175/281, train_loss: 0.0790, step time: 0.2487\n",
      "176/281, train_loss: 0.1379, step time: 0.2484\n",
      "177/281, train_loss: 0.2913, step time: 0.2472\n",
      "178/281, train_loss: 0.1565, step time: 0.2560\n",
      "179/281, train_loss: 0.1862, step time: 0.2501\n",
      "180/281, train_loss: 0.1281, step time: 0.2484\n",
      "181/281, train_loss: 0.0863, step time: 0.2507\n",
      "182/281, train_loss: 0.0850, step time: 0.2536\n",
      "183/281, train_loss: 0.1106, step time: 0.2510\n",
      "184/281, train_loss: 0.0629, step time: 0.2439\n",
      "185/281, train_loss: 0.1859, step time: 0.2439\n",
      "186/281, train_loss: 0.2418, step time: 0.2470\n",
      "187/281, train_loss: 0.1005, step time: 0.2510\n",
      "188/281, train_loss: 0.0660, step time: 0.2508\n",
      "189/281, train_loss: 0.0954, step time: 0.2455\n",
      "190/281, train_loss: 0.1117, step time: 0.2514\n",
      "191/281, train_loss: 0.2595, step time: 0.2536\n",
      "192/281, train_loss: 0.2906, step time: 0.2464\n",
      "193/281, train_loss: 0.1115, step time: 0.2526\n",
      "194/281, train_loss: 0.0815, step time: 0.2480\n",
      "195/281, train_loss: 0.1317, step time: 0.2484\n",
      "196/281, train_loss: 0.0993, step time: 0.2528\n",
      "197/281, train_loss: 0.1659, step time: 0.2536\n",
      "198/281, train_loss: 0.0671, step time: 0.2472\n",
      "199/281, train_loss: 0.2590, step time: 0.2498\n",
      "200/281, train_loss: 0.2568, step time: 0.2492\n",
      "201/281, train_loss: 0.1772, step time: 0.2478\n",
      "202/281, train_loss: 0.0752, step time: 0.2487\n",
      "203/281, train_loss: 0.1078, step time: 0.2435\n",
      "204/281, train_loss: 0.1674, step time: 0.2477\n",
      "205/281, train_loss: 0.1134, step time: 0.2506\n",
      "206/281, train_loss: 0.2811, step time: 0.2502\n",
      "207/281, train_loss: 0.0604, step time: 0.2555\n",
      "208/281, train_loss: 0.0614, step time: 0.2511\n",
      "209/281, train_loss: 0.1434, step time: 0.2506\n",
      "210/281, train_loss: 0.0792, step time: 0.2482\n",
      "211/281, train_loss: 0.1188, step time: 0.2526\n",
      "212/281, train_loss: 0.1096, step time: 0.2514\n",
      "213/281, train_loss: 0.1073, step time: 0.2498\n",
      "214/281, train_loss: 0.1452, step time: 0.2500\n",
      "215/281, train_loss: 0.0808, step time: 0.2528\n",
      "216/281, train_loss: 0.0907, step time: 0.2481\n",
      "217/281, train_loss: 0.0918, step time: 0.2496\n",
      "218/281, train_loss: 0.0905, step time: 0.2480\n",
      "219/281, train_loss: 0.1125, step time: 0.2528\n",
      "220/281, train_loss: 0.2551, step time: 0.2548\n",
      "221/281, train_loss: 0.1013, step time: 0.2518\n",
      "222/281, train_loss: 0.2744, step time: 0.2487\n",
      "223/281, train_loss: 0.0686, step time: 0.2506\n",
      "224/281, train_loss: 0.0697, step time: 0.2486\n",
      "225/281, train_loss: 0.0968, step time: 0.2478\n",
      "226/281, train_loss: 0.1226, step time: 0.2522\n",
      "227/281, train_loss: 0.0738, step time: 0.2463\n",
      "228/281, train_loss: 0.1385, step time: 0.2516\n",
      "229/281, train_loss: 0.1645, step time: 0.2557\n",
      "230/281, train_loss: 0.1477, step time: 0.2558\n",
      "231/281, train_loss: 0.0885, step time: 0.2559\n",
      "232/281, train_loss: 0.1764, step time: 0.2479\n",
      "233/281, train_loss: 0.0752, step time: 0.2511\n",
      "234/281, train_loss: 0.0853, step time: 0.2483\n",
      "235/281, train_loss: 0.0871, step time: 0.2504\n",
      "236/281, train_loss: 0.1356, step time: 0.2455\n",
      "237/281, train_loss: 0.2078, step time: 0.2475\n",
      "238/281, train_loss: 0.1393, step time: 0.2424\n",
      "239/281, train_loss: 0.0737, step time: 0.2430\n",
      "240/281, train_loss: 0.1047, step time: 0.2677\n",
      "241/281, train_loss: 0.2418, step time: 0.2497\n",
      "242/281, train_loss: 0.0685, step time: 0.2425\n",
      "243/281, train_loss: 0.0939, step time: 0.2427\n",
      "244/281, train_loss: 0.2241, step time: 0.2520\n",
      "245/281, train_loss: 0.0955, step time: 0.2486\n",
      "246/281, train_loss: 0.1106, step time: 0.2493\n",
      "247/281, train_loss: 0.0696, step time: 0.2445\n",
      "248/281, train_loss: 0.0899, step time: 0.2559\n",
      "249/281, train_loss: 0.1630, step time: 0.2499\n",
      "250/281, train_loss: 0.0720, step time: 0.2475\n",
      "251/281, train_loss: 0.1336, step time: 0.2480\n",
      "252/281, train_loss: 0.0902, step time: 0.2497\n",
      "253/281, train_loss: 0.0709, step time: 0.2492\n",
      "254/281, train_loss: 0.2812, step time: 0.2527\n",
      "255/281, train_loss: 0.0958, step time: 0.2476\n",
      "256/281, train_loss: 0.1297, step time: 0.2447\n",
      "257/281, train_loss: 0.1864, step time: 0.2532\n",
      "258/281, train_loss: 0.0978, step time: 0.2535\n",
      "259/281, train_loss: 0.0688, step time: 0.2490\n",
      "260/281, train_loss: 0.0713, step time: 0.2438\n",
      "261/281, train_loss: 0.0921, step time: 0.2457\n",
      "262/281, train_loss: 0.2536, step time: 0.2536\n",
      "263/281, train_loss: 0.0799, step time: 0.2483\n",
      "264/281, train_loss: 0.0693, step time: 0.2523\n",
      "265/281, train_loss: 0.1617, step time: 0.2544\n",
      "266/281, train_loss: 0.0583, step time: 0.2584\n",
      "267/281, train_loss: 0.0476, step time: 0.2510\n",
      "268/281, train_loss: 0.0810, step time: 0.2474\n",
      "269/281, train_loss: 0.0712, step time: 0.2454\n",
      "270/281, train_loss: 0.0962, step time: 0.2571\n",
      "271/281, train_loss: 0.1139, step time: 0.2505\n",
      "272/281, train_loss: 0.1168, step time: 0.2489\n",
      "273/281, train_loss: 0.0839, step time: 0.2491\n",
      "274/281, train_loss: 0.0827, step time: 0.2536\n",
      "275/281, train_loss: 0.0801, step time: 0.2493\n",
      "276/281, train_loss: 0.0862, step time: 0.2488\n",
      "277/281, train_loss: 0.0824, step time: 0.2486\n",
      "278/281, train_loss: 0.3086, step time: 0.2530\n",
      "279/281, train_loss: 0.1538, step time: 0.2524\n",
      "280/281, train_loss: 0.1299, step time: 0.2542\n",
      "281/281, train_loss: 0.1283, step time: 0.2553\n",
      "282/281, train_loss: 0.3909, step time: 0.1487\n",
      "epoch 47 average loss: 0.1322\n",
      "current epoch: 47 current mean dice: 0.8647 tc: 0.8586 wt: 0.9056 et: 0.8399\n",
      "best mean dice: 0.8674 at epoch: 46\n",
      "time consuming of epoch 47 is: 418.3725\n",
      "----------\n",
      "epoch 48/200\n",
      "1/281, train_loss: 0.1206, step time: 0.2646\n",
      "2/281, train_loss: 0.1062, step time: 0.2497\n",
      "3/281, train_loss: 0.2235, step time: 0.2527\n",
      "4/281, train_loss: 0.1331, step time: 0.2485\n",
      "5/281, train_loss: 0.2575, step time: 0.2617\n",
      "6/281, train_loss: 0.2535, step time: 0.2672\n",
      "7/281, train_loss: 0.2452, step time: 0.2615\n",
      "8/281, train_loss: 0.1051, step time: 0.2509\n",
      "9/281, train_loss: 0.2916, step time: 0.2543\n",
      "10/281, train_loss: 0.1150, step time: 0.2577\n",
      "11/281, train_loss: 0.0587, step time: 0.2534\n",
      "12/281, train_loss: 0.1434, step time: 0.2602\n",
      "13/281, train_loss: 0.0868, step time: 0.2570\n",
      "14/281, train_loss: 0.1269, step time: 0.2559\n",
      "15/281, train_loss: 0.2454, step time: 0.2550\n",
      "16/281, train_loss: 0.0746, step time: 0.2592\n",
      "17/281, train_loss: 0.0766, step time: 0.2746\n",
      "18/281, train_loss: 0.1420, step time: 0.2762\n",
      "19/281, train_loss: 0.4148, step time: 0.2585\n",
      "20/281, train_loss: 0.1409, step time: 0.2527\n",
      "21/281, train_loss: 0.1526, step time: 0.2541\n",
      "22/281, train_loss: 0.1115, step time: 0.2565\n",
      "23/281, train_loss: 0.0534, step time: 0.2538\n",
      "24/281, train_loss: 0.1501, step time: 0.2527\n",
      "25/281, train_loss: 0.1237, step time: 0.2554\n",
      "26/281, train_loss: 0.2614, step time: 0.2581\n",
      "27/281, train_loss: 0.0947, step time: 0.2570\n",
      "28/281, train_loss: 0.2747, step time: 0.2574\n",
      "29/281, train_loss: 0.0915, step time: 0.2592\n",
      "30/281, train_loss: 0.1341, step time: 0.2573\n",
      "31/281, train_loss: 0.1383, step time: 0.2542\n",
      "32/281, train_loss: 0.1142, step time: 0.2503\n",
      "33/281, train_loss: 0.0622, step time: 0.2533\n",
      "34/281, train_loss: 0.0730, step time: 0.2592\n",
      "35/281, train_loss: 0.1373, step time: 0.2571\n",
      "36/281, train_loss: 0.1056, step time: 0.2550\n",
      "37/281, train_loss: 0.0948, step time: 0.2599\n",
      "38/281, train_loss: 0.1112, step time: 0.2563\n",
      "39/281, train_loss: 0.1186, step time: 0.2595\n",
      "40/281, train_loss: 0.2667, step time: 0.2625\n",
      "41/281, train_loss: 0.0782, step time: 0.2570\n",
      "42/281, train_loss: 0.1041, step time: 0.2521\n",
      "43/281, train_loss: 0.0977, step time: 0.2543\n",
      "44/281, train_loss: 0.0882, step time: 0.2586\n",
      "45/281, train_loss: 0.1663, step time: 0.2583\n",
      "46/281, train_loss: 0.0941, step time: 0.2578\n",
      "47/281, train_loss: 0.1833, step time: 0.2528\n",
      "48/281, train_loss: 0.0619, step time: 0.2558\n",
      "49/281, train_loss: 0.0570, step time: 0.2527\n",
      "50/281, train_loss: 0.0962, step time: 0.2565\n",
      "51/281, train_loss: 0.0975, step time: 0.2569\n",
      "52/281, train_loss: 0.0925, step time: 0.2585\n",
      "53/281, train_loss: 0.0975, step time: 0.2566\n",
      "54/281, train_loss: 0.0891, step time: 0.2532\n",
      "55/281, train_loss: 0.2385, step time: 0.2561\n",
      "56/281, train_loss: 0.0994, step time: 0.2507\n",
      "57/281, train_loss: 0.0793, step time: 0.2527\n",
      "58/281, train_loss: 0.1011, step time: 0.2599\n",
      "59/281, train_loss: 0.1042, step time: 0.2530\n",
      "60/281, train_loss: 0.0878, step time: 0.2497\n",
      "61/281, train_loss: 0.0717, step time: 0.2563\n",
      "62/281, train_loss: 0.1019, step time: 0.2544\n",
      "63/281, train_loss: 0.1271, step time: 0.2560\n",
      "64/281, train_loss: 0.2999, step time: 0.2588\n",
      "65/281, train_loss: 0.1383, step time: 0.2585\n",
      "66/281, train_loss: 0.1285, step time: 0.2609\n",
      "67/281, train_loss: 0.1392, step time: 0.2653\n",
      "68/281, train_loss: 0.0744, step time: 0.2572\n",
      "69/281, train_loss: 0.0636, step time: 0.2585\n",
      "70/281, train_loss: 0.0968, step time: 0.2619\n",
      "71/281, train_loss: 0.0759, step time: 0.2610\n",
      "72/281, train_loss: 0.1058, step time: 0.2588\n",
      "73/281, train_loss: 0.1082, step time: 0.2580\n",
      "74/281, train_loss: 0.0466, step time: 0.2620\n",
      "75/281, train_loss: 0.0599, step time: 0.2592\n",
      "76/281, train_loss: 0.0693, step time: 0.2547\n",
      "77/281, train_loss: 0.1131, step time: 0.2558\n",
      "78/281, train_loss: 0.0661, step time: 0.2548\n",
      "79/281, train_loss: 0.1282, step time: 0.2570\n",
      "80/281, train_loss: 0.1270, step time: 0.2817\n",
      "81/281, train_loss: 0.1167, step time: 0.2599\n",
      "82/281, train_loss: 0.1106, step time: 0.2554\n",
      "83/281, train_loss: 0.2471, step time: 0.2599\n",
      "84/281, train_loss: 0.0870, step time: 0.2607\n",
      "85/281, train_loss: 0.0682, step time: 0.2648\n",
      "86/281, train_loss: 0.0826, step time: 0.2548\n",
      "87/281, train_loss: 0.2630, step time: 0.2577\n",
      "88/281, train_loss: 0.1344, step time: 0.2571\n",
      "89/281, train_loss: 0.0534, step time: 0.2557\n",
      "90/281, train_loss: 0.1001, step time: 0.2571\n",
      "91/281, train_loss: 0.0902, step time: 0.2595\n",
      "92/281, train_loss: 0.0709, step time: 0.2581\n",
      "93/281, train_loss: 0.1272, step time: 0.2611\n",
      "94/281, train_loss: 0.2614, step time: 0.2605\n",
      "95/281, train_loss: 0.1215, step time: 0.2597\n",
      "96/281, train_loss: 0.2438, step time: 0.2630\n",
      "97/281, train_loss: 0.1132, step time: 0.2578\n",
      "98/281, train_loss: 0.0720, step time: 0.2536\n",
      "99/281, train_loss: 0.0711, step time: 0.2554\n",
      "100/281, train_loss: 0.1113, step time: 0.2543\n",
      "101/281, train_loss: 0.0776, step time: 0.2490\n",
      "102/281, train_loss: 0.1172, step time: 0.2532\n",
      "103/281, train_loss: 0.0780, step time: 0.2565\n",
      "104/281, train_loss: 0.1078, step time: 0.2562\n",
      "105/281, train_loss: 0.0963, step time: 0.2590\n",
      "106/281, train_loss: 0.0703, step time: 0.2564\n",
      "107/281, train_loss: 0.0847, step time: 0.2538\n",
      "108/281, train_loss: 0.0756, step time: 0.2562\n",
      "109/281, train_loss: 0.1336, step time: 0.2580\n",
      "110/281, train_loss: 0.0577, step time: 0.2562\n",
      "111/281, train_loss: 0.0910, step time: 0.2544\n",
      "112/281, train_loss: 0.0799, step time: 0.2557\n",
      "113/281, train_loss: 0.1008, step time: 0.2522\n",
      "114/281, train_loss: 0.2667, step time: 0.2542\n",
      "115/281, train_loss: 0.2589, step time: 0.2531\n",
      "116/281, train_loss: 0.2262, step time: 0.2523\n",
      "117/281, train_loss: 0.0966, step time: 0.2507\n",
      "118/281, train_loss: 0.0919, step time: 0.2543\n",
      "119/281, train_loss: 0.0635, step time: 0.2567\n",
      "120/281, train_loss: 0.0948, step time: 0.2543\n",
      "121/281, train_loss: 0.2456, step time: 0.2581\n",
      "122/281, train_loss: 0.0946, step time: 0.2622\n",
      "123/281, train_loss: 0.1321, step time: 0.2582\n",
      "124/281, train_loss: 0.2518, step time: 0.2667\n",
      "125/281, train_loss: 0.2464, step time: 0.2584\n",
      "126/281, train_loss: 0.1067, step time: 0.2573\n",
      "127/281, train_loss: 0.0945, step time: 0.2579\n",
      "128/281, train_loss: 0.0732, step time: 0.2573\n",
      "129/281, train_loss: 0.2742, step time: 0.2550\n",
      "130/281, train_loss: 0.0958, step time: 0.2553\n",
      "131/281, train_loss: 0.0698, step time: 0.2611\n",
      "132/281, train_loss: 0.0935, step time: 0.2579\n",
      "133/281, train_loss: 0.2738, step time: 0.2598\n",
      "134/281, train_loss: 0.0937, step time: 0.2585\n",
      "135/281, train_loss: 0.1567, step time: 0.2590\n",
      "136/281, train_loss: 0.1027, step time: 0.2584\n",
      "137/281, train_loss: 0.1075, step time: 0.2595\n",
      "138/281, train_loss: 0.0739, step time: 0.2576\n",
      "139/281, train_loss: 0.0525, step time: 0.2638\n",
      "140/281, train_loss: 0.1283, step time: 0.2610\n",
      "141/281, train_loss: 0.0810, step time: 0.2525\n",
      "142/281, train_loss: 0.0960, step time: 0.2546\n",
      "143/281, train_loss: 0.0795, step time: 0.2589\n",
      "144/281, train_loss: 0.2492, step time: 0.2594\n",
      "145/281, train_loss: 0.0797, step time: 0.2539\n",
      "146/281, train_loss: 0.1412, step time: 0.2634\n",
      "147/281, train_loss: 0.0907, step time: 0.2518\n",
      "148/281, train_loss: 0.0728, step time: 0.2553\n",
      "149/281, train_loss: 0.0587, step time: 0.2529\n",
      "150/281, train_loss: 0.0925, step time: 0.2575\n",
      "151/281, train_loss: 0.0873, step time: 0.2552\n",
      "152/281, train_loss: 0.4047, step time: 0.2596\n",
      "153/281, train_loss: 0.1162, step time: 0.2595\n",
      "154/281, train_loss: 0.1005, step time: 0.2613\n",
      "155/281, train_loss: 0.1027, step time: 0.2530\n",
      "156/281, train_loss: 0.0799, step time: 0.2553\n",
      "157/281, train_loss: 0.0929, step time: 0.2533\n",
      "158/281, train_loss: 0.0828, step time: 0.2568\n",
      "159/281, train_loss: 0.1268, step time: 0.2645\n",
      "160/281, train_loss: 0.0606, step time: 0.2554\n",
      "161/281, train_loss: 0.0547, step time: 0.2554\n",
      "162/281, train_loss: 0.0888, step time: 0.2509\n",
      "163/281, train_loss: 0.0806, step time: 0.2577\n",
      "164/281, train_loss: 0.0953, step time: 0.2598\n",
      "165/281, train_loss: 0.1072, step time: 0.2516\n",
      "166/281, train_loss: 0.1851, step time: 0.2535\n",
      "167/281, train_loss: 0.2374, step time: 0.2601\n",
      "168/281, train_loss: 0.1044, step time: 0.2509\n",
      "169/281, train_loss: 0.0635, step time: 0.2497\n",
      "170/281, train_loss: 0.2452, step time: 0.2541\n",
      "171/281, train_loss: 0.0708, step time: 0.2541\n",
      "172/281, train_loss: 0.0827, step time: 0.2606\n",
      "173/281, train_loss: 0.0947, step time: 0.2563\n",
      "174/281, train_loss: 0.1137, step time: 0.2559\n",
      "175/281, train_loss: 0.1145, step time: 0.2580\n",
      "176/281, train_loss: 0.0716, step time: 0.2619\n",
      "177/281, train_loss: 0.2514, step time: 0.2573\n",
      "178/281, train_loss: 0.2439, step time: 0.2591\n",
      "179/281, train_loss: 0.0928, step time: 0.2507\n",
      "180/281, train_loss: 0.0546, step time: 0.2646\n",
      "181/281, train_loss: 0.2179, step time: 0.2565\n",
      "182/281, train_loss: 0.1103, step time: 0.2536\n",
      "183/281, train_loss: 0.2819, step time: 0.2615\n",
      "184/281, train_loss: 0.0982, step time: 0.2526\n",
      "185/281, train_loss: 0.0999, step time: 0.2561\n",
      "186/281, train_loss: 0.0887, step time: 0.2550\n",
      "187/281, train_loss: 0.0690, step time: 0.2596\n",
      "188/281, train_loss: 0.1028, step time: 0.2553\n",
      "189/281, train_loss: 0.1186, step time: 0.2565\n",
      "190/281, train_loss: 0.0835, step time: 0.2536\n",
      "191/281, train_loss: 0.1679, step time: 0.2603\n",
      "192/281, train_loss: 0.1337, step time: 0.2576\n",
      "193/281, train_loss: 0.1399, step time: 0.2504\n",
      "194/281, train_loss: 0.0946, step time: 0.2500\n",
      "195/281, train_loss: 0.1446, step time: 0.2563\n",
      "196/281, train_loss: 0.0913, step time: 0.2587\n",
      "197/281, train_loss: 0.1505, step time: 0.2591\n",
      "198/281, train_loss: 0.1100, step time: 0.2555\n",
      "199/281, train_loss: 0.1156, step time: 0.2521\n",
      "200/281, train_loss: 0.1226, step time: 0.2535\n",
      "201/281, train_loss: 0.2348, step time: 0.2581\n",
      "202/281, train_loss: 0.0840, step time: 0.2569\n",
      "203/281, train_loss: 0.0853, step time: 0.2585\n",
      "204/281, train_loss: 0.1637, step time: 0.2595\n",
      "205/281, train_loss: 0.1262, step time: 0.2569\n",
      "206/281, train_loss: 0.1074, step time: 0.2511\n",
      "207/281, train_loss: 0.0706, step time: 0.2549\n",
      "208/281, train_loss: 0.3343, step time: 0.2572\n",
      "209/281, train_loss: 0.0662, step time: 0.2556\n",
      "210/281, train_loss: 0.1362, step time: 0.2524\n",
      "211/281, train_loss: 0.0898, step time: 0.2581\n",
      "212/281, train_loss: 0.1042, step time: 0.2544\n",
      "213/281, train_loss: 0.2485, step time: 0.2511\n",
      "214/281, train_loss: 0.0913, step time: 0.2501\n",
      "215/281, train_loss: 0.3618, step time: 0.2528\n",
      "216/281, train_loss: 0.2882, step time: 0.2577\n",
      "217/281, train_loss: 0.1165, step time: 0.2558\n",
      "218/281, train_loss: 0.0903, step time: 0.2536\n",
      "219/281, train_loss: 0.1270, step time: 0.2551\n",
      "220/281, train_loss: 0.1027, step time: 0.2510\n",
      "221/281, train_loss: 0.1320, step time: 0.2578\n",
      "222/281, train_loss: 0.0874, step time: 0.2525\n",
      "223/281, train_loss: 0.0759, step time: 0.2518\n",
      "224/281, train_loss: 0.2237, step time: 0.2490\n",
      "225/281, train_loss: 0.1217, step time: 0.2557\n",
      "226/281, train_loss: 0.1220, step time: 0.2571\n",
      "227/281, train_loss: 0.1564, step time: 0.2552\n",
      "228/281, train_loss: 0.1174, step time: 0.2541\n",
      "229/281, train_loss: 0.1737, step time: 0.2551\n",
      "230/281, train_loss: 0.0936, step time: 0.2567\n",
      "231/281, train_loss: 0.0899, step time: 0.2608\n",
      "232/281, train_loss: 0.1000, step time: 0.2485\n",
      "233/281, train_loss: 0.1338, step time: 0.2560\n",
      "234/281, train_loss: 0.1107, step time: 0.2510\n",
      "235/281, train_loss: 0.1462, step time: 0.2539\n",
      "236/281, train_loss: 0.2391, step time: 0.2706\n",
      "237/281, train_loss: 0.1060, step time: 0.2820\n",
      "238/281, train_loss: 0.1844, step time: 0.2608\n",
      "239/281, train_loss: 0.2120, step time: 0.2567\n",
      "240/281, train_loss: 0.0974, step time: 0.2559\n",
      "241/281, train_loss: 0.0830, step time: 0.2476\n",
      "242/281, train_loss: 0.1494, step time: 0.2478\n",
      "243/281, train_loss: 0.1018, step time: 0.2495\n",
      "244/281, train_loss: 0.1043, step time: 0.2540\n",
      "245/281, train_loss: 0.1039, step time: 0.2584\n",
      "246/281, train_loss: 0.1284, step time: 0.2509\n",
      "247/281, train_loss: 0.0684, step time: 0.2551\n",
      "248/281, train_loss: 0.1388, step time: 0.2566\n",
      "249/281, train_loss: 0.2417, step time: 0.2561\n",
      "250/281, train_loss: 0.1857, step time: 0.2494\n",
      "251/281, train_loss: 0.1543, step time: 0.2546\n",
      "252/281, train_loss: 0.1160, step time: 0.2505\n",
      "253/281, train_loss: 0.0966, step time: 0.2461\n",
      "254/281, train_loss: 0.2655, step time: 0.2514\n",
      "255/281, train_loss: 0.0848, step time: 0.2559\n",
      "256/281, train_loss: 0.0598, step time: 0.2545\n",
      "257/281, train_loss: 0.1123, step time: 0.2491\n",
      "258/281, train_loss: 0.1192, step time: 0.2471\n",
      "259/281, train_loss: 0.1544, step time: 0.2523\n",
      "260/281, train_loss: 0.0771, step time: 0.2622\n",
      "261/281, train_loss: 0.0887, step time: 0.2680\n",
      "262/281, train_loss: 0.0887, step time: 0.2508\n",
      "263/281, train_loss: 0.1069, step time: 0.2559\n",
      "264/281, train_loss: 0.0957, step time: 0.2507\n",
      "265/281, train_loss: 0.1176, step time: 0.2513\n",
      "266/281, train_loss: 0.0887, step time: 0.2502\n",
      "267/281, train_loss: 0.0990, step time: 0.2530\n",
      "268/281, train_loss: 0.0633, step time: 0.2524\n",
      "269/281, train_loss: 0.0699, step time: 0.2579\n",
      "270/281, train_loss: 0.1174, step time: 0.2586\n",
      "271/281, train_loss: 0.1057, step time: 0.2574\n",
      "272/281, train_loss: 0.0859, step time: 0.2558\n",
      "273/281, train_loss: 0.2635, step time: 0.2523\n",
      "274/281, train_loss: 0.0800, step time: 0.2562\n",
      "275/281, train_loss: 0.0839, step time: 0.2547\n",
      "276/281, train_loss: 0.0883, step time: 0.2599\n",
      "277/281, train_loss: 0.0808, step time: 0.2566\n",
      "278/281, train_loss: 0.0652, step time: 0.2602\n",
      "279/281, train_loss: 0.1145, step time: 0.2561\n",
      "280/281, train_loss: 0.0752, step time: 0.2584\n",
      "281/281, train_loss: 0.2040, step time: 0.2525\n",
      "282/281, train_loss: 0.0883, step time: 0.1508\n",
      "epoch 48 average loss: 0.1267\n",
      "saved new best metric model\n",
      "current epoch: 48 current mean dice: 0.8692 tc: 0.8627 wt: 0.9063 et: 0.8490\n",
      "best mean dice: 0.8692 at epoch: 48\n",
      "time consuming of epoch 48 is: 394.4658\n",
      "----------\n",
      "epoch 49/200\n",
      "1/281, train_loss: 0.0847, step time: 0.2669\n",
      "2/281, train_loss: 0.0525, step time: 0.2548\n",
      "3/281, train_loss: 0.1224, step time: 0.2602\n",
      "4/281, train_loss: 0.0704, step time: 0.2583\n",
      "5/281, train_loss: 0.0639, step time: 0.2518\n",
      "6/281, train_loss: 0.1176, step time: 0.2507\n",
      "7/281, train_loss: 0.0712, step time: 0.2489\n",
      "8/281, train_loss: 0.2718, step time: 0.2483\n",
      "9/281, train_loss: 0.2277, step time: 0.2571\n",
      "10/281, train_loss: 0.0890, step time: 0.2544\n",
      "11/281, train_loss: 0.0939, step time: 0.2528\n",
      "12/281, train_loss: 0.0480, step time: 0.2578\n",
      "13/281, train_loss: 0.2307, step time: 0.2557\n",
      "14/281, train_loss: 0.0727, step time: 0.2560\n",
      "15/281, train_loss: 0.0876, step time: 0.2539\n",
      "16/281, train_loss: 0.0637, step time: 0.2558\n",
      "17/281, train_loss: 0.0704, step time: 0.2462\n",
      "18/281, train_loss: 0.2464, step time: 0.2579\n",
      "19/281, train_loss: 0.0749, step time: 0.2487\n",
      "20/281, train_loss: 0.1540, step time: 0.2499\n",
      "21/281, train_loss: 0.1375, step time: 0.2524\n",
      "22/281, train_loss: 0.1097, step time: 0.2526\n",
      "23/281, train_loss: 0.0787, step time: 0.2568\n",
      "24/281, train_loss: 0.0883, step time: 0.2525\n",
      "25/281, train_loss: 0.1049, step time: 0.2516\n",
      "26/281, train_loss: 0.1209, step time: 0.2505\n",
      "27/281, train_loss: 0.2541, step time: 0.2469\n",
      "28/281, train_loss: 0.1001, step time: 0.2515\n",
      "29/281, train_loss: 0.0984, step time: 0.2587\n",
      "30/281, train_loss: 0.0557, step time: 0.2500\n",
      "31/281, train_loss: 0.2254, step time: 0.2546\n",
      "32/281, train_loss: 0.0929, step time: 0.2531\n",
      "33/281, train_loss: 0.0759, step time: 0.2520\n",
      "34/281, train_loss: 0.1050, step time: 0.2532\n",
      "35/281, train_loss: 0.1083, step time: 0.2540\n",
      "36/281, train_loss: 0.0872, step time: 0.2545\n",
      "37/281, train_loss: 0.1154, step time: 0.2532\n",
      "38/281, train_loss: 0.0707, step time: 0.2548\n",
      "39/281, train_loss: 0.3189, step time: 0.2560\n",
      "40/281, train_loss: 0.1582, step time: 0.2583\n",
      "41/281, train_loss: 0.1062, step time: 0.2540\n",
      "42/281, train_loss: 0.0596, step time: 0.2553\n",
      "43/281, train_loss: 0.0994, step time: 0.2535\n",
      "44/281, train_loss: 0.0966, step time: 0.2514\n",
      "45/281, train_loss: 0.0776, step time: 0.2533\n",
      "46/281, train_loss: 0.0843, step time: 0.2518\n",
      "47/281, train_loss: 0.1104, step time: 0.2514\n",
      "48/281, train_loss: 0.2548, step time: 0.2495\n",
      "49/281, train_loss: 0.0720, step time: 0.2560\n",
      "50/281, train_loss: 0.0605, step time: 0.2571\n",
      "51/281, train_loss: 0.0986, step time: 0.2549\n",
      "52/281, train_loss: 0.1148, step time: 0.2521\n",
      "53/281, train_loss: 0.1173, step time: 0.2510\n",
      "54/281, train_loss: 0.1364, step time: 0.2551\n",
      "55/281, train_loss: 0.0800, step time: 0.2529\n",
      "56/281, train_loss: 0.2751, step time: 0.2567\n",
      "57/281, train_loss: 0.0793, step time: 0.2583\n",
      "58/281, train_loss: 0.1508, step time: 0.2514\n",
      "59/281, train_loss: 0.2317, step time: 0.2492\n",
      "60/281, train_loss: 0.1040, step time: 0.2486\n",
      "61/281, train_loss: 0.0612, step time: 0.2546\n",
      "62/281, train_loss: 0.2640, step time: 0.2601\n",
      "63/281, train_loss: 0.0970, step time: 0.2559\n",
      "64/281, train_loss: 0.0881, step time: 0.2515\n",
      "65/281, train_loss: 0.1031, step time: 0.2554\n",
      "66/281, train_loss: 0.0768, step time: 0.2481\n",
      "67/281, train_loss: 0.1196, step time: 0.2493\n",
      "68/281, train_loss: 0.0861, step time: 0.2502\n",
      "69/281, train_loss: 0.1421, step time: 0.2505\n",
      "70/281, train_loss: 0.0627, step time: 0.2559\n",
      "71/281, train_loss: 0.1120, step time: 0.2563\n",
      "72/281, train_loss: 0.0823, step time: 0.2538\n",
      "73/281, train_loss: 0.2174, step time: 0.2482\n",
      "74/281, train_loss: 0.0474, step time: 0.2574\n",
      "75/281, train_loss: 0.0752, step time: 0.2505\n",
      "76/281, train_loss: 0.0776, step time: 0.2564\n",
      "77/281, train_loss: 0.0888, step time: 0.2546\n",
      "78/281, train_loss: 0.0997, step time: 0.2560\n",
      "79/281, train_loss: 0.0812, step time: 0.2589\n",
      "80/281, train_loss: 0.1360, step time: 0.2510\n",
      "81/281, train_loss: 0.1155, step time: 0.2559\n",
      "82/281, train_loss: 0.2402, step time: 0.2535\n",
      "83/281, train_loss: 0.1932, step time: 0.2586\n",
      "84/281, train_loss: 0.0876, step time: 0.2563\n",
      "85/281, train_loss: 0.0945, step time: 0.2532\n",
      "86/281, train_loss: 0.1459, step time: 0.2520\n",
      "87/281, train_loss: 0.0981, step time: 0.2505\n",
      "88/281, train_loss: 0.0978, step time: 0.2538\n",
      "89/281, train_loss: 0.1398, step time: 0.2545\n",
      "90/281, train_loss: 0.1345, step time: 0.2558\n",
      "91/281, train_loss: 0.2736, step time: 0.2552\n",
      "92/281, train_loss: 0.1501, step time: 0.2561\n",
      "93/281, train_loss: 0.0832, step time: 0.2524\n",
      "94/281, train_loss: 0.2898, step time: 0.2492\n",
      "95/281, train_loss: 0.1176, step time: 0.2497\n",
      "96/281, train_loss: 0.1355, step time: 0.2568\n",
      "97/281, train_loss: 0.1815, step time: 0.2562\n",
      "98/281, train_loss: 0.1152, step time: 0.2556\n",
      "99/281, train_loss: 0.0793, step time: 0.2550\n",
      "100/281, train_loss: 0.0992, step time: 0.2551\n",
      "101/281, train_loss: 0.1084, step time: 0.2507\n",
      "102/281, train_loss: 0.0660, step time: 0.2565\n",
      "103/281, train_loss: 0.2887, step time: 0.2531\n",
      "104/281, train_loss: 0.1265, step time: 0.2562\n",
      "105/281, train_loss: 0.1013, step time: 0.2540\n",
      "106/281, train_loss: 0.1501, step time: 0.2495\n",
      "107/281, train_loss: 0.2642, step time: 0.2522\n",
      "108/281, train_loss: 0.1187, step time: 0.2513\n",
      "109/281, train_loss: 0.0959, step time: 0.2514\n",
      "110/281, train_loss: 0.3974, step time: 0.2509\n",
      "111/281, train_loss: 0.0955, step time: 0.2585\n",
      "112/281, train_loss: 0.0750, step time: 0.2566\n",
      "113/281, train_loss: 0.1158, step time: 0.2528\n",
      "114/281, train_loss: 0.0969, step time: 0.2488\n",
      "115/281, train_loss: 0.0514, step time: 0.2546\n",
      "116/281, train_loss: 0.0797, step time: 0.2526\n",
      "117/281, train_loss: 0.1880, step time: 0.2532\n",
      "118/281, train_loss: 0.1202, step time: 0.2474\n",
      "119/281, train_loss: 0.0968, step time: 0.2512\n",
      "120/281, train_loss: 0.3160, step time: 0.2527\n",
      "121/281, train_loss: 0.0754, step time: 0.2466\n",
      "122/281, train_loss: 0.0650, step time: 0.2471\n",
      "123/281, train_loss: 0.0817, step time: 0.2468\n",
      "124/281, train_loss: 0.1142, step time: 0.2497\n",
      "125/281, train_loss: 0.0850, step time: 0.2462\n",
      "126/281, train_loss: 0.2494, step time: 0.2510\n",
      "127/281, train_loss: 0.0516, step time: 0.2553\n",
      "128/281, train_loss: 0.0799, step time: 0.2562\n",
      "129/281, train_loss: 0.1214, step time: 0.2570\n",
      "130/281, train_loss: 0.0980, step time: 0.2544\n",
      "131/281, train_loss: 0.2928, step time: 0.2547\n",
      "132/281, train_loss: 0.1184, step time: 0.2524\n",
      "133/281, train_loss: 0.0469, step time: 0.2514\n",
      "134/281, train_loss: 0.1139, step time: 0.2545\n",
      "135/281, train_loss: 0.0724, step time: 0.2528\n",
      "136/281, train_loss: 0.1739, step time: 0.2543\n",
      "137/281, train_loss: 0.0860, step time: 0.2534\n",
      "138/281, train_loss: 0.2864, step time: 0.2520\n",
      "139/281, train_loss: 0.1068, step time: 0.2517\n",
      "140/281, train_loss: 0.0906, step time: 0.2539\n",
      "141/281, train_loss: 0.1116, step time: 0.2516\n",
      "142/281, train_loss: 0.1513, step time: 0.2521\n",
      "143/281, train_loss: 0.1370, step time: 0.2508\n",
      "144/281, train_loss: 0.0907, step time: 0.2537\n",
      "145/281, train_loss: 0.1554, step time: 0.2473\n",
      "146/281, train_loss: 0.0661, step time: 0.2482\n",
      "147/281, train_loss: 0.1435, step time: 0.2488\n",
      "148/281, train_loss: 0.2814, step time: 0.2493\n",
      "149/281, train_loss: 0.0734, step time: 0.2536\n",
      "150/281, train_loss: 0.1401, step time: 0.2544\n",
      "151/281, train_loss: 0.0950, step time: 0.2526\n",
      "152/281, train_loss: 0.0834, step time: 0.2461\n",
      "153/281, train_loss: 0.0714, step time: 0.2499\n",
      "154/281, train_loss: 0.0814, step time: 0.2492\n",
      "155/281, train_loss: 0.1016, step time: 0.2501\n",
      "156/281, train_loss: 0.1136, step time: 0.2535\n",
      "157/281, train_loss: 0.0862, step time: 0.2515\n",
      "158/281, train_loss: 0.0963, step time: 0.2561\n",
      "159/281, train_loss: 0.1034, step time: 0.2571\n",
      "160/281, train_loss: 0.0951, step time: 0.2524\n",
      "161/281, train_loss: 0.0947, step time: 0.2490\n",
      "162/281, train_loss: 0.2925, step time: 0.2525\n",
      "163/281, train_loss: 0.0789, step time: 0.2525\n",
      "164/281, train_loss: 0.2586, step time: 0.2476\n",
      "165/281, train_loss: 0.0656, step time: 0.2503\n",
      "166/281, train_loss: 0.3367, step time: 0.2519\n",
      "167/281, train_loss: 0.1339, step time: 0.2546\n",
      "168/281, train_loss: 0.1340, step time: 0.2549\n",
      "169/281, train_loss: 0.0929, step time: 0.2494\n",
      "170/281, train_loss: 0.2284, step time: 0.2468\n",
      "171/281, train_loss: 0.2945, step time: 0.2506\n",
      "172/281, train_loss: 0.2377, step time: 0.2565\n",
      "173/281, train_loss: 0.1089, step time: 0.2509\n",
      "174/281, train_loss: 0.0849, step time: 0.2455\n",
      "175/281, train_loss: 0.2545, step time: 0.2539\n",
      "176/281, train_loss: 0.0848, step time: 0.2552\n",
      "177/281, train_loss: 0.0560, step time: 0.2542\n",
      "178/281, train_loss: 0.0616, step time: 0.2498\n",
      "179/281, train_loss: 0.1060, step time: 0.2534\n",
      "180/281, train_loss: 0.2547, step time: 0.2561\n",
      "181/281, train_loss: 0.1338, step time: 0.2501\n",
      "182/281, train_loss: 0.0834, step time: 0.2528\n",
      "183/281, train_loss: 0.1179, step time: 0.2507\n",
      "184/281, train_loss: 0.0847, step time: 0.2540\n",
      "185/281, train_loss: 0.0894, step time: 0.2557\n",
      "186/281, train_loss: 0.1462, step time: 0.2456\n",
      "187/281, train_loss: 0.1478, step time: 0.2514\n",
      "188/281, train_loss: 0.2388, step time: 0.2517\n",
      "189/281, train_loss: 0.0811, step time: 0.2518\n",
      "190/281, train_loss: 0.2623, step time: 0.2464\n",
      "191/281, train_loss: 0.0695, step time: 0.2507\n",
      "192/281, train_loss: 0.0717, step time: 0.2514\n",
      "193/281, train_loss: 0.2437, step time: 0.2488\n",
      "194/281, train_loss: 0.0655, step time: 0.2479\n",
      "195/281, train_loss: 0.0798, step time: 0.2507\n",
      "196/281, train_loss: 0.0854, step time: 0.2513\n",
      "197/281, train_loss: 0.0766, step time: 0.2503\n",
      "198/281, train_loss: 0.0912, step time: 0.2489\n",
      "199/281, train_loss: 0.1083, step time: 0.2544\n",
      "200/281, train_loss: 0.0788, step time: 0.2471\n",
      "201/281, train_loss: 0.1138, step time: 0.2479\n",
      "202/281, train_loss: 0.0770, step time: 0.2489\n",
      "203/281, train_loss: 0.1479, step time: 0.2499\n",
      "204/281, train_loss: 0.1552, step time: 0.2539\n",
      "205/281, train_loss: 0.1676, step time: 0.2413\n",
      "206/281, train_loss: 0.0611, step time: 0.2826\n",
      "207/281, train_loss: 0.1002, step time: 0.2593\n",
      "208/281, train_loss: 0.0974, step time: 0.2468\n",
      "209/281, train_loss: 0.0998, step time: 0.2490\n",
      "210/281, train_loss: 0.0738, step time: 0.2491\n",
      "211/281, train_loss: 0.0849, step time: 0.2535\n",
      "212/281, train_loss: 0.0939, step time: 0.2518\n",
      "213/281, train_loss: 0.2622, step time: 0.2528\n",
      "214/281, train_loss: 0.1948, step time: 0.2562\n",
      "215/281, train_loss: 0.0930, step time: 0.2504\n",
      "216/281, train_loss: 0.2474, step time: 0.2481\n",
      "217/281, train_loss: 0.1061, step time: 0.2499\n",
      "218/281, train_loss: 0.0807, step time: 0.2498\n",
      "219/281, train_loss: 0.2397, step time: 0.2468\n",
      "220/281, train_loss: 0.0798, step time: 0.2502\n",
      "221/281, train_loss: 0.2625, step time: 0.2491\n",
      "222/281, train_loss: 0.1096, step time: 0.2516\n",
      "223/281, train_loss: 0.0678, step time: 0.2492\n",
      "224/281, train_loss: 0.0559, step time: 0.2441\n",
      "225/281, train_loss: 0.0852, step time: 0.2430\n",
      "226/281, train_loss: 0.1734, step time: 0.2470\n",
      "227/281, train_loss: 0.0715, step time: 0.2526\n",
      "228/281, train_loss: 0.0714, step time: 0.2507\n",
      "229/281, train_loss: 0.0906, step time: 0.2498\n",
      "230/281, train_loss: 0.1013, step time: 0.2531\n",
      "231/281, train_loss: 0.0851, step time: 0.2566\n",
      "232/281, train_loss: 0.0757, step time: 0.2509\n",
      "233/281, train_loss: 0.1327, step time: 0.2511\n",
      "234/281, train_loss: 0.1104, step time: 0.2586\n",
      "235/281, train_loss: 0.0830, step time: 0.2488\n",
      "236/281, train_loss: 0.0848, step time: 0.2503\n",
      "237/281, train_loss: 0.1027, step time: 0.2517\n",
      "238/281, train_loss: 0.0930, step time: 0.2505\n",
      "239/281, train_loss: 0.1052, step time: 0.2557\n",
      "240/281, train_loss: 0.1131, step time: 0.2556\n",
      "241/281, train_loss: 0.0930, step time: 0.2489\n",
      "242/281, train_loss: 0.1197, step time: 0.2484\n",
      "243/281, train_loss: 0.2757, step time: 0.2492\n",
      "244/281, train_loss: 0.2588, step time: 0.2464\n",
      "245/281, train_loss: 0.1017, step time: 0.2441\n",
      "246/281, train_loss: 0.0890, step time: 0.2544\n",
      "247/281, train_loss: 0.1309, step time: 0.2512\n",
      "248/281, train_loss: 0.1031, step time: 0.2441\n",
      "249/281, train_loss: 0.0672, step time: 0.2429\n",
      "250/281, train_loss: 0.1283, step time: 0.2430\n",
      "251/281, train_loss: 0.1181, step time: 0.2431\n",
      "252/281, train_loss: 0.0844, step time: 0.2545\n",
      "253/281, train_loss: 0.1149, step time: 0.2512\n",
      "254/281, train_loss: 0.0991, step time: 0.2456\n",
      "255/281, train_loss: 0.1346, step time: 0.2499\n",
      "256/281, train_loss: 0.0998, step time: 0.2463\n",
      "257/281, train_loss: 0.0878, step time: 0.2470\n",
      "258/281, train_loss: 0.1247, step time: 0.2456\n",
      "259/281, train_loss: 0.0925, step time: 0.2470\n",
      "260/281, train_loss: 0.0792, step time: 0.2490\n",
      "261/281, train_loss: 0.2516, step time: 0.2480\n",
      "262/281, train_loss: 0.1634, step time: 0.2588\n",
      "263/281, train_loss: 0.0760, step time: 0.2529\n",
      "264/281, train_loss: 0.0801, step time: 0.2470\n",
      "265/281, train_loss: 0.1205, step time: 0.2513\n",
      "266/281, train_loss: 0.0859, step time: 0.2496\n",
      "267/281, train_loss: 0.2408, step time: 0.2491\n",
      "268/281, train_loss: 0.1298, step time: 0.2525\n",
      "269/281, train_loss: 0.1160, step time: 0.2501\n",
      "270/281, train_loss: 0.1112, step time: 0.2484\n",
      "271/281, train_loss: 0.0830, step time: 0.2518\n",
      "272/281, train_loss: 0.2581, step time: 0.2527\n",
      "273/281, train_loss: 0.1482, step time: 0.2459\n",
      "274/281, train_loss: 0.1187, step time: 0.2423\n",
      "275/281, train_loss: 0.1000, step time: 0.2524\n",
      "276/281, train_loss: 0.0931, step time: 0.2510\n",
      "277/281, train_loss: 0.0691, step time: 0.2519\n",
      "278/281, train_loss: 0.1245, step time: 0.2519\n",
      "279/281, train_loss: 0.2634, step time: 0.2455\n",
      "280/281, train_loss: 0.1549, step time: 0.2479\n",
      "281/281, train_loss: 0.0530, step time: 0.2441\n",
      "282/281, train_loss: 0.1678, step time: 0.1488\n",
      "epoch 49 average loss: 0.1260\n",
      "saved new best metric model\n",
      "current epoch: 49 current mean dice: 0.8738 tc: 0.8608 wt: 0.9135 et: 0.8587\n",
      "best mean dice: 0.8738 at epoch: 49\n",
      "time consuming of epoch 49 is: 397.0629\n",
      "----------\n",
      "epoch 50/200\n",
      "1/281, train_loss: 0.0978, step time: 0.2565\n",
      "2/281, train_loss: 0.1583, step time: 0.2616\n",
      "3/281, train_loss: 0.0751, step time: 0.2524\n",
      "4/281, train_loss: 0.0933, step time: 0.2487\n",
      "5/281, train_loss: 0.2785, step time: 0.2638\n",
      "6/281, train_loss: 0.1074, step time: 0.2592\n",
      "7/281, train_loss: 0.0634, step time: 0.2596\n",
      "8/281, train_loss: 0.1597, step time: 0.2565\n",
      "9/281, train_loss: 0.0874, step time: 0.2496\n",
      "10/281, train_loss: 0.1256, step time: 0.2470\n",
      "11/281, train_loss: 0.0668, step time: 0.2538\n",
      "12/281, train_loss: 0.0787, step time: 0.2515\n",
      "13/281, train_loss: 0.2674, step time: 0.2602\n",
      "14/281, train_loss: 0.2355, step time: 0.2570\n",
      "15/281, train_loss: 0.1466, step time: 0.2547\n",
      "16/281, train_loss: 0.0840, step time: 0.2557\n",
      "17/281, train_loss: 0.1115, step time: 0.2592\n",
      "18/281, train_loss: 0.1335, step time: 0.2497\n",
      "19/281, train_loss: 0.2904, step time: 0.2535\n",
      "20/281, train_loss: 0.2348, step time: 0.2505\n",
      "21/281, train_loss: 0.0651, step time: 0.2523\n",
      "22/281, train_loss: 0.0552, step time: 0.2492\n",
      "23/281, train_loss: 0.1061, step time: 0.2502\n",
      "24/281, train_loss: 0.0653, step time: 0.2524\n",
      "25/281, train_loss: 0.2253, step time: 0.2534\n",
      "26/281, train_loss: 0.0706, step time: 0.2507\n",
      "27/281, train_loss: 0.0730, step time: 0.2459\n",
      "28/281, train_loss: 0.0901, step time: 0.2498\n",
      "29/281, train_loss: 0.1272, step time: 0.2562\n",
      "30/281, train_loss: 0.4691, step time: 0.2618\n",
      "31/281, train_loss: 0.0545, step time: 0.2679\n",
      "32/281, train_loss: 0.1105, step time: 0.2581\n",
      "33/281, train_loss: 0.2529, step time: 0.2538\n",
      "34/281, train_loss: 0.0929, step time: 0.2475\n",
      "35/281, train_loss: 0.0838, step time: 0.2476\n",
      "36/281, train_loss: 0.1279, step time: 0.2501\n",
      "37/281, train_loss: 0.0946, step time: 0.2504\n",
      "38/281, train_loss: 0.0751, step time: 0.2508\n",
      "39/281, train_loss: 0.2659, step time: 0.2469\n",
      "40/281, train_loss: 0.0780, step time: 0.2504\n",
      "41/281, train_loss: 0.0961, step time: 0.2518\n",
      "42/281, train_loss: 0.2666, step time: 0.2504\n",
      "43/281, train_loss: 0.1124, step time: 0.2555\n",
      "44/281, train_loss: 0.0847, step time: 0.2551\n",
      "45/281, train_loss: 0.0831, step time: 0.2586\n",
      "46/281, train_loss: 0.2486, step time: 0.2569\n",
      "47/281, train_loss: 0.1019, step time: 0.2577\n",
      "48/281, train_loss: 0.0863, step time: 0.2575\n",
      "49/281, train_loss: 0.0625, step time: 0.2533\n",
      "50/281, train_loss: 0.0900, step time: 0.2569\n",
      "51/281, train_loss: 0.1161, step time: 0.2491\n",
      "52/281, train_loss: 0.0812, step time: 0.2474\n",
      "53/281, train_loss: 0.0809, step time: 0.2549\n",
      "54/281, train_loss: 0.1121, step time: 0.2553\n",
      "55/281, train_loss: 0.0997, step time: 0.2569\n",
      "56/281, train_loss: 0.2382, step time: 0.2537\n",
      "57/281, train_loss: 0.0920, step time: 0.2551\n",
      "58/281, train_loss: 0.2500, step time: 0.2556\n",
      "59/281, train_loss: 0.0958, step time: 0.2585\n",
      "60/281, train_loss: 0.0693, step time: 0.2550\n",
      "61/281, train_loss: 0.2421, step time: 0.2545\n",
      "62/281, train_loss: 0.1280, step time: 0.2531\n",
      "63/281, train_loss: 0.0718, step time: 0.2597\n",
      "64/281, train_loss: 0.0857, step time: 0.2577\n",
      "65/281, train_loss: 0.0748, step time: 0.2536\n",
      "66/281, train_loss: 0.1159, step time: 0.2555\n",
      "67/281, train_loss: 0.0660, step time: 0.2612\n",
      "68/281, train_loss: 0.2705, step time: 0.2619\n",
      "69/281, train_loss: 0.0700, step time: 0.2574\n",
      "70/281, train_loss: 0.0570, step time: 0.2563\n",
      "71/281, train_loss: 0.1003, step time: 0.2551\n",
      "72/281, train_loss: 0.0594, step time: 0.2543\n",
      "73/281, train_loss: 0.1108, step time: 0.2492\n",
      "74/281, train_loss: 0.1106, step time: 0.2581\n",
      "75/281, train_loss: 0.1139, step time: 0.2526\n",
      "76/281, train_loss: 0.0806, step time: 0.2515\n",
      "77/281, train_loss: 0.1827, step time: 0.2543\n",
      "78/281, train_loss: 0.1262, step time: 0.2482\n",
      "79/281, train_loss: 0.1102, step time: 0.2590\n",
      "80/281, train_loss: 0.1121, step time: 0.2490\n",
      "81/281, train_loss: 0.1089, step time: 0.2566\n",
      "82/281, train_loss: 0.1040, step time: 0.2532\n",
      "83/281, train_loss: 0.1048, step time: 0.2535\n",
      "84/281, train_loss: 0.2390, step time: 0.2565\n",
      "85/281, train_loss: 0.1236, step time: 0.2556\n",
      "86/281, train_loss: 0.0714, step time: 0.2551\n",
      "87/281, train_loss: 0.0765, step time: 0.2499\n",
      "88/281, train_loss: 0.0670, step time: 0.2482\n",
      "89/281, train_loss: 0.0809, step time: 0.2541\n",
      "90/281, train_loss: 0.0782, step time: 0.2550\n",
      "91/281, train_loss: 0.0838, step time: 0.2535\n",
      "92/281, train_loss: 0.0940, step time: 0.2563\n",
      "93/281, train_loss: 0.0916, step time: 0.2555\n",
      "94/281, train_loss: 0.2839, step time: 0.2544\n",
      "95/281, train_loss: 0.0823, step time: 0.2531\n",
      "96/281, train_loss: 0.1154, step time: 0.2544\n",
      "97/281, train_loss: 0.2791, step time: 0.2551\n",
      "98/281, train_loss: 0.2446, step time: 0.2522\n",
      "99/281, train_loss: 0.1168, step time: 0.2526\n",
      "100/281, train_loss: 0.1241, step time: 0.2510\n",
      "101/281, train_loss: 0.0632, step time: 0.2589\n",
      "102/281, train_loss: 0.0484, step time: 0.2562\n",
      "103/281, train_loss: 0.0853, step time: 0.2488\n",
      "104/281, train_loss: 0.1158, step time: 0.2551\n",
      "105/281, train_loss: 0.0431, step time: 0.2532\n",
      "106/281, train_loss: 0.0570, step time: 0.2598\n",
      "107/281, train_loss: 0.1842, step time: 0.2562\n",
      "108/281, train_loss: 0.1239, step time: 0.2590\n",
      "109/281, train_loss: 0.0720, step time: 0.2617\n",
      "110/281, train_loss: 0.1213, step time: 0.2567\n",
      "111/281, train_loss: 0.0719, step time: 0.2585\n",
      "112/281, train_loss: 0.0961, step time: 0.2564\n",
      "113/281, train_loss: 0.0982, step time: 0.2590\n",
      "114/281, train_loss: 0.0857, step time: 0.2579\n",
      "115/281, train_loss: 0.2863, step time: 0.2566\n",
      "116/281, train_loss: 0.0727, step time: 0.2574\n",
      "117/281, train_loss: 0.1120, step time: 0.2539\n",
      "118/281, train_loss: 0.1056, step time: 0.2533\n",
      "119/281, train_loss: 0.1084, step time: 0.2579\n",
      "120/281, train_loss: 0.0614, step time: 0.2557\n",
      "121/281, train_loss: 0.0766, step time: 0.2560\n",
      "122/281, train_loss: 0.0829, step time: 0.2584\n",
      "123/281, train_loss: 0.0958, step time: 0.2575\n",
      "124/281, train_loss: 0.2342, step time: 0.2562\n",
      "125/281, train_loss: 0.0798, step time: 0.2602\n",
      "126/281, train_loss: 0.0924, step time: 0.2578\n",
      "127/281, train_loss: 0.2857, step time: 0.2559\n",
      "128/281, train_loss: 0.0749, step time: 0.2596\n",
      "129/281, train_loss: 0.0956, step time: 0.2602\n",
      "130/281, train_loss: 0.1011, step time: 0.2506\n",
      "131/281, train_loss: 0.0492, step time: 0.2529\n",
      "132/281, train_loss: 0.0759, step time: 0.2586\n",
      "133/281, train_loss: 0.0535, step time: 0.2521\n",
      "134/281, train_loss: 0.0801, step time: 0.2581\n",
      "135/281, train_loss: 0.2332, step time: 0.2575\n",
      "136/281, train_loss: 0.0944, step time: 0.2593\n",
      "137/281, train_loss: 0.1256, step time: 0.2529\n",
      "138/281, train_loss: 0.2397, step time: 0.2576\n",
      "139/281, train_loss: 0.2507, step time: 0.2587\n",
      "140/281, train_loss: 0.2621, step time: 0.2563\n",
      "141/281, train_loss: 0.1251, step time: 0.2571\n",
      "142/281, train_loss: 0.0940, step time: 0.2546\n",
      "143/281, train_loss: 0.1152, step time: 0.2513\n",
      "144/281, train_loss: 0.1359, step time: 0.2544\n",
      "145/281, train_loss: 0.1137, step time: 0.2513\n",
      "146/281, train_loss: 0.0833, step time: 0.2536\n",
      "147/281, train_loss: 0.0767, step time: 0.2577\n",
      "148/281, train_loss: 0.1349, step time: 0.2574\n",
      "149/281, train_loss: 0.1567, step time: 0.2544\n",
      "150/281, train_loss: 0.1168, step time: 0.2601\n",
      "151/281, train_loss: 0.0938, step time: 0.2568\n",
      "152/281, train_loss: 0.0888, step time: 0.2533\n",
      "153/281, train_loss: 0.0873, step time: 0.2565\n",
      "154/281, train_loss: 0.1099, step time: 0.2541\n",
      "155/281, train_loss: 0.0772, step time: 0.2533\n",
      "156/281, train_loss: 0.0573, step time: 0.2521\n",
      "157/281, train_loss: 0.0538, step time: 0.2569\n",
      "158/281, train_loss: 0.0695, step time: 0.2560\n",
      "159/281, train_loss: 0.1004, step time: 0.2537\n",
      "160/281, train_loss: 0.2590, step time: 0.2600\n",
      "161/281, train_loss: 0.0670, step time: 0.2562\n",
      "162/281, train_loss: 0.1363, step time: 0.2588\n",
      "163/281, train_loss: 0.0870, step time: 0.2546\n",
      "164/281, train_loss: 0.0607, step time: 0.2565\n",
      "165/281, train_loss: 0.0780, step time: 0.2536\n",
      "166/281, train_loss: 0.2497, step time: 0.2589\n",
      "167/281, train_loss: 0.1040, step time: 0.2522\n",
      "168/281, train_loss: 0.0707, step time: 0.2575\n",
      "169/281, train_loss: 0.1445, step time: 0.2592\n",
      "170/281, train_loss: 0.1278, step time: 0.2586\n",
      "171/281, train_loss: 0.0697, step time: 0.2587\n",
      "172/281, train_loss: 0.0654, step time: 0.2638\n",
      "173/281, train_loss: 0.0861, step time: 0.2570\n",
      "174/281, train_loss: 0.1201, step time: 0.2558\n",
      "175/281, train_loss: 0.0758, step time: 0.2518\n",
      "176/281, train_loss: 0.0945, step time: 0.2583\n",
      "177/281, train_loss: 0.1613, step time: 0.2533\n",
      "178/281, train_loss: 0.0846, step time: 0.2557\n",
      "179/281, train_loss: 0.2502, step time: 0.2560\n",
      "180/281, train_loss: 0.1086, step time: 0.2662\n",
      "181/281, train_loss: 0.0947, step time: 0.2623\n",
      "182/281, train_loss: 0.1333, step time: 0.2626\n",
      "183/281, train_loss: 0.1677, step time: 0.2553\n",
      "184/281, train_loss: 0.2234, step time: 0.2617\n",
      "185/281, train_loss: 0.1025, step time: 0.2589\n",
      "186/281, train_loss: 0.0539, step time: 0.2561\n",
      "187/281, train_loss: 0.0995, step time: 0.2589\n",
      "188/281, train_loss: 0.1038, step time: 0.2556\n",
      "189/281, train_loss: 0.2880, step time: 0.2527\n",
      "190/281, train_loss: 0.0963, step time: 0.2507\n",
      "191/281, train_loss: 0.1136, step time: 0.2517\n",
      "192/281, train_loss: 0.1497, step time: 0.2576\n",
      "193/281, train_loss: 0.1026, step time: 0.2792\n",
      "194/281, train_loss: 0.0782, step time: 0.2570\n",
      "195/281, train_loss: 0.1382, step time: 0.2543\n",
      "196/281, train_loss: 0.2713, step time: 0.2561\n",
      "197/281, train_loss: 0.0741, step time: 0.2573\n",
      "198/281, train_loss: 0.1187, step time: 0.2581\n",
      "199/281, train_loss: 0.0808, step time: 0.2573\n",
      "200/281, train_loss: 0.0937, step time: 0.2531\n",
      "201/281, train_loss: 0.1157, step time: 0.2562\n",
      "202/281, train_loss: 0.0584, step time: 0.2598\n",
      "203/281, train_loss: 0.1398, step time: 0.2581\n",
      "204/281, train_loss: 0.1129, step time: 0.2587\n",
      "205/281, train_loss: 0.2939, step time: 0.2594\n",
      "206/281, train_loss: 0.1133, step time: 0.2526\n",
      "207/281, train_loss: 0.1093, step time: 0.2574\n",
      "208/281, train_loss: 0.0974, step time: 0.2549\n",
      "209/281, train_loss: 0.0908, step time: 0.2589\n",
      "210/281, train_loss: 0.1140, step time: 0.2590\n",
      "211/281, train_loss: 0.1393, step time: 0.2570\n",
      "212/281, train_loss: 0.1149, step time: 0.2540\n",
      "213/281, train_loss: 0.0745, step time: 0.2589\n",
      "214/281, train_loss: 0.1317, step time: 0.2491\n",
      "215/281, train_loss: 0.0991, step time: 0.2515\n",
      "216/281, train_loss: 0.2307, step time: 0.2577\n",
      "217/281, train_loss: 0.1061, step time: 0.2514\n",
      "218/281, train_loss: 0.0573, step time: 0.2478\n",
      "219/281, train_loss: 0.1645, step time: 0.2533\n",
      "220/281, train_loss: 0.0557, step time: 0.2579\n",
      "221/281, train_loss: 0.2664, step time: 0.2518\n",
      "222/281, train_loss: 0.1451, step time: 0.2519\n",
      "223/281, train_loss: 0.2997, step time: 0.2492\n",
      "224/281, train_loss: 0.1790, step time: 0.2576\n",
      "225/281, train_loss: 0.0992, step time: 0.2512\n",
      "226/281, train_loss: 0.1225, step time: 0.2535\n",
      "227/281, train_loss: 0.0990, step time: 0.2569\n",
      "228/281, train_loss: 0.1170, step time: 0.2552\n",
      "229/281, train_loss: 0.0876, step time: 0.2597\n",
      "230/281, train_loss: 0.1145, step time: 0.2556\n",
      "231/281, train_loss: 0.2280, step time: 0.2557\n",
      "232/281, train_loss: 0.0895, step time: 0.2563\n",
      "233/281, train_loss: 0.0815, step time: 0.2538\n",
      "234/281, train_loss: 0.1835, step time: 0.2547\n",
      "235/281, train_loss: 0.2520, step time: 0.2548\n",
      "236/281, train_loss: 0.0697, step time: 0.2541\n",
      "237/281, train_loss: 0.2751, step time: 0.2586\n",
      "238/281, train_loss: 0.0935, step time: 0.2608\n",
      "239/281, train_loss: 0.2316, step time: 0.2557\n",
      "240/281, train_loss: 0.1150, step time: 0.2541\n",
      "241/281, train_loss: 0.0649, step time: 0.2522\n",
      "242/281, train_loss: 0.1073, step time: 0.2534\n",
      "243/281, train_loss: 0.0957, step time: 0.2572\n",
      "244/281, train_loss: 0.0784, step time: 0.2536\n",
      "245/281, train_loss: 0.0861, step time: 0.2550\n",
      "246/281, train_loss: 0.1728, step time: 0.2560\n",
      "247/281, train_loss: 0.0869, step time: 0.2580\n",
      "248/281, train_loss: 0.0834, step time: 0.2575\n",
      "249/281, train_loss: 0.0965, step time: 0.2511\n",
      "250/281, train_loss: 0.0993, step time: 0.2570\n",
      "251/281, train_loss: 0.0652, step time: 0.2508\n",
      "252/281, train_loss: 0.1071, step time: 0.2539\n",
      "253/281, train_loss: 0.0709, step time: 0.2510\n",
      "254/281, train_loss: 0.1219, step time: 0.2553\n",
      "255/281, train_loss: 0.1402, step time: 0.2522\n",
      "256/281, train_loss: 0.1211, step time: 0.2526\n",
      "257/281, train_loss: 0.4388, step time: 0.2575\n",
      "258/281, train_loss: 0.1103, step time: 0.2498\n",
      "259/281, train_loss: 0.0728, step time: 0.2506\n",
      "260/281, train_loss: 0.1697, step time: 0.2526\n",
      "261/281, train_loss: 0.0832, step time: 0.2487\n",
      "262/281, train_loss: 0.1559, step time: 0.2448\n",
      "263/281, train_loss: 0.0979, step time: 0.2527\n",
      "264/281, train_loss: 0.1031, step time: 0.2535\n",
      "265/281, train_loss: 0.0962, step time: 0.2486\n",
      "266/281, train_loss: 0.0900, step time: 0.2489\n",
      "267/281, train_loss: 0.1186, step time: 0.2537\n",
      "268/281, train_loss: 0.0785, step time: 0.2499\n",
      "269/281, train_loss: 0.0722, step time: 0.2488\n",
      "270/281, train_loss: 0.0876, step time: 0.2471\n",
      "271/281, train_loss: 0.0802, step time: 0.2526\n",
      "272/281, train_loss: 0.0793, step time: 0.2514\n",
      "273/281, train_loss: 0.3327, step time: 0.2569\n",
      "274/281, train_loss: 0.2207, step time: 0.2505\n",
      "275/281, train_loss: 0.0806, step time: 0.2499\n",
      "276/281, train_loss: 0.0761, step time: 0.2535\n",
      "277/281, train_loss: 0.1301, step time: 0.2524\n",
      "278/281, train_loss: 0.0871, step time: 0.2522\n",
      "279/281, train_loss: 0.1100, step time: 0.2518\n",
      "280/281, train_loss: 0.1089, step time: 0.2533\n",
      "281/281, train_loss: 0.0735, step time: 0.2570\n",
      "282/281, train_loss: 0.0527, step time: 0.1511\n",
      "epoch 50 average loss: 0.1236\n",
      "saved new best metric model\n",
      "current epoch: 50 current mean dice: 0.8744 tc: 0.8656 wt: 0.9151 et: 0.8516\n",
      "best mean dice: 0.8744 at epoch: 50\n",
      "time consuming of epoch 50 is: 422.2942\n",
      "----------\n",
      "epoch 51/200\n",
      "1/281, train_loss: 0.0826, step time: 0.2583\n",
      "2/281, train_loss: 0.1069, step time: 0.2520\n",
      "3/281, train_loss: 0.0837, step time: 0.2516\n",
      "4/281, train_loss: 0.0850, step time: 0.2519\n",
      "5/281, train_loss: 0.0968, step time: 0.2623\n",
      "6/281, train_loss: 0.1189, step time: 0.2547\n",
      "7/281, train_loss: 0.2847, step time: 0.2539\n",
      "8/281, train_loss: 0.1012, step time: 0.2591\n",
      "9/281, train_loss: 0.2736, step time: 0.2523\n",
      "10/281, train_loss: 0.1000, step time: 0.2520\n",
      "11/281, train_loss: 0.0780, step time: 0.2517\n",
      "12/281, train_loss: 0.2264, step time: 0.2522\n",
      "13/281, train_loss: 0.0775, step time: 0.2593\n",
      "14/281, train_loss: 0.0753, step time: 0.2581\n",
      "15/281, train_loss: 0.1231, step time: 0.2661\n",
      "16/281, train_loss: 0.2601, step time: 0.2613\n",
      "17/281, train_loss: 0.0974, step time: 0.2556\n",
      "18/281, train_loss: 0.1308, step time: 0.2563\n",
      "19/281, train_loss: 0.0849, step time: 0.2556\n",
      "20/281, train_loss: 0.1042, step time: 0.2559\n",
      "21/281, train_loss: 0.2307, step time: 0.2605\n",
      "22/281, train_loss: 0.2744, step time: 0.2581\n",
      "23/281, train_loss: 0.0992, step time: 0.2815\n",
      "24/281, train_loss: 0.0824, step time: 0.2606\n",
      "25/281, train_loss: 0.2739, step time: 0.2486\n",
      "26/281, train_loss: 0.1009, step time: 0.2532\n",
      "27/281, train_loss: 0.0867, step time: 0.2540\n",
      "28/281, train_loss: 0.0962, step time: 0.2510\n",
      "29/281, train_loss: 0.0829, step time: 0.2591\n",
      "30/281, train_loss: 0.1361, step time: 0.2570\n",
      "31/281, train_loss: 0.0785, step time: 0.2674\n",
      "32/281, train_loss: 0.1032, step time: 0.2526\n",
      "33/281, train_loss: 0.1291, step time: 0.2541\n",
      "34/281, train_loss: 0.1032, step time: 0.2770\n",
      "35/281, train_loss: 0.1179, step time: 0.2503\n",
      "36/281, train_loss: 0.1502, step time: 0.2522\n",
      "37/281, train_loss: 0.0939, step time: 0.2568\n",
      "38/281, train_loss: 0.0900, step time: 0.2584\n",
      "39/281, train_loss: 0.0937, step time: 0.2595\n",
      "40/281, train_loss: 0.1052, step time: 0.2528\n",
      "41/281, train_loss: 0.0649, step time: 0.2509\n",
      "42/281, train_loss: 0.0701, step time: 0.2508\n",
      "43/281, train_loss: 0.1143, step time: 0.2481\n",
      "44/281, train_loss: 0.0505, step time: 0.2509\n",
      "45/281, train_loss: 0.1002, step time: 0.2555\n",
      "46/281, train_loss: 0.2230, step time: 0.2585\n",
      "47/281, train_loss: 0.1609, step time: 0.2554\n",
      "48/281, train_loss: 0.0911, step time: 0.2561\n",
      "49/281, train_loss: 0.0755, step time: 0.2544\n",
      "50/281, train_loss: 0.1430, step time: 0.2544\n",
      "51/281, train_loss: 0.0716, step time: 0.2473\n",
      "52/281, train_loss: 0.1162, step time: 0.2548\n",
      "53/281, train_loss: 0.0736, step time: 0.2528\n",
      "54/281, train_loss: 0.0836, step time: 0.2541\n",
      "55/281, train_loss: 0.2649, step time: 0.2571\n",
      "56/281, train_loss: 0.1539, step time: 0.2495\n",
      "57/281, train_loss: 0.2342, step time: 0.2511\n",
      "58/281, train_loss: 0.0980, step time: 0.2473\n",
      "59/281, train_loss: 0.0611, step time: 0.2649\n",
      "60/281, train_loss: 0.0624, step time: 0.2485\n",
      "61/281, train_loss: 0.1291, step time: 0.2590\n",
      "62/281, train_loss: 0.0855, step time: 0.2593\n",
      "63/281, train_loss: 0.0957, step time: 0.2535\n",
      "64/281, train_loss: 0.0837, step time: 0.2554\n",
      "65/281, train_loss: 0.0828, step time: 0.2582\n",
      "66/281, train_loss: 0.0776, step time: 0.2581\n",
      "67/281, train_loss: 0.0858, step time: 0.2545\n",
      "68/281, train_loss: 0.2568, step time: 0.2541\n",
      "69/281, train_loss: 0.0795, step time: 0.2583\n",
      "70/281, train_loss: 0.0892, step time: 0.2539\n",
      "71/281, train_loss: 0.1307, step time: 0.2599\n",
      "72/281, train_loss: 0.0868, step time: 0.2534\n",
      "73/281, train_loss: 0.0725, step time: 0.2513\n",
      "74/281, train_loss: 0.0707, step time: 0.2477\n",
      "75/281, train_loss: 0.1148, step time: 0.2593\n",
      "76/281, train_loss: 0.1117, step time: 0.2552\n",
      "77/281, train_loss: 0.2429, step time: 0.2504\n",
      "78/281, train_loss: 0.0889, step time: 0.2524\n",
      "79/281, train_loss: 0.1157, step time: 0.2490\n",
      "80/281, train_loss: 0.0766, step time: 0.2543\n",
      "81/281, train_loss: 0.0913, step time: 0.2526\n",
      "82/281, train_loss: 0.0775, step time: 0.2507\n",
      "83/281, train_loss: 0.0616, step time: 0.2584\n",
      "84/281, train_loss: 0.1270, step time: 0.2573\n",
      "85/281, train_loss: 0.1710, step time: 0.2595\n",
      "86/281, train_loss: 0.0787, step time: 0.2544\n",
      "87/281, train_loss: 0.1617, step time: 0.2540\n",
      "88/281, train_loss: 0.0984, step time: 0.2563\n",
      "89/281, train_loss: 0.2533, step time: 0.2570\n",
      "90/281, train_loss: 0.0826, step time: 0.2569\n",
      "91/281, train_loss: 0.3302, step time: 0.2562\n",
      "92/281, train_loss: 0.1644, step time: 0.2529\n",
      "93/281, train_loss: 0.0791, step time: 0.2574\n",
      "94/281, train_loss: 0.0695, step time: 0.2541\n",
      "95/281, train_loss: 0.1109, step time: 0.2529\n",
      "96/281, train_loss: 0.0523, step time: 0.2549\n",
      "97/281, train_loss: 0.0726, step time: 0.2560\n",
      "98/281, train_loss: 0.0894, step time: 0.2503\n",
      "99/281, train_loss: 0.2587, step time: 0.2557\n",
      "100/281, train_loss: 0.0801, step time: 0.2529\n",
      "101/281, train_loss: 0.0793, step time: 0.2555\n",
      "102/281, train_loss: 0.1180, step time: 0.2553\n",
      "103/281, train_loss: 0.0797, step time: 0.2480\n",
      "104/281, train_loss: 0.2976, step time: 0.2569\n",
      "105/281, train_loss: 0.0874, step time: 0.2494\n",
      "106/281, train_loss: 0.0743, step time: 0.2530\n",
      "107/281, train_loss: 0.2567, step time: 0.2539\n",
      "108/281, train_loss: 0.1194, step time: 0.2508\n",
      "109/281, train_loss: 0.0641, step time: 0.2531\n",
      "110/281, train_loss: 0.0542, step time: 0.2486\n",
      "111/281, train_loss: 0.1116, step time: 0.2594\n",
      "112/281, train_loss: 0.0466, step time: 0.2551\n",
      "113/281, train_loss: 0.0744, step time: 0.2552\n",
      "114/281, train_loss: 0.1179, step time: 0.2542\n",
      "115/281, train_loss: 0.0696, step time: 0.2567\n",
      "116/281, train_loss: 0.2678, step time: 0.2504\n",
      "117/281, train_loss: 0.3047, step time: 0.2521\n",
      "118/281, train_loss: 0.0562, step time: 0.2492\n",
      "119/281, train_loss: 0.1400, step time: 0.2548\n",
      "120/281, train_loss: 0.1224, step time: 0.2530\n",
      "121/281, train_loss: 0.1066, step time: 0.2490\n",
      "122/281, train_loss: 0.0807, step time: 0.2565\n",
      "123/281, train_loss: 0.2727, step time: 0.2521\n",
      "124/281, train_loss: 0.1218, step time: 0.2505\n",
      "125/281, train_loss: 0.1562, step time: 0.2560\n",
      "126/281, train_loss: 0.1038, step time: 0.2559\n",
      "127/281, train_loss: 0.1034, step time: 0.2533\n",
      "128/281, train_loss: 0.1182, step time: 0.2544\n",
      "129/281, train_loss: 0.1155, step time: 0.2470\n",
      "130/281, train_loss: 0.0978, step time: 0.2527\n",
      "131/281, train_loss: 0.2314, step time: 0.2491\n",
      "132/281, train_loss: 0.1162, step time: 0.2531\n",
      "133/281, train_loss: 0.0787, step time: 0.2546\n",
      "134/281, train_loss: 0.1753, step time: 0.2584\n",
      "135/281, train_loss: 0.0550, step time: 0.2559\n",
      "136/281, train_loss: 0.1171, step time: 0.2696\n",
      "137/281, train_loss: 0.1089, step time: 0.2513\n",
      "138/281, train_loss: 0.0679, step time: 0.2492\n",
      "139/281, train_loss: 0.0706, step time: 0.2557\n",
      "140/281, train_loss: 0.0963, step time: 0.2557\n",
      "141/281, train_loss: 0.1047, step time: 0.2598\n",
      "142/281, train_loss: 0.1211, step time: 0.2531\n",
      "143/281, train_loss: 0.1070, step time: 0.2484\n",
      "144/281, train_loss: 0.1028, step time: 0.2546\n",
      "145/281, train_loss: 0.1057, step time: 0.2552\n",
      "146/281, train_loss: 0.2093, step time: 0.2531\n",
      "147/281, train_loss: 0.0598, step time: 0.2492\n",
      "148/281, train_loss: 0.0678, step time: 0.2517\n",
      "149/281, train_loss: 0.2160, step time: 0.2541\n",
      "150/281, train_loss: 0.1222, step time: 0.2537\n",
      "151/281, train_loss: 0.0810, step time: 0.2567\n",
      "152/281, train_loss: 0.1338, step time: 0.2554\n",
      "153/281, train_loss: 0.2479, step time: 0.2494\n",
      "154/281, train_loss: 0.1811, step time: 0.2534\n",
      "155/281, train_loss: 0.2865, step time: 0.2534\n",
      "156/281, train_loss: 0.0587, step time: 0.2537\n",
      "157/281, train_loss: 0.0735, step time: 0.2527\n",
      "158/281, train_loss: 0.0996, step time: 0.2521\n",
      "159/281, train_loss: 0.1264, step time: 0.2539\n",
      "160/281, train_loss: 0.0917, step time: 0.2505\n",
      "161/281, train_loss: 0.0651, step time: 0.2521\n",
      "162/281, train_loss: 0.1282, step time: 0.2521\n",
      "163/281, train_loss: 0.1366, step time: 0.2497\n",
      "164/281, train_loss: 0.0934, step time: 0.2492\n",
      "165/281, train_loss: 0.1451, step time: 0.2540\n",
      "166/281, train_loss: 0.1356, step time: 0.2503\n",
      "167/281, train_loss: 0.1199, step time: 0.2531\n",
      "168/281, train_loss: 0.1903, step time: 0.2549\n",
      "169/281, train_loss: 0.0808, step time: 0.2538\n",
      "170/281, train_loss: 0.0548, step time: 0.2528\n",
      "171/281, train_loss: 0.0467, step time: 0.2522\n",
      "172/281, train_loss: 0.0865, step time: 0.2557\n",
      "173/281, train_loss: 0.0700, step time: 0.2563\n",
      "174/281, train_loss: 0.0744, step time: 0.2549\n",
      "175/281, train_loss: 0.0835, step time: 0.2551\n",
      "176/281, train_loss: 0.1323, step time: 0.2539\n",
      "177/281, train_loss: 0.2344, step time: 0.2576\n",
      "178/281, train_loss: 0.1011, step time: 0.2565\n",
      "179/281, train_loss: 0.0431, step time: 0.2515\n",
      "180/281, train_loss: 0.0541, step time: 0.2438\n",
      "181/281, train_loss: 0.0971, step time: 0.2444\n",
      "182/281, train_loss: 0.1703, step time: 0.2438\n",
      "183/281, train_loss: 0.1121, step time: 0.2469\n",
      "184/281, train_loss: 0.0809, step time: 0.2552\n",
      "185/281, train_loss: 0.1152, step time: 0.2479\n",
      "186/281, train_loss: 0.0680, step time: 0.2476\n",
      "187/281, train_loss: 0.4051, step time: 0.2464\n",
      "188/281, train_loss: 0.1545, step time: 0.2503\n",
      "189/281, train_loss: 0.0643, step time: 0.2513\n",
      "190/281, train_loss: 0.1693, step time: 0.2475\n",
      "191/281, train_loss: 0.0603, step time: 0.2455\n",
      "192/281, train_loss: 0.1134, step time: 0.2516\n",
      "193/281, train_loss: 0.1416, step time: 0.2485\n",
      "194/281, train_loss: 0.0942, step time: 0.2467\n",
      "195/281, train_loss: 0.0672, step time: 0.2502\n",
      "196/281, train_loss: 0.1129, step time: 0.2542\n",
      "197/281, train_loss: 0.0881, step time: 0.2506\n",
      "198/281, train_loss: 0.1246, step time: 0.2474\n",
      "199/281, train_loss: 0.1507, step time: 0.2481\n",
      "200/281, train_loss: 0.2740, step time: 0.2487\n",
      "201/281, train_loss: 0.0623, step time: 0.2422\n",
      "202/281, train_loss: 0.1054, step time: 0.2477\n",
      "203/281, train_loss: 0.0970, step time: 0.2535\n",
      "204/281, train_loss: 0.0991, step time: 0.2487\n",
      "205/281, train_loss: 0.2835, step time: 0.2482\n",
      "206/281, train_loss: 0.0863, step time: 0.2503\n",
      "207/281, train_loss: 0.1470, step time: 0.2493\n",
      "208/281, train_loss: 0.1316, step time: 0.2526\n",
      "209/281, train_loss: 0.0673, step time: 0.2515\n",
      "210/281, train_loss: 0.2668, step time: 0.2428\n",
      "211/281, train_loss: 0.2352, step time: 0.2422\n",
      "212/281, train_loss: 0.2836, step time: 0.2545\n",
      "213/281, train_loss: 0.2329, step time: 0.2506\n",
      "214/281, train_loss: 0.1140, step time: 0.2514\n",
      "215/281, train_loss: 0.0951, step time: 0.2496\n",
      "216/281, train_loss: 0.0955, step time: 0.2537\n",
      "217/281, train_loss: 0.0974, step time: 0.2529\n",
      "218/281, train_loss: 0.1122, step time: 0.2459\n",
      "219/281, train_loss: 0.2236, step time: 0.2435\n",
      "220/281, train_loss: 0.1255, step time: 0.2511\n",
      "221/281, train_loss: 0.0922, step time: 0.2502\n",
      "222/281, train_loss: 0.1306, step time: 0.2454\n",
      "223/281, train_loss: 0.0877, step time: 0.2468\n",
      "224/281, train_loss: 0.0579, step time: 0.2489\n",
      "225/281, train_loss: 0.2468, step time: 0.2511\n",
      "226/281, train_loss: 0.0980, step time: 0.2551\n",
      "227/281, train_loss: 0.0586, step time: 0.2569\n",
      "228/281, train_loss: 0.1253, step time: 0.2570\n",
      "229/281, train_loss: 0.1150, step time: 0.2542\n",
      "230/281, train_loss: 0.1139, step time: 0.2517\n",
      "231/281, train_loss: 0.0606, step time: 0.2508\n",
      "232/281, train_loss: 0.1477, step time: 0.2570\n",
      "233/281, train_loss: 0.0761, step time: 0.2512\n",
      "234/281, train_loss: 0.0574, step time: 0.2486\n",
      "235/281, train_loss: 0.2793, step time: 0.2442\n",
      "236/281, train_loss: 0.1359, step time: 0.2452\n",
      "237/281, train_loss: 0.3941, step time: 0.2461\n",
      "238/281, train_loss: 0.0767, step time: 0.2497\n",
      "239/281, train_loss: 0.0693, step time: 0.2499\n",
      "240/281, train_loss: 0.1105, step time: 0.2530\n",
      "241/281, train_loss: 0.1340, step time: 0.2490\n",
      "242/281, train_loss: 0.0522, step time: 0.2440\n",
      "243/281, train_loss: 0.1031, step time: 0.2417\n",
      "244/281, train_loss: 0.0928, step time: 0.2490\n",
      "245/281, train_loss: 0.0651, step time: 0.2485\n",
      "246/281, train_loss: 0.2954, step time: 0.2513\n",
      "247/281, train_loss: 0.0781, step time: 0.2481\n",
      "248/281, train_loss: 0.0973, step time: 0.2527\n",
      "249/281, train_loss: 0.0907, step time: 0.2462\n",
      "250/281, train_loss: 0.0684, step time: 0.2528\n",
      "251/281, train_loss: 0.1317, step time: 0.2458\n",
      "252/281, train_loss: 0.1016, step time: 0.2464\n",
      "253/281, train_loss: 0.0897, step time: 0.2464\n",
      "254/281, train_loss: 0.0821, step time: 0.2476\n",
      "255/281, train_loss: 0.0986, step time: 0.2502\n",
      "256/281, train_loss: 0.0861, step time: 0.2509\n",
      "257/281, train_loss: 0.3169, step time: 0.2477\n",
      "258/281, train_loss: 0.1243, step time: 0.2511\n",
      "259/281, train_loss: 0.0905, step time: 0.2503\n",
      "260/281, train_loss: 0.0806, step time: 0.2529\n",
      "261/281, train_loss: 0.1057, step time: 0.2515\n",
      "262/281, train_loss: 0.0540, step time: 0.2473\n",
      "263/281, train_loss: 0.2759, step time: 0.2441\n",
      "264/281, train_loss: 0.2486, step time: 0.2435\n",
      "265/281, train_loss: 0.0973, step time: 0.2468\n",
      "266/281, train_loss: 0.1103, step time: 0.2461\n",
      "267/281, train_loss: 0.2620, step time: 0.2480\n",
      "268/281, train_loss: 0.0945, step time: 0.2455\n",
      "269/281, train_loss: 0.0968, step time: 0.2514\n",
      "270/281, train_loss: 0.0792, step time: 0.2514\n",
      "271/281, train_loss: 0.2383, step time: 0.2510\n",
      "272/281, train_loss: 0.1365, step time: 0.2538\n",
      "273/281, train_loss: 0.0660, step time: 0.2541\n",
      "274/281, train_loss: 0.1193, step time: 0.2538\n",
      "275/281, train_loss: 0.0838, step time: 0.2500\n",
      "276/281, train_loss: 0.1070, step time: 0.2510\n",
      "277/281, train_loss: 0.0813, step time: 0.2546\n",
      "278/281, train_loss: 0.1083, step time: 0.2465\n",
      "279/281, train_loss: 0.1142, step time: 0.2503\n",
      "280/281, train_loss: 0.1831, step time: 0.2449\n",
      "281/281, train_loss: 0.0955, step time: 0.2449\n",
      "282/281, train_loss: 0.1201, step time: 0.1479\n",
      "epoch 51 average loss: 0.1243\n",
      "current epoch: 51 current mean dice: 0.8708 tc: 0.8597 wt: 0.9153 et: 0.8479\n",
      "best mean dice: 0.8744 at epoch: 50\n",
      "time consuming of epoch 51 is: 405.6465\n",
      "----------\n",
      "epoch 52/200\n",
      "1/281, train_loss: 0.0637, step time: 0.2564\n",
      "2/281, train_loss: 0.0830, step time: 0.2509\n",
      "3/281, train_loss: 0.2753, step time: 0.2524\n",
      "4/281, train_loss: 0.2309, step time: 0.2539\n",
      "5/281, train_loss: 0.2773, step time: 0.2548\n",
      "6/281, train_loss: 0.2693, step time: 0.2532\n",
      "7/281, train_loss: 0.2640, step time: 0.2512\n",
      "8/281, train_loss: 0.0970, step time: 0.2514\n",
      "9/281, train_loss: 0.1087, step time: 0.2482\n",
      "10/281, train_loss: 0.0824, step time: 0.2428\n",
      "11/281, train_loss: 0.3102, step time: 0.2535\n",
      "12/281, train_loss: 0.0870, step time: 0.2496\n",
      "13/281, train_loss: 0.1089, step time: 0.2518\n",
      "14/281, train_loss: 0.0790, step time: 0.2534\n",
      "15/281, train_loss: 0.0663, step time: 0.2529\n",
      "16/281, train_loss: 0.1192, step time: 0.2537\n",
      "17/281, train_loss: 0.1423, step time: 0.2498\n",
      "18/281, train_loss: 0.2449, step time: 0.2605\n",
      "19/281, train_loss: 0.2845, step time: 0.2654\n",
      "20/281, train_loss: 0.0840, step time: 0.2557\n",
      "21/281, train_loss: 0.0921, step time: 0.2571\n",
      "22/281, train_loss: 0.0714, step time: 0.2514\n",
      "23/281, train_loss: 0.0966, step time: 0.2505\n",
      "24/281, train_loss: 0.1135, step time: 0.2493\n",
      "25/281, train_loss: 0.0903, step time: 0.2523\n",
      "26/281, train_loss: 0.2796, step time: 0.2511\n",
      "27/281, train_loss: 0.0999, step time: 0.2475\n",
      "28/281, train_loss: 0.1090, step time: 0.2463\n",
      "29/281, train_loss: 0.0921, step time: 0.2475\n",
      "30/281, train_loss: 0.1183, step time: 0.2494\n",
      "31/281, train_loss: 0.0717, step time: 0.2491\n",
      "32/281, train_loss: 0.0854, step time: 0.2481\n",
      "33/281, train_loss: 0.1188, step time: 0.2496\n",
      "34/281, train_loss: 0.1044, step time: 0.2510\n",
      "35/281, train_loss: 0.0896, step time: 0.2440\n",
      "36/281, train_loss: 0.1115, step time: 0.2495\n",
      "37/281, train_loss: 0.0534, step time: 0.2484\n",
      "38/281, train_loss: 0.0869, step time: 0.2600\n",
      "39/281, train_loss: 0.1389, step time: 0.2481\n",
      "40/281, train_loss: 0.0858, step time: 0.2474\n",
      "41/281, train_loss: 0.1489, step time: 0.2489\n",
      "42/281, train_loss: 0.2581, step time: 0.2505\n",
      "43/281, train_loss: 0.0755, step time: 0.2535\n",
      "44/281, train_loss: 0.1257, step time: 0.2503\n",
      "45/281, train_loss: 0.0753, step time: 0.2499\n",
      "46/281, train_loss: 0.0819, step time: 0.2494\n",
      "47/281, train_loss: 0.1306, step time: 0.2519\n",
      "48/281, train_loss: 0.2463, step time: 0.2464\n",
      "49/281, train_loss: 0.0802, step time: 0.2512\n",
      "50/281, train_loss: 0.0954, step time: 0.2574\n",
      "51/281, train_loss: 0.0954, step time: 0.2716\n",
      "52/281, train_loss: 0.0949, step time: 0.2527\n",
      "53/281, train_loss: 0.0612, step time: 0.2508\n",
      "54/281, train_loss: 0.0953, step time: 0.2511\n",
      "55/281, train_loss: 0.1415, step time: 0.2526\n",
      "56/281, train_loss: 0.0601, step time: 0.2473\n",
      "57/281, train_loss: 0.2469, step time: 0.2447\n",
      "58/281, train_loss: 0.1064, step time: 0.2534\n",
      "59/281, train_loss: 0.0982, step time: 0.2561\n",
      "60/281, train_loss: 0.0608, step time: 0.2470\n",
      "61/281, train_loss: 0.0820, step time: 0.2530\n",
      "62/281, train_loss: 0.1469, step time: 0.2555\n",
      "63/281, train_loss: 0.0851, step time: 0.2544\n",
      "64/281, train_loss: 0.0811, step time: 0.2471\n",
      "65/281, train_loss: 0.0696, step time: 0.2494\n",
      "66/281, train_loss: 0.0622, step time: 0.2461\n",
      "67/281, train_loss: 0.1468, step time: 0.2479\n",
      "68/281, train_loss: 0.1703, step time: 0.2532\n",
      "69/281, train_loss: 0.0561, step time: 0.2460\n",
      "70/281, train_loss: 0.1294, step time: 0.2473\n",
      "71/281, train_loss: 0.1171, step time: 0.2517\n",
      "72/281, train_loss: 0.0666, step time: 0.2508\n",
      "73/281, train_loss: 0.0979, step time: 0.2469\n",
      "74/281, train_loss: 0.1004, step time: 0.2528\n",
      "75/281, train_loss: 0.0654, step time: 0.2541\n",
      "76/281, train_loss: 0.1257, step time: 0.2477\n",
      "77/281, train_loss: 0.1402, step time: 0.2509\n",
      "78/281, train_loss: 0.1168, step time: 0.2509\n",
      "79/281, train_loss: 0.0653, step time: 0.2562\n",
      "80/281, train_loss: 0.1210, step time: 0.2514\n",
      "81/281, train_loss: 0.2437, step time: 0.2510\n",
      "82/281, train_loss: 0.2431, step time: 0.2509\n",
      "83/281, train_loss: 0.0904, step time: 0.2544\n",
      "84/281, train_loss: 0.1204, step time: 0.2508\n",
      "85/281, train_loss: 0.0670, step time: 0.2572\n",
      "86/281, train_loss: 0.1481, step time: 0.2531\n",
      "87/281, train_loss: 0.0670, step time: 0.2505\n",
      "88/281, train_loss: 0.1447, step time: 0.2529\n",
      "89/281, train_loss: 0.0653, step time: 0.2503\n",
      "90/281, train_loss: 0.0896, step time: 0.2555\n",
      "91/281, train_loss: 0.0860, step time: 0.2537\n",
      "92/281, train_loss: 0.1356, step time: 0.2485\n",
      "93/281, train_loss: 0.1035, step time: 0.2513\n",
      "94/281, train_loss: 0.0999, step time: 0.2509\n",
      "95/281, train_loss: 0.2606, step time: 0.2506\n",
      "96/281, train_loss: 0.1296, step time: 0.2516\n",
      "97/281, train_loss: 0.0889, step time: 0.2451\n",
      "98/281, train_loss: 0.2252, step time: 0.2516\n",
      "99/281, train_loss: 0.1372, step time: 0.2508\n",
      "100/281, train_loss: 0.0815, step time: 0.2527\n",
      "101/281, train_loss: 0.0432, step time: 0.2477\n",
      "102/281, train_loss: 0.0787, step time: 0.2631\n",
      "103/281, train_loss: 0.1344, step time: 0.2532\n",
      "104/281, train_loss: 0.0889, step time: 0.2480\n",
      "105/281, train_loss: 0.2282, step time: 0.2459\n",
      "106/281, train_loss: 0.0500, step time: 0.2586\n",
      "107/281, train_loss: 0.1256, step time: 0.2553\n",
      "108/281, train_loss: 0.2599, step time: 0.2582\n",
      "109/281, train_loss: 0.1043, step time: 0.2516\n",
      "110/281, train_loss: 0.1422, step time: 0.2508\n",
      "111/281, train_loss: 0.0663, step time: 0.2529\n",
      "112/281, train_loss: 0.1116, step time: 0.2535\n",
      "113/281, train_loss: 0.1429, step time: 0.2553\n",
      "114/281, train_loss: 0.0755, step time: 0.2519\n",
      "115/281, train_loss: 0.0693, step time: 0.2536\n",
      "116/281, train_loss: 0.0630, step time: 0.2520\n",
      "117/281, train_loss: 0.0815, step time: 0.2486\n",
      "118/281, train_loss: 0.1019, step time: 0.2537\n",
      "119/281, train_loss: 0.1008, step time: 0.2514\n",
      "120/281, train_loss: 0.0517, step time: 0.2539\n",
      "121/281, train_loss: 0.2636, step time: 0.2552\n",
      "122/281, train_loss: 0.0551, step time: 0.2470\n",
      "123/281, train_loss: 0.1116, step time: 0.2489\n",
      "124/281, train_loss: 0.1048, step time: 0.2490\n",
      "125/281, train_loss: 0.2817, step time: 0.2502\n",
      "126/281, train_loss: 0.2070, step time: 0.2524\n",
      "127/281, train_loss: 0.1056, step time: 0.2502\n",
      "128/281, train_loss: 0.0852, step time: 0.2516\n",
      "129/281, train_loss: 0.0760, step time: 0.2554\n",
      "130/281, train_loss: 0.1211, step time: 0.2562\n",
      "131/281, train_loss: 0.0868, step time: 0.2554\n",
      "132/281, train_loss: 0.2615, step time: 0.2479\n",
      "133/281, train_loss: 0.1359, step time: 0.2492\n",
      "134/281, train_loss: 0.1158, step time: 0.2576\n",
      "135/281, train_loss: 0.0752, step time: 0.2542\n",
      "136/281, train_loss: 0.1416, step time: 0.2506\n",
      "137/281, train_loss: 0.2456, step time: 0.2489\n",
      "138/281, train_loss: 0.1008, step time: 0.2536\n",
      "139/281, train_loss: 0.1030, step time: 0.2536\n",
      "140/281, train_loss: 0.0968, step time: 0.2530\n",
      "141/281, train_loss: 0.1027, step time: 0.2519\n",
      "142/281, train_loss: 0.1030, step time: 0.2592\n",
      "143/281, train_loss: 0.2592, step time: 0.2500\n",
      "144/281, train_loss: 0.0783, step time: 0.2487\n",
      "145/281, train_loss: 0.1528, step time: 0.2485\n",
      "146/281, train_loss: 0.2501, step time: 0.2490\n",
      "147/281, train_loss: 0.0965, step time: 0.2509\n",
      "148/281, train_loss: 0.2678, step time: 0.2499\n",
      "149/281, train_loss: 0.0895, step time: 0.2507\n",
      "150/281, train_loss: 0.0873, step time: 0.2520\n",
      "151/281, train_loss: 0.0678, step time: 0.2463\n",
      "152/281, train_loss: 0.0759, step time: 0.2533\n",
      "153/281, train_loss: 0.1084, step time: 0.2483\n",
      "154/281, train_loss: 0.2612, step time: 0.2518\n",
      "155/281, train_loss: 0.0841, step time: 0.2473\n",
      "156/281, train_loss: 0.0973, step time: 0.2495\n",
      "157/281, train_loss: 0.2841, step time: 0.2467\n",
      "158/281, train_loss: 0.0790, step time: 0.2490\n",
      "159/281, train_loss: 0.1367, step time: 0.2585\n",
      "160/281, train_loss: 0.2306, step time: 0.2565\n",
      "161/281, train_loss: 0.0706, step time: 0.2551\n",
      "162/281, train_loss: 0.1232, step time: 0.2510\n",
      "163/281, train_loss: 0.0636, step time: 0.2509\n",
      "164/281, train_loss: 0.0982, step time: 0.2495\n",
      "165/281, train_loss: 0.3220, step time: 0.2487\n",
      "166/281, train_loss: 0.1443, step time: 0.2545\n",
      "167/281, train_loss: 0.2257, step time: 0.2592\n",
      "168/281, train_loss: 0.2975, step time: 0.2541\n",
      "169/281, train_loss: 0.0929, step time: 0.2477\n",
      "170/281, train_loss: 0.1116, step time: 0.2549\n",
      "171/281, train_loss: 0.0943, step time: 0.2569\n",
      "172/281, train_loss: 0.1129, step time: 0.2540\n",
      "173/281, train_loss: 0.1312, step time: 0.2471\n",
      "174/281, train_loss: 0.0969, step time: 0.2521\n",
      "175/281, train_loss: 0.0792, step time: 0.2557\n",
      "176/281, train_loss: 0.0786, step time: 0.2531\n",
      "177/281, train_loss: 0.1288, step time: 0.2504\n",
      "178/281, train_loss: 0.0923, step time: 0.2527\n",
      "179/281, train_loss: 0.3271, step time: 0.2510\n",
      "180/281, train_loss: 0.0918, step time: 0.2527\n",
      "181/281, train_loss: 0.0772, step time: 0.2493\n",
      "182/281, train_loss: 0.1000, step time: 0.2528\n",
      "183/281, train_loss: 0.1345, step time: 0.2493\n",
      "184/281, train_loss: 0.0858, step time: 0.2526\n",
      "185/281, train_loss: 0.1093, step time: 0.2561\n",
      "186/281, train_loss: 0.2662, step time: 0.2563\n",
      "187/281, train_loss: 0.1427, step time: 0.2540\n",
      "188/281, train_loss: 0.0761, step time: 0.2516\n",
      "189/281, train_loss: 0.0932, step time: 0.2579\n",
      "190/281, train_loss: 0.0924, step time: 0.2551\n",
      "191/281, train_loss: 0.0988, step time: 0.2497\n",
      "192/281, train_loss: 0.1062, step time: 0.2498\n",
      "193/281, train_loss: 0.0647, step time: 0.2463\n",
      "194/281, train_loss: 0.0956, step time: 0.2573\n",
      "195/281, train_loss: 0.2397, step time: 0.2531\n",
      "196/281, train_loss: 0.0901, step time: 0.2481\n",
      "197/281, train_loss: 0.1158, step time: 0.2518\n",
      "198/281, train_loss: 0.0681, step time: 0.2509\n",
      "199/281, train_loss: 0.2301, step time: 0.2530\n",
      "200/281, train_loss: 0.0843, step time: 0.2482\n",
      "201/281, train_loss: 0.0891, step time: 0.2518\n",
      "202/281, train_loss: 0.0554, step time: 0.2547\n",
      "203/281, train_loss: 0.1180, step time: 0.2563\n",
      "204/281, train_loss: 0.2558, step time: 0.2518\n",
      "205/281, train_loss: 0.0817, step time: 0.2487\n",
      "206/281, train_loss: 0.2832, step time: 0.2530\n",
      "207/281, train_loss: 0.0983, step time: 0.2514\n",
      "208/281, train_loss: 0.1517, step time: 0.2469\n",
      "209/281, train_loss: 0.0860, step time: 0.2494\n",
      "210/281, train_loss: 0.1681, step time: 0.2562\n",
      "211/281, train_loss: 0.0678, step time: 0.2575\n",
      "212/281, train_loss: 0.1271, step time: 0.2536\n",
      "213/281, train_loss: 0.0737, step time: 0.2499\n",
      "214/281, train_loss: 0.2502, step time: 0.2533\n",
      "215/281, train_loss: 0.0919, step time: 0.2558\n",
      "216/281, train_loss: 0.0731, step time: 0.2553\n",
      "217/281, train_loss: 0.0723, step time: 0.2565\n",
      "218/281, train_loss: 0.0569, step time: 0.2568\n",
      "219/281, train_loss: 0.1303, step time: 0.2547\n",
      "220/281, train_loss: 0.0666, step time: 0.2510\n",
      "221/281, train_loss: 0.0980, step time: 0.2524\n",
      "222/281, train_loss: 0.0838, step time: 0.2797\n",
      "223/281, train_loss: 0.1175, step time: 0.2529\n",
      "224/281, train_loss: 0.0894, step time: 0.2551\n",
      "225/281, train_loss: 0.1148, step time: 0.2594\n",
      "226/281, train_loss: 0.0580, step time: 0.2567\n",
      "227/281, train_loss: 0.0858, step time: 0.2523\n",
      "228/281, train_loss: 0.0954, step time: 0.2537\n",
      "229/281, train_loss: 0.1317, step time: 0.2576\n",
      "230/281, train_loss: 0.0567, step time: 0.2538\n",
      "231/281, train_loss: 0.0788, step time: 0.2595\n",
      "232/281, train_loss: 0.2495, step time: 0.2573\n",
      "233/281, train_loss: 0.0659, step time: 0.2577\n",
      "234/281, train_loss: 0.0865, step time: 0.2547\n",
      "235/281, train_loss: 0.1090, step time: 0.2542\n",
      "236/281, train_loss: 0.0746, step time: 0.2532\n",
      "237/281, train_loss: 0.2384, step time: 0.2586\n",
      "238/281, train_loss: 0.0753, step time: 0.2612\n",
      "239/281, train_loss: 0.0840, step time: 0.2582\n",
      "240/281, train_loss: 0.1066, step time: 0.2499\n",
      "241/281, train_loss: 0.0568, step time: 0.2530\n",
      "242/281, train_loss: 0.1237, step time: 0.2562\n",
      "243/281, train_loss: 0.0669, step time: 0.2561\n",
      "244/281, train_loss: 0.1127, step time: 0.2557\n",
      "245/281, train_loss: 0.0704, step time: 0.2604\n",
      "246/281, train_loss: 0.1666, step time: 0.2573\n",
      "247/281, train_loss: 0.1273, step time: 0.2584\n",
      "248/281, train_loss: 0.2731, step time: 0.2601\n",
      "249/281, train_loss: 0.0871, step time: 0.2535\n",
      "250/281, train_loss: 0.1249, step time: 0.2543\n",
      "251/281, train_loss: 0.0750, step time: 0.2543\n",
      "252/281, train_loss: 0.0821, step time: 0.2562\n",
      "253/281, train_loss: 0.0719, step time: 0.2633\n",
      "254/281, train_loss: 0.1128, step time: 0.2546\n",
      "255/281, train_loss: 0.1366, step time: 0.2544\n",
      "256/281, train_loss: 0.0914, step time: 0.2542\n",
      "257/281, train_loss: 0.0834, step time: 0.2577\n",
      "258/281, train_loss: 0.1094, step time: 0.2574\n",
      "259/281, train_loss: 0.2746, step time: 0.2572\n",
      "260/281, train_loss: 0.0536, step time: 0.2532\n",
      "261/281, train_loss: 0.1025, step time: 0.2525\n",
      "262/281, train_loss: 0.0942, step time: 0.2546\n",
      "263/281, train_loss: 0.0861, step time: 0.2489\n",
      "264/281, train_loss: 0.0583, step time: 0.2506\n",
      "265/281, train_loss: 0.1216, step time: 0.2514\n",
      "266/281, train_loss: 0.1153, step time: 0.2509\n",
      "267/281, train_loss: 0.2706, step time: 0.2522\n",
      "268/281, train_loss: 0.2664, step time: 0.2493\n",
      "269/281, train_loss: 0.1210, step time: 0.2545\n",
      "270/281, train_loss: 0.1192, step time: 0.2523\n",
      "271/281, train_loss: 0.0693, step time: 0.2538\n",
      "272/281, train_loss: 0.1121, step time: 0.2501\n",
      "273/281, train_loss: 0.1019, step time: 0.2503\n",
      "274/281, train_loss: 0.1017, step time: 0.2513\n",
      "275/281, train_loss: 0.0861, step time: 0.2493\n",
      "276/281, train_loss: 0.0706, step time: 0.2562\n",
      "277/281, train_loss: 0.2515, step time: 0.2488\n",
      "278/281, train_loss: 0.0575, step time: 0.2533\n",
      "279/281, train_loss: 0.0991, step time: 0.2570\n",
      "280/281, train_loss: 0.0807, step time: 0.2540\n",
      "281/281, train_loss: 0.0934, step time: 0.2576\n",
      "282/281, train_loss: 0.0703, step time: 0.1505\n",
      "epoch 52 average loss: 0.1232\n",
      "saved new best metric model\n",
      "current epoch: 52 current mean dice: 0.8748 tc: 0.8663 wt: 0.9111 et: 0.8564\n",
      "best mean dice: 0.8748 at epoch: 52\n",
      "time consuming of epoch 52 is: 415.8680\n",
      "----------\n",
      "epoch 53/200\n",
      "1/281, train_loss: 0.1074, step time: 0.2874\n",
      "2/281, train_loss: 0.1111, step time: 0.2547\n",
      "3/281, train_loss: 0.0973, step time: 0.2497\n",
      "4/281, train_loss: 0.0937, step time: 0.2530\n",
      "5/281, train_loss: 0.1601, step time: 0.2532\n",
      "6/281, train_loss: 0.1170, step time: 0.2675\n",
      "7/281, train_loss: 0.0903, step time: 0.2565\n",
      "8/281, train_loss: 0.1298, step time: 0.2568\n",
      "9/281, train_loss: 0.1009, step time: 0.2530\n",
      "10/281, train_loss: 0.1253, step time: 0.2628\n",
      "11/281, train_loss: 0.1118, step time: 0.2619\n",
      "12/281, train_loss: 0.1032, step time: 0.2586\n",
      "13/281, train_loss: 0.0657, step time: 0.2541\n",
      "14/281, train_loss: 0.2247, step time: 0.2557\n",
      "15/281, train_loss: 0.0853, step time: 0.2560\n",
      "16/281, train_loss: 0.2471, step time: 0.2561\n",
      "17/281, train_loss: 0.0874, step time: 0.2576\n",
      "18/281, train_loss: 0.1158, step time: 0.2571\n",
      "19/281, train_loss: 0.2615, step time: 0.2595\n",
      "20/281, train_loss: 0.2568, step time: 0.2625\n",
      "21/281, train_loss: 0.0493, step time: 0.2590\n",
      "22/281, train_loss: 0.0822, step time: 0.2563\n",
      "23/281, train_loss: 0.0561, step time: 0.2579\n",
      "24/281, train_loss: 0.2831, step time: 0.2548\n",
      "25/281, train_loss: 0.2657, step time: 0.2584\n",
      "26/281, train_loss: 0.0914, step time: 0.2592\n",
      "27/281, train_loss: 0.0706, step time: 0.2585\n",
      "28/281, train_loss: 0.0719, step time: 0.2563\n",
      "29/281, train_loss: 0.0890, step time: 0.2548\n",
      "30/281, train_loss: 0.0760, step time: 0.2552\n",
      "31/281, train_loss: 0.1396, step time: 0.2495\n",
      "32/281, train_loss: 0.1030, step time: 0.2552\n",
      "33/281, train_loss: 0.1264, step time: 0.2546\n",
      "34/281, train_loss: 0.4015, step time: 0.2511\n",
      "35/281, train_loss: 0.1027, step time: 0.2510\n",
      "36/281, train_loss: 0.0822, step time: 0.2468\n",
      "37/281, train_loss: 0.2452, step time: 0.2492\n",
      "38/281, train_loss: 0.1106, step time: 0.2569\n",
      "39/281, train_loss: 0.1067, step time: 0.2508\n",
      "40/281, train_loss: 0.0537, step time: 0.2497\n",
      "41/281, train_loss: 0.0916, step time: 0.2588\n",
      "42/281, train_loss: 0.1137, step time: 0.2507\n",
      "43/281, train_loss: 0.2562, step time: 0.2496\n",
      "44/281, train_loss: 0.0712, step time: 0.2518\n",
      "45/281, train_loss: 0.1234, step time: 0.2607\n",
      "46/281, train_loss: 0.1016, step time: 0.2574\n",
      "47/281, train_loss: 0.2483, step time: 0.2588\n",
      "48/281, train_loss: 0.0979, step time: 0.2653\n",
      "49/281, train_loss: 0.2976, step time: 0.2527\n",
      "50/281, train_loss: 0.1011, step time: 0.2488\n",
      "51/281, train_loss: 0.1523, step time: 0.2484\n",
      "52/281, train_loss: 0.0995, step time: 0.2442\n",
      "53/281, train_loss: 0.1032, step time: 0.2587\n",
      "54/281, train_loss: 0.2562, step time: 0.2586\n",
      "55/281, train_loss: 0.0926, step time: 0.2552\n",
      "56/281, train_loss: 0.1224, step time: 0.2554\n",
      "57/281, train_loss: 0.2564, step time: 0.2549\n",
      "58/281, train_loss: 0.0869, step time: 0.2513\n",
      "59/281, train_loss: 0.1213, step time: 0.2525\n",
      "60/281, train_loss: 0.0886, step time: 0.2515\n",
      "61/281, train_loss: 0.1060, step time: 0.2605\n",
      "62/281, train_loss: 0.0792, step time: 0.2576\n",
      "63/281, train_loss: 0.0654, step time: 0.2510\n",
      "64/281, train_loss: 0.1053, step time: 0.2599\n",
      "65/281, train_loss: 0.1372, step time: 0.2541\n",
      "66/281, train_loss: 0.1011, step time: 0.2524\n",
      "67/281, train_loss: 0.0947, step time: 0.2561\n",
      "68/281, train_loss: 0.0633, step time: 0.2523\n",
      "69/281, train_loss: 0.1060, step time: 0.2576\n",
      "70/281, train_loss: 0.0640, step time: 0.2573\n",
      "71/281, train_loss: 0.0813, step time: 0.2519\n",
      "72/281, train_loss: 0.1138, step time: 0.2596\n",
      "73/281, train_loss: 0.1669, step time: 0.2518\n",
      "74/281, train_loss: 0.1493, step time: 0.2526\n",
      "75/281, train_loss: 0.0782, step time: 0.2558\n",
      "76/281, train_loss: 0.1202, step time: 0.2564\n",
      "77/281, train_loss: 0.1006, step time: 0.2578\n",
      "78/281, train_loss: 0.2291, step time: 0.2561\n",
      "79/281, train_loss: 0.1033, step time: 0.2576\n",
      "80/281, train_loss: 0.1071, step time: 0.2565\n",
      "81/281, train_loss: 0.0539, step time: 0.2549\n",
      "82/281, train_loss: 0.2527, step time: 0.2577\n",
      "83/281, train_loss: 0.1012, step time: 0.2605\n",
      "84/281, train_loss: 0.2842, step time: 0.2559\n",
      "85/281, train_loss: 0.0866, step time: 0.2585\n",
      "86/281, train_loss: 0.0609, step time: 0.2591\n",
      "87/281, train_loss: 0.1313, step time: 0.2570\n",
      "88/281, train_loss: 0.0771, step time: 0.2561\n",
      "89/281, train_loss: 0.0881, step time: 0.2586\n",
      "90/281, train_loss: 0.1005, step time: 0.2736\n",
      "91/281, train_loss: 0.2376, step time: 0.2543\n",
      "92/281, train_loss: 0.0599, step time: 0.2508\n",
      "93/281, train_loss: 0.1471, step time: 0.2525\n",
      "94/281, train_loss: 0.0527, step time: 0.2506\n",
      "95/281, train_loss: 0.0738, step time: 0.2550\n",
      "96/281, train_loss: 0.0587, step time: 0.2562\n",
      "97/281, train_loss: 0.1372, step time: 0.2634\n",
      "98/281, train_loss: 0.2868, step time: 0.2584\n",
      "99/281, train_loss: 0.0869, step time: 0.2556\n",
      "100/281, train_loss: 0.1021, step time: 0.2578\n",
      "101/281, train_loss: 0.0846, step time: 0.2610\n",
      "102/281, train_loss: 0.1709, step time: 0.2571\n",
      "103/281, train_loss: 0.0911, step time: 0.2567\n",
      "104/281, train_loss: 0.1250, step time: 0.2576\n",
      "105/281, train_loss: 0.1440, step time: 0.2606\n",
      "106/281, train_loss: 0.0778, step time: 0.2539\n",
      "107/281, train_loss: 0.1515, step time: 0.2565\n",
      "108/281, train_loss: 0.0735, step time: 0.2592\n",
      "109/281, train_loss: 0.1187, step time: 0.2575\n",
      "110/281, train_loss: 0.1321, step time: 0.2539\n",
      "111/281, train_loss: 0.1407, step time: 0.2613\n",
      "112/281, train_loss: 0.0979, step time: 0.2568\n",
      "113/281, train_loss: 0.0926, step time: 0.2563\n",
      "114/281, train_loss: 0.1158, step time: 0.2565\n",
      "115/281, train_loss: 0.0760, step time: 0.2551\n",
      "116/281, train_loss: 0.0924, step time: 0.2538\n",
      "117/281, train_loss: 0.0897, step time: 0.2605\n",
      "118/281, train_loss: 0.2514, step time: 0.2546\n",
      "119/281, train_loss: 0.0750, step time: 0.2563\n",
      "120/281, train_loss: 0.0915, step time: 0.2632\n",
      "121/281, train_loss: 0.0817, step time: 0.2509\n",
      "122/281, train_loss: 0.1038, step time: 0.2543\n",
      "123/281, train_loss: 0.0723, step time: 0.2565\n",
      "124/281, train_loss: 0.1244, step time: 0.2547\n",
      "125/281, train_loss: 0.0801, step time: 0.2598\n",
      "126/281, train_loss: 0.2359, step time: 0.2526\n",
      "127/281, train_loss: 0.2499, step time: 0.2576\n",
      "128/281, train_loss: 0.0907, step time: 0.2892\n",
      "129/281, train_loss: 0.0660, step time: 0.2720\n",
      "130/281, train_loss: 0.2396, step time: 0.2552\n",
      "131/281, train_loss: 0.0607, step time: 0.2536\n",
      "132/281, train_loss: 0.1075, step time: 0.2571\n",
      "133/281, train_loss: 0.0631, step time: 0.2529\n",
      "134/281, train_loss: 0.0745, step time: 0.2529\n",
      "135/281, train_loss: 0.0765, step time: 0.2529\n",
      "136/281, train_loss: 0.1178, step time: 0.2543\n",
      "137/281, train_loss: 0.0860, step time: 0.2574\n",
      "138/281, train_loss: 0.1578, step time: 0.2570\n",
      "139/281, train_loss: 0.1182, step time: 0.2630\n",
      "140/281, train_loss: 0.1135, step time: 0.2544\n",
      "141/281, train_loss: 0.1084, step time: 0.2600\n",
      "142/281, train_loss: 0.1205, step time: 0.2595\n",
      "143/281, train_loss: 0.3046, step time: 0.2580\n",
      "144/281, train_loss: 0.0976, step time: 0.2584\n",
      "145/281, train_loss: 0.2456, step time: 0.2563\n",
      "146/281, train_loss: 0.1579, step time: 0.2587\n",
      "147/281, train_loss: 0.3028, step time: 0.2589\n",
      "148/281, train_loss: 0.1018, step time: 0.2580\n",
      "149/281, train_loss: 0.0974, step time: 0.2559\n",
      "150/281, train_loss: 0.1222, step time: 0.2486\n",
      "151/281, train_loss: 0.0958, step time: 0.2541\n",
      "152/281, train_loss: 0.0676, step time: 0.2538\n",
      "153/281, train_loss: 0.0790, step time: 0.2541\n",
      "154/281, train_loss: 0.0900, step time: 0.2557\n",
      "155/281, train_loss: 0.0664, step time: 0.2510\n",
      "156/281, train_loss: 0.1390, step time: 0.2541\n",
      "157/281, train_loss: 0.2323, step time: 0.2512\n",
      "158/281, train_loss: 0.0783, step time: 0.2513\n",
      "159/281, train_loss: 0.2200, step time: 0.2517\n",
      "160/281, train_loss: 0.1376, step time: 0.2506\n",
      "161/281, train_loss: 0.0964, step time: 0.2504\n",
      "162/281, train_loss: 0.1118, step time: 0.2569\n",
      "163/281, train_loss: 0.0692, step time: 0.2554\n",
      "164/281, train_loss: 0.1323, step time: 0.2543\n",
      "165/281, train_loss: 0.2535, step time: 0.2519\n",
      "166/281, train_loss: 0.1052, step time: 0.2506\n",
      "167/281, train_loss: 0.1031, step time: 0.2548\n",
      "168/281, train_loss: 0.1063, step time: 0.2570\n",
      "169/281, train_loss: 0.0964, step time: 0.2581\n",
      "170/281, train_loss: 0.0779, step time: 0.2549\n",
      "171/281, train_loss: 0.0930, step time: 0.2643\n",
      "172/281, train_loss: 0.0750, step time: 0.2578\n",
      "173/281, train_loss: 0.1038, step time: 0.2590\n",
      "174/281, train_loss: 0.2302, step time: 0.2579\n",
      "175/281, train_loss: 0.0697, step time: 0.2590\n",
      "176/281, train_loss: 0.0563, step time: 0.2555\n",
      "177/281, train_loss: 0.1088, step time: 0.2505\n",
      "178/281, train_loss: 0.0637, step time: 0.2557\n",
      "179/281, train_loss: 0.1019, step time: 0.2576\n",
      "180/281, train_loss: 0.0959, step time: 0.2555\n",
      "181/281, train_loss: 0.1695, step time: 0.2535\n",
      "182/281, train_loss: 0.0454, step time: 0.2564\n",
      "183/281, train_loss: 0.0947, step time: 0.2560\n",
      "184/281, train_loss: 0.0774, step time: 0.2588\n",
      "185/281, train_loss: 0.1578, step time: 0.2554\n",
      "186/281, train_loss: 0.0826, step time: 0.2626\n",
      "187/281, train_loss: 0.0601, step time: 0.2606\n",
      "188/281, train_loss: 0.1048, step time: 0.2571\n",
      "189/281, train_loss: 0.2417, step time: 0.2577\n",
      "190/281, train_loss: 0.0941, step time: 0.2631\n",
      "191/281, train_loss: 0.0818, step time: 0.2581\n",
      "192/281, train_loss: 0.0852, step time: 0.2578\n",
      "193/281, train_loss: 0.0541, step time: 0.2518\n",
      "194/281, train_loss: 0.0932, step time: 0.2529\n",
      "195/281, train_loss: 0.0474, step time: 0.2501\n",
      "196/281, train_loss: 0.0976, step time: 0.2576\n",
      "197/281, train_loss: 0.0872, step time: 0.2577\n",
      "198/281, train_loss: 0.4161, step time: 0.2626\n",
      "199/281, train_loss: 0.0797, step time: 0.2592\n",
      "200/281, train_loss: 0.0995, step time: 0.2584\n",
      "201/281, train_loss: 0.0814, step time: 0.2567\n",
      "202/281, train_loss: 0.1169, step time: 0.2534\n",
      "203/281, train_loss: 0.1717, step time: 0.2526\n",
      "204/281, train_loss: 0.1132, step time: 0.2576\n",
      "205/281, train_loss: 0.0836, step time: 0.2549\n",
      "206/281, train_loss: 0.0579, step time: 0.2588\n",
      "207/281, train_loss: 0.1034, step time: 0.2572\n",
      "208/281, train_loss: 0.0970, step time: 0.2545\n",
      "209/281, train_loss: 0.1094, step time: 0.2525\n",
      "210/281, train_loss: 0.0537, step time: 0.2569\n",
      "211/281, train_loss: 0.1438, step time: 0.2556\n",
      "212/281, train_loss: 0.0777, step time: 0.2542\n",
      "213/281, train_loss: 0.0494, step time: 0.2578\n",
      "214/281, train_loss: 0.1038, step time: 0.2579\n",
      "215/281, train_loss: 0.2520, step time: 0.2590\n",
      "216/281, train_loss: 0.0943, step time: 0.2554\n",
      "217/281, train_loss: 0.1623, step time: 0.2511\n",
      "218/281, train_loss: 0.0775, step time: 0.2548\n",
      "219/281, train_loss: 0.0954, step time: 0.2554\n",
      "220/281, train_loss: 0.0555, step time: 0.2552\n",
      "221/281, train_loss: 0.1220, step time: 0.2514\n",
      "222/281, train_loss: 0.2411, step time: 0.2566\n",
      "223/281, train_loss: 0.0810, step time: 0.2552\n",
      "224/281, train_loss: 0.0529, step time: 0.2582\n",
      "225/281, train_loss: 0.0675, step time: 0.2598\n",
      "226/281, train_loss: 0.1218, step time: 0.2619\n",
      "227/281, train_loss: 0.0626, step time: 0.2594\n",
      "228/281, train_loss: 0.0838, step time: 0.2557\n",
      "229/281, train_loss: 0.1220, step time: 0.2538\n",
      "230/281, train_loss: 0.2608, step time: 0.2569\n",
      "231/281, train_loss: 0.2427, step time: 0.2572\n",
      "232/281, train_loss: 0.0501, step time: 0.2573\n",
      "233/281, train_loss: 0.1230, step time: 0.2552\n",
      "234/281, train_loss: 0.0925, step time: 0.2572\n",
      "235/281, train_loss: 0.0590, step time: 0.2568\n",
      "236/281, train_loss: 0.1177, step time: 0.2549\n",
      "237/281, train_loss: 0.1116, step time: 0.2631\n",
      "238/281, train_loss: 0.1275, step time: 0.2622\n",
      "239/281, train_loss: 0.0876, step time: 0.2588\n",
      "240/281, train_loss: 0.3030, step time: 0.2575\n",
      "241/281, train_loss: 0.0684, step time: 0.2533\n",
      "242/281, train_loss: 0.0914, step time: 0.2480\n",
      "243/281, train_loss: 0.0905, step time: 0.2536\n",
      "244/281, train_loss: 0.0739, step time: 0.2495\n",
      "245/281, train_loss: 0.2639, step time: 0.2505\n",
      "246/281, train_loss: 0.1343, step time: 0.2541\n",
      "247/281, train_loss: 0.1540, step time: 0.2525\n",
      "248/281, train_loss: 0.0579, step time: 0.2525\n",
      "249/281, train_loss: 0.2336, step time: 0.2545\n",
      "250/281, train_loss: 0.1212, step time: 0.2622\n",
      "251/281, train_loss: 0.0893, step time: 0.3108\n",
      "252/281, train_loss: 0.1118, step time: 0.2566\n",
      "253/281, train_loss: 0.0857, step time: 0.2515\n",
      "254/281, train_loss: 0.0818, step time: 0.2511\n",
      "255/281, train_loss: 0.0937, step time: 0.2560\n",
      "256/281, train_loss: 0.2625, step time: 0.2563\n",
      "257/281, train_loss: 0.0896, step time: 0.2543\n",
      "258/281, train_loss: 0.1074, step time: 0.2582\n",
      "259/281, train_loss: 0.0589, step time: 0.2565\n",
      "260/281, train_loss: 0.0919, step time: 0.2524\n",
      "261/281, train_loss: 0.0724, step time: 0.2523\n",
      "262/281, train_loss: 0.0988, step time: 0.2542\n",
      "263/281, train_loss: 0.1714, step time: 0.2542\n",
      "264/281, train_loss: 0.0758, step time: 0.2514\n",
      "265/281, train_loss: 0.0785, step time: 0.2480\n",
      "266/281, train_loss: 0.2439, step time: 0.2466\n",
      "267/281, train_loss: 0.0765, step time: 0.2507\n",
      "268/281, train_loss: 0.0736, step time: 0.2510\n",
      "269/281, train_loss: 0.2628, step time: 0.2560\n",
      "270/281, train_loss: 0.0909, step time: 0.2553\n",
      "271/281, train_loss: 0.1582, step time: 0.2493\n",
      "272/281, train_loss: 0.0996, step time: 0.2484\n",
      "273/281, train_loss: 0.1138, step time: 0.2510\n",
      "274/281, train_loss: 0.0402, step time: 0.2493\n",
      "275/281, train_loss: 0.1340, step time: 0.2536\n",
      "276/281, train_loss: 0.1184, step time: 0.2518\n",
      "277/281, train_loss: 0.1144, step time: 0.2521\n",
      "278/281, train_loss: 0.1072, step time: 0.2534\n",
      "279/281, train_loss: 0.2470, step time: 0.2513\n",
      "280/281, train_loss: 0.0838, step time: 0.2487\n",
      "281/281, train_loss: 0.0835, step time: 0.2513\n",
      "282/281, train_loss: 0.0611, step time: 0.1515\n",
      "epoch 53 average loss: 0.1217\n",
      "current epoch: 53 current mean dice: 0.8746 tc: 0.8644 wt: 0.9159 et: 0.8537\n",
      "best mean dice: 0.8748 at epoch: 52\n",
      "time consuming of epoch 53 is: 407.1275\n",
      "----------\n",
      "epoch 54/200\n",
      "1/281, train_loss: 0.0964, step time: 0.2543\n",
      "2/281, train_loss: 0.0688, step time: 0.2521\n",
      "3/281, train_loss: 0.2658, step time: 0.2547\n",
      "4/281, train_loss: 0.1006, step time: 0.2539\n",
      "5/281, train_loss: 0.0672, step time: 0.2547\n",
      "6/281, train_loss: 0.1326, step time: 0.2587\n",
      "7/281, train_loss: 0.3901, step time: 0.2802\n",
      "8/281, train_loss: 0.1036, step time: 0.2526\n",
      "9/281, train_loss: 0.0793, step time: 0.2608\n",
      "10/281, train_loss: 0.1061, step time: 0.2642\n",
      "11/281, train_loss: 0.0781, step time: 0.2530\n",
      "12/281, train_loss: 0.1057, step time: 0.2490\n",
      "13/281, train_loss: 0.0651, step time: 0.2521\n",
      "14/281, train_loss: 0.3065, step time: 0.2494\n",
      "15/281, train_loss: 0.0508, step time: 0.2496\n",
      "16/281, train_loss: 0.0734, step time: 0.2494\n",
      "17/281, train_loss: 0.1057, step time: 0.2644\n",
      "18/281, train_loss: 0.0810, step time: 0.2588\n",
      "19/281, train_loss: 0.2422, step time: 0.2558\n",
      "20/281, train_loss: 0.2566, step time: 0.2499\n",
      "21/281, train_loss: 0.1498, step time: 0.2524\n",
      "22/281, train_loss: 0.0649, step time: 0.2525\n",
      "23/281, train_loss: 0.0592, step time: 0.2557\n",
      "24/281, train_loss: 0.1449, step time: 0.2505\n",
      "25/281, train_loss: 0.1116, step time: 0.2869\n",
      "26/281, train_loss: 0.2564, step time: 0.2725\n",
      "27/281, train_loss: 0.0734, step time: 0.2545\n",
      "28/281, train_loss: 0.0549, step time: 0.2565\n",
      "29/281, train_loss: 0.0429, step time: 0.2575\n",
      "30/281, train_loss: 0.1298, step time: 0.2651\n",
      "31/281, train_loss: 0.0792, step time: 0.2552\n",
      "32/281, train_loss: 0.0949, step time: 0.2487\n",
      "33/281, train_loss: 0.0989, step time: 0.2581\n",
      "34/281, train_loss: 0.0857, step time: 0.2606\n",
      "35/281, train_loss: 0.0688, step time: 0.2578\n",
      "36/281, train_loss: 0.0501, step time: 0.2637\n",
      "37/281, train_loss: 0.0756, step time: 0.2514\n",
      "38/281, train_loss: 0.0761, step time: 0.2464\n",
      "39/281, train_loss: 0.2487, step time: 0.2477\n",
      "40/281, train_loss: 0.0703, step time: 0.2586\n",
      "41/281, train_loss: 0.0693, step time: 0.2492\n",
      "42/281, train_loss: 0.0905, step time: 0.2550\n",
      "43/281, train_loss: 0.1128, step time: 0.2601\n",
      "44/281, train_loss: 0.0923, step time: 0.2565\n",
      "45/281, train_loss: 0.0947, step time: 0.2561\n",
      "46/281, train_loss: 0.0912, step time: 0.2569\n",
      "47/281, train_loss: 0.1031, step time: 0.2559\n",
      "48/281, train_loss: 0.0953, step time: 0.2483\n",
      "49/281, train_loss: 0.0744, step time: 0.2474\n",
      "50/281, train_loss: 0.1000, step time: 0.2487\n",
      "51/281, train_loss: 0.0845, step time: 0.2494\n",
      "52/281, train_loss: 0.1627, step time: 0.2472\n",
      "53/281, train_loss: 0.0616, step time: 0.2491\n",
      "54/281, train_loss: 0.1040, step time: 0.2485\n",
      "55/281, train_loss: 0.1366, step time: 0.2529\n",
      "56/281, train_loss: 0.1007, step time: 0.2447\n",
      "57/281, train_loss: 0.2411, step time: 0.2522\n",
      "58/281, train_loss: 0.0888, step time: 0.2530\n",
      "59/281, train_loss: 0.0798, step time: 0.2537\n",
      "60/281, train_loss: 0.0942, step time: 0.2497\n",
      "61/281, train_loss: 0.0843, step time: 0.2574\n",
      "62/281, train_loss: 0.1008, step time: 0.2536\n",
      "63/281, train_loss: 0.0999, step time: 0.2446\n",
      "64/281, train_loss: 0.0912, step time: 0.2565\n",
      "65/281, train_loss: 0.2217, step time: 0.2476\n",
      "66/281, train_loss: 0.0863, step time: 0.2494\n",
      "67/281, train_loss: 0.0654, step time: 0.2469\n",
      "68/281, train_loss: 0.2421, step time: 0.2520\n",
      "69/281, train_loss: 0.1145, step time: 0.2547\n",
      "70/281, train_loss: 0.0530, step time: 0.2516\n",
      "71/281, train_loss: 0.1139, step time: 0.2527\n",
      "72/281, train_loss: 0.0777, step time: 0.2505\n",
      "73/281, train_loss: 0.1210, step time: 0.2504\n",
      "74/281, train_loss: 0.0812, step time: 0.2525\n",
      "75/281, train_loss: 0.0580, step time: 0.2466\n",
      "76/281, train_loss: 0.0864, step time: 0.2502\n",
      "77/281, train_loss: 0.0760, step time: 0.2596\n",
      "78/281, train_loss: 0.2371, step time: 0.2503\n",
      "79/281, train_loss: 0.1342, step time: 0.2526\n",
      "80/281, train_loss: 0.0785, step time: 0.2478\n",
      "81/281, train_loss: 0.0549, step time: 0.2470\n",
      "82/281, train_loss: 0.0712, step time: 0.2507\n",
      "83/281, train_loss: 0.2353, step time: 0.2513\n",
      "84/281, train_loss: 0.2477, step time: 0.2563\n",
      "85/281, train_loss: 0.2421, step time: 0.2639\n",
      "86/281, train_loss: 0.0700, step time: 0.2546\n",
      "87/281, train_loss: 0.0696, step time: 0.2501\n",
      "88/281, train_loss: 0.0763, step time: 0.2534\n",
      "89/281, train_loss: 0.1404, step time: 0.2545\n",
      "90/281, train_loss: 0.0915, step time: 0.2535\n",
      "91/281, train_loss: 0.0929, step time: 0.2510\n",
      "92/281, train_loss: 0.0785, step time: 0.2496\n",
      "93/281, train_loss: 0.0945, step time: 0.2551\n",
      "94/281, train_loss: 0.0725, step time: 0.2576\n",
      "95/281, train_loss: 0.1074, step time: 0.2553\n",
      "96/281, train_loss: 0.1078, step time: 0.2522\n",
      "97/281, train_loss: 0.1584, step time: 0.2529\n",
      "98/281, train_loss: 0.0509, step time: 0.2495\n",
      "99/281, train_loss: 0.0681, step time: 0.2514\n",
      "100/281, train_loss: 0.0773, step time: 0.2546\n",
      "101/281, train_loss: 0.1120, step time: 0.2476\n",
      "102/281, train_loss: 0.0849, step time: 0.2514\n",
      "103/281, train_loss: 0.2285, step time: 0.2512\n",
      "104/281, train_loss: 0.1071, step time: 0.2496\n",
      "105/281, train_loss: 0.2872, step time: 0.2543\n",
      "106/281, train_loss: 0.0950, step time: 0.2511\n",
      "107/281, train_loss: 0.0850, step time: 0.2477\n",
      "108/281, train_loss: 0.1106, step time: 0.2502\n",
      "109/281, train_loss: 0.2564, step time: 0.2519\n",
      "110/281, train_loss: 0.0662, step time: 0.2575\n",
      "111/281, train_loss: 0.1221, step time: 0.2570\n",
      "112/281, train_loss: 0.1175, step time: 0.2420\n",
      "113/281, train_loss: 0.1342, step time: 0.2543\n",
      "114/281, train_loss: 0.0822, step time: 0.2549\n",
      "115/281, train_loss: 0.1546, step time: 0.2515\n",
      "116/281, train_loss: 0.0989, step time: 0.2511\n",
      "117/281, train_loss: 0.1009, step time: 0.2554\n",
      "118/281, train_loss: 0.2291, step time: 0.2484\n",
      "119/281, train_loss: 0.0629, step time: 0.2462\n",
      "120/281, train_loss: 0.1111, step time: 0.2510\n",
      "121/281, train_loss: 0.2550, step time: 0.2547\n",
      "122/281, train_loss: 0.0874, step time: 0.2513\n",
      "123/281, train_loss: 0.1794, step time: 0.2497\n",
      "124/281, train_loss: 0.0929, step time: 0.2456\n",
      "125/281, train_loss: 0.0805, step time: 0.2563\n",
      "126/281, train_loss: 0.1387, step time: 0.2468\n",
      "127/281, train_loss: 0.0651, step time: 0.2469\n",
      "128/281, train_loss: 0.0998, step time: 0.2511\n",
      "129/281, train_loss: 0.2481, step time: 0.2489\n",
      "130/281, train_loss: 0.0981, step time: 0.2510\n",
      "131/281, train_loss: 0.1224, step time: 0.2495\n",
      "132/281, train_loss: 0.2395, step time: 0.2499\n",
      "133/281, train_loss: 0.0787, step time: 0.2510\n",
      "134/281, train_loss: 0.3068, step time: 0.2520\n",
      "135/281, train_loss: 0.0915, step time: 0.2501\n",
      "136/281, train_loss: 0.0826, step time: 0.2516\n",
      "137/281, train_loss: 0.1202, step time: 0.2500\n",
      "138/281, train_loss: 0.0627, step time: 0.2519\n",
      "139/281, train_loss: 0.1355, step time: 0.2497\n",
      "140/281, train_loss: 0.1451, step time: 0.2550\n",
      "141/281, train_loss: 0.0859, step time: 0.2512\n",
      "142/281, train_loss: 0.1374, step time: 0.2525\n",
      "143/281, train_loss: 0.1124, step time: 0.2487\n",
      "144/281, train_loss: 0.1118, step time: 0.2502\n",
      "145/281, train_loss: 0.0673, step time: 0.2462\n",
      "146/281, train_loss: 0.2732, step time: 0.2520\n",
      "147/281, train_loss: 0.1269, step time: 0.2525\n",
      "148/281, train_loss: 0.0641, step time: 0.2525\n",
      "149/281, train_loss: 0.1428, step time: 0.2496\n",
      "150/281, train_loss: 0.0740, step time: 0.2470\n",
      "151/281, train_loss: 0.0747, step time: 0.2460\n",
      "152/281, train_loss: 0.0710, step time: 0.2488\n",
      "153/281, train_loss: 0.1056, step time: 0.2493\n",
      "154/281, train_loss: 0.0474, step time: 0.2487\n",
      "155/281, train_loss: 0.0518, step time: 0.2482\n",
      "156/281, train_loss: 0.2116, step time: 0.2526\n",
      "157/281, train_loss: 0.2246, step time: 0.2481\n",
      "158/281, train_loss: 0.0837, step time: 0.2552\n",
      "159/281, train_loss: 0.0759, step time: 0.2543\n",
      "160/281, train_loss: 0.0797, step time: 0.2504\n",
      "161/281, train_loss: 0.0652, step time: 0.2456\n",
      "162/281, train_loss: 0.0913, step time: 0.2475\n",
      "163/281, train_loss: 0.2525, step time: 0.2501\n",
      "164/281, train_loss: 0.2086, step time: 0.2530\n",
      "165/281, train_loss: 0.1537, step time: 0.2463\n",
      "166/281, train_loss: 0.0570, step time: 0.2435\n",
      "167/281, train_loss: 0.2480, step time: 0.2472\n",
      "168/281, train_loss: 0.0802, step time: 0.2528\n",
      "169/281, train_loss: 0.0799, step time: 0.2540\n",
      "170/281, train_loss: 0.1113, step time: 0.2484\n",
      "171/281, train_loss: 0.0551, step time: 0.2500\n",
      "172/281, train_loss: 0.0916, step time: 0.2490\n",
      "173/281, train_loss: 0.0642, step time: 0.2506\n",
      "174/281, train_loss: 0.3188, step time: 0.2502\n",
      "175/281, train_loss: 0.1077, step time: 0.2551\n",
      "176/281, train_loss: 0.0586, step time: 0.2510\n",
      "177/281, train_loss: 0.0852, step time: 0.2532\n",
      "178/281, train_loss: 0.1122, step time: 0.2502\n",
      "179/281, train_loss: 0.1567, step time: 0.2508\n",
      "180/281, train_loss: 0.2094, step time: 0.2506\n",
      "181/281, train_loss: 0.2550, step time: 0.2469\n",
      "182/281, train_loss: 0.1488, step time: 0.2434\n",
      "183/281, train_loss: 0.1230, step time: 0.2486\n",
      "184/281, train_loss: 0.0854, step time: 0.2530\n",
      "185/281, train_loss: 0.1163, step time: 0.2558\n",
      "186/281, train_loss: 0.0924, step time: 0.2509\n",
      "187/281, train_loss: 0.0735, step time: 0.2433\n",
      "188/281, train_loss: 0.1474, step time: 0.2445\n",
      "189/281, train_loss: 0.0609, step time: 0.2492\n",
      "190/281, train_loss: 0.1148, step time: 0.2501\n",
      "191/281, train_loss: 0.0977, step time: 0.2469\n",
      "192/281, train_loss: 0.1209, step time: 0.2515\n",
      "193/281, train_loss: 0.1580, step time: 0.2549\n",
      "194/281, train_loss: 0.2843, step time: 0.2536\n",
      "195/281, train_loss: 0.1072, step time: 0.2546\n",
      "196/281, train_loss: 0.2350, step time: 0.2533\n",
      "197/281, train_loss: 0.0942, step time: 0.2514\n",
      "198/281, train_loss: 0.2323, step time: 0.2456\n",
      "199/281, train_loss: 0.0868, step time: 0.2467\n",
      "200/281, train_loss: 0.2303, step time: 0.2757\n",
      "201/281, train_loss: 0.2863, step time: 0.2550\n",
      "202/281, train_loss: 0.0984, step time: 0.2439\n",
      "203/281, train_loss: 0.2798, step time: 0.2444\n",
      "204/281, train_loss: 0.0849, step time: 0.2531\n",
      "205/281, train_loss: 0.0632, step time: 0.2529\n",
      "206/281, train_loss: 0.0657, step time: 0.2483\n",
      "207/281, train_loss: 0.4030, step time: 0.2499\n",
      "208/281, train_loss: 0.0749, step time: 0.2444\n",
      "209/281, train_loss: 0.0887, step time: 0.2477\n",
      "210/281, train_loss: 0.1384, step time: 0.2512\n",
      "211/281, train_loss: 0.1280, step time: 0.2531\n",
      "212/281, train_loss: 0.0608, step time: 0.2486\n",
      "213/281, train_loss: 0.0477, step time: 0.2464\n",
      "214/281, train_loss: 0.1100, step time: 0.2519\n",
      "215/281, train_loss: 0.0988, step time: 0.2532\n",
      "216/281, train_loss: 0.2910, step time: 0.2505\n",
      "217/281, train_loss: 0.0801, step time: 0.2524\n",
      "218/281, train_loss: 0.1571, step time: 0.2451\n",
      "219/281, train_loss: 0.1543, step time: 0.2600\n",
      "220/281, train_loss: 0.2286, step time: 0.2474\n",
      "221/281, train_loss: 0.1099, step time: 0.2530\n",
      "222/281, train_loss: 0.1551, step time: 0.2561\n",
      "223/281, train_loss: 0.0776, step time: 0.2544\n",
      "224/281, train_loss: 0.0665, step time: 0.2489\n",
      "225/281, train_loss: 0.0844, step time: 0.2547\n",
      "226/281, train_loss: 0.0981, step time: 0.2565\n",
      "227/281, train_loss: 0.1380, step time: 0.2567\n",
      "228/281, train_loss: 0.0932, step time: 0.2461\n",
      "229/281, train_loss: 0.0775, step time: 0.2510\n",
      "230/281, train_loss: 0.0899, step time: 0.2486\n",
      "231/281, train_loss: 0.1033, step time: 0.2551\n",
      "232/281, train_loss: 0.1352, step time: 0.2491\n",
      "233/281, train_loss: 0.0851, step time: 0.2503\n",
      "234/281, train_loss: 0.0776, step time: 0.2432\n",
      "235/281, train_loss: 0.0864, step time: 0.2457\n",
      "236/281, train_loss: 0.0953, step time: 0.2492\n",
      "237/281, train_loss: 0.0674, step time: 0.2504\n",
      "238/281, train_loss: 0.2548, step time: 0.2526\n",
      "239/281, train_loss: 0.2274, step time: 0.2467\n",
      "240/281, train_loss: 0.0786, step time: 0.2469\n",
      "241/281, train_loss: 0.1013, step time: 0.2573\n",
      "242/281, train_loss: 0.0682, step time: 0.2760\n",
      "243/281, train_loss: 0.1748, step time: 0.2620\n",
      "244/281, train_loss: 0.0546, step time: 0.2489\n",
      "245/281, train_loss: 0.0747, step time: 0.2496\n",
      "246/281, train_loss: 0.1433, step time: 0.2503\n",
      "247/281, train_loss: 0.1034, step time: 0.2498\n",
      "248/281, train_loss: 0.1120, step time: 0.2534\n",
      "249/281, train_loss: 0.0746, step time: 0.2503\n",
      "250/281, train_loss: 0.0879, step time: 0.2479\n",
      "251/281, train_loss: 0.0931, step time: 0.2441\n",
      "252/281, train_loss: 0.1635, step time: 0.2522\n",
      "253/281, train_loss: 0.0998, step time: 0.2575\n",
      "254/281, train_loss: 0.0610, step time: 0.2492\n",
      "255/281, train_loss: 0.1823, step time: 0.2541\n",
      "256/281, train_loss: 0.0899, step time: 0.2497\n",
      "257/281, train_loss: 0.0789, step time: 0.2506\n",
      "258/281, train_loss: 0.1094, step time: 0.2490\n",
      "259/281, train_loss: 0.1366, step time: 0.2526\n",
      "260/281, train_loss: 0.1160, step time: 0.2449\n",
      "261/281, train_loss: 0.0791, step time: 0.2466\n",
      "262/281, train_loss: 0.0723, step time: 0.2569\n",
      "263/281, train_loss: 0.1028, step time: 0.2538\n",
      "264/281, train_loss: 0.1019, step time: 0.2493\n",
      "265/281, train_loss: 0.2301, step time: 0.2473\n",
      "266/281, train_loss: 0.1163, step time: 0.2550\n",
      "267/281, train_loss: 0.2920, step time: 0.2555\n",
      "268/281, train_loss: 0.1171, step time: 0.2544\n",
      "269/281, train_loss: 0.1008, step time: 0.2528\n",
      "270/281, train_loss: 0.1293, step time: 0.2497\n",
      "271/281, train_loss: 0.1148, step time: 0.2465\n",
      "272/281, train_loss: 0.0850, step time: 0.2424\n",
      "273/281, train_loss: 0.1420, step time: 0.2493\n",
      "274/281, train_loss: 0.0548, step time: 0.2529\n",
      "275/281, train_loss: 0.0874, step time: 0.2500\n",
      "276/281, train_loss: 0.2807, step time: 0.2465\n",
      "277/281, train_loss: 0.0776, step time: 0.2471\n",
      "278/281, train_loss: 0.0880, step time: 0.2468\n",
      "279/281, train_loss: 0.1347, step time: 0.2517\n",
      "280/281, train_loss: 0.1233, step time: 0.2521\n",
      "281/281, train_loss: 0.1424, step time: 0.2502\n",
      "282/281, train_loss: 0.1333, step time: 0.1472\n",
      "epoch 54 average loss: 0.1222\n",
      "current epoch: 54 current mean dice: 0.8699 tc: 0.8473 wt: 0.9154 et: 0.8597\n",
      "best mean dice: 0.8748 at epoch: 52\n",
      "time consuming of epoch 54 is: 379.8997\n",
      "----------\n",
      "epoch 55/200\n",
      "1/281, train_loss: 0.2363, step time: 0.2520\n",
      "2/281, train_loss: 0.0516, step time: 0.2567\n",
      "3/281, train_loss: 0.0552, step time: 0.2577\n",
      "4/281, train_loss: 0.1216, step time: 0.2526\n",
      "5/281, train_loss: 0.2341, step time: 0.2543\n",
      "6/281, train_loss: 0.1230, step time: 0.2516\n",
      "7/281, train_loss: 0.1175, step time: 0.2513\n",
      "8/281, train_loss: 0.1025, step time: 0.2560\n",
      "9/281, train_loss: 0.0570, step time: 0.2514\n",
      "10/281, train_loss: 0.3027, step time: 0.2493\n",
      "11/281, train_loss: 0.0728, step time: 0.2592\n",
      "12/281, train_loss: 0.1224, step time: 0.2595\n",
      "13/281, train_loss: 0.1059, step time: 0.2583\n",
      "14/281, train_loss: 0.0798, step time: 0.2568\n",
      "15/281, train_loss: 0.1025, step time: 0.2639\n",
      "16/281, train_loss: 0.0721, step time: 0.2607\n",
      "17/281, train_loss: 0.1071, step time: 0.2584\n",
      "18/281, train_loss: 0.1249, step time: 0.2509\n",
      "19/281, train_loss: 0.1469, step time: 0.2572\n",
      "20/281, train_loss: 0.1557, step time: 0.2533\n",
      "21/281, train_loss: 0.0738, step time: 0.2524\n",
      "22/281, train_loss: 0.0762, step time: 0.2596\n",
      "23/281, train_loss: 0.0802, step time: 0.2681\n",
      "24/281, train_loss: 0.1058, step time: 0.2606\n",
      "25/281, train_loss: 0.1421, step time: 0.2557\n",
      "26/281, train_loss: 0.1142, step time: 0.2885\n",
      "27/281, train_loss: 0.2187, step time: 0.2646\n",
      "28/281, train_loss: 0.1051, step time: 0.2591\n",
      "29/281, train_loss: 0.2539, step time: 0.2643\n",
      "30/281, train_loss: 0.1111, step time: 0.2542\n",
      "31/281, train_loss: 0.0739, step time: 0.2535\n",
      "32/281, train_loss: 0.1113, step time: 0.2511\n",
      "33/281, train_loss: 0.1046, step time: 0.2561\n",
      "34/281, train_loss: 0.0786, step time: 0.2583\n",
      "35/281, train_loss: 0.2452, step time: 0.2624\n",
      "36/281, train_loss: 0.0976, step time: 0.2559\n",
      "37/281, train_loss: 0.2999, step time: 0.2582\n",
      "38/281, train_loss: 0.2640, step time: 0.2565\n",
      "39/281, train_loss: 0.1385, step time: 0.2638\n",
      "40/281, train_loss: 0.1382, step time: 0.2576\n",
      "41/281, train_loss: 0.0867, step time: 0.2512\n",
      "42/281, train_loss: 0.0771, step time: 0.2561\n",
      "43/281, train_loss: 0.2436, step time: 0.2599\n",
      "44/281, train_loss: 0.0908, step time: 0.2564\n",
      "45/281, train_loss: 0.0985, step time: 0.2546\n",
      "46/281, train_loss: 0.0774, step time: 0.2503\n",
      "47/281, train_loss: 0.2323, step time: 0.2526\n",
      "48/281, train_loss: 0.0761, step time: 0.2513\n",
      "49/281, train_loss: 0.0789, step time: 0.2504\n",
      "50/281, train_loss: 0.0752, step time: 0.2595\n",
      "51/281, train_loss: 0.1069, step time: 0.2623\n",
      "52/281, train_loss: 0.1132, step time: 0.2558\n",
      "53/281, train_loss: 0.0886, step time: 0.2574\n",
      "54/281, train_loss: 0.2866, step time: 0.2548\n",
      "55/281, train_loss: 0.1041, step time: 0.2570\n",
      "56/281, train_loss: 0.0819, step time: 0.2550\n",
      "57/281, train_loss: 0.1252, step time: 0.2564\n",
      "58/281, train_loss: 0.0748, step time: 0.2557\n",
      "59/281, train_loss: 0.0725, step time: 0.2549\n",
      "60/281, train_loss: 0.0746, step time: 0.2556\n",
      "61/281, train_loss: 0.1156, step time: 0.2548\n",
      "62/281, train_loss: 0.1430, step time: 0.2551\n",
      "63/281, train_loss: 0.0895, step time: 0.2575\n",
      "64/281, train_loss: 0.0696, step time: 0.2535\n",
      "65/281, train_loss: 0.1255, step time: 0.2598\n",
      "66/281, train_loss: 0.2292, step time: 0.2585\n",
      "67/281, train_loss: 0.2179, step time: 0.2568\n",
      "68/281, train_loss: 0.2689, step time: 0.2571\n",
      "69/281, train_loss: 0.0792, step time: 0.2572\n",
      "70/281, train_loss: 0.1026, step time: 0.2580\n",
      "71/281, train_loss: 0.2770, step time: 0.2591\n",
      "72/281, train_loss: 0.0598, step time: 0.2604\n",
      "73/281, train_loss: 0.1200, step time: 0.2574\n",
      "74/281, train_loss: 0.1020, step time: 0.2555\n",
      "75/281, train_loss: 0.1383, step time: 0.2566\n",
      "76/281, train_loss: 0.2409, step time: 0.2528\n",
      "77/281, train_loss: 0.0895, step time: 0.2533\n",
      "78/281, train_loss: 0.1176, step time: 0.2567\n",
      "79/281, train_loss: 0.1021, step time: 0.2550\n",
      "80/281, train_loss: 0.1151, step time: 0.2556\n",
      "81/281, train_loss: 0.1074, step time: 0.2554\n",
      "82/281, train_loss: 0.0898, step time: 0.2538\n",
      "83/281, train_loss: 0.1116, step time: 0.2548\n",
      "84/281, train_loss: 0.0686, step time: 0.2614\n",
      "85/281, train_loss: 0.0701, step time: 0.2506\n",
      "86/281, train_loss: 0.0670, step time: 0.2474\n",
      "87/281, train_loss: 0.2430, step time: 0.2508\n",
      "88/281, train_loss: 0.0858, step time: 0.2532\n",
      "89/281, train_loss: 0.0623, step time: 0.2510\n",
      "90/281, train_loss: 0.1076, step time: 0.2505\n",
      "91/281, train_loss: 0.1697, step time: 0.2490\n",
      "92/281, train_loss: 0.0889, step time: 0.2527\n",
      "93/281, train_loss: 0.0838, step time: 0.2518\n",
      "94/281, train_loss: 0.0726, step time: 0.2500\n",
      "95/281, train_loss: 0.0809, step time: 0.2531\n",
      "96/281, train_loss: 0.0899, step time: 0.2515\n",
      "97/281, train_loss: 0.0916, step time: 0.2561\n",
      "98/281, train_loss: 0.0782, step time: 0.2560\n",
      "99/281, train_loss: 0.0962, step time: 0.2564\n",
      "100/281, train_loss: 0.1086, step time: 0.2563\n",
      "101/281, train_loss: 0.1100, step time: 0.2552\n",
      "102/281, train_loss: 0.1154, step time: 0.2585\n",
      "103/281, train_loss: 0.1185, step time: 0.2558\n",
      "104/281, train_loss: 0.1131, step time: 0.2589\n",
      "105/281, train_loss: 0.0770, step time: 0.2616\n",
      "106/281, train_loss: 0.1070, step time: 0.2574\n",
      "107/281, train_loss: 0.0905, step time: 0.2534\n",
      "108/281, train_loss: 0.0666, step time: 0.2559\n",
      "109/281, train_loss: 0.1454, step time: 0.2628\n",
      "110/281, train_loss: 0.1177, step time: 0.2577\n",
      "111/281, train_loss: 0.0680, step time: 0.2567\n",
      "112/281, train_loss: 0.0673, step time: 0.2538\n",
      "113/281, train_loss: 0.1407, step time: 0.2517\n",
      "114/281, train_loss: 0.0957, step time: 0.2536\n",
      "115/281, train_loss: 0.2646, step time: 0.2556\n",
      "116/281, train_loss: 0.1130, step time: 0.2585\n",
      "117/281, train_loss: 0.1242, step time: 0.2568\n",
      "118/281, train_loss: 0.0705, step time: 0.2543\n",
      "119/281, train_loss: 0.0667, step time: 0.2584\n",
      "120/281, train_loss: 0.1550, step time: 0.2598\n",
      "121/281, train_loss: 0.1052, step time: 0.2582\n",
      "122/281, train_loss: 0.1031, step time: 0.2507\n",
      "123/281, train_loss: 0.0700, step time: 0.2515\n",
      "124/281, train_loss: 0.1219, step time: 0.2535\n",
      "125/281, train_loss: 0.1098, step time: 0.2560\n",
      "126/281, train_loss: 0.0965, step time: 0.2570\n",
      "127/281, train_loss: 0.2231, step time: 0.2564\n",
      "128/281, train_loss: 0.1086, step time: 0.2585\n",
      "129/281, train_loss: 0.0986, step time: 0.2617\n",
      "130/281, train_loss: 0.0711, step time: 0.2614\n",
      "131/281, train_loss: 0.0725, step time: 0.2608\n",
      "132/281, train_loss: 0.0594, step time: 0.2566\n",
      "133/281, train_loss: 0.0435, step time: 0.2577\n",
      "134/281, train_loss: 0.0866, step time: 0.2517\n",
      "135/281, train_loss: 0.1093, step time: 0.2553\n",
      "136/281, train_loss: 0.1218, step time: 0.2564\n",
      "137/281, train_loss: 0.0964, step time: 0.2599\n",
      "138/281, train_loss: 0.1575, step time: 0.2558\n",
      "139/281, train_loss: 0.0916, step time: 0.2523\n",
      "140/281, train_loss: 0.1002, step time: 0.2563\n",
      "141/281, train_loss: 0.0674, step time: 0.2538\n",
      "142/281, train_loss: 0.2698, step time: 0.2521\n",
      "143/281, train_loss: 0.1487, step time: 0.2536\n",
      "144/281, train_loss: 0.0494, step time: 0.2531\n",
      "145/281, train_loss: 0.2560, step time: 0.2578\n",
      "146/281, train_loss: 0.1432, step time: 0.2517\n",
      "147/281, train_loss: 0.1356, step time: 0.2555\n",
      "148/281, train_loss: 0.0782, step time: 0.2568\n",
      "149/281, train_loss: 0.1167, step time: 0.2563\n",
      "150/281, train_loss: 0.1170, step time: 0.2554\n",
      "151/281, train_loss: 0.0750, step time: 0.2580\n",
      "152/281, train_loss: 0.0847, step time: 0.2594\n",
      "153/281, train_loss: 0.0824, step time: 0.2584\n",
      "154/281, train_loss: 0.1180, step time: 0.2558\n",
      "155/281, train_loss: 0.0637, step time: 0.2555\n",
      "156/281, train_loss: 0.2043, step time: 0.2534\n",
      "157/281, train_loss: 0.1241, step time: 0.2576\n",
      "158/281, train_loss: 0.0739, step time: 0.2535\n",
      "159/281, train_loss: 0.1664, step time: 0.2558\n",
      "160/281, train_loss: 0.1045, step time: 0.2605\n",
      "161/281, train_loss: 0.1350, step time: 0.2539\n",
      "162/281, train_loss: 0.0781, step time: 0.2560\n",
      "163/281, train_loss: 0.1130, step time: 0.2507\n",
      "164/281, train_loss: 0.0941, step time: 0.2533\n",
      "165/281, train_loss: 0.0809, step time: 0.2583\n",
      "166/281, train_loss: 0.0812, step time: 0.2576\n",
      "167/281, train_loss: 0.2334, step time: 0.2525\n",
      "168/281, train_loss: 0.0656, step time: 0.2556\n",
      "169/281, train_loss: 0.2600, step time: 0.2560\n",
      "170/281, train_loss: 0.0711, step time: 0.2535\n",
      "171/281, train_loss: 0.0810, step time: 0.2526\n",
      "172/281, train_loss: 0.1562, step time: 0.2580\n",
      "173/281, train_loss: 0.2360, step time: 0.2574\n",
      "174/281, train_loss: 0.2292, step time: 0.2531\n",
      "175/281, train_loss: 0.0717, step time: 0.2557\n",
      "176/281, train_loss: 0.0806, step time: 0.2579\n",
      "177/281, train_loss: 0.1094, step time: 0.2620\n",
      "178/281, train_loss: 0.0696, step time: 0.2546\n",
      "179/281, train_loss: 0.1095, step time: 0.2582\n",
      "180/281, train_loss: 0.0965, step time: 0.2527\n",
      "181/281, train_loss: 0.1306, step time: 0.2561\n",
      "182/281, train_loss: 0.0941, step time: 0.2516\n",
      "183/281, train_loss: 0.0561, step time: 0.2515\n",
      "184/281, train_loss: 0.1726, step time: 0.2566\n",
      "185/281, train_loss: 0.0779, step time: 0.2526\n",
      "186/281, train_loss: 0.1608, step time: 0.2516\n",
      "187/281, train_loss: 0.1046, step time: 0.2582\n",
      "188/281, train_loss: 0.2586, step time: 0.2507\n",
      "189/281, train_loss: 0.0964, step time: 0.2537\n",
      "190/281, train_loss: 0.2231, step time: 0.2582\n",
      "191/281, train_loss: 0.2552, step time: 0.2527\n",
      "192/281, train_loss: 0.1166, step time: 0.2551\n",
      "193/281, train_loss: 0.0909, step time: 0.2530\n",
      "194/281, train_loss: 0.0760, step time: 0.2510\n",
      "195/281, train_loss: 0.2647, step time: 0.2500\n",
      "196/281, train_loss: 0.2322, step time: 0.2556\n",
      "197/281, train_loss: 0.0501, step time: 0.2519\n",
      "198/281, train_loss: 0.0950, step time: 0.2540\n",
      "199/281, train_loss: 0.2877, step time: 0.2554\n",
      "200/281, train_loss: 0.1008, step time: 0.2548\n",
      "201/281, train_loss: 0.0785, step time: 0.2578\n",
      "202/281, train_loss: 0.0821, step time: 0.2571\n",
      "203/281, train_loss: 0.1489, step time: 0.2521\n",
      "204/281, train_loss: 0.1171, step time: 0.2541\n",
      "205/281, train_loss: 0.0829, step time: 0.2581\n",
      "206/281, train_loss: 0.2464, step time: 0.2556\n",
      "207/281, train_loss: 0.0654, step time: 0.2587\n",
      "208/281, train_loss: 0.1129, step time: 0.2548\n",
      "209/281, train_loss: 0.0837, step time: 0.2611\n",
      "210/281, train_loss: 0.0756, step time: 0.2570\n",
      "211/281, train_loss: 0.0571, step time: 0.2632\n",
      "212/281, train_loss: 0.1413, step time: 0.2662\n",
      "213/281, train_loss: 0.0671, step time: 0.2632\n",
      "214/281, train_loss: 0.0991, step time: 0.2590\n",
      "215/281, train_loss: 0.2505, step time: 0.2597\n",
      "216/281, train_loss: 0.1344, step time: 0.2555\n",
      "217/281, train_loss: 0.0833, step time: 0.2570\n",
      "218/281, train_loss: 0.0877, step time: 0.2608\n",
      "219/281, train_loss: 0.1218, step time: 0.2525\n",
      "220/281, train_loss: 0.0729, step time: 0.2516\n",
      "221/281, train_loss: 0.0743, step time: 0.2600\n",
      "222/281, train_loss: 0.0889, step time: 0.2575\n",
      "223/281, train_loss: 0.0415, step time: 0.2574\n",
      "224/281, train_loss: 0.0525, step time: 0.2534\n",
      "225/281, train_loss: 0.0669, step time: 0.2573\n",
      "226/281, train_loss: 0.2632, step time: 0.2569\n",
      "227/281, train_loss: 0.0863, step time: 0.2531\n",
      "228/281, train_loss: 0.2360, step time: 0.2555\n",
      "229/281, train_loss: 0.1014, step time: 0.2553\n",
      "230/281, train_loss: 0.0803, step time: 0.2540\n",
      "231/281, train_loss: 0.2602, step time: 0.2546\n",
      "232/281, train_loss: 0.0666, step time: 0.2561\n",
      "233/281, train_loss: 0.1084, step time: 0.2565\n",
      "234/281, train_loss: 0.1248, step time: 0.2590\n",
      "235/281, train_loss: 0.0534, step time: 0.2563\n",
      "236/281, train_loss: 0.0888, step time: 0.2617\n",
      "237/281, train_loss: 0.0805, step time: 0.2505\n",
      "238/281, train_loss: 0.2484, step time: 0.2519\n",
      "239/281, train_loss: 0.0789, step time: 0.2505\n",
      "240/281, train_loss: 0.0800, step time: 0.2515\n",
      "241/281, train_loss: 0.1390, step time: 0.2579\n",
      "242/281, train_loss: 0.0926, step time: 0.2560\n",
      "243/281, train_loss: 0.2300, step time: 0.2497\n",
      "244/281, train_loss: 0.1062, step time: 0.2529\n",
      "245/281, train_loss: 0.2410, step time: 0.2517\n",
      "246/281, train_loss: 0.2663, step time: 0.2581\n",
      "247/281, train_loss: 0.0775, step time: 0.2518\n",
      "248/281, train_loss: 0.1408, step time: 0.2604\n",
      "249/281, train_loss: 0.0879, step time: 0.2551\n",
      "250/281, train_loss: 0.1204, step time: 0.2522\n",
      "251/281, train_loss: 0.0708, step time: 0.2490\n",
      "252/281, train_loss: 0.0854, step time: 0.2523\n",
      "253/281, train_loss: 0.0868, step time: 0.2576\n",
      "254/281, train_loss: 0.2609, step time: 0.2526\n",
      "255/281, train_loss: 0.0782, step time: 0.2503\n",
      "256/281, train_loss: 0.0980, step time: 0.2529\n",
      "257/281, train_loss: 0.0886, step time: 0.2556\n",
      "258/281, train_loss: 0.2904, step time: 0.2524\n",
      "259/281, train_loss: 0.2499, step time: 0.2569\n",
      "260/281, train_loss: 0.0970, step time: 0.2536\n",
      "261/281, train_loss: 0.2364, step time: 0.2487\n",
      "262/281, train_loss: 0.1230, step time: 0.2511\n",
      "263/281, train_loss: 0.0767, step time: 0.2491\n",
      "264/281, train_loss: 0.0647, step time: 0.2570\n",
      "265/281, train_loss: 0.1496, step time: 0.2573\n",
      "266/281, train_loss: 0.2187, step time: 0.2524\n",
      "267/281, train_loss: 0.1170, step time: 0.2511\n",
      "268/281, train_loss: 0.0637, step time: 0.2512\n",
      "269/281, train_loss: 0.2559, step time: 0.2563\n",
      "270/281, train_loss: 0.0566, step time: 0.2522\n",
      "271/281, train_loss: 0.1380, step time: 0.2538\n",
      "272/281, train_loss: 0.1400, step time: 0.2589\n",
      "273/281, train_loss: 0.0507, step time: 0.2547\n",
      "274/281, train_loss: 0.0813, step time: 0.2555\n",
      "275/281, train_loss: 0.0751, step time: 0.2530\n",
      "276/281, train_loss: 0.0578, step time: 0.2535\n",
      "277/281, train_loss: 0.1294, step time: 0.2502\n",
      "278/281, train_loss: 0.1186, step time: 0.2582\n",
      "279/281, train_loss: 0.0782, step time: 0.2578\n",
      "280/281, train_loss: 0.0850, step time: 0.2609\n",
      "281/281, train_loss: 0.1133, step time: 0.2575\n",
      "282/281, train_loss: 0.0489, step time: 0.1541\n",
      "epoch 55 average loss: 0.1220\n",
      "saved new best metric model\n",
      "current epoch: 55 current mean dice: 0.8802 tc: 0.8738 wt: 0.9159 et: 0.8603\n",
      "best mean dice: 0.8802 at epoch: 55\n",
      "time consuming of epoch 55 is: 380.9980\n",
      "----------\n",
      "epoch 56/200\n",
      "1/281, train_loss: 0.1473, step time: 0.2511\n",
      "2/281, train_loss: 0.0807, step time: 0.2572\n",
      "3/281, train_loss: 0.1271, step time: 0.2523\n",
      "4/281, train_loss: 0.1025, step time: 0.2585\n",
      "5/281, train_loss: 0.1003, step time: 0.2595\n",
      "6/281, train_loss: 0.1107, step time: 0.2605\n",
      "7/281, train_loss: 0.1262, step time: 0.2569\n",
      "8/281, train_loss: 0.0758, step time: 0.2571\n",
      "9/281, train_loss: 0.2426, step time: 0.2561\n",
      "10/281, train_loss: 0.0824, step time: 0.2549\n",
      "11/281, train_loss: 0.0777, step time: 0.2587\n",
      "12/281, train_loss: 0.0660, step time: 0.2590\n",
      "13/281, train_loss: 0.1131, step time: 0.2578\n",
      "14/281, train_loss: 0.1038, step time: 0.2524\n",
      "15/281, train_loss: 0.1017, step time: 0.2596\n",
      "16/281, train_loss: 0.1253, step time: 0.2631\n",
      "17/281, train_loss: 0.1056, step time: 0.2631\n",
      "18/281, train_loss: 0.0857, step time: 0.2556\n",
      "19/281, train_loss: 0.0765, step time: 0.2529\n",
      "20/281, train_loss: 0.0766, step time: 0.2589\n",
      "21/281, train_loss: 0.0908, step time: 0.2633\n",
      "22/281, train_loss: 0.0775, step time: 0.2560\n",
      "23/281, train_loss: 0.0814, step time: 0.2573\n",
      "24/281, train_loss: 0.0873, step time: 0.2552\n",
      "25/281, train_loss: 0.1214, step time: 0.2614\n",
      "26/281, train_loss: 0.1250, step time: 0.2521\n",
      "27/281, train_loss: 0.0922, step time: 0.2543\n",
      "28/281, train_loss: 0.0463, step time: 0.2501\n",
      "29/281, train_loss: 0.0928, step time: 0.2502\n",
      "30/281, train_loss: 0.2582, step time: 0.2602\n",
      "31/281, train_loss: 0.1184, step time: 0.2609\n",
      "32/281, train_loss: 0.0878, step time: 0.2881\n",
      "33/281, train_loss: 0.0671, step time: 0.2546\n",
      "34/281, train_loss: 0.0965, step time: 0.2508\n",
      "35/281, train_loss: 0.2306, step time: 0.2588\n",
      "36/281, train_loss: 0.0864, step time: 0.2592\n",
      "37/281, train_loss: 0.1126, step time: 0.2540\n",
      "38/281, train_loss: 0.0720, step time: 0.2493\n",
      "39/281, train_loss: 0.0886, step time: 0.2548\n",
      "40/281, train_loss: 0.0587, step time: 0.2472\n",
      "41/281, train_loss: 0.2731, step time: 0.2510\n",
      "42/281, train_loss: 0.0800, step time: 0.2554\n",
      "43/281, train_loss: 0.0808, step time: 0.2526\n",
      "44/281, train_loss: 0.0935, step time: 0.2575\n",
      "45/281, train_loss: 0.1414, step time: 0.2566\n",
      "46/281, train_loss: 0.0867, step time: 0.2472\n",
      "47/281, train_loss: 0.1007, step time: 0.2600\n",
      "48/281, train_loss: 0.1088, step time: 0.2595\n",
      "49/281, train_loss: 0.0506, step time: 0.2565\n",
      "50/281, train_loss: 0.0934, step time: 0.2574\n",
      "51/281, train_loss: 0.0732, step time: 0.2590\n",
      "52/281, train_loss: 0.0816, step time: 0.2585\n",
      "53/281, train_loss: 0.2916, step time: 0.2610\n",
      "54/281, train_loss: 0.0856, step time: 0.2545\n",
      "55/281, train_loss: 0.0890, step time: 0.2547\n",
      "56/281, train_loss: 0.0849, step time: 0.2601\n",
      "57/281, train_loss: 0.1139, step time: 0.2628\n",
      "58/281, train_loss: 0.0913, step time: 0.2528\n",
      "59/281, train_loss: 0.3988, step time: 0.2595\n",
      "60/281, train_loss: 0.2488, step time: 0.2522\n",
      "61/281, train_loss: 0.0533, step time: 0.2473\n",
      "62/281, train_loss: 0.0783, step time: 0.2536\n",
      "63/281, train_loss: 0.0712, step time: 0.2527\n",
      "64/281, train_loss: 0.0740, step time: 0.2521\n",
      "65/281, train_loss: 0.0600, step time: 0.2508\n",
      "66/281, train_loss: 0.0681, step time: 0.2511\n",
      "67/281, train_loss: 0.1264, step time: 0.2536\n",
      "68/281, train_loss: 0.0888, step time: 0.2565\n",
      "69/281, train_loss: 0.0692, step time: 0.2508\n",
      "70/281, train_loss: 0.1014, step time: 0.2591\n",
      "71/281, train_loss: 0.2284, step time: 0.2570\n",
      "72/281, train_loss: 0.1200, step time: 0.2567\n",
      "73/281, train_loss: 0.1073, step time: 0.2570\n",
      "74/281, train_loss: 0.1925, step time: 0.2502\n",
      "75/281, train_loss: 0.1462, step time: 0.2518\n",
      "76/281, train_loss: 0.0746, step time: 0.2510\n",
      "77/281, train_loss: 0.0541, step time: 0.2498\n",
      "78/281, train_loss: 0.0547, step time: 0.2642\n",
      "79/281, train_loss: 0.0679, step time: 0.2510\n",
      "80/281, train_loss: 0.0850, step time: 0.2493\n",
      "81/281, train_loss: 0.0804, step time: 0.2549\n",
      "82/281, train_loss: 0.0572, step time: 0.2542\n",
      "83/281, train_loss: 0.1170, step time: 0.2508\n",
      "84/281, train_loss: 0.0881, step time: 0.2522\n",
      "85/281, train_loss: 0.0649, step time: 0.2482\n",
      "86/281, train_loss: 0.1238, step time: 0.2585\n",
      "87/281, train_loss: 0.1129, step time: 0.2580\n",
      "88/281, train_loss: 0.0905, step time: 0.2576\n",
      "89/281, train_loss: 0.2405, step time: 0.2581\n",
      "90/281, train_loss: 0.0808, step time: 0.2552\n",
      "91/281, train_loss: 0.2631, step time: 0.2522\n",
      "92/281, train_loss: 0.0978, step time: 0.2487\n",
      "93/281, train_loss: 0.0806, step time: 0.2553\n",
      "94/281, train_loss: 0.0949, step time: 0.2548\n",
      "95/281, train_loss: 0.0982, step time: 0.2574\n",
      "96/281, train_loss: 0.0906, step time: 0.2498\n",
      "97/281, train_loss: 0.0620, step time: 0.2549\n",
      "98/281, train_loss: 0.0749, step time: 0.2560\n",
      "99/281, train_loss: 0.1346, step time: 0.2547\n",
      "100/281, train_loss: 0.2374, step time: 0.2469\n",
      "101/281, train_loss: 0.2520, step time: 0.2548\n",
      "102/281, train_loss: 0.0991, step time: 0.2575\n",
      "103/281, train_loss: 0.0749, step time: 0.2597\n",
      "104/281, train_loss: 0.1176, step time: 0.2549\n",
      "105/281, train_loss: 0.0756, step time: 0.2601\n",
      "106/281, train_loss: 0.0910, step time: 0.2546\n",
      "107/281, train_loss: 0.0849, step time: 0.2568\n",
      "108/281, train_loss: 0.0765, step time: 0.2535\n",
      "109/281, train_loss: 0.0772, step time: 0.2542\n",
      "110/281, train_loss: 0.1197, step time: 0.2520\n",
      "111/281, train_loss: 0.0646, step time: 0.2535\n",
      "112/281, train_loss: 0.0748, step time: 0.2538\n",
      "113/281, train_loss: 0.1160, step time: 0.2508\n",
      "114/281, train_loss: 0.1187, step time: 0.2481\n",
      "115/281, train_loss: 0.1367, step time: 0.2555\n",
      "116/281, train_loss: 0.0733, step time: 0.2567\n",
      "117/281, train_loss: 0.1368, step time: 0.2507\n",
      "118/281, train_loss: 0.0885, step time: 0.2585\n",
      "119/281, train_loss: 0.1192, step time: 0.2556\n",
      "120/281, train_loss: 0.0651, step time: 0.2544\n",
      "121/281, train_loss: 0.0872, step time: 0.2562\n",
      "122/281, train_loss: 0.0784, step time: 0.2455\n",
      "123/281, train_loss: 0.0903, step time: 0.2468\n",
      "124/281, train_loss: 0.0610, step time: 0.2460\n",
      "125/281, train_loss: 0.0889, step time: 0.2720\n",
      "126/281, train_loss: 0.2466, step time: 0.2571\n",
      "127/281, train_loss: 0.1012, step time: 0.2518\n",
      "128/281, train_loss: 0.2464, step time: 0.2675\n",
      "129/281, train_loss: 0.0731, step time: 0.2501\n",
      "130/281, train_loss: 0.3434, step time: 0.2571\n",
      "131/281, train_loss: 0.1082, step time: 0.2566\n",
      "132/281, train_loss: 0.1232, step time: 0.2557\n",
      "133/281, train_loss: 0.0715, step time: 0.2528\n",
      "134/281, train_loss: 0.0977, step time: 0.2554\n",
      "135/281, train_loss: 0.0974, step time: 0.2529\n",
      "136/281, train_loss: 0.1298, step time: 0.2525\n",
      "137/281, train_loss: 0.2653, step time: 0.2539\n",
      "138/281, train_loss: 0.1508, step time: 0.2558\n",
      "139/281, train_loss: 0.1016, step time: 0.2669\n",
      "140/281, train_loss: 0.1486, step time: 0.2940\n",
      "141/281, train_loss: 0.2341, step time: 0.2550\n",
      "142/281, train_loss: 0.0506, step time: 0.2501\n",
      "143/281, train_loss: 0.0712, step time: 0.2480\n",
      "144/281, train_loss: 0.2918, step time: 0.2475\n",
      "145/281, train_loss: 0.0566, step time: 0.2592\n",
      "146/281, train_loss: 0.0849, step time: 0.2557\n",
      "147/281, train_loss: 0.0988, step time: 0.2558\n",
      "148/281, train_loss: 0.0978, step time: 0.2559\n",
      "149/281, train_loss: 0.1416, step time: 0.2564\n",
      "150/281, train_loss: 0.1110, step time: 0.2551\n",
      "151/281, train_loss: 0.0842, step time: 0.2596\n",
      "152/281, train_loss: 0.0574, step time: 0.2572\n",
      "153/281, train_loss: 0.2489, step time: 0.2483\n",
      "154/281, train_loss: 0.1227, step time: 0.2525\n",
      "155/281, train_loss: 0.0882, step time: 0.2498\n",
      "156/281, train_loss: 0.1664, step time: 0.2552\n",
      "157/281, train_loss: 0.0425, step time: 0.2551\n",
      "158/281, train_loss: 0.0713, step time: 0.2567\n",
      "159/281, train_loss: 0.0722, step time: 0.2464\n",
      "160/281, train_loss: 0.2405, step time: 0.2460\n",
      "161/281, train_loss: 0.2264, step time: 0.2478\n",
      "162/281, train_loss: 0.2617, step time: 0.2546\n",
      "163/281, train_loss: 0.1032, step time: 0.2566\n",
      "164/281, train_loss: 0.0975, step time: 0.2537\n",
      "165/281, train_loss: 0.1081, step time: 0.2538\n",
      "166/281, train_loss: 0.0463, step time: 0.2518\n",
      "167/281, train_loss: 0.1727, step time: 0.2554\n",
      "168/281, train_loss: 0.1474, step time: 0.2533\n",
      "169/281, train_loss: 0.0835, step time: 0.2499\n",
      "170/281, train_loss: 0.2515, step time: 0.2476\n",
      "171/281, train_loss: 0.1129, step time: 0.2553\n",
      "172/281, train_loss: 0.0934, step time: 0.2545\n",
      "173/281, train_loss: 0.0659, step time: 0.2527\n",
      "174/281, train_loss: 0.1140, step time: 0.2487\n",
      "175/281, train_loss: 0.0457, step time: 0.2520\n",
      "176/281, train_loss: 0.1208, step time: 0.2541\n",
      "177/281, train_loss: 0.0691, step time: 0.2525\n",
      "178/281, train_loss: 0.0499, step time: 0.2474\n",
      "179/281, train_loss: 0.0885, step time: 0.2509\n",
      "180/281, train_loss: 0.4323, step time: 0.2489\n",
      "181/281, train_loss: 0.0528, step time: 0.2510\n",
      "182/281, train_loss: 0.0935, step time: 0.2481\n",
      "183/281, train_loss: 0.1313, step time: 0.2507\n",
      "184/281, train_loss: 0.0895, step time: 0.2489\n",
      "185/281, train_loss: 0.1434, step time: 0.2498\n",
      "186/281, train_loss: 0.1131, step time: 0.2482\n",
      "187/281, train_loss: 0.0615, step time: 0.2519\n",
      "188/281, train_loss: 0.1100, step time: 0.2463\n",
      "189/281, train_loss: 0.0796, step time: 0.2438\n",
      "190/281, train_loss: 0.1448, step time: 0.2476\n",
      "191/281, train_loss: 0.1089, step time: 0.2514\n",
      "192/281, train_loss: 0.0843, step time: 0.2500\n",
      "193/281, train_loss: 0.0853, step time: 0.2463\n",
      "194/281, train_loss: 0.1055, step time: 0.2549\n",
      "195/281, train_loss: 0.2820, step time: 0.2537\n",
      "196/281, train_loss: 0.1558, step time: 0.2536\n",
      "197/281, train_loss: 0.0398, step time: 0.2505\n",
      "198/281, train_loss: 0.1690, step time: 0.2581\n",
      "199/281, train_loss: 0.2626, step time: 0.2528\n",
      "200/281, train_loss: 0.1098, step time: 0.2568\n",
      "201/281, train_loss: 0.0934, step time: 0.2536\n",
      "202/281, train_loss: 0.0708, step time: 0.2550\n",
      "203/281, train_loss: 0.2514, step time: 0.2579\n",
      "204/281, train_loss: 0.0681, step time: 0.2519\n",
      "205/281, train_loss: 0.0840, step time: 0.2504\n",
      "206/281, train_loss: 0.0483, step time: 0.2516\n",
      "207/281, train_loss: 0.1467, step time: 0.2567\n",
      "208/281, train_loss: 0.2356, step time: 0.2563\n",
      "209/281, train_loss: 0.1101, step time: 0.2526\n",
      "210/281, train_loss: 0.2561, step time: 0.2511\n",
      "211/281, train_loss: 0.0481, step time: 0.2540\n",
      "212/281, train_loss: 0.1044, step time: 0.2532\n",
      "213/281, train_loss: 0.1033, step time: 0.2505\n",
      "214/281, train_loss: 0.0896, step time: 0.2556\n",
      "215/281, train_loss: 0.0839, step time: 0.2545\n",
      "216/281, train_loss: 0.1669, step time: 0.2549\n",
      "217/281, train_loss: 0.1525, step time: 0.2533\n",
      "218/281, train_loss: 0.0622, step time: 0.2502\n",
      "219/281, train_loss: 0.0888, step time: 0.2521\n",
      "220/281, train_loss: 0.1065, step time: 0.2543\n",
      "221/281, train_loss: 0.0819, step time: 0.2479\n",
      "222/281, train_loss: 0.1113, step time: 0.2457\n",
      "223/281, train_loss: 0.0964, step time: 0.2490\n",
      "224/281, train_loss: 0.1146, step time: 0.2510\n",
      "225/281, train_loss: 0.2811, step time: 0.2482\n",
      "226/281, train_loss: 0.1064, step time: 0.2482\n",
      "227/281, train_loss: 0.2372, step time: 0.2505\n",
      "228/281, train_loss: 0.2567, step time: 0.2527\n",
      "229/281, train_loss: 0.2311, step time: 0.2507\n",
      "230/281, train_loss: 0.0614, step time: 0.2472\n",
      "231/281, train_loss: 0.0952, step time: 0.2506\n",
      "232/281, train_loss: 0.0921, step time: 0.2532\n",
      "233/281, train_loss: 0.2339, step time: 0.2473\n",
      "234/281, train_loss: 0.0580, step time: 0.2510\n",
      "235/281, train_loss: 0.0787, step time: 0.2540\n",
      "236/281, train_loss: 0.2702, step time: 0.2543\n",
      "237/281, train_loss: 0.0973, step time: 0.2463\n",
      "238/281, train_loss: 0.0702, step time: 0.2492\n",
      "239/281, train_loss: 0.0669, step time: 0.2557\n",
      "240/281, train_loss: 0.0920, step time: 0.2497\n",
      "241/281, train_loss: 0.1101, step time: 0.2488\n",
      "242/281, train_loss: 0.1275, step time: 0.2519\n",
      "243/281, train_loss: 0.0778, step time: 0.2480\n",
      "244/281, train_loss: 0.1448, step time: 0.2453\n",
      "245/281, train_loss: 0.2698, step time: 0.2520\n",
      "246/281, train_loss: 0.0942, step time: 0.2494\n",
      "247/281, train_loss: 0.1096, step time: 0.2497\n",
      "248/281, train_loss: 0.0665, step time: 0.2555\n",
      "249/281, train_loss: 0.2707, step time: 0.2577\n",
      "250/281, train_loss: 0.0745, step time: 0.2507\n",
      "251/281, train_loss: 0.2800, step time: 0.2541\n",
      "252/281, train_loss: 0.1020, step time: 0.2528\n",
      "253/281, train_loss: 0.1132, step time: 0.2469\n",
      "254/281, train_loss: 0.0724, step time: 0.2451\n",
      "255/281, train_loss: 0.0945, step time: 0.2501\n",
      "256/281, train_loss: 0.0544, step time: 0.2506\n",
      "257/281, train_loss: 0.0824, step time: 0.2524\n",
      "258/281, train_loss: 0.2460, step time: 0.2448\n",
      "259/281, train_loss: 0.0790, step time: 0.2526\n",
      "260/281, train_loss: 0.0509, step time: 0.2534\n",
      "261/281, train_loss: 0.0781, step time: 0.2472\n",
      "262/281, train_loss: 0.2516, step time: 0.2486\n",
      "263/281, train_loss: 0.0464, step time: 0.2551\n",
      "264/281, train_loss: 0.1048, step time: 0.2510\n",
      "265/281, train_loss: 0.2302, step time: 0.2534\n",
      "266/281, train_loss: 0.0816, step time: 0.2555\n",
      "267/281, train_loss: 0.0777, step time: 0.2519\n",
      "268/281, train_loss: 0.0967, step time: 0.2506\n",
      "269/281, train_loss: 0.0490, step time: 0.2429\n",
      "270/281, train_loss: 0.0843, step time: 0.2453\n",
      "271/281, train_loss: 0.0810, step time: 0.2568\n",
      "272/281, train_loss: 0.0556, step time: 0.2570\n",
      "273/281, train_loss: 0.0707, step time: 0.2525\n",
      "274/281, train_loss: 0.1146, step time: 0.2576\n",
      "275/281, train_loss: 0.1262, step time: 0.2468\n",
      "276/281, train_loss: 0.0540, step time: 0.2450\n",
      "277/281, train_loss: 0.0992, step time: 0.2464\n",
      "278/281, train_loss: 0.2743, step time: 0.2544\n",
      "279/281, train_loss: 0.0584, step time: 0.2472\n",
      "280/281, train_loss: 0.0685, step time: 0.2459\n",
      "281/281, train_loss: 0.0772, step time: 0.2466\n",
      "282/281, train_loss: 0.1274, step time: 0.1507\n",
      "epoch 56 average loss: 0.1183\n",
      "current epoch: 56 current mean dice: 0.8772 tc: 0.8690 wt: 0.9166 et: 0.8552\n",
      "best mean dice: 0.8802 at epoch: 55\n",
      "time consuming of epoch 56 is: 392.9415\n",
      "----------\n",
      "epoch 57/200\n",
      "1/281, train_loss: 0.1373, step time: 0.2625\n",
      "2/281, train_loss: 0.1049, step time: 0.2515\n",
      "3/281, train_loss: 0.0580, step time: 0.2567\n",
      "4/281, train_loss: 0.0929, step time: 0.2542\n",
      "5/281, train_loss: 0.0927, step time: 0.2468\n",
      "6/281, train_loss: 0.0925, step time: 0.2465\n",
      "7/281, train_loss: 0.0470, step time: 0.2523\n",
      "8/281, train_loss: 0.0881, step time: 0.2712\n",
      "9/281, train_loss: 0.0966, step time: 0.2476\n",
      "10/281, train_loss: 0.1095, step time: 0.2494\n",
      "11/281, train_loss: 0.1072, step time: 0.2480\n",
      "12/281, train_loss: 0.2878, step time: 0.2540\n",
      "13/281, train_loss: 0.1151, step time: 0.2467\n",
      "14/281, train_loss: 0.0645, step time: 0.2523\n",
      "15/281, train_loss: 0.0869, step time: 0.2561\n",
      "16/281, train_loss: 0.0701, step time: 0.2658\n",
      "17/281, train_loss: 0.0812, step time: 0.2470\n",
      "18/281, train_loss: 0.0959, step time: 0.2451\n",
      "19/281, train_loss: 0.1709, step time: 0.2484\n",
      "20/281, train_loss: 0.0821, step time: 0.2498\n",
      "21/281, train_loss: 0.1101, step time: 0.2580\n",
      "22/281, train_loss: 0.1177, step time: 0.2477\n",
      "23/281, train_loss: 0.0918, step time: 0.2444\n",
      "24/281, train_loss: 0.0808, step time: 0.2500\n",
      "25/281, train_loss: 0.1229, step time: 0.2471\n",
      "26/281, train_loss: 0.0872, step time: 0.2502\n",
      "27/281, train_loss: 0.2516, step time: 0.2529\n",
      "28/281, train_loss: 0.2315, step time: 0.2509\n",
      "29/281, train_loss: 0.2094, step time: 0.2466\n",
      "30/281, train_loss: 0.0783, step time: 0.2496\n",
      "31/281, train_loss: 0.0729, step time: 0.2587\n",
      "32/281, train_loss: 0.1360, step time: 0.2566\n",
      "33/281, train_loss: 0.0976, step time: 0.2496\n",
      "34/281, train_loss: 0.0608, step time: 0.2525\n",
      "35/281, train_loss: 0.1207, step time: 0.2608\n",
      "36/281, train_loss: 0.1108, step time: 0.2568\n",
      "37/281, train_loss: 0.0777, step time: 0.2541\n",
      "38/281, train_loss: 0.2705, step time: 0.2537\n",
      "39/281, train_loss: 0.0905, step time: 0.2520\n",
      "40/281, train_loss: 0.1094, step time: 0.2524\n",
      "41/281, train_loss: 0.0713, step time: 0.2539\n",
      "42/281, train_loss: 0.0615, step time: 0.2531\n",
      "43/281, train_loss: 0.2353, step time: 0.2558\n",
      "44/281, train_loss: 0.0622, step time: 0.2554\n",
      "45/281, train_loss: 0.2692, step time: 0.2529\n",
      "46/281, train_loss: 0.2581, step time: 0.2564\n",
      "47/281, train_loss: 0.1155, step time: 0.2523\n",
      "48/281, train_loss: 0.1093, step time: 0.2555\n",
      "49/281, train_loss: 0.2590, step time: 0.2562\n",
      "50/281, train_loss: 0.0957, step time: 0.2577\n",
      "51/281, train_loss: 0.0974, step time: 0.2573\n",
      "52/281, train_loss: 0.1070, step time: 0.2584\n",
      "53/281, train_loss: 0.0966, step time: 0.2575\n",
      "54/281, train_loss: 0.2496, step time: 0.2539\n",
      "55/281, train_loss: 0.0862, step time: 0.2540\n",
      "56/281, train_loss: 0.1409, step time: 0.2579\n",
      "57/281, train_loss: 0.2306, step time: 0.2597\n",
      "58/281, train_loss: 0.0674, step time: 0.2633\n",
      "59/281, train_loss: 0.2509, step time: 0.2655\n",
      "60/281, train_loss: 0.0656, step time: 0.2598\n",
      "61/281, train_loss: 0.1024, step time: 0.2597\n",
      "62/281, train_loss: 0.0671, step time: 0.2600\n",
      "63/281, train_loss: 0.1177, step time: 0.2551\n",
      "64/281, train_loss: 0.1017, step time: 0.2550\n",
      "65/281, train_loss: 0.0781, step time: 0.2626\n",
      "66/281, train_loss: 0.1551, step time: 0.2582\n",
      "67/281, train_loss: 0.1939, step time: 0.2554\n",
      "68/281, train_loss: 0.1693, step time: 0.2511\n",
      "69/281, train_loss: 0.2434, step time: 0.2595\n",
      "70/281, train_loss: 0.1015, step time: 0.2584\n",
      "71/281, train_loss: 0.0573, step time: 0.2605\n",
      "72/281, train_loss: 0.0901, step time: 0.2576\n",
      "73/281, train_loss: 0.0671, step time: 0.2614\n",
      "74/281, train_loss: 0.2526, step time: 0.2586\n",
      "75/281, train_loss: 0.0598, step time: 0.2561\n",
      "76/281, train_loss: 0.0933, step time: 0.2590\n",
      "77/281, train_loss: 0.1169, step time: 0.2594\n",
      "78/281, train_loss: 0.1104, step time: 0.2603\n",
      "79/281, train_loss: 0.0749, step time: 0.2575\n",
      "80/281, train_loss: 0.1192, step time: 0.2579\n",
      "81/281, train_loss: 0.0814, step time: 0.2608\n",
      "82/281, train_loss: 0.0954, step time: 0.2570\n",
      "83/281, train_loss: 0.0880, step time: 0.2566\n",
      "84/281, train_loss: 0.0803, step time: 0.2606\n",
      "85/281, train_loss: 0.0986, step time: 0.2543\n",
      "86/281, train_loss: 0.1091, step time: 0.2535\n",
      "87/281, train_loss: 0.0915, step time: 0.2553\n",
      "88/281, train_loss: 0.0830, step time: 0.2590\n",
      "89/281, train_loss: 0.1148, step time: 0.2596\n",
      "90/281, train_loss: 0.0583, step time: 0.2594\n",
      "91/281, train_loss: 0.1727, step time: 0.2610\n",
      "92/281, train_loss: 0.0559, step time: 0.2563\n",
      "93/281, train_loss: 0.1189, step time: 0.2587\n",
      "94/281, train_loss: 0.1117, step time: 0.2568\n",
      "95/281, train_loss: 0.0826, step time: 0.2575\n",
      "96/281, train_loss: 0.0980, step time: 0.2588\n",
      "97/281, train_loss: 0.1079, step time: 0.2589\n",
      "98/281, train_loss: 0.0885, step time: 0.2596\n",
      "99/281, train_loss: 0.2422, step time: 0.2562\n",
      "100/281, train_loss: 0.0690, step time: 0.2492\n",
      "101/281, train_loss: 0.2244, step time: 0.2597\n",
      "102/281, train_loss: 0.2851, step time: 0.2606\n",
      "103/281, train_loss: 0.0967, step time: 0.2586\n",
      "104/281, train_loss: 0.0690, step time: 0.2586\n",
      "105/281, train_loss: 0.1000, step time: 0.2553\n",
      "106/281, train_loss: 0.1012, step time: 0.2591\n",
      "107/281, train_loss: 0.0548, step time: 0.2584\n",
      "108/281, train_loss: 0.0674, step time: 0.2585\n",
      "109/281, train_loss: 0.1108, step time: 0.2621\n",
      "110/281, train_loss: 0.1449, step time: 0.2621\n",
      "111/281, train_loss: 0.1092, step time: 0.2573\n",
      "112/281, train_loss: 0.0810, step time: 0.2518\n",
      "113/281, train_loss: 0.0855, step time: 0.2617\n",
      "114/281, train_loss: 0.0718, step time: 0.2601\n",
      "115/281, train_loss: 0.0620, step time: 0.2614\n",
      "116/281, train_loss: 0.1180, step time: 0.2537\n",
      "117/281, train_loss: 0.0939, step time: 0.2536\n",
      "118/281, train_loss: 0.0653, step time: 0.2527\n",
      "119/281, train_loss: 0.0523, step time: 0.2782\n",
      "120/281, train_loss: 0.0683, step time: 0.2537\n",
      "121/281, train_loss: 0.0619, step time: 0.2560\n",
      "122/281, train_loss: 0.0977, step time: 0.2542\n",
      "123/281, train_loss: 0.0600, step time: 0.2561\n",
      "124/281, train_loss: 0.0970, step time: 0.2589\n",
      "125/281, train_loss: 0.1043, step time: 0.2644\n",
      "126/281, train_loss: 0.0770, step time: 0.2602\n",
      "127/281, train_loss: 0.0796, step time: 0.2571\n",
      "128/281, train_loss: 0.1025, step time: 0.2520\n",
      "129/281, train_loss: 0.0687, step time: 0.2523\n",
      "130/281, train_loss: 0.1004, step time: 0.2494\n",
      "131/281, train_loss: 0.0865, step time: 0.2528\n",
      "132/281, train_loss: 0.2103, step time: 0.2506\n",
      "133/281, train_loss: 0.0800, step time: 0.2531\n",
      "134/281, train_loss: 0.1757, step time: 0.2481\n",
      "135/281, train_loss: 0.1168, step time: 0.2486\n",
      "136/281, train_loss: 0.1142, step time: 0.2514\n",
      "137/281, train_loss: 0.0703, step time: 0.2498\n",
      "138/281, train_loss: 0.1273, step time: 0.2512\n",
      "139/281, train_loss: 0.0777, step time: 0.2595\n",
      "140/281, train_loss: 0.1030, step time: 0.2537\n",
      "141/281, train_loss: 0.0805, step time: 0.2463\n",
      "142/281, train_loss: 0.0766, step time: 0.2481\n",
      "143/281, train_loss: 0.0582, step time: 0.2509\n",
      "144/281, train_loss: 0.1070, step time: 0.2497\n",
      "145/281, train_loss: 0.2354, step time: 0.2558\n",
      "146/281, train_loss: 0.0733, step time: 0.2555\n",
      "147/281, train_loss: 0.2405, step time: 0.2535\n",
      "148/281, train_loss: 0.0592, step time: 0.2521\n",
      "149/281, train_loss: 0.1044, step time: 0.2497\n",
      "150/281, train_loss: 0.2289, step time: 0.2519\n",
      "151/281, train_loss: 0.0844, step time: 0.2521\n",
      "152/281, train_loss: 0.0559, step time: 0.2573\n",
      "153/281, train_loss: 0.1211, step time: 0.2559\n",
      "154/281, train_loss: 0.0693, step time: 0.2573\n",
      "155/281, train_loss: 0.0905, step time: 0.2528\n",
      "156/281, train_loss: 0.0674, step time: 0.2521\n",
      "157/281, train_loss: 0.0551, step time: 0.2542\n",
      "158/281, train_loss: 0.0600, step time: 0.2546\n",
      "159/281, train_loss: 0.0734, step time: 0.2604\n",
      "160/281, train_loss: 0.0964, step time: 0.2585\n",
      "161/281, train_loss: 0.0814, step time: 0.2521\n",
      "162/281, train_loss: 0.0687, step time: 0.2505\n",
      "163/281, train_loss: 0.1713, step time: 0.2590\n",
      "164/281, train_loss: 0.1256, step time: 0.2585\n",
      "165/281, train_loss: 0.0980, step time: 0.2580\n",
      "166/281, train_loss: 0.0428, step time: 0.2623\n",
      "167/281, train_loss: 0.1148, step time: 0.2569\n",
      "168/281, train_loss: 0.0835, step time: 0.2598\n",
      "169/281, train_loss: 0.0668, step time: 0.2576\n",
      "170/281, train_loss: 0.1174, step time: 0.2585\n",
      "171/281, train_loss: 0.0862, step time: 0.2552\n",
      "172/281, train_loss: 0.1169, step time: 0.2524\n",
      "173/281, train_loss: 0.1207, step time: 0.2588\n",
      "174/281, train_loss: 0.1072, step time: 0.2585\n",
      "175/281, train_loss: 0.0665, step time: 0.2569\n",
      "176/281, train_loss: 0.1022, step time: 0.2538\n",
      "177/281, train_loss: 0.0685, step time: 0.2515\n",
      "178/281, train_loss: 0.0912, step time: 0.2568\n",
      "179/281, train_loss: 0.1061, step time: 0.2560\n",
      "180/281, train_loss: 0.0932, step time: 0.2567\n",
      "181/281, train_loss: 0.0631, step time: 0.2622\n",
      "182/281, train_loss: 0.0857, step time: 0.2569\n",
      "183/281, train_loss: 0.2331, step time: 0.2565\n",
      "184/281, train_loss: 0.0839, step time: 0.2525\n",
      "185/281, train_loss: 0.0891, step time: 0.2532\n",
      "186/281, train_loss: 0.2930, step time: 0.2631\n",
      "187/281, train_loss: 0.0769, step time: 0.2585\n",
      "188/281, train_loss: 0.2424, step time: 0.2590\n",
      "189/281, train_loss: 0.2471, step time: 0.2647\n",
      "190/281, train_loss: 0.2784, step time: 0.2588\n",
      "191/281, train_loss: 0.0769, step time: 0.2551\n",
      "192/281, train_loss: 0.0674, step time: 0.2575\n",
      "193/281, train_loss: 0.2663, step time: 0.2528\n",
      "194/281, train_loss: 0.0922, step time: 0.2545\n",
      "195/281, train_loss: 0.0604, step time: 0.2583\n",
      "196/281, train_loss: 0.2635, step time: 0.2570\n",
      "197/281, train_loss: 0.2360, step time: 0.2618\n",
      "198/281, train_loss: 0.1217, step time: 0.2609\n",
      "199/281, train_loss: 0.2553, step time: 0.2588\n",
      "200/281, train_loss: 0.1070, step time: 0.2555\n",
      "201/281, train_loss: 0.2228, step time: 0.2599\n",
      "202/281, train_loss: 0.1002, step time: 0.2603\n",
      "203/281, train_loss: 0.0886, step time: 0.2550\n",
      "204/281, train_loss: 0.0696, step time: 0.2519\n",
      "205/281, train_loss: 0.2437, step time: 0.2536\n",
      "206/281, train_loss: 0.2184, step time: 0.2547\n",
      "207/281, train_loss: 0.1038, step time: 0.2549\n",
      "208/281, train_loss: 0.1280, step time: 0.2525\n",
      "209/281, train_loss: 0.0915, step time: 0.2512\n",
      "210/281, train_loss: 0.2346, step time: 0.2538\n",
      "211/281, train_loss: 0.0698, step time: 0.2552\n",
      "212/281, train_loss: 0.0668, step time: 0.2527\n",
      "213/281, train_loss: 0.0456, step time: 0.2624\n",
      "214/281, train_loss: 0.0728, step time: 0.2610\n",
      "215/281, train_loss: 0.1346, step time: 0.2536\n",
      "216/281, train_loss: 0.0532, step time: 0.2542\n",
      "217/281, train_loss: 0.1534, step time: 0.2547\n",
      "218/281, train_loss: 0.1298, step time: 0.2575\n",
      "219/281, train_loss: 0.1010, step time: 0.2557\n",
      "220/281, train_loss: 0.1355, step time: 0.2557\n",
      "221/281, train_loss: 0.0602, step time: 0.2599\n",
      "222/281, train_loss: 0.1144, step time: 0.2606\n",
      "223/281, train_loss: 0.0968, step time: 0.2618\n",
      "224/281, train_loss: 0.0831, step time: 0.2599\n",
      "225/281, train_loss: 0.0945, step time: 0.2565\n",
      "226/281, train_loss: 0.1232, step time: 0.2589\n",
      "227/281, train_loss: 0.0651, step time: 0.2651\n",
      "228/281, train_loss: 0.0997, step time: 0.2520\n",
      "229/281, train_loss: 0.0584, step time: 0.2544\n",
      "230/281, train_loss: 0.1490, step time: 0.2580\n",
      "231/281, train_loss: 0.0827, step time: 0.2565\n",
      "232/281, train_loss: 0.0662, step time: 0.2549\n",
      "233/281, train_loss: 0.0899, step time: 0.2552\n",
      "234/281, train_loss: 0.0982, step time: 0.2528\n",
      "235/281, train_loss: 0.0621, step time: 0.2523\n",
      "236/281, train_loss: 0.0904, step time: 0.2507\n",
      "237/281, train_loss: 0.3342, step time: 0.2589\n",
      "238/281, train_loss: 0.0829, step time: 0.2574\n",
      "239/281, train_loss: 0.0943, step time: 0.2580\n",
      "240/281, train_loss: 0.1134, step time: 0.2543\n",
      "241/281, train_loss: 0.0791, step time: 0.2563\n",
      "242/281, train_loss: 0.1001, step time: 0.2585\n",
      "243/281, train_loss: 0.2255, step time: 0.2595\n",
      "244/281, train_loss: 0.1063, step time: 0.2528\n",
      "245/281, train_loss: 0.2477, step time: 0.2508\n",
      "246/281, train_loss: 0.1252, step time: 0.2541\n",
      "247/281, train_loss: 0.2836, step time: 0.2559\n",
      "248/281, train_loss: 0.1153, step time: 0.2565\n",
      "249/281, train_loss: 0.0986, step time: 0.2538\n",
      "250/281, train_loss: 0.0635, step time: 0.2543\n",
      "251/281, train_loss: 0.1059, step time: 0.2569\n",
      "252/281, train_loss: 0.0765, step time: 0.2555\n",
      "253/281, train_loss: 0.1200, step time: 0.2563\n",
      "254/281, train_loss: 0.0576, step time: 0.2500\n",
      "255/281, train_loss: 0.2609, step time: 0.2547\n",
      "256/281, train_loss: 0.0723, step time: 0.2482\n",
      "257/281, train_loss: 0.0837, step time: 0.2459\n",
      "258/281, train_loss: 0.0995, step time: 0.2536\n",
      "259/281, train_loss: 0.0818, step time: 0.2573\n",
      "260/281, train_loss: 0.2408, step time: 0.2514\n",
      "261/281, train_loss: 0.0704, step time: 0.2555\n",
      "262/281, train_loss: 0.1109, step time: 0.2521\n",
      "263/281, train_loss: 0.0957, step time: 0.2513\n",
      "264/281, train_loss: 0.0990, step time: 0.2553\n",
      "265/281, train_loss: 0.0734, step time: 0.2525\n",
      "266/281, train_loss: 0.0868, step time: 0.2571\n",
      "267/281, train_loss: 0.2499, step time: 0.2573\n",
      "268/281, train_loss: 0.0689, step time: 0.2525\n",
      "269/281, train_loss: 0.0951, step time: 0.2520\n",
      "270/281, train_loss: 0.0756, step time: 0.2524\n",
      "271/281, train_loss: 0.0955, step time: 0.2512\n",
      "272/281, train_loss: 0.0781, step time: 0.2514\n",
      "273/281, train_loss: 0.0771, step time: 0.2572\n",
      "274/281, train_loss: 0.0496, step time: 0.2572\n",
      "275/281, train_loss: 0.0645, step time: 0.2521\n",
      "276/281, train_loss: 0.1044, step time: 0.2510\n",
      "277/281, train_loss: 0.2758, step time: 0.2493\n",
      "278/281, train_loss: 0.0973, step time: 0.2510\n",
      "279/281, train_loss: 0.2382, step time: 0.2499\n",
      "280/281, train_loss: 0.2459, step time: 0.2473\n",
      "281/281, train_loss: 0.0609, step time: 0.2502\n",
      "282/281, train_loss: 0.1446, step time: 0.1485\n",
      "epoch 57 average loss: 0.1169\n",
      "saved new best metric model\n",
      "current epoch: 57 current mean dice: 0.8804 tc: 0.8751 wt: 0.9139 et: 0.8607\n",
      "best mean dice: 0.8804 at epoch: 57\n",
      "time consuming of epoch 57 is: 395.9187\n",
      "----------\n",
      "epoch 58/200\n",
      "1/281, train_loss: 0.0974, step time: 0.2638\n",
      "2/281, train_loss: 0.1035, step time: 0.2521\n",
      "3/281, train_loss: 0.0875, step time: 0.2535\n",
      "4/281, train_loss: 0.0859, step time: 0.2560\n",
      "5/281, train_loss: 0.0742, step time: 0.2577\n",
      "6/281, train_loss: 0.2467, step time: 0.2583\n",
      "7/281, train_loss: 0.1001, step time: 0.2491\n",
      "8/281, train_loss: 0.1273, step time: 0.2467\n",
      "9/281, train_loss: 0.0885, step time: 0.2514\n",
      "10/281, train_loss: 0.0609, step time: 0.2480\n",
      "11/281, train_loss: 0.0978, step time: 0.2496\n",
      "12/281, train_loss: 0.0913, step time: 0.2541\n",
      "13/281, train_loss: 0.0793, step time: 0.2583\n",
      "14/281, train_loss: 0.1123, step time: 0.2515\n",
      "15/281, train_loss: 0.0506, step time: 0.2511\n",
      "16/281, train_loss: 0.1229, step time: 0.2568\n",
      "17/281, train_loss: 0.2681, step time: 0.2584\n",
      "18/281, train_loss: 0.0834, step time: 0.2567\n",
      "19/281, train_loss: 0.0564, step time: 0.2526\n",
      "20/281, train_loss: 0.2242, step time: 0.2478\n",
      "21/281, train_loss: 0.1407, step time: 0.2523\n",
      "22/281, train_loss: 0.1681, step time: 0.2430\n",
      "23/281, train_loss: 0.2252, step time: 0.2523\n",
      "24/281, train_loss: 0.0905, step time: 0.2597\n",
      "25/281, train_loss: 0.0836, step time: 0.2494\n",
      "26/281, train_loss: 0.0746, step time: 0.2512\n",
      "27/281, train_loss: 0.1075, step time: 0.2502\n",
      "28/281, train_loss: 0.0930, step time: 0.2540\n",
      "29/281, train_loss: 0.0976, step time: 0.2477\n",
      "30/281, train_loss: 0.0917, step time: 0.2500\n",
      "31/281, train_loss: 0.0887, step time: 0.2565\n",
      "32/281, train_loss: 0.0687, step time: 0.2520\n",
      "33/281, train_loss: 0.0670, step time: 0.2543\n",
      "34/281, train_loss: 0.0653, step time: 0.2621\n",
      "35/281, train_loss: 0.0815, step time: 0.2468\n",
      "36/281, train_loss: 0.1017, step time: 0.2540\n",
      "37/281, train_loss: 0.0504, step time: 0.2510\n",
      "38/281, train_loss: 0.1227, step time: 0.2495\n",
      "39/281, train_loss: 0.0916, step time: 0.2503\n",
      "40/281, train_loss: 0.0972, step time: 0.2537\n",
      "41/281, train_loss: 0.1078, step time: 0.2551\n",
      "42/281, train_loss: 0.2581, step time: 0.2515\n",
      "43/281, train_loss: 0.0502, step time: 0.2492\n",
      "44/281, train_loss: 0.0960, step time: 0.2526\n",
      "45/281, train_loss: 0.0895, step time: 0.2491\n",
      "46/281, train_loss: 0.0975, step time: 0.2485\n",
      "47/281, train_loss: 0.0666, step time: 0.2470\n",
      "48/281, train_loss: 0.0703, step time: 0.2509\n",
      "49/281, train_loss: 0.0517, step time: 0.2518\n",
      "50/281, train_loss: 0.1239, step time: 0.2548\n",
      "51/281, train_loss: 0.0913, step time: 0.2490\n",
      "52/281, train_loss: 0.0947, step time: 0.2432\n",
      "53/281, train_loss: 0.0686, step time: 0.2439\n",
      "54/281, train_loss: 0.1206, step time: 0.2499\n",
      "55/281, train_loss: 0.0886, step time: 0.2518\n",
      "56/281, train_loss: 0.0973, step time: 0.2474\n",
      "57/281, train_loss: 0.0703, step time: 0.2518\n",
      "58/281, train_loss: 0.1081, step time: 0.2575\n",
      "59/281, train_loss: 0.0964, step time: 0.2677\n",
      "60/281, train_loss: 0.0757, step time: 0.2616\n",
      "61/281, train_loss: 0.0727, step time: 0.2596\n",
      "62/281, train_loss: 0.0650, step time: 0.2546\n",
      "63/281, train_loss: 0.1076, step time: 0.2546\n",
      "64/281, train_loss: 0.0722, step time: 0.2493\n",
      "65/281, train_loss: 0.2665, step time: 0.2521\n",
      "66/281, train_loss: 0.0597, step time: 0.2536\n",
      "67/281, train_loss: 0.0469, step time: 0.2461\n",
      "68/281, train_loss: 0.0773, step time: 0.2483\n",
      "69/281, train_loss: 0.1081, step time: 0.2591\n",
      "70/281, train_loss: 0.1169, step time: 0.2557\n",
      "71/281, train_loss: 0.0823, step time: 0.2564\n",
      "72/281, train_loss: 0.1376, step time: 0.2517\n",
      "73/281, train_loss: 0.0947, step time: 0.2590\n",
      "74/281, train_loss: 0.2589, step time: 0.2516\n",
      "75/281, train_loss: 0.0990, step time: 0.2559\n",
      "76/281, train_loss: 0.1134, step time: 0.2602\n",
      "77/281, train_loss: 0.0746, step time: 0.2546\n",
      "78/281, train_loss: 0.0777, step time: 0.2574\n",
      "79/281, train_loss: 0.1143, step time: 0.2530\n",
      "80/281, train_loss: 0.2617, step time: 0.2510\n",
      "81/281, train_loss: 0.1020, step time: 0.2503\n",
      "82/281, train_loss: 0.0735, step time: 0.2533\n",
      "83/281, train_loss: 0.0716, step time: 0.2544\n",
      "84/281, train_loss: 0.1086, step time: 0.2546\n",
      "85/281, train_loss: 0.0957, step time: 0.2543\n",
      "86/281, train_loss: 0.0637, step time: 0.2523\n",
      "87/281, train_loss: 0.0661, step time: 0.2527\n",
      "88/281, train_loss: 0.0798, step time: 0.2551\n",
      "89/281, train_loss: 0.1200, step time: 0.2486\n",
      "90/281, train_loss: 0.0852, step time: 0.2580\n",
      "91/281, train_loss: 0.0745, step time: 0.2654\n",
      "92/281, train_loss: 0.1286, step time: 0.2693\n",
      "93/281, train_loss: 0.1042, step time: 0.2539\n",
      "94/281, train_loss: 0.2260, step time: 0.2535\n",
      "95/281, train_loss: 0.0901, step time: 0.2578\n",
      "96/281, train_loss: 0.1155, step time: 0.2492\n",
      "97/281, train_loss: 0.0775, step time: 0.2524\n",
      "98/281, train_loss: 0.2539, step time: 0.2471\n",
      "99/281, train_loss: 0.0558, step time: 0.2526\n",
      "100/281, train_loss: 0.0882, step time: 0.2498\n",
      "101/281, train_loss: 0.1002, step time: 0.2555\n",
      "102/281, train_loss: 0.1402, step time: 0.2517\n",
      "103/281, train_loss: 0.0694, step time: 0.2504\n",
      "104/281, train_loss: 0.2248, step time: 0.2548\n",
      "105/281, train_loss: 0.0673, step time: 0.2583\n",
      "106/281, train_loss: 0.0995, step time: 0.2534\n",
      "107/281, train_loss: 0.2850, step time: 0.2547\n",
      "108/281, train_loss: 0.0879, step time: 0.2561\n",
      "109/281, train_loss: 0.3029, step time: 0.2536\n",
      "110/281, train_loss: 0.1311, step time: 0.2509\n",
      "111/281, train_loss: 0.1838, step time: 0.2549\n",
      "112/281, train_loss: 0.1048, step time: 0.2541\n",
      "113/281, train_loss: 0.0879, step time: 0.2559\n",
      "114/281, train_loss: 0.1505, step time: 0.2476\n",
      "115/281, train_loss: 0.0746, step time: 0.2512\n",
      "116/281, train_loss: 0.0841, step time: 0.2522\n",
      "117/281, train_loss: 0.0937, step time: 0.2530\n",
      "118/281, train_loss: 0.1095, step time: 0.2597\n",
      "119/281, train_loss: 0.0794, step time: 0.2549\n",
      "120/281, train_loss: 0.0620, step time: 0.2622\n",
      "121/281, train_loss: 0.1140, step time: 0.2707\n",
      "122/281, train_loss: 0.0927, step time: 0.2583\n",
      "123/281, train_loss: 0.1063, step time: 0.2544\n",
      "124/281, train_loss: 0.0723, step time: 0.2499\n",
      "125/281, train_loss: 0.2544, step time: 0.2489\n",
      "126/281, train_loss: 0.0800, step time: 0.2530\n",
      "127/281, train_loss: 0.1125, step time: 0.2633\n",
      "128/281, train_loss: 0.0919, step time: 0.2508\n",
      "129/281, train_loss: 0.0834, step time: 0.2462\n",
      "130/281, train_loss: 0.0671, step time: 0.2492\n",
      "131/281, train_loss: 0.0556, step time: 0.2558\n",
      "132/281, train_loss: 0.1045, step time: 0.2536\n",
      "133/281, train_loss: 0.0590, step time: 0.2511\n",
      "134/281, train_loss: 0.0986, step time: 0.2531\n",
      "135/281, train_loss: 0.1183, step time: 0.2536\n",
      "136/281, train_loss: 0.2641, step time: 0.2474\n",
      "137/281, train_loss: 0.0914, step time: 0.2510\n",
      "138/281, train_loss: 0.0478, step time: 0.2691\n",
      "139/281, train_loss: 0.2557, step time: 0.2455\n",
      "140/281, train_loss: 0.2293, step time: 0.2502\n",
      "141/281, train_loss: 0.2150, step time: 0.2541\n",
      "142/281, train_loss: 0.2371, step time: 0.2536\n",
      "143/281, train_loss: 0.0784, step time: 0.2521\n",
      "144/281, train_loss: 0.0861, step time: 0.2518\n",
      "145/281, train_loss: 0.0664, step time: 0.2513\n",
      "146/281, train_loss: 0.0852, step time: 0.2521\n",
      "147/281, train_loss: 0.0664, step time: 0.2503\n",
      "148/281, train_loss: 0.0890, step time: 0.2495\n",
      "149/281, train_loss: 0.0762, step time: 0.2534\n",
      "150/281, train_loss: 0.0910, step time: 0.2528\n",
      "151/281, train_loss: 0.0976, step time: 0.2512\n",
      "152/281, train_loss: 0.0696, step time: 0.2554\n",
      "153/281, train_loss: 0.2221, step time: 0.2533\n",
      "154/281, train_loss: 0.1000, step time: 0.2492\n",
      "155/281, train_loss: 0.0859, step time: 0.2510\n",
      "156/281, train_loss: 0.2593, step time: 0.2526\n",
      "157/281, train_loss: 0.1213, step time: 0.2488\n",
      "158/281, train_loss: 0.1059, step time: 0.2486\n",
      "159/281, train_loss: 0.0714, step time: 0.2506\n",
      "160/281, train_loss: 0.0814, step time: 0.2492\n",
      "161/281, train_loss: 0.0809, step time: 0.2514\n",
      "162/281, train_loss: 0.1374, step time: 0.2480\n",
      "163/281, train_loss: 0.0614, step time: 0.2484\n",
      "164/281, train_loss: 0.0889, step time: 0.2494\n",
      "165/281, train_loss: 0.1083, step time: 0.2453\n",
      "166/281, train_loss: 0.2588, step time: 0.2557\n",
      "167/281, train_loss: 0.0889, step time: 0.2505\n",
      "168/281, train_loss: 0.0653, step time: 0.2508\n",
      "169/281, train_loss: 0.2318, step time: 0.2435\n",
      "170/281, train_loss: 0.0751, step time: 0.2557\n",
      "171/281, train_loss: 0.1129, step time: 0.2583\n",
      "172/281, train_loss: 0.0553, step time: 0.2538\n",
      "173/281, train_loss: 0.1408, step time: 0.2507\n",
      "174/281, train_loss: 0.0874, step time: 0.2522\n",
      "175/281, train_loss: 0.1066, step time: 0.2533\n",
      "176/281, train_loss: 0.2418, step time: 0.2560\n",
      "177/281, train_loss: 0.0598, step time: 0.2607\n",
      "178/281, train_loss: 0.2380, step time: 0.2545\n",
      "179/281, train_loss: 0.0646, step time: 0.2547\n",
      "180/281, train_loss: 0.0842, step time: 0.2535\n",
      "181/281, train_loss: 0.0572, step time: 0.2487\n",
      "182/281, train_loss: 0.0713, step time: 0.2540\n",
      "183/281, train_loss: 0.0691, step time: 0.2561\n",
      "184/281, train_loss: 0.0866, step time: 0.2513\n",
      "185/281, train_loss: 0.0803, step time: 0.2509\n",
      "186/281, train_loss: 0.3093, step time: 0.2543\n",
      "187/281, train_loss: 0.2284, step time: 0.2581\n",
      "188/281, train_loss: 0.0940, step time: 0.2573\n",
      "189/281, train_loss: 0.1010, step time: 0.2590\n",
      "190/281, train_loss: 0.2540, step time: 0.2607\n",
      "191/281, train_loss: 0.0726, step time: 0.2571\n",
      "192/281, train_loss: 0.0819, step time: 0.2598\n",
      "193/281, train_loss: 0.0855, step time: 0.2551\n",
      "194/281, train_loss: 0.0954, step time: 0.2538\n",
      "195/281, train_loss: 0.0574, step time: 0.2554\n",
      "196/281, train_loss: 0.2753, step time: 0.2569\n",
      "197/281, train_loss: 0.0934, step time: 0.2578\n",
      "198/281, train_loss: 0.1012, step time: 0.2598\n",
      "199/281, train_loss: 0.1005, step time: 0.2573\n",
      "200/281, train_loss: 0.0881, step time: 0.2560\n",
      "201/281, train_loss: 0.2407, step time: 0.2532\n",
      "202/281, train_loss: 0.2572, step time: 0.2580\n",
      "203/281, train_loss: 0.0914, step time: 0.2588\n",
      "204/281, train_loss: 0.0908, step time: 0.2554\n",
      "205/281, train_loss: 0.2996, step time: 0.2556\n",
      "206/281, train_loss: 0.1329, step time: 0.2550\n",
      "207/281, train_loss: 0.0758, step time: 0.2596\n",
      "208/281, train_loss: 0.0949, step time: 0.2566\n",
      "209/281, train_loss: 0.0601, step time: 0.2566\n",
      "210/281, train_loss: 0.0995, step time: 0.2528\n",
      "211/281, train_loss: 0.0775, step time: 0.2559\n",
      "212/281, train_loss: 0.0702, step time: 0.2568\n",
      "213/281, train_loss: 0.0743, step time: 0.2569\n",
      "214/281, train_loss: 0.0881, step time: 0.2605\n",
      "215/281, train_loss: 0.0652, step time: 0.2587\n",
      "216/281, train_loss: 0.1166, step time: 0.2549\n",
      "217/281, train_loss: 0.0659, step time: 0.2529\n",
      "218/281, train_loss: 0.1093, step time: 0.2568\n",
      "219/281, train_loss: 0.0936, step time: 0.2611\n",
      "220/281, train_loss: 0.2775, step time: 0.2576\n",
      "221/281, train_loss: 0.0928, step time: 0.2525\n",
      "222/281, train_loss: 0.2129, step time: 0.2663\n",
      "223/281, train_loss: 0.0960, step time: 0.2562\n",
      "224/281, train_loss: 0.1632, step time: 0.2563\n",
      "225/281, train_loss: 0.0924, step time: 0.2546\n",
      "226/281, train_loss: 0.0908, step time: 0.2572\n",
      "227/281, train_loss: 0.0980, step time: 0.2565\n",
      "228/281, train_loss: 0.0634, step time: 0.2565\n",
      "229/281, train_loss: 0.0938, step time: 0.2555\n",
      "230/281, train_loss: 0.2538, step time: 0.2611\n",
      "231/281, train_loss: 0.0965, step time: 0.2559\n",
      "232/281, train_loss: 0.0656, step time: 0.2556\n",
      "233/281, train_loss: 0.2739, step time: 0.2532\n",
      "234/281, train_loss: 0.1057, step time: 0.2578\n",
      "235/281, train_loss: 0.2460, step time: 0.2606\n",
      "236/281, train_loss: 0.1191, step time: 0.2576\n",
      "237/281, train_loss: 0.0948, step time: 0.2554\n",
      "238/281, train_loss: 0.0801, step time: 0.2522\n",
      "239/281, train_loss: 0.2505, step time: 0.2505\n",
      "240/281, train_loss: 0.0782, step time: 0.2521\n",
      "241/281, train_loss: 0.1184, step time: 0.2520\n",
      "242/281, train_loss: 0.1180, step time: 0.2597\n",
      "243/281, train_loss: 0.1258, step time: 0.2596\n",
      "244/281, train_loss: 0.2337, step time: 0.2509\n",
      "245/281, train_loss: 0.2432, step time: 0.2681\n",
      "246/281, train_loss: 0.1034, step time: 0.2557\n",
      "247/281, train_loss: 0.1587, step time: 0.2544\n",
      "248/281, train_loss: 0.2958, step time: 0.2556\n",
      "249/281, train_loss: 0.1235, step time: 0.2579\n",
      "250/281, train_loss: 0.1118, step time: 0.2628\n",
      "251/281, train_loss: 0.2314, step time: 0.2612\n",
      "252/281, train_loss: 0.2253, step time: 0.2602\n",
      "253/281, train_loss: 0.0578, step time: 0.2539\n",
      "254/281, train_loss: 0.1357, step time: 0.2530\n",
      "255/281, train_loss: 0.1442, step time: 0.2531\n",
      "256/281, train_loss: 0.0873, step time: 0.2515\n",
      "257/281, train_loss: 0.1193, step time: 0.2639\n",
      "258/281, train_loss: 0.0670, step time: 0.2535\n",
      "259/281, train_loss: 0.1148, step time: 0.2535\n",
      "260/281, train_loss: 0.0778, step time: 0.2550\n",
      "261/281, train_loss: 0.0865, step time: 0.2636\n",
      "262/281, train_loss: 0.0524, step time: 0.2566\n",
      "263/281, train_loss: 0.1121, step time: 0.2585\n",
      "264/281, train_loss: 0.0725, step time: 0.2611\n",
      "265/281, train_loss: 0.1206, step time: 0.2578\n",
      "266/281, train_loss: 0.0489, step time: 0.2533\n",
      "267/281, train_loss: 0.0537, step time: 0.2526\n",
      "268/281, train_loss: 0.0681, step time: 0.2541\n",
      "269/281, train_loss: 0.0873, step time: 0.2618\n",
      "270/281, train_loss: 0.0795, step time: 0.2560\n",
      "271/281, train_loss: 0.2723, step time: 0.2553\n",
      "272/281, train_loss: 0.0924, step time: 0.2575\n",
      "273/281, train_loss: 0.1004, step time: 0.2552\n",
      "274/281, train_loss: 0.1022, step time: 0.2540\n",
      "275/281, train_loss: 0.1496, step time: 0.2527\n",
      "276/281, train_loss: 0.2437, step time: 0.2566\n",
      "277/281, train_loss: 0.1289, step time: 0.2498\n",
      "278/281, train_loss: 0.0833, step time: 0.2511\n",
      "279/281, train_loss: 0.1342, step time: 0.2485\n",
      "280/281, train_loss: 0.0779, step time: 0.2507\n",
      "281/281, train_loss: 0.1389, step time: 0.2522\n",
      "282/281, train_loss: 0.0880, step time: 0.1536\n",
      "epoch 58 average loss: 0.1172\n",
      "current epoch: 58 current mean dice: 0.8794 tc: 0.8763 wt: 0.9147 et: 0.8574\n",
      "best mean dice: 0.8804 at epoch: 57\n",
      "time consuming of epoch 58 is: 389.3106\n",
      "----------\n",
      "epoch 59/200\n",
      "1/281, train_loss: 0.1343, step time: 0.2694\n",
      "2/281, train_loss: 0.1114, step time: 0.2568\n",
      "3/281, train_loss: 0.0835, step time: 0.2797\n",
      "4/281, train_loss: 0.0884, step time: 0.2673\n",
      "5/281, train_loss: 0.0781, step time: 0.2684\n",
      "6/281, train_loss: 0.0662, step time: 0.2613\n",
      "7/281, train_loss: 0.1057, step time: 0.2570\n",
      "8/281, train_loss: 0.1074, step time: 0.2580\n",
      "9/281, train_loss: 0.2412, step time: 0.2536\n",
      "10/281, train_loss: 0.0509, step time: 0.2553\n",
      "11/281, train_loss: 0.0915, step time: 0.2510\n",
      "12/281, train_loss: 0.2581, step time: 0.2527\n",
      "13/281, train_loss: 0.3132, step time: 0.2528\n",
      "14/281, train_loss: 0.0804, step time: 0.2561\n",
      "15/281, train_loss: 0.0969, step time: 0.2527\n",
      "16/281, train_loss: 0.0667, step time: 0.2496\n",
      "17/281, train_loss: 0.1091, step time: 0.2511\n",
      "18/281, train_loss: 0.2585, step time: 0.2544\n",
      "19/281, train_loss: 0.0968, step time: 0.2524\n",
      "20/281, train_loss: 0.0619, step time: 0.2534\n",
      "21/281, train_loss: 0.1050, step time: 0.2521\n",
      "22/281, train_loss: 0.2063, step time: 0.2603\n",
      "23/281, train_loss: 0.2499, step time: 0.2602\n",
      "24/281, train_loss: 0.0865, step time: 0.2582\n",
      "25/281, train_loss: 0.1010, step time: 0.2528\n",
      "26/281, train_loss: 0.0613, step time: 0.2523\n",
      "27/281, train_loss: 0.1059, step time: 0.2570\n",
      "28/281, train_loss: 0.0868, step time: 0.2594\n",
      "29/281, train_loss: 0.0650, step time: 0.2481\n",
      "30/281, train_loss: 0.1390, step time: 0.2698\n",
      "31/281, train_loss: 0.0820, step time: 0.2666\n",
      "32/281, train_loss: 0.1021, step time: 0.2572\n",
      "33/281, train_loss: 0.1020, step time: 0.2573\n",
      "34/281, train_loss: 0.1006, step time: 0.2534\n",
      "35/281, train_loss: 0.1130, step time: 0.2527\n",
      "36/281, train_loss: 0.0848, step time: 0.2598\n",
      "37/281, train_loss: 0.1142, step time: 0.2524\n",
      "38/281, train_loss: 0.0656, step time: 0.2644\n",
      "39/281, train_loss: 0.0920, step time: 0.2538\n",
      "40/281, train_loss: 0.3042, step time: 0.2549\n",
      "41/281, train_loss: 0.1068, step time: 0.2633\n",
      "42/281, train_loss: 0.0554, step time: 0.2573\n",
      "43/281, train_loss: 0.0829, step time: 0.2579\n",
      "44/281, train_loss: 0.0907, step time: 0.2506\n",
      "45/281, train_loss: 0.0611, step time: 0.2592\n",
      "46/281, train_loss: 0.0537, step time: 0.2589\n",
      "47/281, train_loss: 0.1517, step time: 0.2604\n",
      "48/281, train_loss: 0.0435, step time: 0.2631\n",
      "49/281, train_loss: 0.0883, step time: 0.2573\n",
      "50/281, train_loss: 0.1133, step time: 0.2572\n",
      "51/281, train_loss: 0.0499, step time: 0.2600\n",
      "52/281, train_loss: 0.1364, step time: 0.2567\n",
      "53/281, train_loss: 0.0704, step time: 0.2532\n",
      "54/281, train_loss: 0.0918, step time: 0.2586\n",
      "55/281, train_loss: 0.0671, step time: 0.2612\n",
      "56/281, train_loss: 0.0813, step time: 0.2677\n",
      "57/281, train_loss: 0.2381, step time: 0.2548\n",
      "58/281, train_loss: 0.0914, step time: 0.2558\n",
      "59/281, train_loss: 0.0739, step time: 0.2597\n",
      "60/281, train_loss: 0.1195, step time: 0.2580\n",
      "61/281, train_loss: 0.0761, step time: 0.2491\n",
      "62/281, train_loss: 0.0810, step time: 0.2495\n",
      "63/281, train_loss: 0.2981, step time: 0.2558\n",
      "64/281, train_loss: 0.0600, step time: 0.2511\n",
      "65/281, train_loss: 0.1211, step time: 0.2537\n",
      "66/281, train_loss: 0.2483, step time: 0.2599\n",
      "67/281, train_loss: 0.1174, step time: 0.2557\n",
      "68/281, train_loss: 0.1274, step time: 0.2575\n",
      "69/281, train_loss: 0.0794, step time: 0.2571\n",
      "70/281, train_loss: 0.0989, step time: 0.2578\n",
      "71/281, train_loss: 0.2639, step time: 0.2515\n",
      "72/281, train_loss: 0.0870, step time: 0.2566\n",
      "73/281, train_loss: 0.0712, step time: 0.2620\n",
      "74/281, train_loss: 0.0404, step time: 0.2581\n",
      "75/281, train_loss: 0.0868, step time: 0.2547\n",
      "76/281, train_loss: 0.1144, step time: 0.2542\n",
      "77/281, train_loss: 0.0758, step time: 0.2552\n",
      "78/281, train_loss: 0.0680, step time: 0.2574\n",
      "79/281, train_loss: 0.0825, step time: 0.2562\n",
      "80/281, train_loss: 0.2675, step time: 0.2518\n",
      "81/281, train_loss: 0.1202, step time: 0.2570\n",
      "82/281, train_loss: 0.1470, step time: 0.2523\n",
      "83/281, train_loss: 0.0913, step time: 0.2532\n",
      "84/281, train_loss: 0.0743, step time: 0.2572\n",
      "85/281, train_loss: 0.1108, step time: 0.2563\n",
      "86/281, train_loss: 0.0930, step time: 0.2582\n",
      "87/281, train_loss: 0.0874, step time: 0.2550\n",
      "88/281, train_loss: 0.2283, step time: 0.2531\n",
      "89/281, train_loss: 0.0506, step time: 0.2646\n",
      "90/281, train_loss: 0.1469, step time: 0.2613\n",
      "91/281, train_loss: 0.2523, step time: 0.2544\n",
      "92/281, train_loss: 0.2284, step time: 0.2520\n",
      "93/281, train_loss: 0.0953, step time: 0.2597\n",
      "94/281, train_loss: 0.1126, step time: 0.2617\n",
      "95/281, train_loss: 0.0733, step time: 0.2556\n",
      "96/281, train_loss: 0.1403, step time: 0.2551\n",
      "97/281, train_loss: 0.0886, step time: 0.2478\n",
      "98/281, train_loss: 0.2547, step time: 0.2538\n",
      "99/281, train_loss: 0.0871, step time: 0.2721\n",
      "100/281, train_loss: 0.2558, step time: 0.2532\n",
      "101/281, train_loss: 0.0882, step time: 0.2522\n",
      "102/281, train_loss: 0.0706, step time: 0.2565\n",
      "103/281, train_loss: 0.0983, step time: 0.2524\n",
      "104/281, train_loss: 0.0685, step time: 0.2520\n",
      "105/281, train_loss: 0.0381, step time: 0.2501\n",
      "106/281, train_loss: 0.0971, step time: 0.2545\n",
      "107/281, train_loss: 0.0898, step time: 0.2522\n",
      "108/281, train_loss: 0.0459, step time: 0.2507\n",
      "109/281, train_loss: 0.1365, step time: 0.2569\n",
      "110/281, train_loss: 0.0926, step time: 0.2578\n",
      "111/281, train_loss: 0.0852, step time: 0.2546\n",
      "112/281, train_loss: 0.0763, step time: 0.2536\n",
      "113/281, train_loss: 0.0485, step time: 0.2498\n",
      "114/281, train_loss: 0.0499, step time: 0.2517\n",
      "115/281, train_loss: 0.0862, step time: 0.2475\n",
      "116/281, train_loss: 0.0698, step time: 0.2647\n",
      "117/281, train_loss: 0.0816, step time: 0.2511\n",
      "118/281, train_loss: 0.0547, step time: 0.2520\n",
      "119/281, train_loss: 0.1034, step time: 0.2511\n",
      "120/281, train_loss: 0.0909, step time: 0.2529\n",
      "121/281, train_loss: 0.0866, step time: 0.2516\n",
      "122/281, train_loss: 0.1105, step time: 0.2535\n",
      "123/281, train_loss: 0.2746, step time: 0.2579\n",
      "124/281, train_loss: 0.0609, step time: 0.2483\n",
      "125/281, train_loss: 0.2477, step time: 0.2511\n",
      "126/281, train_loss: 0.0842, step time: 0.2482\n",
      "127/281, train_loss: 0.1111, step time: 0.2503\n",
      "128/281, train_loss: 0.0776, step time: 0.2526\n",
      "129/281, train_loss: 0.1182, step time: 0.2517\n",
      "130/281, train_loss: 0.0999, step time: 0.2562\n",
      "131/281, train_loss: 0.0688, step time: 0.2648\n",
      "132/281, train_loss: 0.0546, step time: 0.2599\n",
      "133/281, train_loss: 0.0919, step time: 0.2667\n",
      "134/281, train_loss: 0.2610, step time: 0.2634\n",
      "135/281, train_loss: 0.0759, step time: 0.2796\n",
      "136/281, train_loss: 0.3162, step time: 0.2589\n",
      "137/281, train_loss: 0.2617, step time: 0.2534\n",
      "138/281, train_loss: 0.1175, step time: 0.2568\n",
      "139/281, train_loss: 0.1650, step time: 0.2565\n",
      "140/281, train_loss: 0.0754, step time: 0.2550\n",
      "141/281, train_loss: 0.1083, step time: 0.2533\n",
      "142/281, train_loss: 0.1063, step time: 0.2545\n",
      "143/281, train_loss: 0.1452, step time: 0.2526\n",
      "144/281, train_loss: 0.0923, step time: 0.2580\n",
      "145/281, train_loss: 0.0767, step time: 0.2555\n",
      "146/281, train_loss: 0.2663, step time: 0.2556\n",
      "147/281, train_loss: 0.1172, step time: 0.2514\n",
      "148/281, train_loss: 0.1194, step time: 0.2500\n",
      "149/281, train_loss: 0.0734, step time: 0.2526\n",
      "150/281, train_loss: 0.1922, step time: 0.2528\n",
      "151/281, train_loss: 0.0791, step time: 0.2508\n",
      "152/281, train_loss: 0.1430, step time: 0.2498\n",
      "153/281, train_loss: 0.0778, step time: 0.2507\n",
      "154/281, train_loss: 0.1130, step time: 0.2570\n",
      "155/281, train_loss: 0.0971, step time: 0.2565\n",
      "156/281, train_loss: 0.0944, step time: 0.2520\n",
      "157/281, train_loss: 0.0764, step time: 0.2489\n",
      "158/281, train_loss: 0.2376, step time: 0.2535\n",
      "159/281, train_loss: 0.2497, step time: 0.2509\n",
      "160/281, train_loss: 0.0933, step time: 0.2525\n",
      "161/281, train_loss: 0.1120, step time: 0.2579\n",
      "162/281, train_loss: 0.0780, step time: 0.2544\n",
      "163/281, train_loss: 0.2405, step time: 0.2525\n",
      "164/281, train_loss: 0.0890, step time: 0.2510\n",
      "165/281, train_loss: 0.2349, step time: 0.2523\n",
      "166/281, train_loss: 0.0837, step time: 0.2522\n",
      "167/281, train_loss: 0.1294, step time: 0.2539\n",
      "168/281, train_loss: 0.0902, step time: 0.2489\n",
      "169/281, train_loss: 0.0718, step time: 0.2623\n",
      "170/281, train_loss: 0.0961, step time: 0.2577\n",
      "171/281, train_loss: 0.1285, step time: 0.2543\n",
      "172/281, train_loss: 0.0931, step time: 0.2558\n",
      "173/281, train_loss: 0.0486, step time: 0.2543\n",
      "174/281, train_loss: 0.0904, step time: 0.2539\n",
      "175/281, train_loss: 0.0784, step time: 0.2542\n",
      "176/281, train_loss: 0.1323, step time: 0.2545\n",
      "177/281, train_loss: 0.0566, step time: 0.2553\n",
      "178/281, train_loss: 0.1583, step time: 0.2559\n",
      "179/281, train_loss: 0.0848, step time: 0.2533\n",
      "180/281, train_loss: 0.1182, step time: 0.2552\n",
      "181/281, train_loss: 0.0780, step time: 0.2534\n",
      "182/281, train_loss: 0.1058, step time: 0.2519\n",
      "183/281, train_loss: 0.0554, step time: 0.2543\n",
      "184/281, train_loss: 0.1629, step time: 0.2626\n",
      "185/281, train_loss: 0.2487, step time: 0.2522\n",
      "186/281, train_loss: 0.0714, step time: 0.2535\n",
      "187/281, train_loss: 0.0697, step time: 0.2533\n",
      "188/281, train_loss: 0.0646, step time: 0.2504\n",
      "189/281, train_loss: 0.0615, step time: 0.2572\n",
      "190/281, train_loss: 0.2456, step time: 0.2606\n",
      "191/281, train_loss: 0.2546, step time: 0.2539\n",
      "192/281, train_loss: 0.0775, step time: 0.2531\n",
      "193/281, train_loss: 0.1008, step time: 0.2531\n",
      "194/281, train_loss: 0.1368, step time: 0.2494\n",
      "195/281, train_loss: 0.0801, step time: 0.2476\n",
      "196/281, train_loss: 0.1003, step time: 0.2450\n",
      "197/281, train_loss: 0.0873, step time: 0.2546\n",
      "198/281, train_loss: 0.0939, step time: 0.2529\n",
      "199/281, train_loss: 0.0630, step time: 0.2521\n",
      "200/281, train_loss: 0.1284, step time: 0.2568\n",
      "201/281, train_loss: 0.1186, step time: 0.2514\n",
      "202/281, train_loss: 0.2741, step time: 0.2539\n",
      "203/281, train_loss: 0.1091, step time: 0.2469\n",
      "204/281, train_loss: 0.2472, step time: 0.2476\n",
      "205/281, train_loss: 0.0565, step time: 0.2564\n",
      "206/281, train_loss: 0.0747, step time: 0.2527\n",
      "207/281, train_loss: 0.1326, step time: 0.2490\n",
      "208/281, train_loss: 0.0783, step time: 0.2479\n",
      "209/281, train_loss: 0.0907, step time: 0.2525\n",
      "210/281, train_loss: 0.0767, step time: 0.2511\n",
      "211/281, train_loss: 0.0772, step time: 0.2503\n",
      "212/281, train_loss: 0.0846, step time: 0.2502\n",
      "213/281, train_loss: 0.0694, step time: 0.2518\n",
      "214/281, train_loss: 0.2423, step time: 0.2565\n",
      "215/281, train_loss: 0.1247, step time: 0.2512\n",
      "216/281, train_loss: 0.2518, step time: 0.2446\n",
      "217/281, train_loss: 0.0808, step time: 0.2523\n",
      "218/281, train_loss: 0.2434, step time: 0.2505\n",
      "219/281, train_loss: 0.0875, step time: 0.2515\n",
      "220/281, train_loss: 0.0986, step time: 0.2542\n",
      "221/281, train_loss: 0.2253, step time: 0.2533\n",
      "222/281, train_loss: 0.3204, step time: 0.2521\n",
      "223/281, train_loss: 0.2247, step time: 0.2511\n",
      "224/281, train_loss: 0.2347, step time: 0.2499\n",
      "225/281, train_loss: 0.0836, step time: 0.2597\n",
      "226/281, train_loss: 0.0832, step time: 0.2522\n",
      "227/281, train_loss: 0.0953, step time: 0.2560\n",
      "228/281, train_loss: 0.2664, step time: 0.2572\n",
      "229/281, train_loss: 0.0739, step time: 0.2617\n",
      "230/281, train_loss: 0.0854, step time: 0.2549\n",
      "231/281, train_loss: 0.0613, step time: 0.2498\n",
      "232/281, train_loss: 0.0900, step time: 0.2523\n",
      "233/281, train_loss: 0.0947, step time: 0.2562\n",
      "234/281, train_loss: 0.0851, step time: 0.2515\n",
      "235/281, train_loss: 0.1174, step time: 0.2527\n",
      "236/281, train_loss: 0.1465, step time: 0.2485\n",
      "237/281, train_loss: 0.0967, step time: 0.2576\n",
      "238/281, train_loss: 0.1343, step time: 0.2546\n",
      "239/281, train_loss: 0.0824, step time: 0.2508\n",
      "240/281, train_loss: 0.1310, step time: 0.2546\n",
      "241/281, train_loss: 0.0950, step time: 0.2521\n",
      "242/281, train_loss: 0.1027, step time: 0.2555\n",
      "243/281, train_loss: 0.0985, step time: 0.2564\n",
      "244/281, train_loss: 0.2290, step time: 0.2506\n",
      "245/281, train_loss: 0.0621, step time: 0.2493\n",
      "246/281, train_loss: 0.0417, step time: 0.2491\n",
      "247/281, train_loss: 0.0882, step time: 0.2560\n",
      "248/281, train_loss: 0.0663, step time: 0.2510\n",
      "249/281, train_loss: 0.0463, step time: 0.2503\n",
      "250/281, train_loss: 0.1267, step time: 0.2525\n",
      "251/281, train_loss: 0.0745, step time: 0.2511\n",
      "252/281, train_loss: 0.0821, step time: 0.2506\n",
      "253/281, train_loss: 0.0553, step time: 0.2499\n",
      "254/281, train_loss: 0.0876, step time: 0.2503\n",
      "255/281, train_loss: 0.0931, step time: 0.2547\n",
      "256/281, train_loss: 0.2531, step time: 0.2455\n",
      "257/281, train_loss: 0.0910, step time: 0.2561\n",
      "258/281, train_loss: 0.1236, step time: 0.2521\n",
      "259/281, train_loss: 0.2668, step time: 0.2490\n",
      "260/281, train_loss: 0.0679, step time: 0.2485\n",
      "261/281, train_loss: 0.2214, step time: 0.2579\n",
      "262/281, train_loss: 0.1278, step time: 0.2557\n",
      "263/281, train_loss: 0.0780, step time: 0.2496\n",
      "264/281, train_loss: 0.0920, step time: 0.2474\n",
      "265/281, train_loss: 0.0807, step time: 0.2487\n",
      "266/281, train_loss: 0.0511, step time: 0.2509\n",
      "267/281, train_loss: 0.0992, step time: 0.2476\n",
      "268/281, train_loss: 0.0933, step time: 0.2503\n",
      "269/281, train_loss: 0.0763, step time: 0.2544\n",
      "270/281, train_loss: 0.0640, step time: 0.2507\n",
      "271/281, train_loss: 0.1019, step time: 0.2515\n",
      "272/281, train_loss: 0.0724, step time: 0.2528\n",
      "273/281, train_loss: 0.0757, step time: 0.2466\n",
      "274/281, train_loss: 0.1187, step time: 0.2474\n",
      "275/281, train_loss: 0.0948, step time: 0.2507\n",
      "276/281, train_loss: 0.0610, step time: 0.2507\n",
      "277/281, train_loss: 0.1026, step time: 0.2512\n",
      "278/281, train_loss: 0.1072, step time: 0.2515\n",
      "279/281, train_loss: 0.1140, step time: 0.2532\n",
      "280/281, train_loss: 0.0849, step time: 0.2536\n",
      "281/281, train_loss: 0.2699, step time: 0.2488\n",
      "282/281, train_loss: 0.0865, step time: 0.1499\n",
      "epoch 59 average loss: 0.1174\n",
      "current epoch: 59 current mean dice: 0.8793 tc: 0.8707 wt: 0.9141 et: 0.8624\n",
      "best mean dice: 0.8804 at epoch: 57\n",
      "time consuming of epoch 59 is: 365.8670\n",
      "----------\n",
      "epoch 60/200\n",
      "1/281, train_loss: 0.0656, step time: 0.2565\n",
      "2/281, train_loss: 0.0656, step time: 0.2786\n",
      "3/281, train_loss: 0.2493, step time: 0.2578\n",
      "4/281, train_loss: 0.0483, step time: 0.2478\n",
      "5/281, train_loss: 0.0785, step time: 0.2474\n",
      "6/281, train_loss: 0.0827, step time: 0.2559\n",
      "7/281, train_loss: 0.1269, step time: 0.2609\n",
      "8/281, train_loss: 0.0778, step time: 0.2547\n",
      "9/281, train_loss: 0.1090, step time: 0.2508\n",
      "10/281, train_loss: 0.2399, step time: 0.2428\n",
      "11/281, train_loss: 0.2320, step time: 0.2449\n",
      "12/281, train_loss: 0.0738, step time: 0.2454\n",
      "13/281, train_loss: 0.1374, step time: 0.2507\n",
      "14/281, train_loss: 0.1213, step time: 0.2514\n",
      "15/281, train_loss: 0.0791, step time: 0.2502\n",
      "16/281, train_loss: 0.0668, step time: 0.2464\n",
      "17/281, train_loss: 0.1517, step time: 0.2433\n",
      "18/281, train_loss: 0.0577, step time: 0.2537\n",
      "19/281, train_loss: 0.1028, step time: 0.2549\n",
      "20/281, train_loss: 0.0724, step time: 0.2495\n",
      "21/281, train_loss: 0.0724, step time: 0.2525\n",
      "22/281, train_loss: 0.1167, step time: 0.2507\n",
      "23/281, train_loss: 0.0818, step time: 0.2580\n",
      "24/281, train_loss: 0.0603, step time: 0.2561\n",
      "25/281, train_loss: 0.0858, step time: 0.2533\n",
      "26/281, train_loss: 0.0857, step time: 0.2523\n",
      "27/281, train_loss: 0.1112, step time: 0.2505\n",
      "28/281, train_loss: 0.0955, step time: 0.2541\n",
      "29/281, train_loss: 0.1418, step time: 0.2475\n",
      "30/281, train_loss: 0.0885, step time: 0.2515\n",
      "31/281, train_loss: 0.0816, step time: 0.2459\n",
      "32/281, train_loss: 0.2375, step time: 0.2426\n",
      "33/281, train_loss: 0.0784, step time: 0.2432\n",
      "34/281, train_loss: 0.1044, step time: 0.2509\n",
      "35/281, train_loss: 0.0993, step time: 0.2510\n",
      "36/281, train_loss: 0.0462, step time: 0.2491\n",
      "37/281, train_loss: 0.0585, step time: 0.2489\n",
      "38/281, train_loss: 0.0501, step time: 0.2551\n",
      "39/281, train_loss: 0.2569, step time: 0.2577\n",
      "40/281, train_loss: 0.1003, step time: 0.2554\n",
      "41/281, train_loss: 0.1225, step time: 0.2514\n",
      "42/281, train_loss: 0.0496, step time: 0.2544\n",
      "43/281, train_loss: 0.1004, step time: 0.2541\n",
      "44/281, train_loss: 0.0671, step time: 0.2561\n",
      "45/281, train_loss: 0.0775, step time: 0.2477\n",
      "46/281, train_loss: 0.1197, step time: 0.2574\n",
      "47/281, train_loss: 0.0587, step time: 0.2467\n",
      "48/281, train_loss: 0.2702, step time: 0.2468\n",
      "49/281, train_loss: 0.2469, step time: 0.2519\n",
      "50/281, train_loss: 0.0806, step time: 0.2500\n",
      "51/281, train_loss: 0.0604, step time: 0.2520\n",
      "52/281, train_loss: 0.1025, step time: 0.2505\n",
      "53/281, train_loss: 0.0827, step time: 0.2535\n",
      "54/281, train_loss: 0.0901, step time: 0.2467\n",
      "55/281, train_loss: 0.2150, step time: 0.2494\n",
      "56/281, train_loss: 0.0892, step time: 0.2544\n",
      "57/281, train_loss: 0.0909, step time: 0.2564\n",
      "58/281, train_loss: 0.0647, step time: 0.2719\n",
      "59/281, train_loss: 0.1217, step time: 0.2568\n",
      "60/281, train_loss: 0.1368, step time: 0.2549\n",
      "61/281, train_loss: 0.0813, step time: 0.2469\n",
      "62/281, train_loss: 0.0861, step time: 0.2539\n",
      "63/281, train_loss: 0.0835, step time: 0.2541\n",
      "64/281, train_loss: 0.1053, step time: 0.2592\n",
      "65/281, train_loss: 0.0900, step time: 0.2549\n",
      "66/281, train_loss: 0.0635, step time: 0.2533\n",
      "67/281, train_loss: 0.0658, step time: 0.2477\n",
      "68/281, train_loss: 0.0997, step time: 0.2544\n",
      "69/281, train_loss: 0.1139, step time: 0.2512\n",
      "70/281, train_loss: 0.0736, step time: 0.2522\n",
      "71/281, train_loss: 0.1058, step time: 0.2490\n",
      "72/281, train_loss: 0.0750, step time: 0.2511\n",
      "73/281, train_loss: 0.0940, step time: 0.2508\n",
      "74/281, train_loss: 0.0603, step time: 0.2573\n",
      "75/281, train_loss: 0.1066, step time: 0.2525\n",
      "76/281, train_loss: 0.1780, step time: 0.2541\n",
      "77/281, train_loss: 0.0665, step time: 0.2509\n",
      "78/281, train_loss: 0.1376, step time: 0.2506\n",
      "79/281, train_loss: 0.0903, step time: 0.2542\n",
      "80/281, train_loss: 0.1066, step time: 0.2549\n",
      "81/281, train_loss: 0.0645, step time: 0.2577\n",
      "82/281, train_loss: 0.1073, step time: 0.2507\n",
      "83/281, train_loss: 0.0694, step time: 0.2531\n",
      "84/281, train_loss: 0.0730, step time: 0.2525\n",
      "85/281, train_loss: 0.1125, step time: 0.2541\n",
      "86/281, train_loss: 0.0892, step time: 0.2461\n",
      "87/281, train_loss: 0.1483, step time: 0.2470\n",
      "88/281, train_loss: 0.0682, step time: 0.2594\n",
      "89/281, train_loss: 0.3599, step time: 0.2557\n",
      "90/281, train_loss: 0.0566, step time: 0.2496\n",
      "91/281, train_loss: 0.0575, step time: 0.2493\n",
      "92/281, train_loss: 0.1044, step time: 0.2527\n",
      "93/281, train_loss: 0.1067, step time: 0.2521\n",
      "94/281, train_loss: 0.0892, step time: 0.2541\n",
      "95/281, train_loss: 0.1174, step time: 0.2567\n",
      "96/281, train_loss: 0.2162, step time: 0.2589\n",
      "97/281, train_loss: 0.0654, step time: 0.2516\n",
      "98/281, train_loss: 0.0967, step time: 0.2553\n",
      "99/281, train_loss: 0.1209, step time: 0.2539\n",
      "100/281, train_loss: 0.2903, step time: 0.2478\n",
      "101/281, train_loss: 0.1186, step time: 0.2511\n",
      "102/281, train_loss: 0.0731, step time: 0.2519\n",
      "103/281, train_loss: 0.1019, step time: 0.2480\n",
      "104/281, train_loss: 0.4276, step time: 0.2488\n",
      "105/281, train_loss: 0.0844, step time: 0.2450\n",
      "106/281, train_loss: 0.0819, step time: 0.2500\n",
      "107/281, train_loss: 0.0969, step time: 0.2520\n",
      "108/281, train_loss: 0.0826, step time: 0.2523\n",
      "109/281, train_loss: 0.0649, step time: 0.2595\n",
      "110/281, train_loss: 0.1023, step time: 0.2612\n",
      "111/281, train_loss: 0.0955, step time: 0.2585\n",
      "112/281, train_loss: 0.2298, step time: 0.2563\n",
      "113/281, train_loss: 0.2497, step time: 0.2514\n",
      "114/281, train_loss: 0.0869, step time: 0.2519\n",
      "115/281, train_loss: 0.1021, step time: 0.2521\n",
      "116/281, train_loss: 0.0854, step time: 0.2473\n",
      "117/281, train_loss: 0.0922, step time: 0.2470\n",
      "118/281, train_loss: 0.0920, step time: 0.2485\n",
      "119/281, train_loss: 0.0890, step time: 0.2480\n",
      "120/281, train_loss: 0.0609, step time: 0.2469\n",
      "121/281, train_loss: 0.0512, step time: 0.2465\n",
      "122/281, train_loss: 0.0953, step time: 0.2522\n",
      "123/281, train_loss: 0.0733, step time: 0.2528\n",
      "124/281, train_loss: 0.1125, step time: 0.2528\n",
      "125/281, train_loss: 0.0603, step time: 0.2539\n",
      "126/281, train_loss: 0.2067, step time: 0.2493\n",
      "127/281, train_loss: 0.0900, step time: 0.2556\n",
      "128/281, train_loss: 0.0623, step time: 0.2585\n",
      "129/281, train_loss: 0.0713, step time: 0.2604\n",
      "130/281, train_loss: 0.0958, step time: 0.2574\n",
      "131/281, train_loss: 0.0572, step time: 0.2547\n",
      "132/281, train_loss: 0.1249, step time: 0.2548\n",
      "133/281, train_loss: 0.0735, step time: 0.2600\n",
      "134/281, train_loss: 0.0477, step time: 0.2602\n",
      "135/281, train_loss: 0.1998, step time: 0.2562\n",
      "136/281, train_loss: 0.0850, step time: 0.2581\n",
      "137/281, train_loss: 0.2188, step time: 0.2564\n",
      "138/281, train_loss: 0.1560, step time: 0.2555\n",
      "139/281, train_loss: 0.1472, step time: 0.2563\n",
      "140/281, train_loss: 0.0955, step time: 0.2586\n",
      "141/281, train_loss: 0.0921, step time: 0.2550\n",
      "142/281, train_loss: 0.0761, step time: 0.2612\n",
      "143/281, train_loss: 0.0667, step time: 0.2541\n",
      "144/281, train_loss: 0.1682, step time: 0.2526\n",
      "145/281, train_loss: 0.0788, step time: 0.2552\n",
      "146/281, train_loss: 0.2381, step time: 0.2567\n",
      "147/281, train_loss: 0.0835, step time: 0.2565\n",
      "148/281, train_loss: 0.0703, step time: 0.2582\n",
      "149/281, train_loss: 0.0861, step time: 0.2578\n",
      "150/281, train_loss: 0.0612, step time: 0.2560\n",
      "151/281, train_loss: 0.0858, step time: 0.2564\n",
      "152/281, train_loss: 0.0799, step time: 0.2594\n",
      "153/281, train_loss: 0.1053, step time: 0.2558\n",
      "154/281, train_loss: 0.1055, step time: 0.2563\n",
      "155/281, train_loss: 0.0511, step time: 0.2557\n",
      "156/281, train_loss: 0.0836, step time: 0.2558\n",
      "157/281, train_loss: 0.0866, step time: 0.2574\n",
      "158/281, train_loss: 0.0989, step time: 0.2573\n",
      "159/281, train_loss: 0.0830, step time: 0.2568\n",
      "160/281, train_loss: 0.1208, step time: 0.2558\n",
      "161/281, train_loss: 0.0790, step time: 0.2519\n",
      "162/281, train_loss: 0.0734, step time: 0.2522\n",
      "163/281, train_loss: 0.0967, step time: 0.2518\n",
      "164/281, train_loss: 0.2469, step time: 0.2593\n",
      "165/281, train_loss: 0.0981, step time: 0.2618\n",
      "166/281, train_loss: 0.2524, step time: 0.2520\n",
      "167/281, train_loss: 0.1093, step time: 0.2563\n",
      "168/281, train_loss: 0.1375, step time: 0.2577\n",
      "169/281, train_loss: 0.0931, step time: 0.2558\n",
      "170/281, train_loss: 0.2795, step time: 0.2537\n",
      "171/281, train_loss: 0.2555, step time: 0.2514\n",
      "172/281, train_loss: 0.2439, step time: 0.2577\n",
      "173/281, train_loss: 0.0776, step time: 0.2565\n",
      "174/281, train_loss: 0.2365, step time: 0.2529\n",
      "175/281, train_loss: 0.1295, step time: 0.2570\n",
      "176/281, train_loss: 0.0702, step time: 0.2539\n",
      "177/281, train_loss: 0.0797, step time: 0.2548\n",
      "178/281, train_loss: 0.1321, step time: 0.2558\n",
      "179/281, train_loss: 0.0855, step time: 0.2565\n",
      "180/281, train_loss: 0.0807, step time: 0.2507\n",
      "181/281, train_loss: 0.0610, step time: 0.2544\n",
      "182/281, train_loss: 0.0692, step time: 0.2560\n",
      "183/281, train_loss: 0.0977, step time: 0.2581\n",
      "184/281, train_loss: 0.1088, step time: 0.2571\n",
      "185/281, train_loss: 0.0701, step time: 0.2524\n",
      "186/281, train_loss: 0.0852, step time: 0.2502\n",
      "187/281, train_loss: 0.2151, step time: 0.2502\n",
      "188/281, train_loss: 0.0917, step time: 0.2522\n",
      "189/281, train_loss: 0.2748, step time: 0.2562\n",
      "190/281, train_loss: 0.2386, step time: 0.2525\n",
      "191/281, train_loss: 0.2628, step time: 0.2525\n",
      "192/281, train_loss: 0.0724, step time: 0.2558\n",
      "193/281, train_loss: 0.1041, step time: 0.2565\n",
      "194/281, train_loss: 0.0887, step time: 0.2585\n",
      "195/281, train_loss: 0.0735, step time: 0.2587\n",
      "196/281, train_loss: 0.0534, step time: 0.2545\n",
      "197/281, train_loss: 0.1354, step time: 0.2547\n",
      "198/281, train_loss: 0.1039, step time: 0.2538\n",
      "199/281, train_loss: 0.2747, step time: 0.2556\n",
      "200/281, train_loss: 0.0859, step time: 0.2541\n",
      "201/281, train_loss: 0.0410, step time: 0.2562\n",
      "202/281, train_loss: 0.0664, step time: 0.2541\n",
      "203/281, train_loss: 0.0932, step time: 0.2549\n",
      "204/281, train_loss: 0.0883, step time: 0.2529\n",
      "205/281, train_loss: 0.0763, step time: 0.2550\n",
      "206/281, train_loss: 0.0729, step time: 0.2571\n",
      "207/281, train_loss: 0.0844, step time: 0.2524\n",
      "208/281, train_loss: 0.0664, step time: 0.2527\n",
      "209/281, train_loss: 0.0829, step time: 0.2623\n",
      "210/281, train_loss: 0.0989, step time: 0.2519\n",
      "211/281, train_loss: 0.2819, step time: 0.2582\n",
      "212/281, train_loss: 0.0532, step time: 0.2599\n",
      "213/281, train_loss: 0.1207, step time: 0.2529\n",
      "214/281, train_loss: 0.0556, step time: 0.2538\n",
      "215/281, train_loss: 0.0521, step time: 0.2554\n",
      "216/281, train_loss: 0.2452, step time: 0.2533\n",
      "217/281, train_loss: 0.1130, step time: 0.2564\n",
      "218/281, train_loss: 0.2432, step time: 0.2550\n",
      "219/281, train_loss: 0.0983, step time: 0.2550\n",
      "220/281, train_loss: 0.0741, step time: 0.2513\n",
      "221/281, train_loss: 0.1155, step time: 0.2502\n",
      "222/281, train_loss: 0.1041, step time: 0.2460\n",
      "223/281, train_loss: 0.2332, step time: 0.2471\n",
      "224/281, train_loss: 0.0756, step time: 0.2561\n",
      "225/281, train_loss: 0.1147, step time: 0.2541\n",
      "226/281, train_loss: 0.0610, step time: 0.2512\n",
      "227/281, train_loss: 0.1058, step time: 0.2530\n",
      "228/281, train_loss: 0.0915, step time: 0.2586\n",
      "229/281, train_loss: 0.2638, step time: 0.2609\n",
      "230/281, train_loss: 0.0662, step time: 0.2568\n",
      "231/281, train_loss: 0.1187, step time: 0.2589\n",
      "232/281, train_loss: 0.0928, step time: 0.2578\n",
      "233/281, train_loss: 0.0782, step time: 0.2600\n",
      "234/281, train_loss: 0.1053, step time: 0.2567\n",
      "235/281, train_loss: 0.0713, step time: 0.2565\n",
      "236/281, train_loss: 0.0677, step time: 0.2538\n",
      "237/281, train_loss: 0.1608, step time: 0.2529\n",
      "238/281, train_loss: 0.1198, step time: 0.2579\n",
      "239/281, train_loss: 0.0803, step time: 0.2568\n",
      "240/281, train_loss: 0.0687, step time: 0.2599\n",
      "241/281, train_loss: 0.2369, step time: 0.2570\n",
      "242/281, train_loss: 0.0978, step time: 0.2540\n",
      "243/281, train_loss: 0.1353, step time: 0.2567\n",
      "244/281, train_loss: 0.0952, step time: 0.2547\n",
      "245/281, train_loss: 0.0565, step time: 0.2558\n",
      "246/281, train_loss: 0.1037, step time: 0.2581\n",
      "247/281, train_loss: 0.2139, step time: 0.2551\n",
      "248/281, train_loss: 0.1043, step time: 0.2535\n",
      "249/281, train_loss: 0.1155, step time: 0.2552\n",
      "250/281, train_loss: 0.2586, step time: 0.2530\n",
      "251/281, train_loss: 0.2576, step time: 0.2541\n",
      "252/281, train_loss: 0.0887, step time: 0.2533\n",
      "253/281, train_loss: 0.2697, step time: 0.2557\n",
      "254/281, train_loss: 0.1045, step time: 0.2499\n",
      "255/281, train_loss: 0.0941, step time: 0.2505\n",
      "256/281, train_loss: 0.0573, step time: 0.2534\n",
      "257/281, train_loss: 0.2854, step time: 0.2518\n",
      "258/281, train_loss: 0.0705, step time: 0.2557\n",
      "259/281, train_loss: 0.0801, step time: 0.2518\n",
      "260/281, train_loss: 0.0811, step time: 0.2571\n",
      "261/281, train_loss: 0.0962, step time: 0.2558\n",
      "262/281, train_loss: 0.0970, step time: 0.2551\n",
      "263/281, train_loss: 0.2093, step time: 0.2573\n",
      "264/281, train_loss: 0.0768, step time: 0.2679\n",
      "265/281, train_loss: 0.2463, step time: 0.2519\n",
      "266/281, train_loss: 0.0775, step time: 0.2512\n",
      "267/281, train_loss: 0.1108, step time: 0.2551\n",
      "268/281, train_loss: 0.2604, step time: 0.2589\n",
      "269/281, train_loss: 0.0753, step time: 0.2522\n",
      "270/281, train_loss: 0.0901, step time: 0.2507\n",
      "271/281, train_loss: 0.0878, step time: 0.2705\n",
      "272/281, train_loss: 0.0783, step time: 0.2549\n",
      "273/281, train_loss: 0.2608, step time: 0.2545\n",
      "274/281, train_loss: 0.0890, step time: 0.2543\n",
      "275/281, train_loss: 0.0572, step time: 0.2561\n",
      "276/281, train_loss: 0.0486, step time: 0.2617\n",
      "277/281, train_loss: 0.1044, step time: 0.2542\n",
      "278/281, train_loss: 0.0914, step time: 0.2536\n",
      "279/281, train_loss: 0.0963, step time: 0.2514\n",
      "280/281, train_loss: 0.0734, step time: 0.2568\n",
      "281/281, train_loss: 0.0851, step time: 0.2555\n",
      "282/281, train_loss: 0.0758, step time: 0.1518\n",
      "epoch 60 average loss: 0.1146\n",
      "saved new best metric model\n",
      "current epoch: 60 current mean dice: 0.8805 tc: 0.8746 wt: 0.9160 et: 0.8595\n",
      "best mean dice: 0.8805 at epoch: 60\n",
      "time consuming of epoch 60 is: 391.8390\n",
      "----------\n",
      "epoch 61/200\n",
      "1/281, train_loss: 0.1058, step time: 0.2568\n",
      "2/281, train_loss: 0.0754, step time: 0.2583\n",
      "3/281, train_loss: 0.0764, step time: 0.2575\n",
      "4/281, train_loss: 0.0818, step time: 0.2531\n",
      "5/281, train_loss: 0.0898, step time: 0.2518\n",
      "6/281, train_loss: 0.1344, step time: 0.2536\n",
      "7/281, train_loss: 0.0598, step time: 0.2547\n",
      "8/281, train_loss: 0.0635, step time: 0.2590\n",
      "9/281, train_loss: 0.1211, step time: 0.2586\n",
      "10/281, train_loss: 0.1252, step time: 0.2556\n",
      "11/281, train_loss: 0.0524, step time: 0.2542\n",
      "12/281, train_loss: 0.0686, step time: 0.2591\n",
      "13/281, train_loss: 0.1048, step time: 0.2550\n",
      "14/281, train_loss: 0.0536, step time: 0.2495\n",
      "15/281, train_loss: 0.0564, step time: 0.2528\n",
      "16/281, train_loss: 0.1196, step time: 0.2566\n",
      "17/281, train_loss: 0.0922, step time: 0.2492\n",
      "18/281, train_loss: 0.0978, step time: 0.2548\n",
      "19/281, train_loss: 0.1061, step time: 0.2562\n",
      "20/281, train_loss: 0.1026, step time: 0.2577\n",
      "21/281, train_loss: 0.0879, step time: 0.2526\n",
      "22/281, train_loss: 0.0949, step time: 0.2524\n",
      "23/281, train_loss: 0.0597, step time: 0.2560\n",
      "24/281, train_loss: 0.0844, step time: 0.2566\n",
      "25/281, train_loss: 0.1321, step time: 0.2552\n",
      "26/281, train_loss: 0.2285, step time: 0.2489\n",
      "27/281, train_loss: 0.0973, step time: 0.2492\n",
      "28/281, train_loss: 0.1217, step time: 0.2530\n",
      "29/281, train_loss: 0.1042, step time: 0.2586\n",
      "30/281, train_loss: 0.1349, step time: 0.2561\n",
      "31/281, train_loss: 0.0575, step time: 0.2578\n",
      "32/281, train_loss: 0.0914, step time: 0.2576\n",
      "33/281, train_loss: 0.0579, step time: 0.2929\n",
      "34/281, train_loss: 0.0685, step time: 0.2570\n",
      "35/281, train_loss: 0.0686, step time: 0.2543\n",
      "36/281, train_loss: 0.0943, step time: 0.2513\n",
      "37/281, train_loss: 0.2626, step time: 0.2548\n",
      "38/281, train_loss: 0.0714, step time: 0.2542\n",
      "39/281, train_loss: 0.0695, step time: 0.2491\n",
      "40/281, train_loss: 0.0976, step time: 0.2529\n",
      "41/281, train_loss: 0.1333, step time: 0.2556\n",
      "42/281, train_loss: 0.2295, step time: 0.2594\n",
      "43/281, train_loss: 0.0962, step time: 0.2524\n",
      "44/281, train_loss: 0.0897, step time: 0.2488\n",
      "45/281, train_loss: 0.0568, step time: 0.2508\n",
      "46/281, train_loss: 0.0746, step time: 0.2621\n",
      "47/281, train_loss: 0.0686, step time: 0.2576\n",
      "48/281, train_loss: 0.0855, step time: 0.2527\n",
      "49/281, train_loss: 0.0664, step time: 0.2513\n",
      "50/281, train_loss: 0.0939, step time: 0.2526\n",
      "51/281, train_loss: 0.0936, step time: 0.2586\n",
      "52/281, train_loss: 0.0823, step time: 0.2584\n",
      "53/281, train_loss: 0.1276, step time: 0.2557\n",
      "54/281, train_loss: 0.3349, step time: 0.2774\n",
      "55/281, train_loss: 0.0787, step time: 0.2484\n",
      "56/281, train_loss: 0.4378, step time: 0.2518\n",
      "57/281, train_loss: 0.2392, step time: 0.2538\n",
      "58/281, train_loss: 0.1254, step time: 0.2535\n",
      "59/281, train_loss: 0.2852, step time: 0.2541\n",
      "60/281, train_loss: 0.0613, step time: 0.2492\n",
      "61/281, train_loss: 0.1064, step time: 0.2537\n",
      "62/281, train_loss: 0.0738, step time: 0.2547\n",
      "63/281, train_loss: 0.1309, step time: 0.2471\n",
      "64/281, train_loss: 0.1038, step time: 0.2479\n",
      "65/281, train_loss: 0.1451, step time: 0.2488\n",
      "66/281, train_loss: 0.1064, step time: 0.2495\n",
      "67/281, train_loss: 0.2295, step time: 0.2573\n",
      "68/281, train_loss: 0.0626, step time: 0.2582\n",
      "69/281, train_loss: 0.0945, step time: 0.2483\n",
      "70/281, train_loss: 0.0729, step time: 0.2519\n",
      "71/281, train_loss: 0.0867, step time: 0.2608\n",
      "72/281, train_loss: 0.0906, step time: 0.2549\n",
      "73/281, train_loss: 0.0763, step time: 0.2508\n",
      "74/281, train_loss: 0.0797, step time: 0.2538\n",
      "75/281, train_loss: 0.1401, step time: 0.2506\n",
      "76/281, train_loss: 0.1187, step time: 0.2491\n",
      "77/281, train_loss: 0.0963, step time: 0.2457\n",
      "78/281, train_loss: 0.0905, step time: 0.2535\n",
      "79/281, train_loss: 0.2281, step time: 0.2510\n",
      "80/281, train_loss: 0.0796, step time: 0.2554\n",
      "81/281, train_loss: 0.0641, step time: 0.2562\n",
      "82/281, train_loss: 0.0752, step time: 0.2548\n",
      "83/281, train_loss: 0.0704, step time: 0.2545\n",
      "84/281, train_loss: 0.1142, step time: 0.2502\n",
      "85/281, train_loss: 0.1000, step time: 0.2530\n",
      "86/281, train_loss: 0.0967, step time: 0.2541\n",
      "87/281, train_loss: 0.2280, step time: 0.2568\n",
      "88/281, train_loss: 0.0872, step time: 0.2595\n",
      "89/281, train_loss: 0.0657, step time: 0.2539\n",
      "90/281, train_loss: 0.0857, step time: 0.2505\n",
      "91/281, train_loss: 0.0620, step time: 0.2538\n",
      "92/281, train_loss: 0.0486, step time: 0.2536\n",
      "93/281, train_loss: 0.0782, step time: 0.2521\n",
      "94/281, train_loss: 0.2465, step time: 0.2495\n",
      "95/281, train_loss: 0.1032, step time: 0.2567\n",
      "96/281, train_loss: 0.0828, step time: 0.2527\n",
      "97/281, train_loss: 0.1297, step time: 0.2475\n",
      "98/281, train_loss: 0.1236, step time: 0.2481\n",
      "99/281, train_loss: 0.0976, step time: 0.2515\n",
      "100/281, train_loss: 0.0844, step time: 0.2517\n",
      "101/281, train_loss: 0.0783, step time: 0.2513\n",
      "102/281, train_loss: 0.0844, step time: 0.2501\n",
      "103/281, train_loss: 0.2404, step time: 0.2539\n",
      "104/281, train_loss: 0.0875, step time: 0.2509\n",
      "105/281, train_loss: 0.1007, step time: 0.2537\n",
      "106/281, train_loss: 0.2441, step time: 0.2541\n",
      "107/281, train_loss: 0.1043, step time: 0.2529\n",
      "108/281, train_loss: 0.1353, step time: 0.2497\n",
      "109/281, train_loss: 0.0568, step time: 0.2536\n",
      "110/281, train_loss: 0.2155, step time: 0.2520\n",
      "111/281, train_loss: 0.0957, step time: 0.2562\n",
      "112/281, train_loss: 0.0874, step time: 0.2525\n",
      "113/281, train_loss: 0.0574, step time: 0.2529\n",
      "114/281, train_loss: 0.1240, step time: 0.2513\n",
      "115/281, train_loss: 0.0728, step time: 0.2492\n",
      "116/281, train_loss: 0.0636, step time: 0.2506\n",
      "117/281, train_loss: 0.0883, step time: 0.2512\n",
      "118/281, train_loss: 0.0621, step time: 0.2470\n",
      "119/281, train_loss: 0.2403, step time: 0.2494\n",
      "120/281, train_loss: 0.0749, step time: 0.2506\n",
      "121/281, train_loss: 0.1233, step time: 0.2589\n",
      "122/281, train_loss: 0.0977, step time: 0.2680\n",
      "123/281, train_loss: 0.0728, step time: 0.2512\n",
      "124/281, train_loss: 0.1352, step time: 0.2556\n",
      "125/281, train_loss: 0.1277, step time: 0.2505\n",
      "126/281, train_loss: 0.0765, step time: 0.2479\n",
      "127/281, train_loss: 0.2279, step time: 0.2500\n",
      "128/281, train_loss: 0.2757, step time: 0.2497\n",
      "129/281, train_loss: 0.1304, step time: 0.2515\n",
      "130/281, train_loss: 0.0997, step time: 0.2500\n",
      "131/281, train_loss: 0.0776, step time: 0.2580\n",
      "132/281, train_loss: 0.4175, step time: 0.2510\n",
      "133/281, train_loss: 0.0899, step time: 0.2512\n",
      "134/281, train_loss: 0.0771, step time: 0.2518\n",
      "135/281, train_loss: 0.1042, step time: 0.2506\n",
      "136/281, train_loss: 0.0760, step time: 0.2508\n",
      "137/281, train_loss: 0.0671, step time: 0.2556\n",
      "138/281, train_loss: 0.1013, step time: 0.2558\n",
      "139/281, train_loss: 0.0746, step time: 0.2522\n",
      "140/281, train_loss: 0.1215, step time: 0.2544\n",
      "141/281, train_loss: 0.0745, step time: 0.2595\n",
      "142/281, train_loss: 0.0647, step time: 0.2567\n",
      "143/281, train_loss: 0.1063, step time: 0.2604\n",
      "144/281, train_loss: 0.0829, step time: 0.2539\n",
      "145/281, train_loss: 0.0681, step time: 0.2493\n",
      "146/281, train_loss: 0.0548, step time: 0.2523\n",
      "147/281, train_loss: 0.0947, step time: 0.2523\n",
      "148/281, train_loss: 0.0986, step time: 0.2570\n",
      "149/281, train_loss: 0.1372, step time: 0.2536\n",
      "150/281, train_loss: 0.2302, step time: 0.2489\n",
      "151/281, train_loss: 0.2717, step time: 0.2564\n",
      "152/281, train_loss: 0.0565, step time: 0.2468\n",
      "153/281, train_loss: 0.0981, step time: 0.2526\n",
      "154/281, train_loss: 0.0674, step time: 0.2496\n",
      "155/281, train_loss: 0.1127, step time: 0.2627\n",
      "156/281, train_loss: 0.0495, step time: 0.2532\n",
      "157/281, train_loss: 0.2648, step time: 0.2529\n",
      "158/281, train_loss: 0.0885, step time: 0.2506\n",
      "159/281, train_loss: 0.2637, step time: 0.2603\n",
      "160/281, train_loss: 0.1023, step time: 0.2546\n",
      "161/281, train_loss: 0.0923, step time: 0.2552\n",
      "162/281, train_loss: 0.2255, step time: 0.2526\n",
      "163/281, train_loss: 0.0809, step time: 0.2514\n",
      "164/281, train_loss: 0.2376, step time: 0.2508\n",
      "165/281, train_loss: 0.0785, step time: 0.2524\n",
      "166/281, train_loss: 0.1317, step time: 0.2505\n",
      "167/281, train_loss: 0.0717, step time: 0.2512\n",
      "168/281, train_loss: 0.2527, step time: 0.2570\n",
      "169/281, train_loss: 0.0620, step time: 0.2569\n",
      "170/281, train_loss: 0.1285, step time: 0.2547\n",
      "171/281, train_loss: 0.0959, step time: 0.2655\n",
      "172/281, train_loss: 0.1308, step time: 0.2531\n",
      "173/281, train_loss: 0.0699, step time: 0.2522\n",
      "174/281, train_loss: 0.0769, step time: 0.2531\n",
      "175/281, train_loss: 0.0541, step time: 0.2615\n",
      "176/281, train_loss: 0.1255, step time: 0.2558\n",
      "177/281, train_loss: 0.1274, step time: 0.2545\n",
      "178/281, train_loss: 0.0619, step time: 0.2530\n",
      "179/281, train_loss: 0.2299, step time: 0.2549\n",
      "180/281, train_loss: 0.1478, step time: 0.2560\n",
      "181/281, train_loss: 0.1064, step time: 0.2500\n",
      "182/281, train_loss: 0.0715, step time: 0.2531\n",
      "183/281, train_loss: 0.2453, step time: 0.2557\n",
      "184/281, train_loss: 0.2599, step time: 0.2537\n",
      "185/281, train_loss: 0.2397, step time: 0.2560\n",
      "186/281, train_loss: 0.0825, step time: 0.2544\n",
      "187/281, train_loss: 0.0820, step time: 0.2486\n",
      "188/281, train_loss: 0.0663, step time: 0.2534\n",
      "189/281, train_loss: 0.2451, step time: 0.2550\n",
      "190/281, train_loss: 0.0977, step time: 0.2546\n",
      "191/281, train_loss: 0.0901, step time: 0.2570\n",
      "192/281, train_loss: 0.0931, step time: 0.2537\n",
      "193/281, train_loss: 0.0807, step time: 0.2497\n",
      "194/281, train_loss: 0.0830, step time: 0.2482\n",
      "195/281, train_loss: 0.1102, step time: 0.2535\n",
      "196/281, train_loss: 0.0635, step time: 0.2516\n",
      "197/281, train_loss: 0.0816, step time: 0.2688\n",
      "198/281, train_loss: 0.0669, step time: 0.2552\n",
      "199/281, train_loss: 0.0827, step time: 0.2527\n",
      "200/281, train_loss: 0.0681, step time: 0.2511\n",
      "201/281, train_loss: 0.1291, step time: 0.2549\n",
      "202/281, train_loss: 0.0663, step time: 0.2561\n",
      "203/281, train_loss: 0.0890, step time: 0.2577\n",
      "204/281, train_loss: 0.2590, step time: 0.2625\n",
      "205/281, train_loss: 0.0635, step time: 0.2550\n",
      "206/281, train_loss: 0.1312, step time: 0.2538\n",
      "207/281, train_loss: 0.0977, step time: 0.2516\n",
      "208/281, train_loss: 0.2300, step time: 0.2538\n",
      "209/281, train_loss: 0.0989, step time: 0.2560\n",
      "210/281, train_loss: 0.1003, step time: 0.2540\n",
      "211/281, train_loss: 0.0660, step time: 0.2538\n",
      "212/281, train_loss: 0.0949, step time: 0.2495\n",
      "213/281, train_loss: 0.1764, step time: 0.2488\n",
      "214/281, train_loss: 0.1183, step time: 0.2517\n",
      "215/281, train_loss: 0.0641, step time: 0.2534\n",
      "216/281, train_loss: 0.0988, step time: 0.2477\n",
      "217/281, train_loss: 0.0863, step time: 0.2540\n",
      "218/281, train_loss: 0.2703, step time: 0.2501\n",
      "219/281, train_loss: 0.1083, step time: 0.2539\n",
      "220/281, train_loss: 0.2259, step time: 0.2544\n",
      "221/281, train_loss: 0.2366, step time: 0.2538\n",
      "222/281, train_loss: 0.2440, step time: 0.2585\n",
      "223/281, train_loss: 0.0582, step time: 0.2514\n",
      "224/281, train_loss: 0.0758, step time: 0.2485\n",
      "225/281, train_loss: 0.1050, step time: 0.2498\n",
      "226/281, train_loss: 0.1039, step time: 0.2580\n",
      "227/281, train_loss: 0.0372, step time: 0.2541\n",
      "228/281, train_loss: 0.2563, step time: 0.2502\n",
      "229/281, train_loss: 0.0951, step time: 0.2495\n",
      "230/281, train_loss: 0.0881, step time: 0.2521\n",
      "231/281, train_loss: 0.0909, step time: 0.2576\n",
      "232/281, train_loss: 0.1035, step time: 0.2524\n",
      "233/281, train_loss: 0.0877, step time: 0.2472\n",
      "234/281, train_loss: 0.0772, step time: 0.2518\n",
      "235/281, train_loss: 0.0749, step time: 0.2460\n",
      "236/281, train_loss: 0.0699, step time: 0.2483\n",
      "237/281, train_loss: 0.1017, step time: 0.2465\n",
      "238/281, train_loss: 0.0595, step time: 0.2562\n",
      "239/281, train_loss: 0.0561, step time: 0.2572\n",
      "240/281, train_loss: 0.0423, step time: 0.2539\n",
      "241/281, train_loss: 0.0997, step time: 0.2482\n",
      "242/281, train_loss: 0.1083, step time: 0.2458\n",
      "243/281, train_loss: 0.0989, step time: 0.2421\n",
      "244/281, train_loss: 0.0446, step time: 0.2440\n",
      "245/281, train_loss: 0.1570, step time: 0.2523\n",
      "246/281, train_loss: 0.0959, step time: 0.2496\n",
      "247/281, train_loss: 0.1307, step time: 0.2477\n",
      "248/281, train_loss: 0.0564, step time: 0.2561\n",
      "249/281, train_loss: 0.2451, step time: 0.2470\n",
      "250/281, train_loss: 0.1134, step time: 0.2457\n",
      "251/281, train_loss: 0.1109, step time: 0.2450\n",
      "252/281, train_loss: 0.1205, step time: 0.2526\n",
      "253/281, train_loss: 0.0860, step time: 0.2475\n",
      "254/281, train_loss: 0.0575, step time: 0.2576\n",
      "255/281, train_loss: 0.0900, step time: 0.2514\n",
      "256/281, train_loss: 0.0687, step time: 0.2503\n",
      "257/281, train_loss: 0.0617, step time: 0.2545\n",
      "258/281, train_loss: 0.0854, step time: 0.2498\n",
      "259/281, train_loss: 0.2619, step time: 0.2562\n",
      "260/281, train_loss: 0.0804, step time: 0.2541\n",
      "261/281, train_loss: 0.0927, step time: 0.2548\n",
      "262/281, train_loss: 0.1196, step time: 0.2489\n",
      "263/281, train_loss: 0.0822, step time: 0.2530\n",
      "264/281, train_loss: 0.1024, step time: 0.2525\n",
      "265/281, train_loss: 0.1128, step time: 0.2563\n",
      "266/281, train_loss: 0.0685, step time: 0.2497\n",
      "267/281, train_loss: 0.1057, step time: 0.2585\n",
      "268/281, train_loss: 0.1229, step time: 0.2497\n",
      "269/281, train_loss: 0.0727, step time: 0.2516\n",
      "270/281, train_loss: 0.0892, step time: 0.2512\n",
      "271/281, train_loss: 0.1574, step time: 0.2526\n",
      "272/281, train_loss: 0.0713, step time: 0.2509\n",
      "273/281, train_loss: 0.1261, step time: 0.2524\n",
      "274/281, train_loss: 0.2957, step time: 0.2496\n",
      "275/281, train_loss: 0.1021, step time: 0.2477\n",
      "276/281, train_loss: 0.0860, step time: 0.2505\n",
      "277/281, train_loss: 0.2484, step time: 0.2513\n",
      "278/281, train_loss: 0.0990, step time: 0.2480\n",
      "279/281, train_loss: 0.0751, step time: 0.2477\n",
      "280/281, train_loss: 0.0775, step time: 0.2506\n",
      "281/281, train_loss: 0.2211, step time: 0.2507\n",
      "282/281, train_loss: 0.1244, step time: 0.1470\n",
      "epoch 61 average loss: 0.1153\n",
      "current epoch: 61 current mean dice: 0.8734 tc: 0.8659 wt: 0.9071 et: 0.8578\n",
      "best mean dice: 0.8805 at epoch: 60\n",
      "time consuming of epoch 61 is: 400.3078\n",
      "----------\n",
      "epoch 62/200\n",
      "1/281, train_loss: 0.0496, step time: 0.2554\n",
      "2/281, train_loss: 0.1553, step time: 0.2607\n",
      "3/281, train_loss: 0.1500, step time: 0.2523\n",
      "4/281, train_loss: 0.0739, step time: 0.2507\n",
      "5/281, train_loss: 0.0877, step time: 0.2470\n",
      "6/281, train_loss: 0.0842, step time: 0.2556\n",
      "7/281, train_loss: 0.0608, step time: 0.2526\n",
      "8/281, train_loss: 0.0895, step time: 0.2533\n",
      "9/281, train_loss: 0.1023, step time: 0.2567\n",
      "10/281, train_loss: 0.0712, step time: 0.2508\n",
      "11/281, train_loss: 0.2105, step time: 0.2498\n",
      "12/281, train_loss: 0.0833, step time: 0.2510\n",
      "13/281, train_loss: 0.3052, step time: 0.2484\n",
      "14/281, train_loss: 0.2434, step time: 0.2485\n",
      "15/281, train_loss: 0.0866, step time: 0.2519\n",
      "16/281, train_loss: 0.2516, step time: 0.2493\n",
      "17/281, train_loss: 0.0673, step time: 0.2534\n",
      "18/281, train_loss: 0.0720, step time: 0.2483\n",
      "19/281, train_loss: 0.0882, step time: 0.2423\n",
      "20/281, train_loss: 0.1071, step time: 0.2453\n",
      "21/281, train_loss: 0.1200, step time: 0.2476\n",
      "22/281, train_loss: 0.1093, step time: 0.2512\n",
      "23/281, train_loss: 0.0693, step time: 0.2515\n",
      "24/281, train_loss: 0.1384, step time: 0.2526\n",
      "25/281, train_loss: 0.2190, step time: 0.2526\n",
      "26/281, train_loss: 0.1044, step time: 0.2562\n",
      "27/281, train_loss: 0.0731, step time: 0.2521\n",
      "28/281, train_loss: 0.1054, step time: 0.2535\n",
      "29/281, train_loss: 0.0858, step time: 0.2551\n",
      "30/281, train_loss: 0.0894, step time: 0.2558\n",
      "31/281, train_loss: 0.0724, step time: 0.2572\n",
      "32/281, train_loss: 0.1208, step time: 0.2571\n",
      "33/281, train_loss: 0.1370, step time: 0.2553\n",
      "34/281, train_loss: 0.0505, step time: 0.2538\n",
      "35/281, train_loss: 0.1393, step time: 0.2500\n",
      "36/281, train_loss: 0.0654, step time: 0.2498\n",
      "37/281, train_loss: 0.0619, step time: 0.2521\n",
      "38/281, train_loss: 0.0951, step time: 0.2622\n",
      "39/281, train_loss: 0.0864, step time: 0.2553\n",
      "40/281, train_loss: 0.0960, step time: 0.2570\n",
      "41/281, train_loss: 0.0694, step time: 0.2540\n",
      "42/281, train_loss: 0.1483, step time: 0.2599\n",
      "43/281, train_loss: 0.0876, step time: 0.2520\n",
      "44/281, train_loss: 0.2403, step time: 0.2521\n",
      "45/281, train_loss: 0.1192, step time: 0.2553\n",
      "46/281, train_loss: 0.2672, step time: 0.2587\n",
      "47/281, train_loss: 0.0756, step time: 0.2524\n",
      "48/281, train_loss: 0.0883, step time: 0.2469\n",
      "49/281, train_loss: 0.1257, step time: 0.2491\n",
      "50/281, train_loss: 0.0759, step time: 0.2512\n",
      "51/281, train_loss: 0.0861, step time: 0.2552\n",
      "52/281, train_loss: 0.2475, step time: 0.2539\n",
      "53/281, train_loss: 0.1210, step time: 0.2542\n",
      "54/281, train_loss: 0.0712, step time: 0.2551\n",
      "55/281, train_loss: 0.1045, step time: 0.2511\n",
      "56/281, train_loss: 0.0986, step time: 0.2487\n",
      "57/281, train_loss: 0.0913, step time: 0.2477\n",
      "58/281, train_loss: 0.0597, step time: 0.2480\n",
      "59/281, train_loss: 0.0593, step time: 0.2527\n",
      "60/281, train_loss: 0.0569, step time: 0.2537\n",
      "61/281, train_loss: 0.0950, step time: 0.2486\n",
      "62/281, train_loss: 0.0469, step time: 0.2520\n",
      "63/281, train_loss: 0.1186, step time: 0.2522\n",
      "64/281, train_loss: 0.4146, step time: 0.2517\n",
      "65/281, train_loss: 0.0531, step time: 0.2540\n",
      "66/281, train_loss: 0.0905, step time: 0.2592\n",
      "67/281, train_loss: 0.1334, step time: 0.2525\n",
      "68/281, train_loss: 0.0600, step time: 0.2472\n",
      "69/281, train_loss: 0.1072, step time: 0.2437\n",
      "70/281, train_loss: 0.1113, step time: 0.2457\n",
      "71/281, train_loss: 0.1483, step time: 0.2517\n",
      "72/281, train_loss: 0.0898, step time: 0.2508\n",
      "73/281, train_loss: 0.0807, step time: 0.2534\n",
      "74/281, train_loss: 0.0748, step time: 0.2542\n",
      "75/281, train_loss: 0.0767, step time: 0.2540\n",
      "76/281, train_loss: 0.1017, step time: 0.2543\n",
      "77/281, train_loss: 0.2616, step time: 0.2540\n",
      "78/281, train_loss: 0.0573, step time: 0.2539\n",
      "79/281, train_loss: 0.0613, step time: 0.2544\n",
      "80/281, train_loss: 0.0789, step time: 0.2511\n",
      "81/281, train_loss: 0.4038, step time: 0.2502\n",
      "82/281, train_loss: 0.0717, step time: 0.2505\n",
      "83/281, train_loss: 0.1049, step time: 0.2542\n",
      "84/281, train_loss: 0.2233, step time: 0.2551\n",
      "85/281, train_loss: 0.0837, step time: 0.2586\n",
      "86/281, train_loss: 0.2155, step time: 0.2525\n",
      "87/281, train_loss: 0.0811, step time: 0.2522\n",
      "88/281, train_loss: 0.0938, step time: 0.2494\n",
      "89/281, train_loss: 0.1376, step time: 0.2527\n",
      "90/281, train_loss: 0.4278, step time: 0.2686\n",
      "91/281, train_loss: 0.0797, step time: 0.2534\n",
      "92/281, train_loss: 0.0979, step time: 0.2465\n",
      "93/281, train_loss: 0.0757, step time: 0.2499\n",
      "94/281, train_loss: 0.1142, step time: 0.2516\n",
      "95/281, train_loss: 0.1905, step time: 0.2546\n",
      "96/281, train_loss: 0.1194, step time: 0.2497\n",
      "97/281, train_loss: 0.0744, step time: 0.2525\n",
      "98/281, train_loss: 0.0950, step time: 0.2585\n",
      "99/281, train_loss: 0.0954, step time: 0.2536\n",
      "100/281, train_loss: 0.0822, step time: 0.2561\n",
      "101/281, train_loss: 0.1222, step time: 0.2527\n",
      "102/281, train_loss: 0.0737, step time: 0.2525\n",
      "103/281, train_loss: 0.1133, step time: 0.2568\n",
      "104/281, train_loss: 0.0818, step time: 0.2634\n",
      "105/281, train_loss: 0.0797, step time: 0.2559\n",
      "106/281, train_loss: 0.1154, step time: 0.2563\n",
      "107/281, train_loss: 0.0698, step time: 0.2520\n",
      "108/281, train_loss: 0.1078, step time: 0.2506\n",
      "109/281, train_loss: 0.0782, step time: 0.2554\n",
      "110/281, train_loss: 0.0856, step time: 0.2588\n",
      "111/281, train_loss: 0.1064, step time: 0.2533\n",
      "112/281, train_loss: 0.0645, step time: 0.2500\n",
      "113/281, train_loss: 0.0832, step time: 0.2535\n",
      "114/281, train_loss: 0.1310, step time: 0.2561\n",
      "115/281, train_loss: 0.0813, step time: 0.2562\n",
      "116/281, train_loss: 0.0521, step time: 0.2507\n",
      "117/281, train_loss: 0.1109, step time: 0.2516\n",
      "118/281, train_loss: 0.1085, step time: 0.2496\n",
      "119/281, train_loss: 0.1655, step time: 0.2510\n",
      "120/281, train_loss: 0.2276, step time: 0.2618\n",
      "121/281, train_loss: 0.2865, step time: 0.2520\n",
      "122/281, train_loss: 0.1201, step time: 0.2498\n",
      "123/281, train_loss: 0.0714, step time: 0.2490\n",
      "124/281, train_loss: 0.0980, step time: 0.2519\n",
      "125/281, train_loss: 0.0852, step time: 0.2542\n",
      "126/281, train_loss: 0.0734, step time: 0.2568\n",
      "127/281, train_loss: 0.0955, step time: 0.2541\n",
      "128/281, train_loss: 0.2150, step time: 0.2520\n",
      "129/281, train_loss: 0.1091, step time: 0.2533\n",
      "130/281, train_loss: 0.1359, step time: 0.2488\n",
      "131/281, train_loss: 0.0644, step time: 0.2557\n",
      "132/281, train_loss: 0.2564, step time: 0.2548\n",
      "133/281, train_loss: 0.2318, step time: 0.2560\n",
      "134/281, train_loss: 0.0613, step time: 0.2491\n",
      "135/281, train_loss: 0.0714, step time: 0.2552\n",
      "136/281, train_loss: 0.1029, step time: 0.2591\n",
      "137/281, train_loss: 0.0853, step time: 0.2556\n",
      "138/281, train_loss: 0.0893, step time: 0.2554\n",
      "139/281, train_loss: 0.0671, step time: 0.2520\n",
      "140/281, train_loss: 0.1459, step time: 0.2472\n",
      "141/281, train_loss: 0.2477, step time: 0.2516\n",
      "142/281, train_loss: 0.0544, step time: 0.2430\n",
      "143/281, train_loss: 0.0884, step time: 0.2482\n",
      "144/281, train_loss: 0.0892, step time: 0.2513\n",
      "145/281, train_loss: 0.2252, step time: 0.2526\n",
      "146/281, train_loss: 0.1590, step time: 0.2479\n",
      "147/281, train_loss: 0.0947, step time: 0.2474\n",
      "148/281, train_loss: 0.3951, step time: 0.2533\n",
      "149/281, train_loss: 0.0539, step time: 0.2518\n",
      "150/281, train_loss: 0.0993, step time: 0.2495\n",
      "151/281, train_loss: 0.0789, step time: 0.2452\n",
      "152/281, train_loss: 0.0473, step time: 0.2499\n",
      "153/281, train_loss: 0.0882, step time: 0.2509\n",
      "154/281, train_loss: 0.1024, step time: 0.2559\n",
      "155/281, train_loss: 0.0600, step time: 0.2452\n",
      "156/281, train_loss: 0.0797, step time: 0.2525\n",
      "157/281, train_loss: 0.0827, step time: 0.2497\n",
      "158/281, train_loss: 0.0877, step time: 0.2523\n",
      "159/281, train_loss: 0.0918, step time: 0.2525\n",
      "160/281, train_loss: 0.0441, step time: 0.2543\n",
      "161/281, train_loss: 0.0537, step time: 0.2556\n",
      "162/281, train_loss: 0.0743, step time: 0.2558\n",
      "163/281, train_loss: 0.2599, step time: 0.2564\n",
      "164/281, train_loss: 0.0864, step time: 0.2604\n",
      "165/281, train_loss: 0.0761, step time: 0.2545\n",
      "166/281, train_loss: 0.0530, step time: 0.2535\n",
      "167/281, train_loss: 0.0932, step time: 0.2552\n",
      "168/281, train_loss: 0.0887, step time: 0.2811\n",
      "169/281, train_loss: 0.0938, step time: 0.2567\n",
      "170/281, train_loss: 0.0759, step time: 0.2576\n",
      "171/281, train_loss: 0.0630, step time: 0.2523\n",
      "172/281, train_loss: 0.0983, step time: 0.2519\n",
      "173/281, train_loss: 0.2453, step time: 0.2552\n",
      "174/281, train_loss: 0.0989, step time: 0.2564\n",
      "175/281, train_loss: 0.1178, step time: 0.2544\n",
      "176/281, train_loss: 0.1501, step time: 0.2551\n",
      "177/281, train_loss: 0.0751, step time: 0.2568\n",
      "178/281, train_loss: 0.2754, step time: 0.2555\n",
      "179/281, train_loss: 0.0956, step time: 0.2552\n",
      "180/281, train_loss: 0.0679, step time: 0.2501\n",
      "181/281, train_loss: 0.0979, step time: 0.2548\n",
      "182/281, train_loss: 0.1351, step time: 0.2547\n",
      "183/281, train_loss: 0.1384, step time: 0.2569\n",
      "184/281, train_loss: 0.0733, step time: 0.2557\n",
      "185/281, train_loss: 0.0975, step time: 0.2573\n",
      "186/281, train_loss: 0.0897, step time: 0.2615\n",
      "187/281, train_loss: 0.2809, step time: 0.2585\n",
      "188/281, train_loss: 0.1071, step time: 0.2561\n",
      "189/281, train_loss: 0.2307, step time: 0.2533\n",
      "190/281, train_loss: 0.0629, step time: 0.2513\n",
      "191/281, train_loss: 0.1868, step time: 0.2567\n",
      "192/281, train_loss: 0.0655, step time: 0.2505\n",
      "193/281, train_loss: 0.0926, step time: 0.2517\n",
      "194/281, train_loss: 0.0845, step time: 0.2546\n",
      "195/281, train_loss: 0.1039, step time: 0.2581\n",
      "196/281, train_loss: 0.2426, step time: 0.2554\n",
      "197/281, train_loss: 0.0726, step time: 0.2506\n",
      "198/281, train_loss: 0.0919, step time: 0.2534\n",
      "199/281, train_loss: 0.0908, step time: 0.2558\n",
      "200/281, train_loss: 0.0643, step time: 0.2545\n",
      "201/281, train_loss: 0.0828, step time: 0.2583\n",
      "202/281, train_loss: 0.0721, step time: 0.2606\n",
      "203/281, train_loss: 0.0896, step time: 0.2574\n",
      "204/281, train_loss: 0.0596, step time: 0.2536\n",
      "205/281, train_loss: 0.0685, step time: 0.2540\n",
      "206/281, train_loss: 0.0971, step time: 0.2566\n",
      "207/281, train_loss: 0.1435, step time: 0.2558\n",
      "208/281, train_loss: 0.1020, step time: 0.2547\n",
      "209/281, train_loss: 0.0556, step time: 0.2532\n",
      "210/281, train_loss: 0.1081, step time: 0.2588\n",
      "211/281, train_loss: 0.0729, step time: 0.2543\n",
      "212/281, train_loss: 0.0811, step time: 0.2550\n",
      "213/281, train_loss: 0.0882, step time: 0.2572\n",
      "214/281, train_loss: 0.0673, step time: 0.2550\n",
      "215/281, train_loss: 0.0685, step time: 0.2543\n",
      "216/281, train_loss: 0.1641, step time: 0.2620\n",
      "217/281, train_loss: 0.0821, step time: 0.2547\n",
      "218/281, train_loss: 0.0688, step time: 0.2520\n",
      "219/281, train_loss: 0.2405, step time: 0.2518\n",
      "220/281, train_loss: 0.0662, step time: 0.2604\n",
      "221/281, train_loss: 0.0930, step time: 0.2598\n",
      "222/281, train_loss: 0.0669, step time: 0.2544\n",
      "223/281, train_loss: 0.0713, step time: 0.2572\n",
      "224/281, train_loss: 0.1309, step time: 0.2561\n",
      "225/281, train_loss: 0.0977, step time: 0.2548\n",
      "226/281, train_loss: 0.1167, step time: 0.2569\n",
      "227/281, train_loss: 0.0787, step time: 0.2567\n",
      "228/281, train_loss: 0.0974, step time: 0.2503\n",
      "229/281, train_loss: 0.1222, step time: 0.2557\n",
      "230/281, train_loss: 0.1134, step time: 0.2594\n",
      "231/281, train_loss: 0.2114, step time: 0.2569\n",
      "232/281, train_loss: 0.0897, step time: 0.2511\n",
      "233/281, train_loss: 0.0909, step time: 0.2546\n",
      "234/281, train_loss: 0.0764, step time: 0.2666\n",
      "235/281, train_loss: 0.0840, step time: 0.2676\n",
      "236/281, train_loss: 0.0500, step time: 0.2561\n",
      "237/281, train_loss: 0.2785, step time: 0.2548\n",
      "238/281, train_loss: 0.1126, step time: 0.2631\n",
      "239/281, train_loss: 0.0917, step time: 0.2603\n",
      "240/281, train_loss: 0.0797, step time: 0.2569\n",
      "241/281, train_loss: 0.0702, step time: 0.2589\n",
      "242/281, train_loss: 0.2463, step time: 0.2600\n",
      "243/281, train_loss: 0.0730, step time: 0.2563\n",
      "244/281, train_loss: 0.2963, step time: 0.2615\n",
      "245/281, train_loss: 0.2700, step time: 0.2622\n",
      "246/281, train_loss: 0.0675, step time: 0.2571\n",
      "247/281, train_loss: 0.2592, step time: 0.2564\n",
      "248/281, train_loss: 0.0809, step time: 0.2510\n",
      "249/281, train_loss: 0.0873, step time: 0.2526\n",
      "250/281, train_loss: 0.0961, step time: 0.2517\n",
      "251/281, train_loss: 0.0733, step time: 0.2503\n",
      "252/281, train_loss: 0.0967, step time: 0.2512\n",
      "253/281, train_loss: 0.0556, step time: 0.2485\n",
      "254/281, train_loss: 0.0860, step time: 0.2499\n",
      "255/281, train_loss: 0.2446, step time: 0.2482\n",
      "256/281, train_loss: 0.1103, step time: 0.2460\n",
      "257/281, train_loss: 0.2292, step time: 0.2515\n",
      "258/281, train_loss: 0.2360, step time: 0.2497\n",
      "259/281, train_loss: 0.1191, step time: 0.2515\n",
      "260/281, train_loss: 0.0764, step time: 0.2497\n",
      "261/281, train_loss: 0.0849, step time: 0.2484\n",
      "262/281, train_loss: 0.0872, step time: 0.2506\n",
      "263/281, train_loss: 0.1003, step time: 0.2518\n",
      "264/281, train_loss: 0.1005, step time: 0.2502\n",
      "265/281, train_loss: 0.0773, step time: 0.2559\n",
      "266/281, train_loss: 0.0928, step time: 0.2574\n",
      "267/281, train_loss: 0.0743, step time: 0.2601\n",
      "268/281, train_loss: 0.2670, step time: 0.2584\n",
      "269/281, train_loss: 0.2221, step time: 0.2567\n",
      "270/281, train_loss: 0.1151, step time: 0.2565\n",
      "271/281, train_loss: 0.0662, step time: 0.2538\n",
      "272/281, train_loss: 0.0516, step time: 0.2570\n",
      "273/281, train_loss: 0.1356, step time: 0.2583\n",
      "274/281, train_loss: 0.1171, step time: 0.2531\n",
      "275/281, train_loss: 0.0479, step time: 0.2513\n",
      "276/281, train_loss: 0.1110, step time: 0.2544\n",
      "277/281, train_loss: 0.1021, step time: 0.2541\n",
      "278/281, train_loss: 0.0462, step time: 0.2501\n",
      "279/281, train_loss: 0.0983, step time: 0.2572\n",
      "280/281, train_loss: 0.0527, step time: 0.2552\n",
      "281/281, train_loss: 0.1002, step time: 0.2525\n",
      "282/281, train_loss: 0.0778, step time: 0.1505\n",
      "epoch 62 average loss: 0.1152\n",
      "saved new best metric model\n",
      "current epoch: 62 current mean dice: 0.8817 tc: 0.8789 wt: 0.9137 et: 0.8613\n",
      "best mean dice: 0.8817 at epoch: 62\n",
      "time consuming of epoch 62 is: 395.0118\n",
      "----------\n",
      "epoch 63/200\n",
      "1/281, train_loss: 0.2207, step time: 0.2601\n",
      "2/281, train_loss: 0.2619, step time: 0.2559\n",
      "3/281, train_loss: 0.0971, step time: 0.2578\n",
      "4/281, train_loss: 0.1032, step time: 0.2788\n",
      "5/281, train_loss: 0.1083, step time: 0.2692\n",
      "6/281, train_loss: 0.0528, step time: 0.2515\n",
      "7/281, train_loss: 0.0711, step time: 0.2561\n",
      "8/281, train_loss: 0.0921, step time: 0.2558\n",
      "9/281, train_loss: 0.0648, step time: 0.2588\n",
      "10/281, train_loss: 0.0722, step time: 0.2573\n",
      "11/281, train_loss: 0.0784, step time: 0.2515\n",
      "12/281, train_loss: 0.0837, step time: 0.2572\n",
      "13/281, train_loss: 0.0583, step time: 0.2515\n",
      "14/281, train_loss: 0.1039, step time: 0.2533\n",
      "15/281, train_loss: 0.0765, step time: 0.2626\n",
      "16/281, train_loss: 0.0849, step time: 0.2577\n",
      "17/281, train_loss: 0.2441, step time: 0.2570\n",
      "18/281, train_loss: 0.0585, step time: 0.2564\n",
      "19/281, train_loss: 0.0606, step time: 0.2575\n",
      "20/281, train_loss: 0.0626, step time: 0.2553\n",
      "21/281, train_loss: 0.0895, step time: 0.2516\n",
      "22/281, train_loss: 0.1064, step time: 0.2547\n",
      "23/281, train_loss: 0.0647, step time: 0.2591\n",
      "24/281, train_loss: 0.2525, step time: 0.2589\n",
      "25/281, train_loss: 0.0659, step time: 0.2593\n",
      "26/281, train_loss: 0.0931, step time: 0.2576\n",
      "27/281, train_loss: 0.0691, step time: 0.2576\n",
      "28/281, train_loss: 0.2171, step time: 0.2520\n",
      "29/281, train_loss: 0.1718, step time: 0.2512\n",
      "30/281, train_loss: 0.1037, step time: 0.2591\n",
      "31/281, train_loss: 0.1083, step time: 0.2633\n",
      "32/281, train_loss: 0.1107, step time: 0.2546\n",
      "33/281, train_loss: 0.0833, step time: 0.2554\n",
      "34/281, train_loss: 0.0988, step time: 0.2552\n",
      "35/281, train_loss: 0.0675, step time: 0.2651\n",
      "36/281, train_loss: 0.0698, step time: 0.2581\n",
      "37/281, train_loss: 0.0617, step time: 0.2627\n",
      "38/281, train_loss: 0.0865, step time: 0.2678\n",
      "39/281, train_loss: 0.1211, step time: 0.2561\n",
      "40/281, train_loss: 0.0766, step time: 0.2563\n",
      "41/281, train_loss: 0.0830, step time: 0.2526\n",
      "42/281, train_loss: 0.0733, step time: 0.2596\n",
      "43/281, train_loss: 0.2506, step time: 0.2538\n",
      "44/281, train_loss: 0.1164, step time: 0.2562\n",
      "45/281, train_loss: 0.0841, step time: 0.2606\n",
      "46/281, train_loss: 0.0873, step time: 0.2572\n",
      "47/281, train_loss: 0.1059, step time: 0.2544\n",
      "48/281, train_loss: 0.0842, step time: 0.2576\n",
      "49/281, train_loss: 0.0915, step time: 0.2649\n",
      "50/281, train_loss: 0.2545, step time: 0.2584\n",
      "51/281, train_loss: 0.2168, step time: 0.2582\n",
      "52/281, train_loss: 0.0477, step time: 0.2549\n",
      "53/281, train_loss: 0.0741, step time: 0.2554\n",
      "54/281, train_loss: 0.0683, step time: 0.2550\n",
      "55/281, train_loss: 0.2524, step time: 0.2737\n",
      "56/281, train_loss: 0.0805, step time: 0.2538\n",
      "57/281, train_loss: 0.0648, step time: 0.2569\n",
      "58/281, train_loss: 0.2473, step time: 0.2563\n",
      "59/281, train_loss: 0.2148, step time: 0.2498\n",
      "60/281, train_loss: 0.1027, step time: 0.2573\n",
      "61/281, train_loss: 0.0883, step time: 0.2556\n",
      "62/281, train_loss: 0.0909, step time: 0.2505\n",
      "63/281, train_loss: 0.0848, step time: 0.2588\n",
      "64/281, train_loss: 0.2472, step time: 0.2568\n",
      "65/281, train_loss: 0.1003, step time: 0.2598\n",
      "66/281, train_loss: 0.0889, step time: 0.2560\n",
      "67/281, train_loss: 0.0689, step time: 0.2622\n",
      "68/281, train_loss: 0.0888, step time: 0.2588\n",
      "69/281, train_loss: 0.0751, step time: 0.2572\n",
      "70/281, train_loss: 0.0839, step time: 0.2569\n",
      "71/281, train_loss: 0.0821, step time: 0.2600\n",
      "72/281, train_loss: 0.0650, step time: 0.2597\n",
      "73/281, train_loss: 0.0862, step time: 0.2582\n",
      "74/281, train_loss: 0.0745, step time: 0.2571\n",
      "75/281, train_loss: 0.1134, step time: 0.2578\n",
      "76/281, train_loss: 0.0588, step time: 0.2563\n",
      "77/281, train_loss: 0.0944, step time: 0.2575\n",
      "78/281, train_loss: 0.0601, step time: 0.2588\n",
      "79/281, train_loss: 0.0773, step time: 0.2563\n",
      "80/281, train_loss: 0.0749, step time: 0.2560\n",
      "81/281, train_loss: 0.1223, step time: 0.2569\n",
      "82/281, train_loss: 0.0949, step time: 0.2571\n",
      "83/281, train_loss: 0.1362, step time: 0.2625\n",
      "84/281, train_loss: 0.2148, step time: 0.2568\n",
      "85/281, train_loss: 0.1092, step time: 0.2580\n",
      "86/281, train_loss: 0.0808, step time: 0.2593\n",
      "87/281, train_loss: 0.0535, step time: 0.2561\n",
      "88/281, train_loss: 0.1149, step time: 0.2620\n",
      "89/281, train_loss: 0.0361, step time: 0.2610\n",
      "90/281, train_loss: 0.0942, step time: 0.2782\n",
      "91/281, train_loss: 0.0799, step time: 0.2556\n",
      "92/281, train_loss: 0.0605, step time: 0.2570\n",
      "93/281, train_loss: 0.1080, step time: 0.2597\n",
      "94/281, train_loss: 0.0771, step time: 0.2665\n",
      "95/281, train_loss: 0.0643, step time: 0.2592\n",
      "96/281, train_loss: 0.1052, step time: 0.2540\n",
      "97/281, train_loss: 0.0944, step time: 0.2570\n",
      "98/281, train_loss: 0.0748, step time: 0.2561\n",
      "99/281, train_loss: 0.0880, step time: 0.2585\n",
      "100/281, train_loss: 0.0881, step time: 0.2541\n",
      "101/281, train_loss: 0.1804, step time: 0.2548\n",
      "102/281, train_loss: 0.0926, step time: 0.2525\n",
      "103/281, train_loss: 0.1029, step time: 0.2514\n",
      "104/281, train_loss: 0.0780, step time: 0.2539\n",
      "105/281, train_loss: 0.1408, step time: 0.2557\n",
      "106/281, train_loss: 0.2518, step time: 0.2565\n",
      "107/281, train_loss: 0.1195, step time: 0.2599\n",
      "108/281, train_loss: 0.1159, step time: 0.2580\n",
      "109/281, train_loss: 0.2508, step time: 0.2564\n",
      "110/281, train_loss: 0.0721, step time: 0.2583\n",
      "111/281, train_loss: 0.1022, step time: 0.2523\n",
      "112/281, train_loss: 0.0607, step time: 0.2523\n",
      "113/281, train_loss: 0.4273, step time: 0.2587\n",
      "114/281, train_loss: 0.1114, step time: 0.2576\n",
      "115/281, train_loss: 0.1343, step time: 0.2574\n",
      "116/281, train_loss: 0.1171, step time: 0.2615\n",
      "117/281, train_loss: 0.0969, step time: 0.2536\n",
      "118/281, train_loss: 0.0714, step time: 0.2543\n",
      "119/281, train_loss: 0.0571, step time: 0.2519\n",
      "120/281, train_loss: 0.0665, step time: 0.2602\n",
      "121/281, train_loss: 0.1032, step time: 0.2572\n",
      "122/281, train_loss: 0.0583, step time: 0.2590\n",
      "123/281, train_loss: 0.1384, step time: 0.2590\n",
      "124/281, train_loss: 0.1182, step time: 0.2597\n",
      "125/281, train_loss: 0.0908, step time: 0.2600\n",
      "126/281, train_loss: 0.2570, step time: 0.2551\n",
      "127/281, train_loss: 0.0892, step time: 0.2539\n",
      "128/281, train_loss: 0.0543, step time: 0.2578\n",
      "129/281, train_loss: 0.0696, step time: 0.2557\n",
      "130/281, train_loss: 0.1015, step time: 0.2528\n",
      "131/281, train_loss: 0.0415, step time: 0.2529\n",
      "132/281, train_loss: 0.0994, step time: 0.2573\n",
      "133/281, train_loss: 0.1113, step time: 0.2530\n",
      "134/281, train_loss: 0.0881, step time: 0.2569\n",
      "135/281, train_loss: 0.0929, step time: 0.2550\n",
      "136/281, train_loss: 0.2406, step time: 0.2507\n",
      "137/281, train_loss: 0.0948, step time: 0.2538\n",
      "138/281, train_loss: 0.0827, step time: 0.2514\n",
      "139/281, train_loss: 0.2346, step time: 0.2548\n",
      "140/281, train_loss: 0.1156, step time: 0.2489\n",
      "141/281, train_loss: 0.0831, step time: 0.2459\n",
      "142/281, train_loss: 0.0943, step time: 0.2488\n",
      "143/281, train_loss: 0.0862, step time: 0.2529\n",
      "144/281, train_loss: 0.0820, step time: 0.2539\n",
      "145/281, train_loss: 0.2155, step time: 0.2541\n",
      "146/281, train_loss: 0.0725, step time: 0.2562\n",
      "147/281, train_loss: 0.0663, step time: 0.2506\n",
      "148/281, train_loss: 0.0656, step time: 0.2503\n",
      "149/281, train_loss: 0.0606, step time: 0.2513\n",
      "150/281, train_loss: 0.0929, step time: 0.2491\n",
      "151/281, train_loss: 0.2256, step time: 0.2539\n",
      "152/281, train_loss: 0.1183, step time: 0.2514\n",
      "153/281, train_loss: 0.1197, step time: 0.2622\n",
      "154/281, train_loss: 0.1164, step time: 0.2610\n",
      "155/281, train_loss: 0.0953, step time: 0.2583\n",
      "156/281, train_loss: 0.0892, step time: 0.2563\n",
      "157/281, train_loss: 0.1015, step time: 0.2550\n",
      "158/281, train_loss: 0.0649, step time: 0.2547\n",
      "159/281, train_loss: 0.1549, step time: 0.2578\n",
      "160/281, train_loss: 0.0941, step time: 0.2590\n",
      "161/281, train_loss: 0.0704, step time: 0.2502\n",
      "162/281, train_loss: 0.0762, step time: 0.2521\n",
      "163/281, train_loss: 0.0905, step time: 0.2526\n",
      "164/281, train_loss: 0.0694, step time: 0.2517\n",
      "165/281, train_loss: 0.2958, step time: 0.2531\n",
      "166/281, train_loss: 0.0966, step time: 0.2577\n",
      "167/281, train_loss: 0.2262, step time: 0.2493\n",
      "168/281, train_loss: 0.0677, step time: 0.2523\n",
      "169/281, train_loss: 0.0675, step time: 0.2538\n",
      "170/281, train_loss: 0.0856, step time: 0.2525\n",
      "171/281, train_loss: 0.2275, step time: 0.2521\n",
      "172/281, train_loss: 0.0726, step time: 0.2555\n",
      "173/281, train_loss: 0.2608, step time: 0.2582\n",
      "174/281, train_loss: 0.1144, step time: 0.2580\n",
      "175/281, train_loss: 0.0966, step time: 0.2571\n",
      "176/281, train_loss: 0.1012, step time: 0.2500\n",
      "177/281, train_loss: 0.0837, step time: 0.2555\n",
      "178/281, train_loss: 0.0650, step time: 0.2567\n",
      "179/281, train_loss: 0.2270, step time: 0.2511\n",
      "180/281, train_loss: 0.0735, step time: 0.2522\n",
      "181/281, train_loss: 0.2371, step time: 0.2553\n",
      "182/281, train_loss: 0.0610, step time: 0.2587\n",
      "183/281, train_loss: 0.0589, step time: 0.2516\n",
      "184/281, train_loss: 0.0882, step time: 0.2559\n",
      "185/281, train_loss: 0.1018, step time: 0.2598\n",
      "186/281, train_loss: 0.0578, step time: 0.2542\n",
      "187/281, train_loss: 0.1511, step time: 0.2556\n",
      "188/281, train_loss: 0.0701, step time: 0.2521\n",
      "189/281, train_loss: 0.0596, step time: 0.2566\n",
      "190/281, train_loss: 0.0901, step time: 0.2525\n",
      "191/281, train_loss: 0.1105, step time: 0.2581\n",
      "192/281, train_loss: 0.0813, step time: 0.2587\n",
      "193/281, train_loss: 0.1099, step time: 0.2584\n",
      "194/281, train_loss: 0.2296, step time: 0.2584\n",
      "195/281, train_loss: 0.0810, step time: 0.2590\n",
      "196/281, train_loss: 0.0623, step time: 0.2594\n",
      "197/281, train_loss: 0.0609, step time: 0.2568\n",
      "198/281, train_loss: 0.2450, step time: 0.2578\n",
      "199/281, train_loss: 0.0776, step time: 0.2585\n",
      "200/281, train_loss: 0.0797, step time: 0.2527\n",
      "201/281, train_loss: 0.1123, step time: 0.2572\n",
      "202/281, train_loss: 0.0987, step time: 0.2608\n",
      "203/281, train_loss: 0.0867, step time: 0.2571\n",
      "204/281, train_loss: 0.2277, step time: 0.2541\n",
      "205/281, train_loss: 0.0749, step time: 0.2535\n",
      "206/281, train_loss: 0.0326, step time: 0.2580\n",
      "207/281, train_loss: 0.0866, step time: 0.2591\n",
      "208/281, train_loss: 0.2404, step time: 0.2641\n",
      "209/281, train_loss: 0.0924, step time: 0.2604\n",
      "210/281, train_loss: 0.0635, step time: 0.2613\n",
      "211/281, train_loss: 0.0774, step time: 0.2631\n",
      "212/281, train_loss: 0.1131, step time: 0.2578\n",
      "213/281, train_loss: 0.0741, step time: 0.2576\n",
      "214/281, train_loss: 0.1657, step time: 0.2585\n",
      "215/281, train_loss: 0.0902, step time: 0.2522\n",
      "216/281, train_loss: 0.1149, step time: 0.2525\n",
      "217/281, train_loss: 0.1005, step time: 0.2544\n",
      "218/281, train_loss: 0.2679, step time: 0.2567\n",
      "219/281, train_loss: 0.1245, step time: 0.2544\n",
      "220/281, train_loss: 0.1013, step time: 0.2554\n",
      "221/281, train_loss: 0.0433, step time: 0.2597\n",
      "222/281, train_loss: 0.0905, step time: 0.2584\n",
      "223/281, train_loss: 0.1339, step time: 0.2583\n",
      "224/281, train_loss: 0.2818, step time: 0.2616\n",
      "225/281, train_loss: 0.2285, step time: 0.2569\n",
      "226/281, train_loss: 0.0973, step time: 0.2587\n",
      "227/281, train_loss: 0.0815, step time: 0.2572\n",
      "228/281, train_loss: 0.0788, step time: 0.2594\n",
      "229/281, train_loss: 0.0710, step time: 0.2553\n",
      "230/281, train_loss: 0.0933, step time: 0.2545\n",
      "231/281, train_loss: 0.0648, step time: 0.2530\n",
      "232/281, train_loss: 0.0758, step time: 0.2591\n",
      "233/281, train_loss: 0.1095, step time: 0.2578\n",
      "234/281, train_loss: 0.0714, step time: 0.2574\n",
      "235/281, train_loss: 0.0979, step time: 0.2568\n",
      "236/281, train_loss: 0.1200, step time: 0.2627\n",
      "237/281, train_loss: 0.0896, step time: 0.2562\n",
      "238/281, train_loss: 0.0773, step time: 0.2553\n",
      "239/281, train_loss: 0.0814, step time: 0.2712\n",
      "240/281, train_loss: 0.2818, step time: 0.2560\n",
      "241/281, train_loss: 0.1136, step time: 0.2615\n",
      "242/281, train_loss: 0.0833, step time: 0.2633\n",
      "243/281, train_loss: 0.2316, step time: 0.2575\n",
      "244/281, train_loss: 0.0480, step time: 0.2533\n",
      "245/281, train_loss: 0.0715, step time: 0.2498\n",
      "246/281, train_loss: 0.0975, step time: 0.2566\n",
      "247/281, train_loss: 0.0850, step time: 0.2534\n",
      "248/281, train_loss: 0.1111, step time: 0.2569\n",
      "249/281, train_loss: 0.0923, step time: 0.2503\n",
      "250/281, train_loss: 0.0949, step time: 0.2583\n",
      "251/281, train_loss: 0.0797, step time: 0.2596\n",
      "252/281, train_loss: 0.0763, step time: 0.2553\n",
      "253/281, train_loss: 0.1182, step time: 0.2580\n",
      "254/281, train_loss: 0.1206, step time: 0.2617\n",
      "255/281, train_loss: 0.1154, step time: 0.2568\n",
      "256/281, train_loss: 0.0761, step time: 0.2502\n",
      "257/281, train_loss: 0.1053, step time: 0.2548\n",
      "258/281, train_loss: 0.0847, step time: 0.2524\n",
      "259/281, train_loss: 0.0429, step time: 0.2503\n",
      "260/281, train_loss: 0.1366, step time: 0.2524\n",
      "261/281, train_loss: 0.1075, step time: 0.2540\n",
      "262/281, train_loss: 0.0623, step time: 0.2525\n",
      "263/281, train_loss: 0.0485, step time: 0.2551\n",
      "264/281, train_loss: 0.2611, step time: 0.2581\n",
      "265/281, train_loss: 0.0699, step time: 0.2583\n",
      "266/281, train_loss: 0.0628, step time: 0.2503\n",
      "267/281, train_loss: 0.0849, step time: 0.2481\n",
      "268/281, train_loss: 0.1372, step time: 0.2469\n",
      "269/281, train_loss: 0.2746, step time: 0.2521\n",
      "270/281, train_loss: 0.2578, step time: 0.2486\n",
      "271/281, train_loss: 0.1108, step time: 0.2518\n",
      "272/281, train_loss: 0.0876, step time: 0.2517\n",
      "273/281, train_loss: 0.3849, step time: 0.2517\n",
      "274/281, train_loss: 0.2285, step time: 0.2554\n",
      "275/281, train_loss: 0.0858, step time: 0.2490\n",
      "276/281, train_loss: 0.0686, step time: 0.2604\n",
      "277/281, train_loss: 0.1232, step time: 0.2600\n",
      "278/281, train_loss: 0.0663, step time: 0.2609\n",
      "279/281, train_loss: 0.0743, step time: 0.2809\n",
      "280/281, train_loss: 0.0838, step time: 0.2602\n",
      "281/281, train_loss: 0.0629, step time: 0.2585\n",
      "282/281, train_loss: 0.3909, step time: 0.1554\n",
      "epoch 63 average loss: 0.1128\n",
      "saved new best metric model\n",
      "current epoch: 63 current mean dice: 0.8835 tc: 0.8748 wt: 0.9193 et: 0.8651\n",
      "best mean dice: 0.8835 at epoch: 63\n",
      "time consuming of epoch 63 is: 405.2738\n",
      "----------\n",
      "epoch 64/200\n",
      "1/281, train_loss: 0.0944, step time: 0.2674\n",
      "2/281, train_loss: 0.0979, step time: 0.2616\n",
      "3/281, train_loss: 0.1217, step time: 0.2642\n",
      "4/281, train_loss: 0.0801, step time: 0.2584\n",
      "5/281, train_loss: 0.1121, step time: 0.2517\n",
      "6/281, train_loss: 0.0875, step time: 0.2526\n",
      "7/281, train_loss: 0.1348, step time: 0.2568\n",
      "8/281, train_loss: 0.1351, step time: 0.2510\n",
      "9/281, train_loss: 0.0677, step time: 0.2493\n",
      "10/281, train_loss: 0.2258, step time: 0.2485\n",
      "11/281, train_loss: 0.0918, step time: 0.2501\n",
      "12/281, train_loss: 0.4133, step time: 0.2595\n",
      "13/281, train_loss: 0.0614, step time: 0.2527\n",
      "14/281, train_loss: 0.0849, step time: 0.2529\n",
      "15/281, train_loss: 0.0695, step time: 0.2528\n",
      "16/281, train_loss: 0.0565, step time: 0.2538\n",
      "17/281, train_loss: 0.0722, step time: 0.2640\n",
      "18/281, train_loss: 0.0576, step time: 0.2541\n",
      "19/281, train_loss: 0.0779, step time: 0.2558\n",
      "20/281, train_loss: 0.2327, step time: 0.2606\n",
      "21/281, train_loss: 0.0931, step time: 0.2536\n",
      "22/281, train_loss: 0.1088, step time: 0.2533\n",
      "23/281, train_loss: 0.0768, step time: 0.2600\n",
      "24/281, train_loss: 0.0555, step time: 0.2583\n",
      "25/281, train_loss: 0.0710, step time: 0.2543\n",
      "26/281, train_loss: 0.0712, step time: 0.2532\n",
      "27/281, train_loss: 0.0598, step time: 0.2524\n",
      "28/281, train_loss: 0.2326, step time: 0.2564\n",
      "29/281, train_loss: 0.0678, step time: 0.2590\n",
      "30/281, train_loss: 0.0927, step time: 0.2718\n",
      "31/281, train_loss: 0.0787, step time: 0.2796\n",
      "32/281, train_loss: 0.1009, step time: 0.2578\n",
      "33/281, train_loss: 0.0682, step time: 0.2527\n",
      "34/281, train_loss: 0.0781, step time: 0.2564\n",
      "35/281, train_loss: 0.0566, step time: 0.2602\n",
      "36/281, train_loss: 0.2260, step time: 0.2570\n",
      "37/281, train_loss: 0.1135, step time: 0.2641\n",
      "38/281, train_loss: 0.2231, step time: 0.2556\n",
      "39/281, train_loss: 0.0660, step time: 0.2544\n",
      "40/281, train_loss: 0.0776, step time: 0.2567\n",
      "41/281, train_loss: 0.0506, step time: 0.2600\n",
      "42/281, train_loss: 0.1130, step time: 0.2489\n",
      "43/281, train_loss: 0.0772, step time: 0.2511\n",
      "44/281, train_loss: 0.0837, step time: 0.2531\n",
      "45/281, train_loss: 0.1283, step time: 0.2555\n",
      "46/281, train_loss: 0.0802, step time: 0.2531\n",
      "47/281, train_loss: 0.2689, step time: 0.2532\n",
      "48/281, train_loss: 0.2300, step time: 0.2496\n",
      "49/281, train_loss: 0.0748, step time: 0.2636\n",
      "50/281, train_loss: 0.0664, step time: 0.2587\n",
      "51/281, train_loss: 0.2463, step time: 0.2561\n",
      "52/281, train_loss: 0.0676, step time: 0.2553\n",
      "53/281, train_loss: 0.2323, step time: 0.2600\n",
      "54/281, train_loss: 0.0972, step time: 0.2608\n",
      "55/281, train_loss: 0.2499, step time: 0.2560\n",
      "56/281, train_loss: 0.1051, step time: 0.2569\n",
      "57/281, train_loss: 0.1064, step time: 0.2566\n",
      "58/281, train_loss: 0.0575, step time: 0.2592\n",
      "59/281, train_loss: 0.0651, step time: 0.2584\n",
      "60/281, train_loss: 0.1271, step time: 0.2562\n",
      "61/281, train_loss: 0.0418, step time: 0.2627\n",
      "62/281, train_loss: 0.2467, step time: 0.2561\n",
      "63/281, train_loss: 0.0755, step time: 0.2534\n",
      "64/281, train_loss: 0.0774, step time: 0.2626\n",
      "65/281, train_loss: 0.0976, step time: 0.2550\n",
      "66/281, train_loss: 0.0745, step time: 0.2502\n",
      "67/281, train_loss: 0.0936, step time: 0.2549\n",
      "68/281, train_loss: 0.0931, step time: 0.2555\n",
      "69/281, train_loss: 0.0838, step time: 0.2646\n",
      "70/281, train_loss: 0.0991, step time: 0.2517\n",
      "71/281, train_loss: 0.1061, step time: 0.2517\n",
      "72/281, train_loss: 0.1208, step time: 0.2486\n",
      "73/281, train_loss: 0.0945, step time: 0.2578\n",
      "74/281, train_loss: 0.2415, step time: 0.2549\n",
      "75/281, train_loss: 0.2351, step time: 0.2542\n",
      "76/281, train_loss: 0.0610, step time: 0.2531\n",
      "77/281, train_loss: 0.0716, step time: 0.2555\n",
      "78/281, train_loss: 0.0867, step time: 0.2539\n",
      "79/281, train_loss: 0.1023, step time: 0.2540\n",
      "80/281, train_loss: 0.0521, step time: 0.2534\n",
      "81/281, train_loss: 0.3097, step time: 0.2506\n",
      "82/281, train_loss: 0.0724, step time: 0.2561\n",
      "83/281, train_loss: 0.0571, step time: 0.2546\n",
      "84/281, train_loss: 0.2487, step time: 0.2552\n",
      "85/281, train_loss: 0.0694, step time: 0.2540\n",
      "86/281, train_loss: 0.0685, step time: 0.2535\n",
      "87/281, train_loss: 0.0828, step time: 0.2478\n",
      "88/281, train_loss: 0.0899, step time: 0.2523\n",
      "89/281, train_loss: 0.0821, step time: 0.2510\n",
      "90/281, train_loss: 0.0846, step time: 0.2533\n",
      "91/281, train_loss: 0.0545, step time: 0.2526\n",
      "92/281, train_loss: 0.2369, step time: 0.2510\n",
      "93/281, train_loss: 0.3952, step time: 0.2523\n",
      "94/281, train_loss: 0.1051, step time: 0.2550\n",
      "95/281, train_loss: 0.1961, step time: 0.2239\n",
      "96/281, train_loss: 0.1168, step time: 0.2553\n",
      "97/281, train_loss: 0.0977, step time: 0.2606\n",
      "98/281, train_loss: 0.1064, step time: 0.2544\n",
      "99/281, train_loss: 0.0703, step time: 0.2544\n",
      "100/281, train_loss: 0.1070, step time: 0.2517\n",
      "101/281, train_loss: 0.1612, step time: 0.2531\n",
      "102/281, train_loss: 0.0555, step time: 0.2506\n",
      "103/281, train_loss: 0.0765, step time: 0.2494\n",
      "104/281, train_loss: 0.1090, step time: 0.2470\n",
      "105/281, train_loss: 0.1342, step time: 0.2506\n",
      "106/281, train_loss: 0.0925, step time: 0.2568\n",
      "107/281, train_loss: 0.1126, step time: 0.2533\n",
      "108/281, train_loss: 0.2475, step time: 0.2493\n",
      "109/281, train_loss: 0.1075, step time: 0.2502\n",
      "110/281, train_loss: 0.0965, step time: 0.2505\n",
      "111/281, train_loss: 0.1006, step time: 0.2526\n",
      "112/281, train_loss: 0.0772, step time: 0.2524\n",
      "113/281, train_loss: 0.1324, step time: 0.2510\n",
      "114/281, train_loss: 0.0645, step time: 0.2689\n",
      "115/281, train_loss: 0.0765, step time: 0.2527\n",
      "116/281, train_loss: 0.0837, step time: 0.2507\n",
      "117/281, train_loss: 0.0984, step time: 0.2487\n",
      "118/281, train_loss: 0.0911, step time: 0.2497\n",
      "119/281, train_loss: 0.0567, step time: 0.2522\n",
      "120/281, train_loss: 0.0634, step time: 0.2540\n",
      "121/281, train_loss: 0.1750, step time: 0.2548\n",
      "122/281, train_loss: 0.2427, step time: 0.2534\n",
      "123/281, train_loss: 0.0671, step time: 0.2570\n",
      "124/281, train_loss: 0.1421, step time: 0.2594\n",
      "125/281, train_loss: 0.0989, step time: 0.2536\n",
      "126/281, train_loss: 0.0840, step time: 0.2486\n",
      "127/281, train_loss: 0.1125, step time: 0.2528\n",
      "128/281, train_loss: 0.0642, step time: 0.2464\n",
      "129/281, train_loss: 0.2443, step time: 0.2461\n",
      "130/281, train_loss: 0.0494, step time: 0.2487\n",
      "131/281, train_loss: 0.0997, step time: 0.2520\n",
      "132/281, train_loss: 0.2547, step time: 0.2518\n",
      "133/281, train_loss: 0.0789, step time: 0.2495\n",
      "134/281, train_loss: 0.0551, step time: 0.2554\n",
      "135/281, train_loss: 0.0891, step time: 0.2507\n",
      "136/281, train_loss: 0.1210, step time: 0.2543\n",
      "137/281, train_loss: 0.2321, step time: 0.2523\n",
      "138/281, train_loss: 0.2385, step time: 0.2579\n",
      "139/281, train_loss: 0.1193, step time: 0.2581\n",
      "140/281, train_loss: 0.0785, step time: 0.2562\n",
      "141/281, train_loss: 0.0812, step time: 0.2556\n",
      "142/281, train_loss: 0.0611, step time: 0.2553\n",
      "143/281, train_loss: 0.1403, step time: 0.2551\n",
      "144/281, train_loss: 0.1198, step time: 0.2533\n",
      "145/281, train_loss: 0.0830, step time: 0.2498\n",
      "146/281, train_loss: 0.1103, step time: 0.2564\n",
      "147/281, train_loss: 0.1457, step time: 0.2582\n",
      "148/281, train_loss: 0.0664, step time: 0.2544\n",
      "149/281, train_loss: 0.0542, step time: 0.2521\n",
      "150/281, train_loss: 0.1154, step time: 0.2555\n",
      "151/281, train_loss: 0.0862, step time: 0.2554\n",
      "152/281, train_loss: 0.0800, step time: 0.2537\n",
      "153/281, train_loss: 0.1382, step time: 0.2517\n",
      "154/281, train_loss: 0.1522, step time: 0.2568\n",
      "155/281, train_loss: 0.1200, step time: 0.2538\n",
      "156/281, train_loss: 0.0870, step time: 0.2529\n",
      "157/281, train_loss: 0.0897, step time: 0.2557\n",
      "158/281, train_loss: 0.0693, step time: 0.2575\n",
      "159/281, train_loss: 0.1548, step time: 0.2489\n",
      "160/281, train_loss: 0.0801, step time: 0.2471\n",
      "161/281, train_loss: 0.1223, step time: 0.2482\n",
      "162/281, train_loss: 0.0990, step time: 0.2518\n",
      "163/281, train_loss: 0.0846, step time: 0.2501\n",
      "164/281, train_loss: 0.0652, step time: 0.2494\n",
      "165/281, train_loss: 0.1242, step time: 0.2556\n",
      "166/281, train_loss: 0.1169, step time: 0.2528\n",
      "167/281, train_loss: 0.0940, step time: 0.2538\n",
      "168/281, train_loss: 0.0673, step time: 0.2567\n",
      "169/281, train_loss: 0.0643, step time: 0.2579\n",
      "170/281, train_loss: 0.1111, step time: 0.2455\n",
      "171/281, train_loss: 0.0636, step time: 0.2434\n",
      "172/281, train_loss: 0.2487, step time: 0.2507\n",
      "173/281, train_loss: 0.0794, step time: 0.2539\n",
      "174/281, train_loss: 0.0881, step time: 0.2466\n",
      "175/281, train_loss: 0.0810, step time: 0.2539\n",
      "176/281, train_loss: 0.1452, step time: 0.2467\n",
      "177/281, train_loss: 0.2346, step time: 0.2448\n",
      "178/281, train_loss: 0.0919, step time: 0.2460\n",
      "179/281, train_loss: 0.0836, step time: 0.2501\n",
      "180/281, train_loss: 0.0935, step time: 0.2543\n",
      "181/281, train_loss: 0.1313, step time: 0.2501\n",
      "182/281, train_loss: 0.1673, step time: 0.2479\n",
      "183/281, train_loss: 0.1144, step time: 0.2457\n",
      "184/281, train_loss: 0.2605, step time: 0.2495\n",
      "185/281, train_loss: 0.1266, step time: 0.2534\n",
      "186/281, train_loss: 0.0929, step time: 0.2485\n",
      "187/281, train_loss: 0.0730, step time: 0.2507\n",
      "188/281, train_loss: 0.0837, step time: 0.2476\n",
      "189/281, train_loss: 0.0558, step time: 0.2519\n",
      "190/281, train_loss: 0.1083, step time: 0.2533\n",
      "191/281, train_loss: 0.0915, step time: 0.2510\n",
      "192/281, train_loss: 0.1401, step time: 0.2594\n",
      "193/281, train_loss: 0.0478, step time: 0.2462\n",
      "194/281, train_loss: 0.1557, step time: 0.2461\n",
      "195/281, train_loss: 0.0871, step time: 0.2472\n",
      "196/281, train_loss: 0.0891, step time: 0.2485\n",
      "197/281, train_loss: 0.0772, step time: 0.2493\n",
      "198/281, train_loss: 0.0904, step time: 0.2486\n",
      "199/281, train_loss: 0.1277, step time: 0.2535\n",
      "200/281, train_loss: 0.1066, step time: 0.2532\n",
      "201/281, train_loss: 0.0562, step time: 0.2454\n",
      "202/281, train_loss: 0.0456, step time: 0.2467\n",
      "203/281, train_loss: 0.0723, step time: 0.2539\n",
      "204/281, train_loss: 0.0636, step time: 0.2555\n",
      "205/281, train_loss: 0.0893, step time: 0.2538\n",
      "206/281, train_loss: 0.2714, step time: 0.2518\n",
      "207/281, train_loss: 0.0955, step time: 0.2499\n",
      "208/281, train_loss: 0.0482, step time: 0.2584\n",
      "209/281, train_loss: 0.0954, step time: 0.2565\n",
      "210/281, train_loss: 0.0708, step time: 0.2483\n",
      "211/281, train_loss: 0.0821, step time: 0.2501\n",
      "212/281, train_loss: 0.0757, step time: 0.2532\n",
      "213/281, train_loss: 0.0901, step time: 0.2530\n",
      "214/281, train_loss: 0.3814, step time: 0.2548\n",
      "215/281, train_loss: 0.2766, step time: 0.2716\n",
      "216/281, train_loss: 0.0944, step time: 0.2496\n",
      "217/281, train_loss: 0.0685, step time: 0.2536\n",
      "218/281, train_loss: 0.0506, step time: 0.2514\n",
      "219/281, train_loss: 0.2322, step time: 0.2533\n",
      "220/281, train_loss: 0.0878, step time: 0.2555\n",
      "221/281, train_loss: 0.1551, step time: 0.2558\n",
      "222/281, train_loss: 0.1044, step time: 0.2531\n",
      "223/281, train_loss: 0.0569, step time: 0.2493\n",
      "224/281, train_loss: 0.0804, step time: 0.2524\n",
      "225/281, train_loss: 0.2493, step time: 0.2537\n",
      "226/281, train_loss: 0.0926, step time: 0.2499\n",
      "227/281, train_loss: 0.1051, step time: 0.2537\n",
      "228/281, train_loss: 0.0526, step time: 0.2507\n",
      "229/281, train_loss: 0.1052, step time: 0.2514\n",
      "230/281, train_loss: 0.0972, step time: 0.2520\n",
      "231/281, train_loss: 0.0649, step time: 0.2524\n",
      "232/281, train_loss: 0.1343, step time: 0.2548\n",
      "233/281, train_loss: 0.1262, step time: 0.2496\n",
      "234/281, train_loss: 0.2480, step time: 0.2458\n",
      "235/281, train_loss: 0.0695, step time: 0.2423\n",
      "236/281, train_loss: 0.0914, step time: 0.2513\n",
      "237/281, train_loss: 0.0527, step time: 0.2496\n",
      "238/281, train_loss: 0.0682, step time: 0.2499\n",
      "239/281, train_loss: 0.2448, step time: 0.2523\n",
      "240/281, train_loss: 0.0971, step time: 0.2523\n",
      "241/281, train_loss: 0.2532, step time: 0.2524\n",
      "242/281, train_loss: 0.0788, step time: 0.2493\n",
      "243/281, train_loss: 0.0690, step time: 0.2492\n",
      "244/281, train_loss: 0.2449, step time: 0.2573\n",
      "245/281, train_loss: 0.0585, step time: 0.2461\n",
      "246/281, train_loss: 0.1167, step time: 0.2498\n",
      "247/281, train_loss: 0.0955, step time: 0.2504\n",
      "248/281, train_loss: 0.0986, step time: 0.2503\n",
      "249/281, train_loss: 0.0648, step time: 0.2516\n",
      "250/281, train_loss: 0.0877, step time: 0.2539\n",
      "251/281, train_loss: 0.2676, step time: 0.2475\n",
      "252/281, train_loss: 0.2297, step time: 0.2555\n",
      "253/281, train_loss: 0.0653, step time: 0.2541\n",
      "254/281, train_loss: 0.0586, step time: 0.2503\n",
      "255/281, train_loss: 0.1262, step time: 0.2512\n",
      "256/281, train_loss: 0.1249, step time: 0.2492\n",
      "257/281, train_loss: 0.0646, step time: 0.2452\n",
      "258/281, train_loss: 0.1087, step time: 0.2453\n",
      "259/281, train_loss: 0.0961, step time: 0.2462\n",
      "260/281, train_loss: 0.0809, step time: 0.2487\n",
      "261/281, train_loss: 0.0576, step time: 0.2537\n",
      "262/281, train_loss: 0.0746, step time: 0.2540\n",
      "263/281, train_loss: 0.0770, step time: 0.2492\n",
      "264/281, train_loss: 0.0714, step time: 0.2475\n",
      "265/281, train_loss: 0.0830, step time: 0.2486\n",
      "266/281, train_loss: 0.0811, step time: 0.2458\n",
      "267/281, train_loss: 0.1036, step time: 0.2476\n",
      "268/281, train_loss: 0.1210, step time: 0.2520\n",
      "269/281, train_loss: 0.2429, step time: 0.2456\n",
      "270/281, train_loss: 0.1031, step time: 0.2514\n",
      "271/281, train_loss: 0.0958, step time: 0.2451\n",
      "272/281, train_loss: 0.0557, step time: 0.2516\n",
      "273/281, train_loss: 0.1053, step time: 0.2543\n",
      "274/281, train_loss: 0.1033, step time: 0.2550\n",
      "275/281, train_loss: 0.0780, step time: 0.2472\n",
      "276/281, train_loss: 0.1130, step time: 0.2487\n",
      "277/281, train_loss: 0.2466, step time: 0.2549\n",
      "278/281, train_loss: 0.2507, step time: 0.2501\n",
      "279/281, train_loss: 0.1134, step time: 0.2495\n",
      "280/281, train_loss: 0.0746, step time: 0.2493\n",
      "281/281, train_loss: 0.0551, step time: 0.2490\n",
      "282/281, train_loss: 0.1064, step time: 0.1474\n",
      "epoch 64 average loss: 0.1145\n",
      "current epoch: 64 current mean dice: 0.8824 tc: 0.8745 wt: 0.9162 et: 0.8662\n",
      "best mean dice: 0.8835 at epoch: 63\n",
      "time consuming of epoch 64 is: 376.8597\n",
      "----------\n",
      "epoch 65/200\n",
      "1/281, train_loss: 0.1219, step time: 0.2640\n",
      "2/281, train_loss: 0.0775, step time: 0.2600\n",
      "3/281, train_loss: 0.0852, step time: 0.2545\n",
      "4/281, train_loss: 0.0694, step time: 0.2545\n",
      "5/281, train_loss: 0.2463, step time: 0.2541\n",
      "6/281, train_loss: 0.1336, step time: 0.2513\n",
      "7/281, train_loss: 0.0945, step time: 0.2526\n",
      "8/281, train_loss: 0.0704, step time: 0.2567\n",
      "9/281, train_loss: 0.0522, step time: 0.2546\n",
      "10/281, train_loss: 0.0558, step time: 0.2523\n",
      "11/281, train_loss: 0.2691, step time: 0.2563\n",
      "12/281, train_loss: 0.0967, step time: 0.2579\n",
      "13/281, train_loss: 0.1118, step time: 0.2523\n",
      "14/281, train_loss: 0.0858, step time: 0.2546\n",
      "15/281, train_loss: 0.2261, step time: 0.2532\n",
      "16/281, train_loss: 0.0994, step time: 0.2482\n",
      "17/281, train_loss: 0.1046, step time: 0.2569\n",
      "18/281, train_loss: 0.2303, step time: 0.2527\n",
      "19/281, train_loss: 0.0821, step time: 0.2457\n",
      "20/281, train_loss: 0.0496, step time: 0.2534\n",
      "21/281, train_loss: 0.0774, step time: 0.2552\n",
      "22/281, train_loss: 0.0580, step time: 0.2554\n",
      "23/281, train_loss: 0.1035, step time: 0.2557\n",
      "24/281, train_loss: 0.2322, step time: 0.2603\n",
      "25/281, train_loss: 0.0779, step time: 0.2529\n",
      "26/281, train_loss: 0.2442, step time: 0.2551\n",
      "27/281, train_loss: 0.0935, step time: 0.2610\n",
      "28/281, train_loss: 0.0673, step time: 0.2601\n",
      "29/281, train_loss: 0.0394, step time: 0.2487\n",
      "30/281, train_loss: 0.0985, step time: 0.2532\n",
      "31/281, train_loss: 0.1291, step time: 0.2503\n",
      "32/281, train_loss: 0.0980, step time: 0.2524\n",
      "33/281, train_loss: 0.0815, step time: 0.2579\n",
      "34/281, train_loss: 0.2777, step time: 0.2580\n",
      "35/281, train_loss: 0.0667, step time: 0.2499\n",
      "36/281, train_loss: 0.1047, step time: 0.2574\n",
      "37/281, train_loss: 0.0486, step time: 0.2586\n",
      "38/281, train_loss: 0.0508, step time: 0.2514\n",
      "39/281, train_loss: 0.0698, step time: 0.2550\n",
      "40/281, train_loss: 0.1093, step time: 0.2499\n",
      "41/281, train_loss: 0.1202, step time: 0.2554\n",
      "42/281, train_loss: 0.0649, step time: 0.2516\n",
      "43/281, train_loss: 0.0905, step time: 0.2534\n",
      "44/281, train_loss: 0.2325, step time: 0.2497\n",
      "45/281, train_loss: 0.0819, step time: 0.2465\n",
      "46/281, train_loss: 0.1036, step time: 0.2542\n",
      "47/281, train_loss: 0.0609, step time: 0.2480\n",
      "48/281, train_loss: 0.1015, step time: 0.2766\n",
      "49/281, train_loss: 0.0672, step time: 0.2782\n",
      "50/281, train_loss: 0.2290, step time: 0.2591\n",
      "51/281, train_loss: 0.0620, step time: 0.2499\n",
      "52/281, train_loss: 0.0932, step time: 0.2545\n",
      "53/281, train_loss: 0.1147, step time: 0.2560\n",
      "54/281, train_loss: 0.0816, step time: 0.2512\n",
      "55/281, train_loss: 0.0932, step time: 0.2549\n",
      "56/281, train_loss: 0.1012, step time: 0.2606\n",
      "57/281, train_loss: 0.0543, step time: 0.2534\n",
      "58/281, train_loss: 0.1261, step time: 0.2537\n",
      "59/281, train_loss: 0.0508, step time: 0.2490\n",
      "60/281, train_loss: 0.0551, step time: 0.2460\n",
      "61/281, train_loss: 0.1111, step time: 0.2516\n",
      "62/281, train_loss: 0.1158, step time: 0.2590\n",
      "63/281, train_loss: 0.1070, step time: 0.2576\n",
      "64/281, train_loss: 0.1339, step time: 0.2562\n",
      "65/281, train_loss: 0.0635, step time: 0.2509\n",
      "66/281, train_loss: 0.0856, step time: 0.2521\n",
      "67/281, train_loss: 0.0841, step time: 0.2557\n",
      "68/281, train_loss: 0.1522, step time: 0.2527\n",
      "69/281, train_loss: 0.2386, step time: 0.2616\n",
      "70/281, train_loss: 0.0569, step time: 0.2546\n",
      "71/281, train_loss: 0.0900, step time: 0.2506\n",
      "72/281, train_loss: 0.1085, step time: 0.2530\n",
      "73/281, train_loss: 0.0952, step time: 0.2497\n",
      "74/281, train_loss: 0.0668, step time: 0.2600\n",
      "75/281, train_loss: 0.0791, step time: 0.2562\n",
      "76/281, train_loss: 0.0644, step time: 0.2462\n",
      "77/281, train_loss: 0.0782, step time: 0.2477\n",
      "78/281, train_loss: 0.0770, step time: 0.2570\n",
      "79/281, train_loss: 0.0841, step time: 0.2549\n",
      "80/281, train_loss: 0.0833, step time: 0.2490\n",
      "81/281, train_loss: 0.2578, step time: 0.2507\n",
      "82/281, train_loss: 0.2419, step time: 0.2581\n",
      "83/281, train_loss: 0.1080, step time: 0.2552\n",
      "84/281, train_loss: 0.2335, step time: 0.2483\n",
      "85/281, train_loss: 0.0747, step time: 0.2476\n",
      "86/281, train_loss: 0.0673, step time: 0.2500\n",
      "87/281, train_loss: 0.0772, step time: 0.2497\n",
      "88/281, train_loss: 0.0649, step time: 0.2521\n",
      "89/281, train_loss: 0.0835, step time: 0.2738\n",
      "90/281, train_loss: 0.0821, step time: 0.2574\n",
      "91/281, train_loss: 0.1418, step time: 0.2555\n",
      "92/281, train_loss: 0.0592, step time: 0.2514\n",
      "93/281, train_loss: 0.0731, step time: 0.2507\n",
      "94/281, train_loss: 0.0926, step time: 0.2523\n",
      "95/281, train_loss: 0.0429, step time: 0.2509\n",
      "96/281, train_loss: 0.0828, step time: 0.2465\n",
      "97/281, train_loss: 0.0797, step time: 0.2507\n",
      "98/281, train_loss: 0.0855, step time: 0.2560\n",
      "99/281, train_loss: 0.1067, step time: 0.2546\n",
      "100/281, train_loss: 0.1160, step time: 0.2569\n",
      "101/281, train_loss: 0.1033, step time: 0.2551\n",
      "102/281, train_loss: 0.0558, step time: 0.2562\n",
      "103/281, train_loss: 0.1182, step time: 0.2590\n",
      "104/281, train_loss: 0.1628, step time: 0.2592\n",
      "105/281, train_loss: 0.0962, step time: 0.2577\n",
      "106/281, train_loss: 0.2215, step time: 0.2533\n",
      "107/281, train_loss: 0.1961, step time: 0.2498\n",
      "108/281, train_loss: 0.0719, step time: 0.2564\n",
      "109/281, train_loss: 0.0829, step time: 0.2560\n",
      "110/281, train_loss: 0.0914, step time: 0.2551\n",
      "111/281, train_loss: 0.0442, step time: 0.2506\n",
      "112/281, train_loss: 0.0628, step time: 0.2545\n",
      "113/281, train_loss: 0.0556, step time: 0.2480\n",
      "114/281, train_loss: 0.0927, step time: 0.2535\n",
      "115/281, train_loss: 0.1569, step time: 0.2573\n",
      "116/281, train_loss: 0.0812, step time: 0.2563\n",
      "117/281, train_loss: 0.0998, step time: 0.2544\n",
      "118/281, train_loss: 0.0641, step time: 0.2643\n",
      "119/281, train_loss: 0.0521, step time: 0.2569\n",
      "120/281, train_loss: 0.2213, step time: 0.2535\n",
      "121/281, train_loss: 0.0678, step time: 0.2486\n",
      "122/281, train_loss: 0.0558, step time: 0.2515\n",
      "123/281, train_loss: 0.1008, step time: 0.2568\n",
      "124/281, train_loss: 0.0823, step time: 0.2559\n",
      "125/281, train_loss: 0.0521, step time: 0.2510\n",
      "126/281, train_loss: 0.0604, step time: 0.2543\n",
      "127/281, train_loss: 0.0997, step time: 0.2561\n",
      "128/281, train_loss: 0.0952, step time: 0.2925\n",
      "129/281, train_loss: 0.2597, step time: 0.2939\n",
      "130/281, train_loss: 0.4017, step time: 0.2524\n",
      "131/281, train_loss: 0.0561, step time: 0.2563\n",
      "132/281, train_loss: 0.0637, step time: 0.2575\n",
      "133/281, train_loss: 0.1073, step time: 0.2570\n",
      "134/281, train_loss: 0.0942, step time: 0.2613\n",
      "135/281, train_loss: 0.0837, step time: 0.2573\n",
      "136/281, train_loss: 0.2326, step time: 0.2562\n",
      "137/281, train_loss: 0.1172, step time: 0.2548\n",
      "138/281, train_loss: 0.2336, step time: 0.2557\n",
      "139/281, train_loss: 0.1214, step time: 0.2537\n",
      "140/281, train_loss: 0.1081, step time: 0.2570\n",
      "141/281, train_loss: 0.1016, step time: 0.2552\n",
      "142/281, train_loss: 0.0474, step time: 0.2618\n",
      "143/281, train_loss: 0.1002, step time: 0.2520\n",
      "144/281, train_loss: 0.0834, step time: 0.2550\n",
      "145/281, train_loss: 0.0690, step time: 0.2548\n",
      "146/281, train_loss: 0.0849, step time: 0.2578\n",
      "147/281, train_loss: 0.0661, step time: 0.2627\n",
      "148/281, train_loss: 0.0601, step time: 0.2567\n",
      "149/281, train_loss: 0.1453, step time: 0.2462\n",
      "150/281, train_loss: 0.0635, step time: 0.2548\n",
      "151/281, train_loss: 0.0736, step time: 0.2550\n",
      "152/281, train_loss: 0.0894, step time: 0.2563\n",
      "153/281, train_loss: 0.0850, step time: 0.2515\n",
      "154/281, train_loss: 0.1097, step time: 0.2601\n",
      "155/281, train_loss: 0.0891, step time: 0.2581\n",
      "156/281, train_loss: 0.0724, step time: 0.2584\n",
      "157/281, train_loss: 0.1264, step time: 0.2550\n",
      "158/281, train_loss: 0.2331, step time: 0.2529\n",
      "159/281, train_loss: 0.0931, step time: 0.2606\n",
      "160/281, train_loss: 0.0710, step time: 0.2599\n",
      "161/281, train_loss: 0.1280, step time: 0.2571\n",
      "162/281, train_loss: 0.2563, step time: 0.2631\n",
      "163/281, train_loss: 0.0748, step time: 0.2739\n",
      "164/281, train_loss: 0.2417, step time: 0.2761\n",
      "165/281, train_loss: 0.0988, step time: 0.2545\n",
      "166/281, train_loss: 0.0772, step time: 0.2596\n",
      "167/281, train_loss: 0.1136, step time: 0.2566\n",
      "168/281, train_loss: 0.0658, step time: 0.2566\n",
      "169/281, train_loss: 0.2544, step time: 0.2574\n",
      "170/281, train_loss: 0.4194, step time: 0.2623\n",
      "171/281, train_loss: 0.0821, step time: 0.2593\n",
      "172/281, train_loss: 0.2355, step time: 0.2586\n",
      "173/281, train_loss: 0.2499, step time: 0.2569\n",
      "174/281, train_loss: 0.0785, step time: 0.2549\n",
      "175/281, train_loss: 0.0982, step time: 0.2571\n",
      "176/281, train_loss: 0.2795, step time: 0.2599\n",
      "177/281, train_loss: 0.1206, step time: 0.2589\n",
      "178/281, train_loss: 0.0653, step time: 0.2580\n",
      "179/281, train_loss: 0.0736, step time: 0.2607\n",
      "180/281, train_loss: 0.2244, step time: 0.2594\n",
      "181/281, train_loss: 0.0879, step time: 0.2578\n",
      "182/281, train_loss: 0.0573, step time: 0.2558\n",
      "183/281, train_loss: 0.0757, step time: 0.2561\n",
      "184/281, train_loss: 0.0946, step time: 0.2545\n",
      "185/281, train_loss: 0.0612, step time: 0.2538\n",
      "186/281, train_loss: 0.0617, step time: 0.2553\n",
      "187/281, train_loss: 0.1064, step time: 0.2580\n",
      "188/281, train_loss: 0.0488, step time: 0.2589\n",
      "189/281, train_loss: 0.0698, step time: 0.2618\n",
      "190/281, train_loss: 0.0699, step time: 0.2522\n",
      "191/281, train_loss: 0.0728, step time: 0.2555\n",
      "192/281, train_loss: 0.0853, step time: 0.2563\n",
      "193/281, train_loss: 0.1404, step time: 0.2531\n",
      "194/281, train_loss: 0.0960, step time: 0.2563\n",
      "195/281, train_loss: 0.0738, step time: 0.2533\n",
      "196/281, train_loss: 0.0921, step time: 0.2507\n",
      "197/281, train_loss: 0.0766, step time: 0.2534\n",
      "198/281, train_loss: 0.0772, step time: 0.2531\n",
      "199/281, train_loss: 0.0530, step time: 0.2502\n",
      "200/281, train_loss: 0.0734, step time: 0.2493\n",
      "201/281, train_loss: 0.0733, step time: 0.2508\n",
      "202/281, train_loss: 0.0906, step time: 0.2515\n",
      "203/281, train_loss: 0.0890, step time: 0.2499\n",
      "204/281, train_loss: 0.0959, step time: 0.2482\n",
      "205/281, train_loss: 0.1289, step time: 0.2520\n",
      "206/281, train_loss: 0.0953, step time: 0.2512\n",
      "207/281, train_loss: 0.0801, step time: 0.2520\n",
      "208/281, train_loss: 0.1243, step time: 0.2555\n",
      "209/281, train_loss: 0.2701, step time: 0.2556\n",
      "210/281, train_loss: 0.0714, step time: 0.2668\n",
      "211/281, train_loss: 0.2262, step time: 0.2544\n",
      "212/281, train_loss: 0.1106, step time: 0.2579\n",
      "213/281, train_loss: 0.1100, step time: 0.2563\n",
      "214/281, train_loss: 0.0761, step time: 0.2503\n",
      "215/281, train_loss: 0.0948, step time: 0.2562\n",
      "216/281, train_loss: 0.0658, step time: 0.2503\n",
      "217/281, train_loss: 0.2554, step time: 0.2527\n",
      "218/281, train_loss: 0.0639, step time: 0.2512\n",
      "219/281, train_loss: 0.0729, step time: 0.2507\n",
      "220/281, train_loss: 0.0857, step time: 0.2517\n",
      "221/281, train_loss: 0.2614, step time: 0.2534\n",
      "222/281, train_loss: 0.1021, step time: 0.2595\n",
      "223/281, train_loss: 0.2400, step time: 0.2517\n",
      "224/281, train_loss: 0.0646, step time: 0.2477\n",
      "225/281, train_loss: 0.0913, step time: 0.2534\n",
      "226/281, train_loss: 0.1034, step time: 0.2495\n",
      "227/281, train_loss: 0.1079, step time: 0.2539\n",
      "228/281, train_loss: 0.2837, step time: 0.2775\n",
      "229/281, train_loss: 0.0728, step time: 0.2526\n",
      "230/281, train_loss: 0.0717, step time: 0.2537\n",
      "231/281, train_loss: 0.0619, step time: 0.2529\n",
      "232/281, train_loss: 0.0950, step time: 0.2542\n",
      "233/281, train_loss: 0.1001, step time: 0.2573\n",
      "234/281, train_loss: 0.0905, step time: 0.2570\n",
      "235/281, train_loss: 0.0843, step time: 0.2545\n",
      "236/281, train_loss: 0.0795, step time: 0.2535\n",
      "237/281, train_loss: 0.0875, step time: 0.2545\n",
      "238/281, train_loss: 0.1053, step time: 0.2592\n",
      "239/281, train_loss: 0.1003, step time: 0.2601\n",
      "240/281, train_loss: 0.0932, step time: 0.2591\n",
      "241/281, train_loss: 0.0713, step time: 0.2538\n",
      "242/281, train_loss: 0.2215, step time: 0.2631\n",
      "243/281, train_loss: 0.1038, step time: 0.2548\n",
      "244/281, train_loss: 0.0951, step time: 0.2546\n",
      "245/281, train_loss: 0.0728, step time: 0.2543\n",
      "246/281, train_loss: 0.1076, step time: 0.2582\n",
      "247/281, train_loss: 0.0490, step time: 0.2616\n",
      "248/281, train_loss: 0.0800, step time: 0.2569\n",
      "249/281, train_loss: 0.0720, step time: 0.2583\n",
      "250/281, train_loss: 0.1055, step time: 0.2537\n",
      "251/281, train_loss: 0.1353, step time: 0.2603\n",
      "252/281, train_loss: 0.0893, step time: 0.2553\n",
      "253/281, train_loss: 0.1227, step time: 0.2533\n",
      "254/281, train_loss: 0.1034, step time: 0.2605\n",
      "255/281, train_loss: 0.0699, step time: 0.2567\n",
      "256/281, train_loss: 0.0741, step time: 0.2559\n",
      "257/281, train_loss: 0.1113, step time: 0.2588\n",
      "258/281, train_loss: 0.0641, step time: 0.2541\n",
      "259/281, train_loss: 0.0989, step time: 0.2553\n",
      "260/281, train_loss: 0.1404, step time: 0.2575\n",
      "261/281, train_loss: 0.0890, step time: 0.2624\n",
      "262/281, train_loss: 0.0590, step time: 0.2571\n",
      "263/281, train_loss: 0.2477, step time: 0.2599\n",
      "264/281, train_loss: 0.2224, step time: 0.2604\n",
      "265/281, train_loss: 0.0564, step time: 0.2563\n",
      "266/281, train_loss: 0.2760, step time: 0.2572\n",
      "267/281, train_loss: 0.0596, step time: 0.2570\n",
      "268/281, train_loss: 0.2516, step time: 0.2558\n",
      "269/281, train_loss: 0.0657, step time: 0.2538\n",
      "270/281, train_loss: 0.0638, step time: 0.2593\n",
      "271/281, train_loss: 0.2470, step time: 0.2608\n",
      "272/281, train_loss: 0.1038, step time: 0.2547\n",
      "273/281, train_loss: 0.1021, step time: 0.2526\n",
      "274/281, train_loss: 0.1009, step time: 0.2533\n",
      "275/281, train_loss: 0.2358, step time: 0.2564\n",
      "276/281, train_loss: 0.1142, step time: 0.2572\n",
      "277/281, train_loss: 0.2531, step time: 0.2576\n",
      "278/281, train_loss: 0.1399, step time: 0.2576\n",
      "279/281, train_loss: 0.0730, step time: 0.2528\n",
      "280/281, train_loss: 0.0783, step time: 0.2559\n",
      "281/281, train_loss: 0.0400, step time: 0.2533\n",
      "282/281, train_loss: 0.1153, step time: 0.1510\n",
      "epoch 65 average loss: 0.1116\n",
      "current epoch: 65 current mean dice: 0.8798 tc: 0.8726 wt: 0.9130 et: 0.8626\n",
      "best mean dice: 0.8835 at epoch: 63\n",
      "time consuming of epoch 65 is: 382.5038\n",
      "----------\n",
      "epoch 66/200\n",
      "1/281, train_loss: 0.0690, step time: 0.2613\n",
      "2/281, train_loss: 0.0976, step time: 0.2657\n",
      "3/281, train_loss: 0.0794, step time: 0.2560\n",
      "4/281, train_loss: 0.0658, step time: 0.2544\n",
      "5/281, train_loss: 0.2224, step time: 0.2546\n",
      "6/281, train_loss: 0.0730, step time: 0.2531\n",
      "7/281, train_loss: 0.4561, step time: 0.2543\n",
      "8/281, train_loss: 0.1281, step time: 0.2590\n",
      "9/281, train_loss: 0.0813, step time: 0.2548\n",
      "10/281, train_loss: 0.0716, step time: 0.2520\n",
      "11/281, train_loss: 0.1458, step time: 0.2587\n",
      "12/281, train_loss: 0.1208, step time: 0.2539\n",
      "13/281, train_loss: 0.0679, step time: 0.2589\n",
      "14/281, train_loss: 0.2195, step time: 0.2663\n",
      "15/281, train_loss: 0.0740, step time: 0.2587\n",
      "16/281, train_loss: 0.0926, step time: 0.2551\n",
      "17/281, train_loss: 0.0890, step time: 0.2575\n",
      "18/281, train_loss: 0.2660, step time: 0.2536\n",
      "19/281, train_loss: 0.1027, step time: 0.2545\n",
      "20/281, train_loss: 0.0778, step time: 0.2495\n",
      "21/281, train_loss: 0.2617, step time: 0.2535\n",
      "22/281, train_loss: 0.0584, step time: 0.2581\n",
      "23/281, train_loss: 0.0562, step time: 0.2551\n",
      "24/281, train_loss: 0.0692, step time: 0.2564\n",
      "25/281, train_loss: 0.0727, step time: 0.2572\n",
      "26/281, train_loss: 0.1074, step time: 0.2496\n",
      "27/281, train_loss: 0.0797, step time: 0.2485\n",
      "28/281, train_loss: 0.2197, step time: 0.2586\n",
      "29/281, train_loss: 0.1185, step time: 0.2578\n",
      "30/281, train_loss: 0.0986, step time: 0.2640\n",
      "31/281, train_loss: 0.2120, step time: 0.2602\n",
      "32/281, train_loss: 0.2626, step time: 0.2578\n",
      "33/281, train_loss: 0.0611, step time: 0.2593\n",
      "34/281, train_loss: 0.0989, step time: 0.2562\n",
      "35/281, train_loss: 0.1056, step time: 0.2579\n",
      "36/281, train_loss: 0.1044, step time: 0.2655\n",
      "37/281, train_loss: 0.1089, step time: 0.2570\n",
      "38/281, train_loss: 0.1051, step time: 0.2572\n",
      "39/281, train_loss: 0.0734, step time: 0.2572\n",
      "40/281, train_loss: 0.1062, step time: 0.2528\n",
      "41/281, train_loss: 0.1078, step time: 0.2587\n",
      "42/281, train_loss: 0.2712, step time: 0.2598\n",
      "43/281, train_loss: 0.0778, step time: 0.2800\n",
      "44/281, train_loss: 0.2146, step time: 0.2557\n",
      "45/281, train_loss: 0.0827, step time: 0.2595\n",
      "46/281, train_loss: 0.1282, step time: 0.2565\n",
      "47/281, train_loss: 0.1057, step time: 0.2584\n",
      "48/281, train_loss: 0.2193, step time: 0.2555\n",
      "49/281, train_loss: 0.0594, step time: 0.2625\n",
      "50/281, train_loss: 0.0590, step time: 0.2891\n",
      "51/281, train_loss: 0.0637, step time: 0.2709\n",
      "52/281, train_loss: 0.1231, step time: 0.2529\n",
      "53/281, train_loss: 0.2488, step time: 0.2585\n",
      "54/281, train_loss: 0.0821, step time: 0.2570\n",
      "55/281, train_loss: 0.0701, step time: 0.2586\n",
      "56/281, train_loss: 0.1113, step time: 0.2552\n",
      "57/281, train_loss: 0.0961, step time: 0.2554\n",
      "58/281, train_loss: 0.0951, step time: 0.2560\n",
      "59/281, train_loss: 0.2515, step time: 0.2562\n",
      "60/281, train_loss: 0.1741, step time: 0.2525\n",
      "61/281, train_loss: 0.2321, step time: 0.2582\n",
      "62/281, train_loss: 0.0597, step time: 0.2526\n",
      "63/281, train_loss: 0.0693, step time: 0.2628\n",
      "64/281, train_loss: 0.1182, step time: 0.2559\n",
      "65/281, train_loss: 0.0790, step time: 0.2568\n",
      "66/281, train_loss: 0.0964, step time: 0.2560\n",
      "67/281, train_loss: 0.0528, step time: 0.2585\n",
      "68/281, train_loss: 0.0932, step time: 0.2542\n",
      "69/281, train_loss: 0.0929, step time: 0.2591\n",
      "70/281, train_loss: 0.1419, step time: 0.2578\n",
      "71/281, train_loss: 0.0938, step time: 0.2577\n",
      "72/281, train_loss: 0.0917, step time: 0.2598\n",
      "73/281, train_loss: 0.0676, step time: 0.2605\n",
      "74/281, train_loss: 0.0715, step time: 0.2609\n",
      "75/281, train_loss: 0.0590, step time: 0.2571\n",
      "76/281, train_loss: 0.0740, step time: 0.2566\n",
      "77/281, train_loss: 0.0666, step time: 0.2577\n",
      "78/281, train_loss: 0.0706, step time: 0.2550\n",
      "79/281, train_loss: 0.1944, step time: 0.2520\n",
      "80/281, train_loss: 0.0917, step time: 0.2578\n",
      "81/281, train_loss: 0.0733, step time: 0.2594\n",
      "82/281, train_loss: 0.0828, step time: 0.2560\n",
      "83/281, train_loss: 0.1204, step time: 0.2586\n",
      "84/281, train_loss: 0.0915, step time: 0.2600\n",
      "85/281, train_loss: 0.1340, step time: 0.2606\n",
      "86/281, train_loss: 0.2614, step time: 0.2596\n",
      "87/281, train_loss: 0.2706, step time: 0.2566\n",
      "88/281, train_loss: 0.0969, step time: 0.2535\n",
      "89/281, train_loss: 0.0793, step time: 0.2529\n",
      "90/281, train_loss: 0.0913, step time: 0.2565\n",
      "91/281, train_loss: 0.2589, step time: 0.2503\n",
      "92/281, train_loss: 0.2498, step time: 0.2563\n",
      "93/281, train_loss: 0.1416, step time: 0.2606\n",
      "94/281, train_loss: 0.0929, step time: 0.2577\n",
      "95/281, train_loss: 0.0727, step time: 0.2560\n",
      "96/281, train_loss: 0.0704, step time: 0.2498\n",
      "97/281, train_loss: 0.0539, step time: 0.2514\n",
      "98/281, train_loss: 0.2042, step time: 0.2532\n",
      "99/281, train_loss: 0.0878, step time: 0.2591\n",
      "100/281, train_loss: 0.0886, step time: 0.2552\n",
      "101/281, train_loss: 0.0785, step time: 0.2545\n",
      "102/281, train_loss: 0.0767, step time: 0.2545\n",
      "103/281, train_loss: 0.2251, step time: 0.2583\n",
      "104/281, train_loss: 0.0651, step time: 0.2531\n",
      "105/281, train_loss: 0.1073, step time: 0.2575\n",
      "106/281, train_loss: 0.0846, step time: 0.2547\n",
      "107/281, train_loss: 0.0683, step time: 0.2534\n",
      "108/281, train_loss: 0.1150, step time: 0.2604\n",
      "109/281, train_loss: 0.0890, step time: 0.2580\n",
      "110/281, train_loss: 0.0605, step time: 0.2521\n",
      "111/281, train_loss: 0.0792, step time: 0.2536\n",
      "112/281, train_loss: 0.0737, step time: 0.2591\n",
      "113/281, train_loss: 0.0803, step time: 0.2602\n",
      "114/281, train_loss: 0.1111, step time: 0.2593\n",
      "115/281, train_loss: 0.0705, step time: 0.2557\n",
      "116/281, train_loss: 0.1023, step time: 0.2577\n",
      "117/281, train_loss: 0.2751, step time: 0.2564\n",
      "118/281, train_loss: 0.0629, step time: 0.2541\n",
      "119/281, train_loss: 0.0492, step time: 0.2576\n",
      "120/281, train_loss: 0.0855, step time: 0.2560\n",
      "121/281, train_loss: 0.1126, step time: 0.2578\n",
      "122/281, train_loss: 0.0711, step time: 0.2574\n",
      "123/281, train_loss: 0.0626, step time: 0.2552\n",
      "124/281, train_loss: 0.1131, step time: 0.2578\n",
      "125/281, train_loss: 0.0889, step time: 0.2562\n",
      "126/281, train_loss: 0.1469, step time: 0.2559\n",
      "127/281, train_loss: 0.0635, step time: 0.2512\n",
      "128/281, train_loss: 0.0475, step time: 0.2539\n",
      "129/281, train_loss: 0.0640, step time: 0.2553\n",
      "130/281, train_loss: 0.1432, step time: 0.2515\n",
      "131/281, train_loss: 0.1126, step time: 0.2543\n",
      "132/281, train_loss: 0.0955, step time: 0.2661\n",
      "133/281, train_loss: 0.2250, step time: 0.2585\n",
      "134/281, train_loss: 0.0856, step time: 0.2577\n",
      "135/281, train_loss: 0.1197, step time: 0.2565\n",
      "136/281, train_loss: 0.0764, step time: 0.2589\n",
      "137/281, train_loss: 0.0739, step time: 0.2546\n",
      "138/281, train_loss: 0.2319, step time: 0.2568\n",
      "139/281, train_loss: 0.0516, step time: 0.2581\n",
      "140/281, train_loss: 0.1064, step time: 0.2505\n",
      "141/281, train_loss: 0.0770, step time: 0.2521\n",
      "142/281, train_loss: 0.0751, step time: 0.2498\n",
      "143/281, train_loss: 0.0786, step time: 0.2532\n",
      "144/281, train_loss: 0.0852, step time: 0.2537\n",
      "145/281, train_loss: 0.0692, step time: 0.2526\n",
      "146/281, train_loss: 0.0807, step time: 0.2530\n",
      "147/281, train_loss: 0.0872, step time: 0.2500\n",
      "148/281, train_loss: 0.1298, step time: 0.2604\n",
      "149/281, train_loss: 0.2205, step time: 0.2494\n",
      "150/281, train_loss: 0.2366, step time: 0.2533\n",
      "151/281, train_loss: 0.0963, step time: 0.2510\n",
      "152/281, train_loss: 0.0888, step time: 0.2630\n",
      "153/281, train_loss: 0.0792, step time: 0.2538\n",
      "154/281, train_loss: 0.1026, step time: 0.2542\n",
      "155/281, train_loss: 0.0897, step time: 0.2534\n",
      "156/281, train_loss: 0.0632, step time: 0.2532\n",
      "157/281, train_loss: 0.0851, step time: 0.2543\n",
      "158/281, train_loss: 0.0718, step time: 0.2520\n",
      "159/281, train_loss: 0.0939, step time: 0.2543\n",
      "160/281, train_loss: 0.0663, step time: 0.2558\n",
      "161/281, train_loss: 0.1245, step time: 0.2532\n",
      "162/281, train_loss: 0.0534, step time: 0.2494\n",
      "163/281, train_loss: 0.2279, step time: 0.2455\n",
      "164/281, train_loss: 0.0562, step time: 0.2545\n",
      "165/281, train_loss: 0.0827, step time: 0.2568\n",
      "166/281, train_loss: 0.1820, step time: 0.2576\n",
      "167/281, train_loss: 0.1158, step time: 0.2488\n",
      "168/281, train_loss: 0.0786, step time: 0.2578\n",
      "169/281, train_loss: 0.1060, step time: 0.2566\n",
      "170/281, train_loss: 0.1068, step time: 0.2517\n",
      "171/281, train_loss: 0.0725, step time: 0.2533\n",
      "172/281, train_loss: 0.0841, step time: 0.2540\n",
      "173/281, train_loss: 0.0941, step time: 0.2525\n",
      "174/281, train_loss: 0.0611, step time: 0.2579\n",
      "175/281, train_loss: 0.2245, step time: 0.2579\n",
      "176/281, train_loss: 0.0892, step time: 0.2560\n",
      "177/281, train_loss: 0.0929, step time: 0.2564\n",
      "178/281, train_loss: 0.1195, step time: 0.2546\n",
      "179/281, train_loss: 0.2396, step time: 0.2561\n",
      "180/281, train_loss: 0.1403, step time: 0.2567\n",
      "181/281, train_loss: 0.1030, step time: 0.2559\n",
      "182/281, train_loss: 0.0624, step time: 0.2473\n",
      "183/281, train_loss: 0.0919, step time: 0.2494\n",
      "184/281, train_loss: 0.0876, step time: 0.2500\n",
      "185/281, train_loss: 0.0562, step time: 0.2551\n",
      "186/281, train_loss: 0.2897, step time: 0.2512\n",
      "187/281, train_loss: 0.1220, step time: 0.2522\n",
      "188/281, train_loss: 0.0682, step time: 0.2539\n",
      "189/281, train_loss: 0.2735, step time: 0.2559\n",
      "190/281, train_loss: 0.2625, step time: 0.2575\n",
      "191/281, train_loss: 0.0840, step time: 0.2530\n",
      "192/281, train_loss: 0.1115, step time: 0.2537\n",
      "193/281, train_loss: 0.0668, step time: 0.2569\n",
      "194/281, train_loss: 0.0774, step time: 0.2578\n",
      "195/281, train_loss: 0.0740, step time: 0.2537\n",
      "196/281, train_loss: 0.3835, step time: 0.2550\n",
      "197/281, train_loss: 0.0880, step time: 0.2538\n",
      "198/281, train_loss: 0.0838, step time: 0.2549\n",
      "199/281, train_loss: 0.0824, step time: 0.2526\n",
      "200/281, train_loss: 0.2508, step time: 0.2505\n",
      "201/281, train_loss: 0.0684, step time: 0.2595\n",
      "202/281, train_loss: 0.0630, step time: 0.2564\n",
      "203/281, train_loss: 0.0648, step time: 0.2572\n",
      "204/281, train_loss: 0.0680, step time: 0.2532\n",
      "205/281, train_loss: 0.0704, step time: 0.2571\n",
      "206/281, train_loss: 0.1177, step time: 0.2520\n",
      "207/281, train_loss: 0.0983, step time: 0.2469\n",
      "208/281, train_loss: 0.0693, step time: 0.2531\n",
      "209/281, train_loss: 0.0978, step time: 0.2509\n",
      "210/281, train_loss: 0.2534, step time: 0.2482\n",
      "211/281, train_loss: 0.0907, step time: 0.2495\n",
      "212/281, train_loss: 0.0767, step time: 0.2465\n",
      "213/281, train_loss: 0.0807, step time: 0.2534\n",
      "214/281, train_loss: 0.1244, step time: 0.2541\n",
      "215/281, train_loss: 0.1100, step time: 0.2534\n",
      "216/281, train_loss: 0.0469, step time: 0.2531\n",
      "217/281, train_loss: 0.0723, step time: 0.2511\n",
      "218/281, train_loss: 0.0956, step time: 0.2531\n",
      "219/281, train_loss: 0.0585, step time: 0.2500\n",
      "220/281, train_loss: 0.1012, step time: 0.2520\n",
      "221/281, train_loss: 0.0616, step time: 0.2472\n",
      "222/281, train_loss: 0.0839, step time: 0.2454\n",
      "223/281, train_loss: 0.0964, step time: 0.2455\n",
      "224/281, train_loss: 0.0745, step time: 0.2529\n",
      "225/281, train_loss: 0.0471, step time: 0.2479\n",
      "226/281, train_loss: 0.0630, step time: 0.2501\n",
      "227/281, train_loss: 0.3162, step time: 0.2476\n",
      "228/281, train_loss: 0.2511, step time: 0.2515\n",
      "229/281, train_loss: 0.0550, step time: 0.2493\n",
      "230/281, train_loss: 0.0762, step time: 0.2463\n",
      "231/281, train_loss: 0.0570, step time: 0.2494\n",
      "232/281, train_loss: 0.0578, step time: 0.2535\n",
      "233/281, train_loss: 0.0361, step time: 0.2519\n",
      "234/281, train_loss: 0.0697, step time: 0.2484\n",
      "235/281, train_loss: 0.0695, step time: 0.2481\n",
      "236/281, train_loss: 0.0803, step time: 0.2486\n",
      "237/281, train_loss: 0.0465, step time: 0.2502\n",
      "238/281, train_loss: 0.0730, step time: 0.2540\n",
      "239/281, train_loss: 0.1087, step time: 0.2827\n",
      "240/281, train_loss: 0.1006, step time: 0.2493\n",
      "241/281, train_loss: 0.1000, step time: 0.2551\n",
      "242/281, train_loss: 0.0713, step time: 0.2520\n",
      "243/281, train_loss: 0.0892, step time: 0.2510\n",
      "244/281, train_loss: 0.1279, step time: 0.2529\n",
      "245/281, train_loss: 0.0475, step time: 0.2475\n",
      "246/281, train_loss: 0.1225, step time: 0.2472\n",
      "247/281, train_loss: 0.0523, step time: 0.2496\n",
      "248/281, train_loss: 0.0908, step time: 0.2545\n",
      "249/281, train_loss: 0.0618, step time: 0.2493\n",
      "250/281, train_loss: 0.0909, step time: 0.2539\n",
      "251/281, train_loss: 0.0783, step time: 0.2540\n",
      "252/281, train_loss: 0.0519, step time: 0.2544\n",
      "253/281, train_loss: 0.1087, step time: 0.2565\n",
      "254/281, train_loss: 0.0818, step time: 0.2516\n",
      "255/281, train_loss: 0.0764, step time: 0.2558\n",
      "256/281, train_loss: 0.1360, step time: 0.2459\n",
      "257/281, train_loss: 0.0994, step time: 0.2479\n",
      "258/281, train_loss: 0.0926, step time: 0.2580\n",
      "259/281, train_loss: 0.2551, step time: 0.2537\n",
      "260/281, train_loss: 0.2673, step time: 0.2507\n",
      "261/281, train_loss: 0.0762, step time: 0.2539\n",
      "262/281, train_loss: 0.1061, step time: 0.2520\n",
      "263/281, train_loss: 0.1135, step time: 0.2506\n",
      "264/281, train_loss: 0.1201, step time: 0.2505\n",
      "265/281, train_loss: 0.0662, step time: 0.2480\n",
      "266/281, train_loss: 0.1118, step time: 0.2474\n",
      "267/281, train_loss: 0.2337, step time: 0.2446\n",
      "268/281, train_loss: 0.0557, step time: 0.2519\n",
      "269/281, train_loss: 0.0801, step time: 0.2557\n",
      "270/281, train_loss: 0.0862, step time: 0.2472\n",
      "271/281, train_loss: 0.0620, step time: 0.2475\n",
      "272/281, train_loss: 0.1245, step time: 0.2546\n",
      "273/281, train_loss: 0.0848, step time: 0.2510\n",
      "274/281, train_loss: 0.2321, step time: 0.2574\n",
      "275/281, train_loss: 0.2250, step time: 0.2547\n",
      "276/281, train_loss: 0.1008, step time: 0.2470\n",
      "277/281, train_loss: 0.1114, step time: 0.2482\n",
      "278/281, train_loss: 0.0448, step time: 0.2499\n",
      "279/281, train_loss: 0.0638, step time: 0.2460\n",
      "280/281, train_loss: 0.0640, step time: 0.2491\n",
      "281/281, train_loss: 0.0851, step time: 0.2456\n",
      "282/281, train_loss: 0.0719, step time: 0.1512\n",
      "epoch 66 average loss: 0.1111\n",
      "current epoch: 66 current mean dice: 0.8793 tc: 0.8679 wt: 0.9171 et: 0.8626\n",
      "best mean dice: 0.8835 at epoch: 63\n",
      "time consuming of epoch 66 is: 408.7460\n",
      "----------\n",
      "epoch 67/200\n",
      "1/281, train_loss: 0.0794, step time: 0.2661\n",
      "2/281, train_loss: 0.0782, step time: 0.2549\n",
      "3/281, train_loss: 0.1350, step time: 0.2523\n",
      "4/281, train_loss: 0.0910, step time: 0.2538\n",
      "5/281, train_loss: 0.0660, step time: 0.2507\n",
      "6/281, train_loss: 0.2115, step time: 0.2563\n",
      "7/281, train_loss: 0.0763, step time: 0.2618\n",
      "8/281, train_loss: 0.0843, step time: 0.2681\n",
      "9/281, train_loss: 0.0603, step time: 0.2542\n",
      "10/281, train_loss: 0.0980, step time: 0.2553\n",
      "11/281, train_loss: 0.0754, step time: 0.2523\n",
      "12/281, train_loss: 0.1249, step time: 0.2533\n",
      "13/281, train_loss: 0.0533, step time: 0.2573\n",
      "14/281, train_loss: 0.1542, step time: 0.2603\n",
      "15/281, train_loss: 0.0866, step time: 0.2589\n",
      "16/281, train_loss: 0.1122, step time: 0.2519\n",
      "17/281, train_loss: 0.2335, step time: 0.2495\n",
      "18/281, train_loss: 0.0855, step time: 0.2488\n",
      "19/281, train_loss: 0.0701, step time: 0.2549\n",
      "20/281, train_loss: 0.2330, step time: 0.2458\n",
      "21/281, train_loss: 0.0986, step time: 0.2516\n",
      "22/281, train_loss: 0.0839, step time: 0.2493\n",
      "23/281, train_loss: 0.1036, step time: 0.2496\n",
      "24/281, train_loss: 0.0826, step time: 0.2522\n",
      "25/281, train_loss: 0.0576, step time: 0.2577\n",
      "26/281, train_loss: 0.0327, step time: 0.2539\n",
      "27/281, train_loss: 0.0568, step time: 0.2550\n",
      "28/281, train_loss: 0.0606, step time: 0.2527\n",
      "29/281, train_loss: 0.0577, step time: 0.2530\n",
      "30/281, train_loss: 0.0654, step time: 0.2723\n",
      "31/281, train_loss: 0.1396, step time: 0.2756\n",
      "32/281, train_loss: 0.1216, step time: 0.2615\n",
      "33/281, train_loss: 0.2717, step time: 0.2698\n",
      "34/281, train_loss: 0.0652, step time: 0.2567\n",
      "35/281, train_loss: 0.0670, step time: 0.2474\n",
      "36/281, train_loss: 0.0630, step time: 0.2481\n",
      "37/281, train_loss: 0.1065, step time: 0.2501\n",
      "38/281, train_loss: 0.1142, step time: 0.2464\n",
      "39/281, train_loss: 0.0713, step time: 0.2535\n",
      "40/281, train_loss: 0.0632, step time: 0.2548\n",
      "41/281, train_loss: 0.0961, step time: 0.2587\n",
      "42/281, train_loss: 0.0719, step time: 0.2549\n",
      "43/281, train_loss: 0.0679, step time: 0.2554\n",
      "44/281, train_loss: 0.1153, step time: 0.2574\n",
      "45/281, train_loss: 0.0849, step time: 0.2539\n",
      "46/281, train_loss: 0.0874, step time: 0.2558\n",
      "47/281, train_loss: 0.0823, step time: 0.2705\n",
      "48/281, train_loss: 0.0934, step time: 0.2490\n",
      "49/281, train_loss: 0.0730, step time: 0.2463\n",
      "50/281, train_loss: 0.0630, step time: 0.2469\n",
      "51/281, train_loss: 0.2669, step time: 0.2585\n",
      "52/281, train_loss: 0.1312, step time: 0.2519\n",
      "53/281, train_loss: 0.1165, step time: 0.2523\n",
      "54/281, train_loss: 0.0950, step time: 0.2566\n",
      "55/281, train_loss: 0.0772, step time: 0.2449\n",
      "56/281, train_loss: 0.1086, step time: 0.2454\n",
      "57/281, train_loss: 0.0850, step time: 0.2486\n",
      "58/281, train_loss: 0.0808, step time: 0.2510\n",
      "59/281, train_loss: 0.0895, step time: 0.2494\n",
      "60/281, train_loss: 0.0698, step time: 0.2518\n",
      "61/281, train_loss: 0.0811, step time: 0.2549\n",
      "62/281, train_loss: 0.1075, step time: 0.2578\n",
      "63/281, train_loss: 0.1668, step time: 0.2477\n",
      "64/281, train_loss: 0.0571, step time: 0.2527\n",
      "65/281, train_loss: 0.0635, step time: 0.2500\n",
      "66/281, train_loss: 0.0894, step time: 0.2508\n",
      "67/281, train_loss: 0.0874, step time: 0.2663\n",
      "68/281, train_loss: 0.1108, step time: 0.2540\n",
      "69/281, train_loss: 0.0720, step time: 0.2561\n",
      "70/281, train_loss: 0.0733, step time: 0.2514\n",
      "71/281, train_loss: 0.1057, step time: 0.2521\n",
      "72/281, train_loss: 0.1072, step time: 0.2528\n",
      "73/281, train_loss: 0.0862, step time: 0.2573\n",
      "74/281, train_loss: 0.0948, step time: 0.2569\n",
      "75/281, train_loss: 0.0836, step time: 0.2445\n",
      "76/281, train_loss: 0.0715, step time: 0.2526\n",
      "77/281, train_loss: 0.2731, step time: 0.2506\n",
      "78/281, train_loss: 0.0960, step time: 0.2609\n",
      "79/281, train_loss: 0.0502, step time: 0.2479\n",
      "80/281, train_loss: 0.0895, step time: 0.2482\n",
      "81/281, train_loss: 0.0911, step time: 0.2562\n",
      "82/281, train_loss: 0.0733, step time: 0.2565\n",
      "83/281, train_loss: 0.4036, step time: 0.2513\n",
      "84/281, train_loss: 0.2191, step time: 0.2500\n",
      "85/281, train_loss: 0.2653, step time: 0.2552\n",
      "86/281, train_loss: 0.0807, step time: 0.2448\n",
      "87/281, train_loss: 0.1207, step time: 0.2502\n",
      "88/281, train_loss: 0.2244, step time: 0.2453\n",
      "89/281, train_loss: 0.0851, step time: 0.2541\n",
      "90/281, train_loss: 0.0872, step time: 0.2517\n",
      "91/281, train_loss: 0.0862, step time: 0.2463\n",
      "92/281, train_loss: 0.1089, step time: 0.2489\n",
      "93/281, train_loss: 0.2436, step time: 0.2482\n",
      "94/281, train_loss: 0.0814, step time: 0.2525\n",
      "95/281, train_loss: 0.1022, step time: 0.2558\n",
      "96/281, train_loss: 0.0368, step time: 0.2567\n",
      "97/281, train_loss: 0.1026, step time: 0.2565\n",
      "98/281, train_loss: 0.0909, step time: 0.2611\n",
      "99/281, train_loss: 0.0631, step time: 0.2542\n",
      "100/281, train_loss: 0.0698, step time: 0.2484\n",
      "101/281, train_loss: 0.1382, step time: 0.2477\n",
      "102/281, train_loss: 0.1986, step time: 0.2518\n",
      "103/281, train_loss: 0.2377, step time: 0.2474\n",
      "104/281, train_loss: 0.0791, step time: 0.2562\n",
      "105/281, train_loss: 0.0795, step time: 0.2545\n",
      "106/281, train_loss: 0.0772, step time: 0.2526\n",
      "107/281, train_loss: 0.1478, step time: 0.2550\n",
      "108/281, train_loss: 0.0934, step time: 0.2494\n",
      "109/281, train_loss: 0.1004, step time: 0.2475\n",
      "110/281, train_loss: 0.0739, step time: 0.2489\n",
      "111/281, train_loss: 0.0487, step time: 0.2453\n",
      "112/281, train_loss: 0.0970, step time: 0.2471\n",
      "113/281, train_loss: 0.0896, step time: 0.2525\n",
      "114/281, train_loss: 0.1202, step time: 0.2511\n",
      "115/281, train_loss: 0.1143, step time: 0.2518\n",
      "116/281, train_loss: 0.1024, step time: 0.2505\n",
      "117/281, train_loss: 0.0737, step time: 0.2481\n",
      "118/281, train_loss: 0.0745, step time: 0.2549\n",
      "119/281, train_loss: 0.0730, step time: 0.2551\n",
      "120/281, train_loss: 0.0702, step time: 0.2514\n",
      "121/281, train_loss: 0.1727, step time: 0.2479\n",
      "122/281, train_loss: 0.2638, step time: 0.2502\n",
      "123/281, train_loss: 0.2516, step time: 0.2512\n",
      "124/281, train_loss: 0.0916, step time: 0.2507\n",
      "125/281, train_loss: 0.0655, step time: 0.2469\n",
      "126/281, train_loss: 0.1062, step time: 0.2560\n",
      "127/281, train_loss: 0.1125, step time: 0.2535\n",
      "128/281, train_loss: 0.0834, step time: 0.2503\n",
      "129/281, train_loss: 0.0368, step time: 0.2517\n",
      "130/281, train_loss: 0.0895, step time: 0.2575\n",
      "131/281, train_loss: 0.0761, step time: 0.2526\n",
      "132/281, train_loss: 0.1069, step time: 0.2480\n",
      "133/281, train_loss: 0.2519, step time: 0.2509\n",
      "134/281, train_loss: 0.0946, step time: 0.2519\n",
      "135/281, train_loss: 0.0839, step time: 0.2517\n",
      "136/281, train_loss: 0.2317, step time: 0.2534\n",
      "137/281, train_loss: 0.0792, step time: 0.2535\n",
      "138/281, train_loss: 0.0561, step time: 0.2494\n",
      "139/281, train_loss: 0.0901, step time: 0.2541\n",
      "140/281, train_loss: 0.2274, step time: 0.2516\n",
      "141/281, train_loss: 0.2332, step time: 0.2550\n",
      "142/281, train_loss: 0.1079, step time: 0.2523\n",
      "143/281, train_loss: 0.0758, step time: 0.2535\n",
      "144/281, train_loss: 0.1092, step time: 0.2490\n",
      "145/281, train_loss: 0.0838, step time: 0.2490\n",
      "146/281, train_loss: 0.1086, step time: 0.2466\n",
      "147/281, train_loss: 0.0840, step time: 0.2447\n",
      "148/281, train_loss: 0.1054, step time: 0.2492\n",
      "149/281, train_loss: 0.0629, step time: 0.2469\n",
      "150/281, train_loss: 0.0708, step time: 0.2566\n",
      "151/281, train_loss: 0.0809, step time: 0.2552\n",
      "152/281, train_loss: 0.2316, step time: 0.2525\n",
      "153/281, train_loss: 0.2355, step time: 0.2506\n",
      "154/281, train_loss: 0.0580, step time: 0.2536\n",
      "155/281, train_loss: 0.0927, step time: 0.2536\n",
      "156/281, train_loss: 0.0797, step time: 0.2559\n",
      "157/281, train_loss: 0.1093, step time: 0.2550\n",
      "158/281, train_loss: 0.0796, step time: 0.2537\n",
      "159/281, train_loss: 0.0822, step time: 0.2573\n",
      "160/281, train_loss: 0.0478, step time: 0.2549\n",
      "161/281, train_loss: 0.0851, step time: 0.2545\n",
      "162/281, train_loss: 0.2226, step time: 0.2496\n",
      "163/281, train_loss: 0.0868, step time: 0.2512\n",
      "164/281, train_loss: 0.0649, step time: 0.2491\n",
      "165/281, train_loss: 0.2505, step time: 0.2590\n",
      "166/281, train_loss: 0.0835, step time: 0.2468\n",
      "167/281, train_loss: 0.0953, step time: 0.2504\n",
      "168/281, train_loss: 0.1084, step time: 0.2491\n",
      "169/281, train_loss: 0.0639, step time: 0.2480\n",
      "170/281, train_loss: 0.0708, step time: 0.2522\n",
      "171/281, train_loss: 0.2208, step time: 0.2539\n",
      "172/281, train_loss: 0.2386, step time: 0.2478\n",
      "173/281, train_loss: 0.2526, step time: 0.2540\n",
      "174/281, train_loss: 0.2134, step time: 0.2501\n",
      "175/281, train_loss: 0.0707, step time: 0.2539\n",
      "176/281, train_loss: 0.0680, step time: 0.2543\n",
      "177/281, train_loss: 0.0703, step time: 0.2600\n",
      "178/281, train_loss: 0.0999, step time: 0.2528\n",
      "179/281, train_loss: 0.0575, step time: 0.2503\n",
      "180/281, train_loss: 0.1134, step time: 0.2504\n",
      "181/281, train_loss: 0.2658, step time: 0.2521\n",
      "182/281, train_loss: 0.0986, step time: 0.2568\n",
      "183/281, train_loss: 0.0837, step time: 0.2512\n",
      "184/281, train_loss: 0.0654, step time: 0.2531\n",
      "185/281, train_loss: 0.1008, step time: 0.2518\n",
      "186/281, train_loss: 0.0633, step time: 0.2534\n",
      "187/281, train_loss: 0.1251, step time: 0.2568\n",
      "188/281, train_loss: 0.1033, step time: 0.2597\n",
      "189/281, train_loss: 0.2630, step time: 0.2591\n",
      "190/281, train_loss: 0.0757, step time: 0.2536\n",
      "191/281, train_loss: 0.2132, step time: 0.2559\n",
      "192/281, train_loss: 0.0642, step time: 0.2585\n",
      "193/281, train_loss: 0.0685, step time: 0.2545\n",
      "194/281, train_loss: 0.0956, step time: 0.2573\n",
      "195/281, train_loss: 0.2419, step time: 0.2531\n",
      "196/281, train_loss: 0.1059, step time: 0.2567\n",
      "197/281, train_loss: 0.0374, step time: 0.2591\n",
      "198/281, train_loss: 0.0672, step time: 0.2575\n",
      "199/281, train_loss: 0.0473, step time: 0.2556\n",
      "200/281, train_loss: 0.0800, step time: 0.2539\n",
      "201/281, train_loss: 0.0957, step time: 0.2552\n",
      "202/281, train_loss: 0.0536, step time: 0.2562\n",
      "203/281, train_loss: 0.0760, step time: 0.2587\n",
      "204/281, train_loss: 0.0953, step time: 0.2576\n",
      "205/281, train_loss: 0.1084, step time: 0.2576\n",
      "206/281, train_loss: 0.0998, step time: 0.2562\n",
      "207/281, train_loss: 0.2456, step time: 0.2505\n",
      "208/281, train_loss: 0.0690, step time: 0.2528\n",
      "209/281, train_loss: 0.0665, step time: 0.2547\n",
      "210/281, train_loss: 0.0766, step time: 0.2592\n",
      "211/281, train_loss: 0.1109, step time: 0.2598\n",
      "212/281, train_loss: 0.2219, step time: 0.2591\n",
      "213/281, train_loss: 0.0706, step time: 0.2574\n",
      "214/281, train_loss: 0.2278, step time: 0.2580\n",
      "215/281, train_loss: 0.0883, step time: 0.2551\n",
      "216/281, train_loss: 0.0924, step time: 0.2550\n",
      "217/281, train_loss: 0.0815, step time: 0.2565\n",
      "218/281, train_loss: 0.0624, step time: 0.2860\n",
      "219/281, train_loss: 0.0896, step time: 0.2589\n",
      "220/281, train_loss: 0.0792, step time: 0.2484\n",
      "221/281, train_loss: 0.2932, step time: 0.2563\n",
      "222/281, train_loss: 0.0850, step time: 0.2617\n",
      "223/281, train_loss: 0.2420, step time: 0.2568\n",
      "224/281, train_loss: 0.1038, step time: 0.2551\n",
      "225/281, train_loss: 0.0869, step time: 0.2508\n",
      "226/281, train_loss: 0.0884, step time: 0.2547\n",
      "227/281, train_loss: 0.0495, step time: 0.2575\n",
      "228/281, train_loss: 0.0769, step time: 0.2540\n",
      "229/281, train_loss: 0.1154, step time: 0.2522\n",
      "230/281, train_loss: 0.2377, step time: 0.2579\n",
      "231/281, train_loss: 0.1866, step time: 0.2568\n",
      "232/281, train_loss: 0.0867, step time: 0.2543\n",
      "233/281, train_loss: 0.0831, step time: 0.2587\n",
      "234/281, train_loss: 0.0873, step time: 0.2526\n",
      "235/281, train_loss: 0.0750, step time: 0.2508\n",
      "236/281, train_loss: 0.0895, step time: 0.2643\n",
      "237/281, train_loss: 0.1081, step time: 0.2700\n",
      "238/281, train_loss: 0.1191, step time: 0.2527\n",
      "239/281, train_loss: 0.1313, step time: 0.2540\n",
      "240/281, train_loss: 0.0885, step time: 0.2505\n",
      "241/281, train_loss: 0.0911, step time: 0.2506\n",
      "242/281, train_loss: 0.0799, step time: 0.2563\n",
      "243/281, train_loss: 0.2318, step time: 0.2545\n",
      "244/281, train_loss: 0.0771, step time: 0.2505\n",
      "245/281, train_loss: 0.0561, step time: 0.2534\n",
      "246/281, train_loss: 0.2320, step time: 0.2565\n",
      "247/281, train_loss: 0.0687, step time: 0.2532\n",
      "248/281, train_loss: 0.2600, step time: 0.2533\n",
      "249/281, train_loss: 0.0940, step time: 0.2543\n",
      "250/281, train_loss: 0.1003, step time: 0.2505\n",
      "251/281, train_loss: 0.0645, step time: 0.2502\n",
      "252/281, train_loss: 0.2500, step time: 0.2516\n",
      "253/281, train_loss: 0.0520, step time: 0.2535\n",
      "254/281, train_loss: 0.1570, step time: 0.2526\n",
      "255/281, train_loss: 0.0722, step time: 0.2515\n",
      "256/281, train_loss: 0.1275, step time: 0.2504\n",
      "257/281, train_loss: 0.0740, step time: 0.2536\n",
      "258/281, train_loss: 0.0698, step time: 0.2522\n",
      "259/281, train_loss: 0.2303, step time: 0.2501\n",
      "260/281, train_loss: 0.1251, step time: 0.2531\n",
      "261/281, train_loss: 0.0677, step time: 0.2544\n",
      "262/281, train_loss: 0.1429, step time: 0.2472\n",
      "263/281, train_loss: 0.0748, step time: 0.2471\n",
      "264/281, train_loss: 0.0807, step time: 0.2521\n",
      "265/281, train_loss: 0.0617, step time: 0.2500\n",
      "266/281, train_loss: 0.0894, step time: 0.2537\n",
      "267/281, train_loss: 0.0830, step time: 0.2530\n",
      "268/281, train_loss: 0.0697, step time: 0.2540\n",
      "269/281, train_loss: 0.0817, step time: 0.2519\n",
      "270/281, train_loss: 0.0941, step time: 0.2596\n",
      "271/281, train_loss: 0.0609, step time: 0.2607\n",
      "272/281, train_loss: 0.0361, step time: 0.2664\n",
      "273/281, train_loss: 0.0680, step time: 0.2565\n",
      "274/281, train_loss: 0.0734, step time: 0.2516\n",
      "275/281, train_loss: 0.0694, step time: 0.2553\n",
      "276/281, train_loss: 0.1047, step time: 0.2546\n",
      "277/281, train_loss: 0.0536, step time: 0.2594\n",
      "278/281, train_loss: 0.0514, step time: 0.2557\n",
      "279/281, train_loss: 0.0645, step time: 0.2545\n",
      "280/281, train_loss: 0.2293, step time: 0.2530\n",
      "281/281, train_loss: 0.0921, step time: 0.2531\n",
      "282/281, train_loss: 0.1074, step time: 0.1482\n",
      "epoch 67 average loss: 0.1102\n",
      "saved new best metric model\n",
      "current epoch: 67 current mean dice: 0.8851 tc: 0.8800 wt: 0.9146 et: 0.8691\n",
      "best mean dice: 0.8851 at epoch: 67\n",
      "time consuming of epoch 67 is: 390.3167\n",
      "----------\n",
      "epoch 68/200\n",
      "1/281, train_loss: 0.1224, step time: 0.2545\n",
      "2/281, train_loss: 0.0640, step time: 0.2532\n",
      "3/281, train_loss: 0.1021, step time: 0.2528\n",
      "4/281, train_loss: 0.0658, step time: 0.2524\n",
      "5/281, train_loss: 0.0640, step time: 0.2507\n",
      "6/281, train_loss: 0.0983, step time: 0.2617\n",
      "7/281, train_loss: 0.0876, step time: 0.2540\n",
      "8/281, train_loss: 0.2376, step time: 0.2554\n",
      "9/281, train_loss: 0.1227, step time: 0.2528\n",
      "10/281, train_loss: 0.1106, step time: 0.2483\n",
      "11/281, train_loss: 0.2542, step time: 0.2472\n",
      "12/281, train_loss: 0.0451, step time: 0.2474\n",
      "13/281, train_loss: 0.1280, step time: 0.2483\n",
      "14/281, train_loss: 0.0687, step time: 0.2466\n",
      "15/281, train_loss: 0.0970, step time: 0.2467\n",
      "16/281, train_loss: 0.0948, step time: 0.2505\n",
      "17/281, train_loss: 0.0636, step time: 0.2515\n",
      "18/281, train_loss: 0.0718, step time: 0.2506\n",
      "19/281, train_loss: 0.0545, step time: 0.2529\n",
      "20/281, train_loss: 0.0923, step time: 0.2538\n",
      "21/281, train_loss: 0.2599, step time: 0.2572\n",
      "22/281, train_loss: 0.0992, step time: 0.2543\n",
      "23/281, train_loss: 0.0736, step time: 0.2554\n",
      "24/281, train_loss: 0.2289, step time: 0.2577\n",
      "25/281, train_loss: 0.1289, step time: 0.2520\n",
      "26/281, train_loss: 0.0827, step time: 0.2516\n",
      "27/281, train_loss: 0.0930, step time: 0.2518\n",
      "28/281, train_loss: 0.0704, step time: 0.2505\n",
      "29/281, train_loss: 0.0516, step time: 0.2533\n",
      "30/281, train_loss: 0.0888, step time: 0.2527\n",
      "31/281, train_loss: 0.0780, step time: 0.2596\n",
      "32/281, train_loss: 0.1367, step time: 0.2577\n",
      "33/281, train_loss: 0.2263, step time: 0.2521\n",
      "34/281, train_loss: 0.0572, step time: 0.2552\n",
      "35/281, train_loss: 0.0671, step time: 0.2562\n",
      "36/281, train_loss: 0.0752, step time: 0.2495\n",
      "37/281, train_loss: 0.0893, step time: 0.2504\n",
      "38/281, train_loss: 0.0831, step time: 0.2622\n",
      "39/281, train_loss: 0.2330, step time: 0.2577\n",
      "40/281, train_loss: 0.0686, step time: 0.2570\n",
      "41/281, train_loss: 0.0585, step time: 0.2561\n",
      "42/281, train_loss: 0.1025, step time: 0.2510\n",
      "43/281, train_loss: 0.0815, step time: 0.2513\n",
      "44/281, train_loss: 0.0614, step time: 0.2573\n",
      "45/281, train_loss: 0.2229, step time: 0.2514\n",
      "46/281, train_loss: 0.2220, step time: 0.2555\n",
      "47/281, train_loss: 0.0595, step time: 0.2600\n",
      "48/281, train_loss: 0.0699, step time: 0.2563\n",
      "49/281, train_loss: 0.0708, step time: 0.2539\n",
      "50/281, train_loss: 0.0993, step time: 0.2545\n",
      "51/281, train_loss: 0.1108, step time: 0.2596\n",
      "52/281, train_loss: 0.0748, step time: 0.2579\n",
      "53/281, train_loss: 0.0809, step time: 0.2580\n",
      "54/281, train_loss: 0.0805, step time: 0.2592\n",
      "55/281, train_loss: 0.0535, step time: 0.2509\n",
      "56/281, train_loss: 0.1295, step time: 0.2529\n",
      "57/281, train_loss: 0.1426, step time: 0.2502\n",
      "58/281, train_loss: 0.0853, step time: 0.2526\n",
      "59/281, train_loss: 0.1009, step time: 0.2527\n",
      "60/281, train_loss: 0.1208, step time: 0.2553\n",
      "61/281, train_loss: 0.0575, step time: 0.2503\n",
      "62/281, train_loss: 0.0581, step time: 0.2586\n",
      "63/281, train_loss: 0.0707, step time: 0.2529\n",
      "64/281, train_loss: 0.1080, step time: 0.2500\n",
      "65/281, train_loss: 0.2525, step time: 0.2540\n",
      "66/281, train_loss: 0.1061, step time: 0.2770\n",
      "67/281, train_loss: 0.0519, step time: 0.2501\n",
      "68/281, train_loss: 0.0727, step time: 0.2514\n",
      "69/281, train_loss: 0.1041, step time: 0.2559\n",
      "70/281, train_loss: 0.0779, step time: 0.2595\n",
      "71/281, train_loss: 0.0963, step time: 0.2568\n",
      "72/281, train_loss: 0.0744, step time: 0.2620\n",
      "73/281, train_loss: 0.0893, step time: 0.2538\n",
      "74/281, train_loss: 0.0636, step time: 0.2556\n",
      "75/281, train_loss: 0.0628, step time: 0.2513\n",
      "76/281, train_loss: 0.0533, step time: 0.2502\n",
      "77/281, train_loss: 0.2299, step time: 0.2518\n",
      "78/281, train_loss: 0.0620, step time: 0.2551\n",
      "79/281, train_loss: 0.1173, step time: 0.2583\n",
      "80/281, train_loss: 0.0482, step time: 0.2530\n",
      "81/281, train_loss: 0.0757, step time: 0.2509\n",
      "82/281, train_loss: 0.0869, step time: 0.2552\n",
      "83/281, train_loss: 0.0675, step time: 0.2533\n",
      "84/281, train_loss: 0.2612, step time: 0.2577\n",
      "85/281, train_loss: 0.1237, step time: 0.2565\n",
      "86/281, train_loss: 0.0549, step time: 0.2577\n",
      "87/281, train_loss: 0.0705, step time: 0.2669\n",
      "88/281, train_loss: 0.1217, step time: 0.2558\n",
      "89/281, train_loss: 0.0970, step time: 0.2541\n",
      "90/281, train_loss: 0.2265, step time: 0.2572\n",
      "91/281, train_loss: 0.1221, step time: 0.2566\n",
      "92/281, train_loss: 0.1112, step time: 0.2576\n",
      "93/281, train_loss: 0.2416, step time: 0.2565\n",
      "94/281, train_loss: 0.0960, step time: 0.2536\n",
      "95/281, train_loss: 0.0909, step time: 0.2561\n",
      "96/281, train_loss: 0.2220, step time: 0.2531\n",
      "97/281, train_loss: 0.0502, step time: 0.2532\n",
      "98/281, train_loss: 0.0568, step time: 0.2584\n",
      "99/281, train_loss: 0.1034, step time: 0.2518\n",
      "100/281, train_loss: 0.0817, step time: 0.2524\n",
      "101/281, train_loss: 0.0927, step time: 0.2569\n",
      "102/281, train_loss: 0.0563, step time: 0.2535\n",
      "103/281, train_loss: 0.0697, step time: 0.2547\n",
      "104/281, train_loss: 0.1040, step time: 0.2537\n",
      "105/281, train_loss: 0.2379, step time: 0.2513\n",
      "106/281, train_loss: 0.0788, step time: 0.2560\n",
      "107/281, train_loss: 0.0795, step time: 0.2500\n",
      "108/281, train_loss: 0.0814, step time: 0.2541\n",
      "109/281, train_loss: 0.0822, step time: 0.2505\n",
      "110/281, train_loss: 0.0675, step time: 0.2518\n",
      "111/281, train_loss: 0.0784, step time: 0.2602\n",
      "112/281, train_loss: 0.0599, step time: 0.2512\n",
      "113/281, train_loss: 0.0884, step time: 0.2535\n",
      "114/281, train_loss: 0.0686, step time: 0.2574\n",
      "115/281, train_loss: 0.0495, step time: 0.2535\n",
      "116/281, train_loss: 0.0998, step time: 0.2571\n",
      "117/281, train_loss: 0.1312, step time: 0.2561\n",
      "118/281, train_loss: 0.0916, step time: 0.2567\n",
      "119/281, train_loss: 0.0751, step time: 0.2502\n",
      "120/281, train_loss: 0.0797, step time: 0.2510\n",
      "121/281, train_loss: 0.0870, step time: 0.2546\n",
      "122/281, train_loss: 0.0848, step time: 0.2511\n",
      "123/281, train_loss: 0.0791, step time: 0.2585\n",
      "124/281, train_loss: 0.0616, step time: 0.2509\n",
      "125/281, train_loss: 0.0767, step time: 0.2519\n",
      "126/281, train_loss: 0.0600, step time: 0.2497\n",
      "127/281, train_loss: 0.1209, step time: 0.2617\n",
      "128/281, train_loss: 0.1402, step time: 0.2600\n",
      "129/281, train_loss: 0.1149, step time: 0.2574\n",
      "130/281, train_loss: 0.0462, step time: 0.2543\n",
      "131/281, train_loss: 0.2609, step time: 0.2570\n",
      "132/281, train_loss: 0.1128, step time: 0.2596\n",
      "133/281, train_loss: 0.2201, step time: 0.2569\n",
      "134/281, train_loss: 0.2674, step time: 0.2569\n",
      "135/281, train_loss: 0.1105, step time: 0.2608\n",
      "136/281, train_loss: 0.0691, step time: 0.2607\n",
      "137/281, train_loss: 0.2456, step time: 0.2574\n",
      "138/281, train_loss: 0.1260, step time: 0.2594\n",
      "139/281, train_loss: 0.0528, step time: 0.2562\n",
      "140/281, train_loss: 0.0717, step time: 0.2569\n",
      "141/281, train_loss: 0.2087, step time: 0.2524\n",
      "142/281, train_loss: 0.2919, step time: 0.2564\n",
      "143/281, train_loss: 0.0773, step time: 0.2658\n",
      "144/281, train_loss: 0.0492, step time: 0.2593\n",
      "145/281, train_loss: 0.1100, step time: 0.2517\n",
      "146/281, train_loss: 0.1122, step time: 0.2511\n",
      "147/281, train_loss: 0.1167, step time: 0.2585\n",
      "148/281, train_loss: 0.2291, step time: 0.2554\n",
      "149/281, train_loss: 0.0987, step time: 0.2571\n",
      "150/281, train_loss: 0.0627, step time: 0.2579\n",
      "151/281, train_loss: 0.0934, step time: 0.2567\n",
      "152/281, train_loss: 0.1174, step time: 0.2650\n",
      "153/281, train_loss: 0.0709, step time: 0.2572\n",
      "154/281, train_loss: 0.2416, step time: 0.2570\n",
      "155/281, train_loss: 0.1505, step time: 0.2577\n",
      "156/281, train_loss: 0.0816, step time: 0.2555\n",
      "157/281, train_loss: 0.0797, step time: 0.2521\n",
      "158/281, train_loss: 0.0911, step time: 0.2553\n",
      "159/281, train_loss: 0.0721, step time: 0.2548\n",
      "160/281, train_loss: 0.2349, step time: 0.2543\n",
      "161/281, train_loss: 0.2576, step time: 0.2550\n",
      "162/281, train_loss: 0.0924, step time: 0.2550\n",
      "163/281, train_loss: 0.0637, step time: 0.2550\n",
      "164/281, train_loss: 0.1074, step time: 0.2485\n",
      "165/281, train_loss: 0.0812, step time: 0.2473\n",
      "166/281, train_loss: 0.1193, step time: 0.2468\n",
      "167/281, train_loss: 0.0662, step time: 0.2510\n",
      "168/281, train_loss: 0.0847, step time: 0.2534\n",
      "169/281, train_loss: 0.1093, step time: 0.2491\n",
      "170/281, train_loss: 0.1043, step time: 0.2538\n",
      "171/281, train_loss: 0.0899, step time: 0.2499\n",
      "172/281, train_loss: 0.2489, step time: 0.2537\n",
      "173/281, train_loss: 0.0510, step time: 0.2506\n",
      "174/281, train_loss: 0.0743, step time: 0.2512\n",
      "175/281, train_loss: 0.0662, step time: 0.2512\n",
      "176/281, train_loss: 0.0944, step time: 0.2565\n",
      "177/281, train_loss: 0.1101, step time: 0.2541\n",
      "178/281, train_loss: 0.0731, step time: 0.2799\n",
      "179/281, train_loss: 0.0878, step time: 0.2540\n",
      "180/281, train_loss: 0.2871, step time: 0.2547\n",
      "181/281, train_loss: 0.0870, step time: 0.2565\n",
      "182/281, train_loss: 0.1208, step time: 0.2571\n",
      "183/281, train_loss: 0.0640, step time: 0.2589\n",
      "184/281, train_loss: 0.0848, step time: 0.2524\n",
      "185/281, train_loss: 0.1077, step time: 0.2508\n",
      "186/281, train_loss: 0.2561, step time: 0.2507\n",
      "187/281, train_loss: 0.1192, step time: 0.2567\n",
      "188/281, train_loss: 0.0869, step time: 0.2541\n",
      "189/281, train_loss: 0.2734, step time: 0.2510\n",
      "190/281, train_loss: 0.2252, step time: 0.2532\n",
      "191/281, train_loss: 0.0613, step time: 0.2498\n",
      "192/281, train_loss: 0.2327, step time: 0.2501\n",
      "193/281, train_loss: 0.0556, step time: 0.2522\n",
      "194/281, train_loss: 0.0887, step time: 0.2521\n",
      "195/281, train_loss: 0.0839, step time: 0.2558\n",
      "196/281, train_loss: 0.0886, step time: 0.2584\n",
      "197/281, train_loss: 0.2284, step time: 0.2517\n",
      "198/281, train_loss: 0.2150, step time: 0.2549\n",
      "199/281, train_loss: 0.0685, step time: 0.2526\n",
      "200/281, train_loss: 0.0751, step time: 0.2537\n",
      "201/281, train_loss: 0.0527, step time: 0.2506\n",
      "202/281, train_loss: 0.0556, step time: 0.2572\n",
      "203/281, train_loss: 0.1155, step time: 0.2556\n",
      "204/281, train_loss: 0.0688, step time: 0.2550\n",
      "205/281, train_loss: 0.0788, step time: 0.2506\n",
      "206/281, train_loss: 0.2593, step time: 0.2589\n",
      "207/281, train_loss: 0.0971, step time: 0.2507\n",
      "208/281, train_loss: 0.1614, step time: 0.2516\n",
      "209/281, train_loss: 0.0583, step time: 0.2531\n",
      "210/281, train_loss: 0.1023, step time: 0.2521\n",
      "211/281, train_loss: 0.0686, step time: 0.2533\n",
      "212/281, train_loss: 0.1139, step time: 0.2512\n",
      "213/281, train_loss: 0.0718, step time: 0.2542\n",
      "214/281, train_loss: 0.0902, step time: 0.2549\n",
      "215/281, train_loss: 0.0897, step time: 0.2580\n",
      "216/281, train_loss: 0.0750, step time: 0.2547\n",
      "217/281, train_loss: 0.0681, step time: 0.2561\n",
      "218/281, train_loss: 0.0947, step time: 0.2583\n",
      "219/281, train_loss: 0.0768, step time: 0.2571\n",
      "220/281, train_loss: 0.1185, step time: 0.2561\n",
      "221/281, train_loss: 0.0815, step time: 0.2484\n",
      "222/281, train_loss: 0.0762, step time: 0.2512\n",
      "223/281, train_loss: 0.2436, step time: 0.2570\n",
      "224/281, train_loss: 0.1033, step time: 0.2568\n",
      "225/281, train_loss: 0.1303, step time: 0.2533\n",
      "226/281, train_loss: 0.2733, step time: 0.2507\n",
      "227/281, train_loss: 0.0670, step time: 0.2501\n",
      "228/281, train_loss: 0.0946, step time: 0.2568\n",
      "229/281, train_loss: 0.0968, step time: 0.2532\n",
      "230/281, train_loss: 0.0693, step time: 0.2515\n",
      "231/281, train_loss: 0.0810, step time: 0.2481\n",
      "232/281, train_loss: 0.1021, step time: 0.2518\n",
      "233/281, train_loss: 0.0515, step time: 0.2467\n",
      "234/281, train_loss: 0.0851, step time: 0.2531\n",
      "235/281, train_loss: 0.0725, step time: 0.2582\n",
      "236/281, train_loss: 0.0744, step time: 0.2583\n",
      "237/281, train_loss: 0.2200, step time: 0.2514\n",
      "238/281, train_loss: 0.0678, step time: 0.2550\n",
      "239/281, train_loss: 0.0940, step time: 0.2496\n",
      "240/281, train_loss: 0.0716, step time: 0.2527\n",
      "241/281, train_loss: 0.0637, step time: 0.2495\n",
      "242/281, train_loss: 0.0621, step time: 0.2481\n",
      "243/281, train_loss: 0.0793, step time: 0.2500\n",
      "244/281, train_loss: 0.1992, step time: 0.2500\n",
      "245/281, train_loss: 0.0566, step time: 0.2453\n",
      "246/281, train_loss: 0.0899, step time: 0.2504\n",
      "247/281, train_loss: 0.2075, step time: 0.2454\n",
      "248/281, train_loss: 0.0823, step time: 0.2464\n",
      "249/281, train_loss: 0.0769, step time: 0.2512\n",
      "250/281, train_loss: 0.0498, step time: 0.2506\n",
      "251/281, train_loss: 0.0719, step time: 0.2512\n",
      "252/281, train_loss: 0.2447, step time: 0.2482\n",
      "253/281, train_loss: 0.0339, step time: 0.2439\n",
      "254/281, train_loss: 0.1070, step time: 0.2440\n",
      "255/281, train_loss: 0.0701, step time: 0.2448\n",
      "256/281, train_loss: 0.0717, step time: 0.2426\n",
      "257/281, train_loss: 0.0996, step time: 0.2517\n",
      "258/281, train_loss: 0.0779, step time: 0.2512\n",
      "259/281, train_loss: 0.0711, step time: 0.2525\n",
      "260/281, train_loss: 0.1440, step time: 0.2483\n",
      "261/281, train_loss: 0.1084, step time: 0.2497\n",
      "262/281, train_loss: 0.2452, step time: 0.2474\n",
      "263/281, train_loss: 0.0845, step time: 0.2481\n",
      "264/281, train_loss: 0.2201, step time: 0.2421\n",
      "265/281, train_loss: 0.0602, step time: 0.2453\n",
      "266/281, train_loss: 0.2342, step time: 0.2491\n",
      "267/281, train_loss: 0.1537, step time: 0.2436\n",
      "268/281, train_loss: 0.0864, step time: 0.2465\n",
      "269/281, train_loss: 0.0688, step time: 0.2475\n",
      "270/281, train_loss: 0.0477, step time: 0.2482\n",
      "271/281, train_loss: 0.0680, step time: 0.2502\n",
      "272/281, train_loss: 0.2491, step time: 0.2499\n",
      "273/281, train_loss: 0.0875, step time: 0.2492\n",
      "274/281, train_loss: 0.1019, step time: 0.2486\n",
      "275/281, train_loss: 0.0784, step time: 0.2515\n",
      "276/281, train_loss: 0.1057, step time: 0.2479\n",
      "277/281, train_loss: 0.0916, step time: 0.2482\n",
      "278/281, train_loss: 0.0817, step time: 0.2505\n",
      "279/281, train_loss: 0.0708, step time: 0.2509\n",
      "280/281, train_loss: 0.0742, step time: 0.2509\n",
      "281/281, train_loss: 0.0701, step time: 0.2471\n",
      "282/281, train_loss: 0.0509, step time: 0.1495\n",
      "epoch 68 average loss: 0.1089\n",
      "saved new best metric model\n",
      "current epoch: 68 current mean dice: 0.8878 tc: 0.8828 wt: 0.9197 et: 0.8708\n",
      "best mean dice: 0.8878 at epoch: 68\n",
      "time consuming of epoch 68 is: 400.3194\n",
      "----------\n",
      "epoch 69/200\n",
      "1/281, train_loss: 0.0808, step time: 0.2465\n",
      "2/281, train_loss: 0.0845, step time: 0.2401\n",
      "3/281, train_loss: 0.2249, step time: 0.2422\n",
      "4/281, train_loss: 0.2404, step time: 0.2445\n",
      "5/281, train_loss: 0.2356, step time: 0.2477\n",
      "6/281, train_loss: 0.0825, step time: 0.2501\n",
      "7/281, train_loss: 0.0593, step time: 0.2481\n",
      "8/281, train_loss: 0.0882, step time: 0.2565\n",
      "9/281, train_loss: 0.0857, step time: 0.2489\n",
      "10/281, train_loss: 0.1149, step time: 0.2491\n",
      "11/281, train_loss: 0.0643, step time: 0.2488\n",
      "12/281, train_loss: 0.1292, step time: 0.2481\n",
      "13/281, train_loss: 0.0743, step time: 0.2586\n",
      "14/281, train_loss: 0.0863, step time: 0.2543\n",
      "15/281, train_loss: 0.0759, step time: 0.2496\n",
      "16/281, train_loss: 0.0632, step time: 0.2471\n",
      "17/281, train_loss: 0.0747, step time: 0.2500\n",
      "18/281, train_loss: 0.0917, step time: 0.2446\n",
      "19/281, train_loss: 0.0616, step time: 0.2441\n",
      "20/281, train_loss: 0.0730, step time: 0.2455\n",
      "21/281, train_loss: 0.0873, step time: 0.2489\n",
      "22/281, train_loss: 0.0757, step time: 0.2478\n",
      "23/281, train_loss: 0.0787, step time: 0.2428\n",
      "24/281, train_loss: 0.3788, step time: 0.2423\n",
      "25/281, train_loss: 0.0549, step time: 0.2527\n",
      "26/281, train_loss: 0.0760, step time: 0.2536\n",
      "27/281, train_loss: 0.1383, step time: 0.2477\n",
      "28/281, train_loss: 0.0913, step time: 0.2459\n",
      "29/281, train_loss: 0.0513, step time: 0.2489\n",
      "30/281, train_loss: 0.4297, step time: 0.2496\n",
      "31/281, train_loss: 0.2445, step time: 0.2529\n",
      "32/281, train_loss: 0.2259, step time: 0.2475\n",
      "33/281, train_loss: 0.0573, step time: 0.2534\n",
      "34/281, train_loss: 0.1037, step time: 0.2515\n",
      "35/281, train_loss: 0.0337, step time: 0.2491\n",
      "36/281, train_loss: 0.1367, step time: 0.2476\n",
      "37/281, train_loss: 0.0718, step time: 0.2485\n",
      "38/281, train_loss: 0.2564, step time: 0.2524\n",
      "39/281, train_loss: 0.1243, step time: 0.2517\n",
      "40/281, train_loss: 0.0873, step time: 0.2552\n",
      "41/281, train_loss: 0.0917, step time: 0.2490\n",
      "42/281, train_loss: 0.1158, step time: 0.2515\n",
      "43/281, train_loss: 0.0726, step time: 0.2564\n",
      "44/281, train_loss: 0.0993, step time: 0.2518\n",
      "45/281, train_loss: 0.0383, step time: 0.2492\n",
      "46/281, train_loss: 0.0775, step time: 0.2526\n",
      "47/281, train_loss: 0.0865, step time: 0.2459\n",
      "48/281, train_loss: 0.0568, step time: 0.2497\n",
      "49/281, train_loss: 0.0637, step time: 0.2530\n",
      "50/281, train_loss: 0.0981, step time: 0.2614\n",
      "51/281, train_loss: 0.0619, step time: 0.2876\n",
      "52/281, train_loss: 0.2304, step time: 0.2527\n",
      "53/281, train_loss: 0.0803, step time: 0.2487\n",
      "54/281, train_loss: 0.0828, step time: 0.2516\n",
      "55/281, train_loss: 0.0648, step time: 0.2516\n",
      "56/281, train_loss: 0.0826, step time: 0.2534\n",
      "57/281, train_loss: 0.1234, step time: 0.2534\n",
      "58/281, train_loss: 0.1191, step time: 0.2529\n",
      "59/281, train_loss: 0.0750, step time: 0.2516\n",
      "60/281, train_loss: 0.2362, step time: 0.2555\n",
      "61/281, train_loss: 0.0794, step time: 0.2554\n",
      "62/281, train_loss: 0.1049, step time: 0.2517\n",
      "63/281, train_loss: 0.0866, step time: 0.2496\n",
      "64/281, train_loss: 0.0585, step time: 0.2505\n",
      "65/281, train_loss: 0.0711, step time: 0.2599\n",
      "66/281, train_loss: 0.0789, step time: 0.2583\n",
      "67/281, train_loss: 0.1477, step time: 0.2543\n",
      "68/281, train_loss: 0.0787, step time: 0.2538\n",
      "69/281, train_loss: 0.1347, step time: 0.2585\n",
      "70/281, train_loss: 0.0606, step time: 0.2552\n",
      "71/281, train_loss: 0.0860, step time: 0.2553\n",
      "72/281, train_loss: 0.0971, step time: 0.2532\n",
      "73/281, train_loss: 0.0853, step time: 0.2526\n",
      "74/281, train_loss: 0.0428, step time: 0.2527\n",
      "75/281, train_loss: 0.2344, step time: 0.2503\n",
      "76/281, train_loss: 0.0776, step time: 0.2527\n",
      "77/281, train_loss: 0.1049, step time: 0.2469\n",
      "78/281, train_loss: 0.0830, step time: 0.2538\n",
      "79/281, train_loss: 0.0587, step time: 0.2590\n",
      "80/281, train_loss: 0.1550, step time: 0.2601\n",
      "81/281, train_loss: 0.0508, step time: 0.2471\n",
      "82/281, train_loss: 0.1131, step time: 0.2470\n",
      "83/281, train_loss: 0.1000, step time: 0.2457\n",
      "84/281, train_loss: 0.0656, step time: 0.2462\n",
      "85/281, train_loss: 0.0680, step time: 0.2517\n",
      "86/281, train_loss: 0.2342, step time: 0.2535\n",
      "87/281, train_loss: 0.0991, step time: 0.2480\n",
      "88/281, train_loss: 0.0749, step time: 0.2536\n",
      "89/281, train_loss: 0.0870, step time: 0.2510\n",
      "90/281, train_loss: 0.4098, step time: 0.2490\n",
      "91/281, train_loss: 0.0775, step time: 0.2435\n",
      "92/281, train_loss: 0.0695, step time: 0.2449\n",
      "93/281, train_loss: 0.2283, step time: 0.2550\n",
      "94/281, train_loss: 0.1143, step time: 0.2583\n",
      "95/281, train_loss: 0.0832, step time: 0.2514\n",
      "96/281, train_loss: 0.0726, step time: 0.2490\n",
      "97/281, train_loss: 0.2487, step time: 0.2517\n",
      "98/281, train_loss: 0.0750, step time: 0.2510\n",
      "99/281, train_loss: 0.0596, step time: 0.2528\n",
      "100/281, train_loss: 0.1926, step time: 0.2550\n",
      "101/281, train_loss: 0.0809, step time: 0.2543\n",
      "102/281, train_loss: 0.0822, step time: 0.2571\n",
      "103/281, train_loss: 0.2344, step time: 0.2507\n",
      "104/281, train_loss: 0.0775, step time: 0.2531\n",
      "105/281, train_loss: 0.0961, step time: 0.2534\n",
      "106/281, train_loss: 0.0413, step time: 0.2514\n",
      "107/281, train_loss: 0.0738, step time: 0.2527\n",
      "108/281, train_loss: 0.0592, step time: 0.2514\n",
      "109/281, train_loss: 0.1396, step time: 0.2482\n",
      "110/281, train_loss: 0.0627, step time: 0.2570\n",
      "111/281, train_loss: 0.0627, step time: 0.2538\n",
      "112/281, train_loss: 0.1178, step time: 0.2577\n",
      "113/281, train_loss: 0.0796, step time: 0.2515\n",
      "114/281, train_loss: 0.0560, step time: 0.2546\n",
      "115/281, train_loss: 0.0929, step time: 0.2527\n",
      "116/281, train_loss: 0.1060, step time: 0.2539\n",
      "117/281, train_loss: 0.0861, step time: 0.2549\n",
      "118/281, train_loss: 0.1036, step time: 0.2538\n",
      "119/281, train_loss: 0.0848, step time: 0.2577\n",
      "120/281, train_loss: 0.2439, step time: 0.2545\n",
      "121/281, train_loss: 0.0633, step time: 0.2530\n",
      "122/281, train_loss: 0.1014, step time: 0.2547\n",
      "123/281, train_loss: 0.1032, step time: 0.2562\n",
      "124/281, train_loss: 0.1018, step time: 0.2527\n",
      "125/281, train_loss: 0.1287, step time: 0.2545\n",
      "126/281, train_loss: 0.0748, step time: 0.2547\n",
      "127/281, train_loss: 0.2599, step time: 0.2559\n",
      "128/281, train_loss: 0.0821, step time: 0.2541\n",
      "129/281, train_loss: 0.0880, step time: 0.2475\n",
      "130/281, train_loss: 0.0660, step time: 0.2541\n",
      "131/281, train_loss: 0.1192, step time: 0.2540\n",
      "132/281, train_loss: 0.0879, step time: 0.2544\n",
      "133/281, train_loss: 0.0956, step time: 0.2547\n",
      "134/281, train_loss: 0.0590, step time: 0.2538\n",
      "135/281, train_loss: 0.0571, step time: 0.2618\n",
      "136/281, train_loss: 0.0524, step time: 0.2568\n",
      "137/281, train_loss: 0.1091, step time: 0.2531\n",
      "138/281, train_loss: 0.2433, step time: 0.2510\n",
      "139/281, train_loss: 0.0883, step time: 0.2497\n",
      "140/281, train_loss: 0.0709, step time: 0.2530\n",
      "141/281, train_loss: 0.2339, step time: 0.2523\n",
      "142/281, train_loss: 0.0879, step time: 0.2545\n",
      "143/281, train_loss: 0.1077, step time: 0.2528\n",
      "144/281, train_loss: 0.0654, step time: 0.2479\n",
      "145/281, train_loss: 0.0855, step time: 0.2538\n",
      "146/281, train_loss: 0.0692, step time: 0.2584\n",
      "147/281, train_loss: 0.0837, step time: 0.2520\n",
      "148/281, train_loss: 0.0991, step time: 0.2526\n",
      "149/281, train_loss: 0.1107, step time: 0.2539\n",
      "150/281, train_loss: 0.0728, step time: 0.2551\n",
      "151/281, train_loss: 0.2111, step time: 0.2523\n",
      "152/281, train_loss: 0.2335, step time: 0.2553\n",
      "153/281, train_loss: 0.1291, step time: 0.2544\n",
      "154/281, train_loss: 0.0461, step time: 0.2513\n",
      "155/281, train_loss: 0.0681, step time: 0.2565\n",
      "156/281, train_loss: 0.0589, step time: 0.2622\n",
      "157/281, train_loss: 0.0933, step time: 0.2538\n",
      "158/281, train_loss: 0.1011, step time: 0.2564\n",
      "159/281, train_loss: 0.0792, step time: 0.2574\n",
      "160/281, train_loss: 0.2600, step time: 0.2566\n",
      "161/281, train_loss: 0.0887, step time: 0.2537\n",
      "162/281, train_loss: 0.0601, step time: 0.2521\n",
      "163/281, train_loss: 0.1130, step time: 0.2488\n",
      "164/281, train_loss: 0.0436, step time: 0.2507\n",
      "165/281, train_loss: 0.1020, step time: 0.2536\n",
      "166/281, train_loss: 0.0697, step time: 0.2512\n",
      "167/281, train_loss: 0.0794, step time: 0.2490\n",
      "168/281, train_loss: 0.0442, step time: 0.2526\n",
      "169/281, train_loss: 0.0527, step time: 0.2511\n",
      "170/281, train_loss: 0.0619, step time: 0.2584\n",
      "171/281, train_loss: 0.1486, step time: 0.2583\n",
      "172/281, train_loss: 0.1043, step time: 0.2586\n",
      "173/281, train_loss: 0.0832, step time: 0.2514\n",
      "174/281, train_loss: 0.1299, step time: 0.2515\n",
      "175/281, train_loss: 0.0866, step time: 0.2509\n",
      "176/281, train_loss: 0.0766, step time: 0.2490\n",
      "177/281, train_loss: 0.0780, step time: 0.2546\n",
      "178/281, train_loss: 0.1122, step time: 0.2561\n",
      "179/281, train_loss: 0.0769, step time: 0.2587\n",
      "180/281, train_loss: 0.2317, step time: 0.2573\n",
      "181/281, train_loss: 0.0855, step time: 0.2591\n",
      "182/281, train_loss: 0.2471, step time: 0.2581\n",
      "183/281, train_loss: 0.0720, step time: 0.2557\n",
      "184/281, train_loss: 0.0762, step time: 0.2575\n",
      "185/281, train_loss: 0.0905, step time: 0.2550\n",
      "186/281, train_loss: 0.0818, step time: 0.2501\n",
      "187/281, train_loss: 0.0783, step time: 0.2586\n",
      "188/281, train_loss: 0.0617, step time: 0.2504\n",
      "189/281, train_loss: 0.0563, step time: 0.2525\n",
      "190/281, train_loss: 0.0796, step time: 0.2505\n",
      "191/281, train_loss: 0.0779, step time: 0.2559\n",
      "192/281, train_loss: 0.0694, step time: 0.2588\n",
      "193/281, train_loss: 0.0568, step time: 0.2577\n",
      "194/281, train_loss: 0.0508, step time: 0.2555\n",
      "195/281, train_loss: 0.0745, step time: 0.2542\n",
      "196/281, train_loss: 0.0492, step time: 0.2572\n",
      "197/281, train_loss: 0.1111, step time: 0.2574\n",
      "198/281, train_loss: 0.0747, step time: 0.2542\n",
      "199/281, train_loss: 0.1113, step time: 0.2563\n",
      "200/281, train_loss: 0.0430, step time: 0.2605\n",
      "201/281, train_loss: 0.0946, step time: 0.2548\n",
      "202/281, train_loss: 0.0650, step time: 0.2586\n",
      "203/281, train_loss: 0.0496, step time: 0.2596\n",
      "204/281, train_loss: 0.2307, step time: 0.2802\n",
      "205/281, train_loss: 0.1101, step time: 0.2536\n",
      "206/281, train_loss: 0.1001, step time: 0.2556\n",
      "207/281, train_loss: 0.1234, step time: 0.2552\n",
      "208/281, train_loss: 0.0918, step time: 0.2581\n",
      "209/281, train_loss: 0.0834, step time: 0.2620\n",
      "210/281, train_loss: 0.0751, step time: 0.2578\n",
      "211/281, train_loss: 0.2412, step time: 0.2527\n",
      "212/281, train_loss: 0.0508, step time: 0.2577\n",
      "213/281, train_loss: 0.0670, step time: 0.2557\n",
      "214/281, train_loss: 0.0545, step time: 0.2568\n",
      "215/281, train_loss: 0.0800, step time: 0.2544\n",
      "216/281, train_loss: 0.0933, step time: 0.2546\n",
      "217/281, train_loss: 0.0807, step time: 0.2496\n",
      "218/281, train_loss: 0.0845, step time: 0.2553\n",
      "219/281, train_loss: 0.0750, step time: 0.2569\n",
      "220/281, train_loss: 0.0885, step time: 0.2619\n",
      "221/281, train_loss: 0.0820, step time: 0.2578\n",
      "222/281, train_loss: 0.0739, step time: 0.2571\n",
      "223/281, train_loss: 0.0715, step time: 0.2607\n",
      "224/281, train_loss: 0.2256, step time: 0.2603\n",
      "225/281, train_loss: 0.0688, step time: 0.2573\n",
      "226/281, train_loss: 0.2108, step time: 0.2597\n",
      "227/281, train_loss: 0.0921, step time: 0.2572\n",
      "228/281, train_loss: 0.0751, step time: 0.2583\n",
      "229/281, train_loss: 0.0835, step time: 0.2519\n",
      "230/281, train_loss: 0.0816, step time: 0.2537\n",
      "231/281, train_loss: 0.0405, step time: 0.2538\n",
      "232/281, train_loss: 0.0851, step time: 0.2542\n",
      "233/281, train_loss: 0.1228, step time: 0.2589\n",
      "234/281, train_loss: 0.2058, step time: 0.2573\n",
      "235/281, train_loss: 0.0566, step time: 0.2606\n",
      "236/281, train_loss: 0.0472, step time: 0.2607\n",
      "237/281, train_loss: 0.1424, step time: 0.2589\n",
      "238/281, train_loss: 0.0684, step time: 0.2587\n",
      "239/281, train_loss: 0.0718, step time: 0.2576\n",
      "240/281, train_loss: 0.1412, step time: 0.2601\n",
      "241/281, train_loss: 0.0656, step time: 0.2621\n",
      "242/281, train_loss: 0.2628, step time: 0.2657\n",
      "243/281, train_loss: 0.0719, step time: 0.2561\n",
      "244/281, train_loss: 0.0624, step time: 0.2555\n",
      "245/281, train_loss: 0.1112, step time: 0.2547\n",
      "246/281, train_loss: 0.3099, step time: 0.2525\n",
      "247/281, train_loss: 0.0991, step time: 0.2571\n",
      "248/281, train_loss: 0.0550, step time: 0.2551\n",
      "249/281, train_loss: 0.0830, step time: 0.2530\n",
      "250/281, train_loss: 0.2360, step time: 0.2595\n",
      "251/281, train_loss: 0.2459, step time: 0.2634\n",
      "252/281, train_loss: 0.0585, step time: 0.2511\n",
      "253/281, train_loss: 0.0868, step time: 0.2554\n",
      "254/281, train_loss: 0.1127, step time: 0.2515\n",
      "255/281, train_loss: 0.0934, step time: 0.2527\n",
      "256/281, train_loss: 0.0947, step time: 0.2573\n",
      "257/281, train_loss: 0.2607, step time: 0.2600\n",
      "258/281, train_loss: 0.1143, step time: 0.2485\n",
      "259/281, train_loss: 0.0959, step time: 0.2508\n",
      "260/281, train_loss: 0.1181, step time: 0.2519\n",
      "261/281, train_loss: 0.0469, step time: 0.2551\n",
      "262/281, train_loss: 0.0669, step time: 0.2519\n",
      "263/281, train_loss: 0.2427, step time: 0.2539\n",
      "264/281, train_loss: 0.0795, step time: 0.2588\n",
      "265/281, train_loss: 0.0662, step time: 0.2553\n",
      "266/281, train_loss: 0.0828, step time: 0.2542\n",
      "267/281, train_loss: 0.0896, step time: 0.2552\n",
      "268/281, train_loss: 0.2404, step time: 0.2554\n",
      "269/281, train_loss: 0.0578, step time: 0.2483\n",
      "270/281, train_loss: 0.2616, step time: 0.2511\n",
      "271/281, train_loss: 0.1085, step time: 0.2552\n",
      "272/281, train_loss: 0.2753, step time: 0.2588\n",
      "273/281, train_loss: 0.0648, step time: 0.2483\n",
      "274/281, train_loss: 0.0540, step time: 0.2524\n",
      "275/281, train_loss: 0.1053, step time: 0.2525\n",
      "276/281, train_loss: 0.0967, step time: 0.2526\n",
      "277/281, train_loss: 0.2335, step time: 0.2516\n",
      "278/281, train_loss: 0.1193, step time: 0.2563\n",
      "279/281, train_loss: 0.0953, step time: 0.2507\n",
      "280/281, train_loss: 0.0913, step time: 0.2528\n",
      "281/281, train_loss: 0.0729, step time: 0.2512\n",
      "282/281, train_loss: 0.4190, step time: 0.1504\n",
      "epoch 69 average loss: 0.1089\n",
      "current epoch: 69 current mean dice: 0.8848 tc: 0.8759 wt: 0.9179 et: 0.8707\n",
      "best mean dice: 0.8878 at epoch: 68\n",
      "time consuming of epoch 69 is: 409.8692\n",
      "----------\n",
      "epoch 70/200\n",
      "1/281, train_loss: 0.0605, step time: 0.2633\n",
      "2/281, train_loss: 0.0776, step time: 0.2510\n",
      "3/281, train_loss: 0.2485, step time: 0.2496\n",
      "4/281, train_loss: 0.0742, step time: 0.2513\n",
      "5/281, train_loss: 0.0819, step time: 0.2552\n",
      "6/281, train_loss: 0.1174, step time: 0.2533\n",
      "7/281, train_loss: 0.0730, step time: 0.2597\n",
      "8/281, train_loss: 0.0789, step time: 0.2567\n",
      "9/281, train_loss: 0.0720, step time: 0.2514\n",
      "10/281, train_loss: 0.0832, step time: 0.2584\n",
      "11/281, train_loss: 0.1721, step time: 0.2606\n",
      "12/281, train_loss: 0.2322, step time: 0.2629\n",
      "13/281, train_loss: 0.0651, step time: 0.2566\n",
      "14/281, train_loss: 0.0648, step time: 0.2584\n",
      "15/281, train_loss: 0.1188, step time: 0.2594\n",
      "16/281, train_loss: 0.0755, step time: 0.2523\n",
      "17/281, train_loss: 0.1321, step time: 0.2552\n",
      "18/281, train_loss: 0.0973, step time: 0.2620\n",
      "19/281, train_loss: 0.0761, step time: 0.2566\n",
      "20/281, train_loss: 0.0776, step time: 0.2616\n",
      "21/281, train_loss: 0.0884, step time: 0.2600\n",
      "22/281, train_loss: 0.0388, step time: 0.2580\n",
      "23/281, train_loss: 0.1375, step time: 0.2531\n",
      "24/281, train_loss: 0.0794, step time: 0.2556\n",
      "25/281, train_loss: 0.1384, step time: 0.2575\n",
      "26/281, train_loss: 0.1137, step time: 0.2577\n",
      "27/281, train_loss: 0.1083, step time: 0.2511\n",
      "28/281, train_loss: 0.0839, step time: 0.2552\n",
      "29/281, train_loss: 0.0667, step time: 0.2579\n",
      "30/281, train_loss: 0.0729, step time: 0.2502\n",
      "31/281, train_loss: 0.2427, step time: 0.2550\n",
      "32/281, train_loss: 0.0604, step time: 0.2573\n",
      "33/281, train_loss: 0.0758, step time: 0.2622\n",
      "34/281, train_loss: 0.1082, step time: 0.2614\n",
      "35/281, train_loss: 0.1425, step time: 0.2645\n",
      "36/281, train_loss: 0.0506, step time: 0.2576\n",
      "37/281, train_loss: 0.0558, step time: 0.2585\n",
      "38/281, train_loss: 0.1252, step time: 0.2553\n",
      "39/281, train_loss: 0.1173, step time: 0.2508\n",
      "40/281, train_loss: 0.0751, step time: 0.2537\n",
      "41/281, train_loss: 0.0791, step time: 0.2574\n",
      "42/281, train_loss: 0.2326, step time: 0.2488\n",
      "43/281, train_loss: 0.1219, step time: 0.2512\n",
      "44/281, train_loss: 0.0528, step time: 0.2511\n",
      "45/281, train_loss: 0.0889, step time: 0.2514\n",
      "46/281, train_loss: 0.0842, step time: 0.2598\n",
      "47/281, train_loss: 0.0640, step time: 0.2580\n",
      "48/281, train_loss: 0.0847, step time: 0.2590\n",
      "49/281, train_loss: 0.0691, step time: 0.2568\n",
      "50/281, train_loss: 0.0746, step time: 0.2533\n",
      "51/281, train_loss: 0.0736, step time: 0.2577\n",
      "52/281, train_loss: 0.0883, step time: 0.2575\n",
      "53/281, train_loss: 0.0752, step time: 0.2569\n",
      "54/281, train_loss: 0.0831, step time: 0.2528\n",
      "55/281, train_loss: 0.1033, step time: 0.2580\n",
      "56/281, train_loss: 0.2688, step time: 0.2566\n",
      "57/281, train_loss: 0.0832, step time: 0.2588\n",
      "58/281, train_loss: 0.0853, step time: 0.2574\n",
      "59/281, train_loss: 0.2198, step time: 0.2579\n",
      "60/281, train_loss: 0.1208, step time: 0.2581\n",
      "61/281, train_loss: 0.0620, step time: 0.2609\n",
      "62/281, train_loss: 0.0747, step time: 0.2646\n",
      "63/281, train_loss: 0.1292, step time: 0.2628\n",
      "64/281, train_loss: 0.0845, step time: 0.2576\n",
      "65/281, train_loss: 0.0522, step time: 0.2576\n",
      "66/281, train_loss: 0.1032, step time: 0.2598\n",
      "67/281, train_loss: 0.0565, step time: 0.2536\n",
      "68/281, train_loss: 0.0827, step time: 0.2529\n",
      "69/281, train_loss: 0.0844, step time: 0.2696\n",
      "70/281, train_loss: 0.1081, step time: 0.2830\n",
      "71/281, train_loss: 0.0680, step time: 0.2522\n",
      "72/281, train_loss: 0.1093, step time: 0.2593\n",
      "73/281, train_loss: 0.2479, step time: 0.2568\n",
      "74/281, train_loss: 0.2377, step time: 0.2614\n",
      "75/281, train_loss: 0.2396, step time: 0.2573\n",
      "76/281, train_loss: 0.0624, step time: 0.2549\n",
      "77/281, train_loss: 0.1004, step time: 0.2569\n",
      "78/281, train_loss: 0.0564, step time: 0.2592\n",
      "79/281, train_loss: 0.1365, step time: 0.2558\n",
      "80/281, train_loss: 0.2581, step time: 0.2541\n",
      "81/281, train_loss: 0.0942, step time: 0.2532\n",
      "82/281, train_loss: 0.0743, step time: 0.2561\n",
      "83/281, train_loss: 0.2557, step time: 0.2569\n",
      "84/281, train_loss: 0.0883, step time: 0.2566\n",
      "85/281, train_loss: 0.0694, step time: 0.2580\n",
      "86/281, train_loss: 0.0579, step time: 0.2571\n",
      "87/281, train_loss: 0.1035, step time: 0.2596\n",
      "88/281, train_loss: 0.2590, step time: 0.2569\n",
      "89/281, train_loss: 0.0537, step time: 0.2632\n",
      "90/281, train_loss: 0.0616, step time: 0.2597\n",
      "91/281, train_loss: 0.0632, step time: 0.2561\n",
      "92/281, train_loss: 0.0467, step time: 0.2625\n",
      "93/281, train_loss: 0.0776, step time: 0.2563\n",
      "94/281, train_loss: 0.0465, step time: 0.2647\n",
      "95/281, train_loss: 0.0474, step time: 0.2534\n",
      "96/281, train_loss: 0.0793, step time: 0.2589\n",
      "97/281, train_loss: 0.1024, step time: 0.2600\n",
      "98/281, train_loss: 0.2298, step time: 0.2720\n",
      "99/281, train_loss: 0.1385, step time: 0.2583\n",
      "100/281, train_loss: 0.0909, step time: 0.2577\n",
      "101/281, train_loss: 0.1354, step time: 0.2597\n",
      "102/281, train_loss: 0.0882, step time: 0.2571\n",
      "103/281, train_loss: 0.0839, step time: 0.2632\n",
      "104/281, train_loss: 0.0653, step time: 0.2608\n",
      "105/281, train_loss: 0.1122, step time: 0.2589\n",
      "106/281, train_loss: 0.0796, step time: 0.2612\n",
      "107/281, train_loss: 0.0859, step time: 0.2581\n",
      "108/281, train_loss: 0.2569, step time: 0.2620\n",
      "109/281, train_loss: 0.2312, step time: 0.2587\n",
      "110/281, train_loss: 0.0936, step time: 0.2592\n",
      "111/281, train_loss: 0.0807, step time: 0.2574\n",
      "112/281, train_loss: 0.2490, step time: 0.2562\n",
      "113/281, train_loss: 0.0463, step time: 0.2584\n",
      "114/281, train_loss: 0.1181, step time: 0.2542\n",
      "115/281, train_loss: 0.2770, step time: 0.2534\n",
      "116/281, train_loss: 0.0840, step time: 0.2577\n",
      "117/281, train_loss: 0.0827, step time: 0.2556\n",
      "118/281, train_loss: 0.0832, step time: 0.2661\n",
      "119/281, train_loss: 0.2475, step time: 0.2622\n",
      "120/281, train_loss: 0.0845, step time: 0.2541\n",
      "121/281, train_loss: 0.2191, step time: 0.2560\n",
      "122/281, train_loss: 0.0791, step time: 0.2624\n",
      "123/281, train_loss: 0.0755, step time: 0.2604\n",
      "124/281, train_loss: 0.0729, step time: 0.2596\n",
      "125/281, train_loss: 0.2420, step time: 0.2608\n",
      "126/281, train_loss: 0.1209, step time: 0.2562\n",
      "127/281, train_loss: 0.0690, step time: 0.2579\n",
      "128/281, train_loss: 0.1203, step time: 0.2603\n",
      "129/281, train_loss: 0.0865, step time: 0.2550\n",
      "130/281, train_loss: 0.0630, step time: 0.2571\n",
      "131/281, train_loss: 0.2448, step time: 0.2688\n",
      "132/281, train_loss: 0.0562, step time: 0.2688\n",
      "133/281, train_loss: 0.0783, step time: 0.2575\n",
      "134/281, train_loss: 0.0668, step time: 0.2594\n",
      "135/281, train_loss: 0.0688, step time: 0.2600\n",
      "136/281, train_loss: 0.2708, step time: 0.2571\n",
      "137/281, train_loss: 0.0779, step time: 0.2554\n",
      "138/281, train_loss: 0.0404, step time: 0.2601\n",
      "139/281, train_loss: 0.0725, step time: 0.2617\n",
      "140/281, train_loss: 0.0716, step time: 0.2723\n",
      "141/281, train_loss: 0.1287, step time: 0.2574\n",
      "142/281, train_loss: 0.0944, step time: 0.2718\n",
      "143/281, train_loss: 0.0883, step time: 0.2594\n",
      "144/281, train_loss: 0.1357, step time: 0.2610\n",
      "145/281, train_loss: 0.1186, step time: 0.2572\n",
      "146/281, train_loss: 0.0903, step time: 0.2543\n",
      "147/281, train_loss: 0.1071, step time: 0.2596\n",
      "148/281, train_loss: 0.0860, step time: 0.2584\n",
      "149/281, train_loss: 0.0866, step time: 0.2581\n",
      "150/281, train_loss: 0.0789, step time: 0.2625\n",
      "151/281, train_loss: 0.1290, step time: 0.2574\n",
      "152/281, train_loss: 0.0665, step time: 0.2552\n",
      "153/281, train_loss: 0.2364, step time: 0.2596\n",
      "154/281, train_loss: 0.1021, step time: 0.2569\n",
      "155/281, train_loss: 0.3024, step time: 0.2504\n",
      "156/281, train_loss: 0.0724, step time: 0.2642\n",
      "157/281, train_loss: 0.2406, step time: 0.2582\n",
      "158/281, train_loss: 0.1527, step time: 0.2557\n",
      "159/281, train_loss: 0.2427, step time: 0.2593\n",
      "160/281, train_loss: 0.0766, step time: 0.2559\n",
      "161/281, train_loss: 0.0676, step time: 0.2569\n",
      "162/281, train_loss: 0.0770, step time: 0.2573\n",
      "163/281, train_loss: 0.0854, step time: 0.2530\n",
      "164/281, train_loss: 0.0607, step time: 0.2588\n",
      "165/281, train_loss: 0.1320, step time: 0.2582\n",
      "166/281, train_loss: 0.0728, step time: 0.2563\n",
      "167/281, train_loss: 0.0479, step time: 0.2561\n",
      "168/281, train_loss: 0.0992, step time: 0.2630\n",
      "169/281, train_loss: 0.0680, step time: 0.2578\n",
      "170/281, train_loss: 0.0701, step time: 0.2613\n",
      "171/281, train_loss: 0.1031, step time: 0.2556\n",
      "172/281, train_loss: 0.1065, step time: 0.2560\n",
      "173/281, train_loss: 0.0905, step time: 0.2575\n",
      "174/281, train_loss: 0.0691, step time: 0.2548\n",
      "175/281, train_loss: 0.1058, step time: 0.2558\n",
      "176/281, train_loss: 0.0678, step time: 0.2604\n",
      "177/281, train_loss: 0.1066, step time: 0.2607\n",
      "178/281, train_loss: 0.0925, step time: 0.2630\n",
      "179/281, train_loss: 0.2278, step time: 0.2594\n",
      "180/281, train_loss: 0.0646, step time: 0.2527\n",
      "181/281, train_loss: 0.0875, step time: 0.2532\n",
      "182/281, train_loss: 0.0646, step time: 0.2518\n",
      "183/281, train_loss: 0.0820, step time: 0.2502\n",
      "184/281, train_loss: 0.2346, step time: 0.2539\n",
      "185/281, train_loss: 0.1181, step time: 0.2583\n",
      "186/281, train_loss: 0.2406, step time: 0.2542\n",
      "187/281, train_loss: 0.0614, step time: 0.2580\n",
      "188/281, train_loss: 0.2251, step time: 0.2510\n",
      "189/281, train_loss: 0.0897, step time: 0.2518\n",
      "190/281, train_loss: 0.2177, step time: 0.2554\n",
      "191/281, train_loss: 0.2417, step time: 0.2549\n",
      "192/281, train_loss: 0.0771, step time: 0.2542\n",
      "193/281, train_loss: 0.1162, step time: 0.2580\n",
      "194/281, train_loss: 0.1127, step time: 0.2576\n",
      "195/281, train_loss: 0.0919, step time: 0.2538\n",
      "196/281, train_loss: 0.1055, step time: 0.2513\n",
      "197/281, train_loss: 0.0837, step time: 0.2562\n",
      "198/281, train_loss: 0.0515, step time: 0.2509\n",
      "199/281, train_loss: 0.0878, step time: 0.2586\n",
      "200/281, train_loss: 0.0822, step time: 0.2577\n",
      "201/281, train_loss: 0.0873, step time: 0.2583\n",
      "202/281, train_loss: 0.0771, step time: 0.2602\n",
      "203/281, train_loss: 0.1336, step time: 0.2567\n",
      "204/281, train_loss: 0.0808, step time: 0.2585\n",
      "205/281, train_loss: 0.0804, step time: 0.2578\n",
      "206/281, train_loss: 0.0762, step time: 0.2515\n",
      "207/281, train_loss: 0.0987, step time: 0.2546\n",
      "208/281, train_loss: 0.1604, step time: 0.2575\n",
      "209/281, train_loss: 0.0840, step time: 0.2580\n",
      "210/281, train_loss: 0.1312, step time: 0.2569\n",
      "211/281, train_loss: 0.0879, step time: 0.2526\n",
      "212/281, train_loss: 0.1172, step time: 0.2524\n",
      "213/281, train_loss: 0.0949, step time: 0.2515\n",
      "214/281, train_loss: 0.0684, step time: 0.2496\n",
      "215/281, train_loss: 0.0593, step time: 0.2535\n",
      "216/281, train_loss: 0.0964, step time: 0.2554\n",
      "217/281, train_loss: 0.0613, step time: 0.2541\n",
      "218/281, train_loss: 0.2605, step time: 0.2511\n",
      "219/281, train_loss: 0.0624, step time: 0.2608\n",
      "220/281, train_loss: 0.0673, step time: 0.2581\n",
      "221/281, train_loss: 0.0708, step time: 0.2551\n",
      "222/281, train_loss: 0.0782, step time: 0.2482\n",
      "223/281, train_loss: 0.0861, step time: 0.2556\n",
      "224/281, train_loss: 0.0857, step time: 0.2542\n",
      "225/281, train_loss: 0.0760, step time: 0.2545\n",
      "226/281, train_loss: 0.0967, step time: 0.2539\n",
      "227/281, train_loss: 0.0759, step time: 0.2522\n",
      "228/281, train_loss: 0.0783, step time: 0.2516\n",
      "229/281, train_loss: 0.0730, step time: 0.2556\n",
      "230/281, train_loss: 0.0660, step time: 0.2799\n",
      "231/281, train_loss: 0.0721, step time: 0.2552\n",
      "232/281, train_loss: 0.0786, step time: 0.2580\n",
      "233/281, train_loss: 0.0763, step time: 0.2545\n",
      "234/281, train_loss: 0.0701, step time: 0.2541\n",
      "235/281, train_loss: 0.0870, step time: 0.2552\n",
      "236/281, train_loss: 0.1070, step time: 0.2541\n",
      "237/281, train_loss: 0.0846, step time: 0.2580\n",
      "238/281, train_loss: 0.1006, step time: 0.2527\n",
      "239/281, train_loss: 0.0565, step time: 0.2537\n",
      "240/281, train_loss: 0.2299, step time: 0.2630\n",
      "241/281, train_loss: 0.0856, step time: 0.2577\n",
      "242/281, train_loss: 0.0572, step time: 0.2557\n",
      "243/281, train_loss: 0.0602, step time: 0.2501\n",
      "244/281, train_loss: 0.0739, step time: 0.2532\n",
      "245/281, train_loss: 0.0806, step time: 0.2540\n",
      "246/281, train_loss: 0.0846, step time: 0.2552\n",
      "247/281, train_loss: 0.0822, step time: 0.2552\n",
      "248/281, train_loss: 0.0700, step time: 0.2529\n",
      "249/281, train_loss: 0.0867, step time: 0.2500\n",
      "250/281, train_loss: 0.2201, step time: 0.2531\n",
      "251/281, train_loss: 0.1344, step time: 0.2571\n",
      "252/281, train_loss: 0.2624, step time: 0.2534\n",
      "253/281, train_loss: 0.0919, step time: 0.2591\n",
      "254/281, train_loss: 0.0577, step time: 0.2546\n",
      "255/281, train_loss: 0.1023, step time: 0.2595\n",
      "256/281, train_loss: 0.0885, step time: 0.2529\n",
      "257/281, train_loss: 0.1056, step time: 0.2546\n",
      "258/281, train_loss: 0.0378, step time: 0.2548\n",
      "259/281, train_loss: 0.0681, step time: 0.2530\n",
      "260/281, train_loss: 0.2369, step time: 0.2547\n",
      "261/281, train_loss: 0.0673, step time: 0.2554\n",
      "262/281, train_loss: 0.0968, step time: 0.2539\n",
      "263/281, train_loss: 0.0504, step time: 0.2601\n",
      "264/281, train_loss: 0.0495, step time: 0.2539\n",
      "265/281, train_loss: 0.1311, step time: 0.2557\n",
      "266/281, train_loss: 0.0871, step time: 0.2545\n",
      "267/281, train_loss: 0.0688, step time: 0.2564\n",
      "268/281, train_loss: 0.0730, step time: 0.2575\n",
      "269/281, train_loss: 0.0847, step time: 0.2560\n",
      "270/281, train_loss: 0.1085, step time: 0.2542\n",
      "271/281, train_loss: 0.4219, step time: 0.2542\n",
      "272/281, train_loss: 0.0743, step time: 0.2541\n",
      "273/281, train_loss: 0.1016, step time: 0.2551\n",
      "274/281, train_loss: 0.1000, step time: 0.2497\n",
      "275/281, train_loss: 0.2501, step time: 0.2546\n",
      "276/281, train_loss: 0.0587, step time: 0.2554\n",
      "277/281, train_loss: 0.2369, step time: 0.2518\n",
      "278/281, train_loss: 0.0654, step time: 0.2541\n",
      "279/281, train_loss: 0.2214, step time: 0.2540\n",
      "280/281, train_loss: 0.0605, step time: 0.2471\n",
      "281/281, train_loss: 0.2657, step time: 0.2494\n",
      "282/281, train_loss: 0.1136, step time: 0.1501\n",
      "epoch 70 average loss: 0.1096\n",
      "current epoch: 70 current mean dice: 0.8827 tc: 0.8749 wt: 0.9155 et: 0.8683\n",
      "best mean dice: 0.8878 at epoch: 68\n",
      "time consuming of epoch 70 is: 376.5504\n",
      "----------\n",
      "epoch 71/200\n",
      "1/281, train_loss: 0.1079, step time: 0.2599\n",
      "2/281, train_loss: 0.0826, step time: 0.2482\n",
      "3/281, train_loss: 0.0769, step time: 0.2547\n",
      "4/281, train_loss: 0.2575, step time: 0.2531\n",
      "5/281, train_loss: 0.0756, step time: 0.2546\n",
      "6/281, train_loss: 0.0587, step time: 0.2493\n",
      "7/281, train_loss: 0.0574, step time: 0.2534\n",
      "8/281, train_loss: 0.2649, step time: 0.2515\n",
      "9/281, train_loss: 0.1245, step time: 0.2475\n",
      "10/281, train_loss: 0.1112, step time: 0.2580\n",
      "11/281, train_loss: 0.0487, step time: 0.3055\n",
      "12/281, train_loss: 0.1480, step time: 0.2555\n",
      "13/281, train_loss: 0.0537, step time: 0.2593\n",
      "14/281, train_loss: 0.0854, step time: 0.2578\n",
      "15/281, train_loss: 0.0603, step time: 0.2560\n",
      "16/281, train_loss: 0.0756, step time: 0.2485\n",
      "17/281, train_loss: 0.1052, step time: 0.2484\n",
      "18/281, train_loss: 0.1122, step time: 0.2565\n",
      "19/281, train_loss: 0.0919, step time: 0.2538\n",
      "20/281, train_loss: 0.2608, step time: 0.2523\n",
      "21/281, train_loss: 0.0810, step time: 0.2550\n",
      "22/281, train_loss: 0.0962, step time: 0.2515\n",
      "23/281, train_loss: 0.0854, step time: 0.2542\n",
      "24/281, train_loss: 0.2250, step time: 0.2547\n",
      "25/281, train_loss: 0.0712, step time: 0.2542\n",
      "26/281, train_loss: 0.0680, step time: 0.2496\n",
      "27/281, train_loss: 0.0625, step time: 0.2489\n",
      "28/281, train_loss: 0.2595, step time: 0.2587\n",
      "29/281, train_loss: 0.0557, step time: 0.2502\n",
      "30/281, train_loss: 0.0596, step time: 0.2503\n",
      "31/281, train_loss: 0.0824, step time: 0.2506\n",
      "32/281, train_loss: 0.0776, step time: 0.2510\n",
      "33/281, train_loss: 0.0699, step time: 0.2568\n",
      "34/281, train_loss: 0.0573, step time: 0.2538\n",
      "35/281, train_loss: 0.1008, step time: 0.2532\n",
      "36/281, train_loss: 0.0621, step time: 0.2471\n",
      "37/281, train_loss: 0.0676, step time: 0.2588\n",
      "38/281, train_loss: 0.0975, step time: 0.2573\n",
      "39/281, train_loss: 0.0429, step time: 0.2570\n",
      "40/281, train_loss: 0.0550, step time: 0.2544\n",
      "41/281, train_loss: 0.0816, step time: 0.2530\n",
      "42/281, train_loss: 0.0590, step time: 0.2526\n",
      "43/281, train_loss: 0.0625, step time: 0.2502\n",
      "44/281, train_loss: 0.2400, step time: 0.2479\n",
      "45/281, train_loss: 0.2262, step time: 0.2520\n",
      "46/281, train_loss: 0.0945, step time: 0.2529\n",
      "47/281, train_loss: 0.0677, step time: 0.2496\n",
      "48/281, train_loss: 0.0913, step time: 0.2464\n",
      "49/281, train_loss: 0.0811, step time: 0.2501\n",
      "50/281, train_loss: 0.1130, step time: 0.2562\n",
      "51/281, train_loss: 0.0679, step time: 0.2477\n",
      "52/281, train_loss: 0.2165, step time: 0.2462\n",
      "53/281, train_loss: 0.0744, step time: 0.2483\n",
      "54/281, train_loss: 0.0435, step time: 0.2478\n",
      "55/281, train_loss: 0.0746, step time: 0.2486\n",
      "56/281, train_loss: 0.0677, step time: 0.2498\n",
      "57/281, train_loss: 0.1042, step time: 0.2591\n",
      "58/281, train_loss: 0.0629, step time: 0.2452\n",
      "59/281, train_loss: 0.0925, step time: 0.2507\n",
      "60/281, train_loss: 0.0944, step time: 0.2600\n",
      "61/281, train_loss: 0.0629, step time: 0.2584\n",
      "62/281, train_loss: 0.1006, step time: 0.2447\n",
      "63/281, train_loss: 0.1130, step time: 0.2431\n",
      "64/281, train_loss: 0.0725, step time: 0.2562\n",
      "65/281, train_loss: 0.0695, step time: 0.2504\n",
      "66/281, train_loss: 0.0530, step time: 0.2517\n",
      "67/281, train_loss: 0.0581, step time: 0.2485\n",
      "68/281, train_loss: 0.1246, step time: 0.2472\n",
      "69/281, train_loss: 0.0807, step time: 0.2441\n",
      "70/281, train_loss: 0.0919, step time: 0.2484\n",
      "71/281, train_loss: 0.0830, step time: 0.2477\n",
      "72/281, train_loss: 0.0508, step time: 0.2442\n",
      "73/281, train_loss: 0.1048, step time: 0.2454\n",
      "74/281, train_loss: 0.0668, step time: 0.2425\n",
      "75/281, train_loss: 0.1204, step time: 0.2497\n",
      "76/281, train_loss: 0.2456, step time: 0.2436\n",
      "77/281, train_loss: 0.0697, step time: 0.2454\n",
      "78/281, train_loss: 0.0510, step time: 0.2473\n",
      "79/281, train_loss: 0.1115, step time: 0.2522\n",
      "80/281, train_loss: 0.1254, step time: 0.2707\n",
      "81/281, train_loss: 0.2646, step time: 0.2535\n",
      "82/281, train_loss: 0.1662, step time: 0.2527\n",
      "83/281, train_loss: 0.0859, step time: 0.2536\n",
      "84/281, train_loss: 0.0894, step time: 0.2546\n",
      "85/281, train_loss: 0.2226, step time: 0.2521\n",
      "86/281, train_loss: 0.0991, step time: 0.2500\n",
      "87/281, train_loss: 0.0910, step time: 0.2474\n",
      "88/281, train_loss: 0.0509, step time: 0.2542\n",
      "89/281, train_loss: 0.2146, step time: 0.2520\n",
      "90/281, train_loss: 0.2367, step time: 0.2526\n",
      "91/281, train_loss: 0.0801, step time: 0.2506\n",
      "92/281, train_loss: 0.0544, step time: 0.2496\n",
      "93/281, train_loss: 0.0676, step time: 0.2467\n",
      "94/281, train_loss: 0.0768, step time: 0.2515\n",
      "95/281, train_loss: 0.2436, step time: 0.2568\n",
      "96/281, train_loss: 0.0610, step time: 0.2529\n",
      "97/281, train_loss: 0.0859, step time: 0.2601\n",
      "98/281, train_loss: 0.2618, step time: 0.2868\n",
      "99/281, train_loss: 0.0678, step time: 0.2690\n",
      "100/281, train_loss: 0.0773, step time: 0.2519\n",
      "101/281, train_loss: 0.1083, step time: 0.2533\n",
      "102/281, train_loss: 0.2212, step time: 0.2578\n",
      "103/281, train_loss: 0.0579, step time: 0.2568\n",
      "104/281, train_loss: 0.0838, step time: 0.2502\n",
      "105/281, train_loss: 0.1050, step time: 0.2512\n",
      "106/281, train_loss: 0.1033, step time: 0.2489\n",
      "107/281, train_loss: 0.1167, step time: 0.2497\n",
      "108/281, train_loss: 0.0531, step time: 0.2552\n",
      "109/281, train_loss: 0.2155, step time: 0.2543\n",
      "110/281, train_loss: 0.1122, step time: 0.2562\n",
      "111/281, train_loss: 0.2314, step time: 0.2596\n",
      "112/281, train_loss: 0.2247, step time: 0.2501\n",
      "113/281, train_loss: 0.0744, step time: 0.2523\n",
      "114/281, train_loss: 0.2318, step time: 0.2519\n",
      "115/281, train_loss: 0.0988, step time: 0.2530\n",
      "116/281, train_loss: 0.0850, step time: 0.2516\n",
      "117/281, train_loss: 0.2183, step time: 0.2547\n",
      "118/281, train_loss: 0.1114, step time: 0.2608\n",
      "119/281, train_loss: 0.1116, step time: 0.2564\n",
      "120/281, train_loss: 0.1105, step time: 0.2551\n",
      "121/281, train_loss: 0.0584, step time: 0.2576\n",
      "122/281, train_loss: 0.0720, step time: 0.2525\n",
      "123/281, train_loss: 0.1152, step time: 0.2541\n",
      "124/281, train_loss: 0.0873, step time: 0.2636\n",
      "125/281, train_loss: 0.1199, step time: 0.2543\n",
      "126/281, train_loss: 0.0784, step time: 0.2526\n",
      "127/281, train_loss: 0.0594, step time: 0.2546\n",
      "128/281, train_loss: 0.0837, step time: 0.2571\n",
      "129/281, train_loss: 0.1039, step time: 0.2532\n",
      "130/281, train_loss: 0.1103, step time: 0.2542\n",
      "131/281, train_loss: 0.1203, step time: 0.2534\n",
      "132/281, train_loss: 0.0604, step time: 0.2482\n",
      "133/281, train_loss: 0.2890, step time: 0.2810\n",
      "134/281, train_loss: 0.0961, step time: 0.2553\n",
      "135/281, train_loss: 0.0734, step time: 0.2538\n",
      "136/281, train_loss: 0.0917, step time: 0.2590\n",
      "137/281, train_loss: 0.1093, step time: 0.2498\n",
      "138/281, train_loss: 0.0576, step time: 0.2521\n",
      "139/281, train_loss: 0.0731, step time: 0.2488\n",
      "140/281, train_loss: 0.2661, step time: 0.2493\n",
      "141/281, train_loss: 0.0573, step time: 0.2507\n",
      "142/281, train_loss: 0.0828, step time: 0.2558\n",
      "143/281, train_loss: 0.0834, step time: 0.2522\n",
      "144/281, train_loss: 0.0974, step time: 0.2527\n",
      "145/281, train_loss: 0.2530, step time: 0.2522\n",
      "146/281, train_loss: 0.0942, step time: 0.2498\n",
      "147/281, train_loss: 0.0882, step time: 0.2528\n",
      "148/281, train_loss: 0.0821, step time: 0.2481\n",
      "149/281, train_loss: 0.1020, step time: 0.2520\n",
      "150/281, train_loss: 0.2465, step time: 0.2487\n",
      "151/281, train_loss: 0.3029, step time: 0.2475\n",
      "152/281, train_loss: 0.0888, step time: 0.2535\n",
      "153/281, train_loss: 0.1138, step time: 0.2496\n",
      "154/281, train_loss: 0.0640, step time: 0.2532\n",
      "155/281, train_loss: 0.0787, step time: 0.2497\n",
      "156/281, train_loss: 0.1164, step time: 0.2531\n",
      "157/281, train_loss: 0.0801, step time: 0.2615\n",
      "158/281, train_loss: 0.1324, step time: 0.2538\n",
      "159/281, train_loss: 0.0524, step time: 0.2518\n",
      "160/281, train_loss: 0.0693, step time: 0.2528\n",
      "161/281, train_loss: 0.0702, step time: 0.2544\n",
      "162/281, train_loss: 0.0781, step time: 0.2513\n",
      "163/281, train_loss: 0.1147, step time: 0.2544\n",
      "164/281, train_loss: 0.1242, step time: 0.2527\n",
      "165/281, train_loss: 0.0979, step time: 0.2511\n",
      "166/281, train_loss: 0.0572, step time: 0.2592\n",
      "167/281, train_loss: 0.0489, step time: 0.2569\n",
      "168/281, train_loss: 0.0418, step time: 0.2531\n",
      "169/281, train_loss: 0.1205, step time: 0.2581\n",
      "170/281, train_loss: 0.2149, step time: 0.2527\n",
      "171/281, train_loss: 0.0761, step time: 0.2528\n",
      "172/281, train_loss: 0.2682, step time: 0.2620\n",
      "173/281, train_loss: 0.0536, step time: 0.2535\n",
      "174/281, train_loss: 0.1042, step time: 0.2502\n",
      "175/281, train_loss: 0.0423, step time: 0.2513\n",
      "176/281, train_loss: 0.0465, step time: 0.2556\n",
      "177/281, train_loss: 0.0518, step time: 0.2602\n",
      "178/281, train_loss: 0.1033, step time: 0.2560\n",
      "179/281, train_loss: 0.0954, step time: 0.2540\n",
      "180/281, train_loss: 0.0554, step time: 0.2599\n",
      "181/281, train_loss: 0.0712, step time: 0.2585\n",
      "182/281, train_loss: 0.2253, step time: 0.2504\n",
      "183/281, train_loss: 0.0962, step time: 0.2515\n",
      "184/281, train_loss: 0.1201, step time: 0.2508\n",
      "185/281, train_loss: 0.0434, step time: 0.2521\n",
      "186/281, train_loss: 0.0796, step time: 0.2479\n",
      "187/281, train_loss: 0.0923, step time: 0.2550\n",
      "188/281, train_loss: 0.0675, step time: 0.2452\n",
      "189/281, train_loss: 0.0939, step time: 0.2515\n",
      "190/281, train_loss: 0.0868, step time: 0.2499\n",
      "191/281, train_loss: 0.0773, step time: 0.2496\n",
      "192/281, train_loss: 0.0766, step time: 0.2595\n",
      "193/281, train_loss: 0.0653, step time: 0.2589\n",
      "194/281, train_loss: 0.0626, step time: 0.2552\n",
      "195/281, train_loss: 0.0638, step time: 0.2585\n",
      "196/281, train_loss: 0.0483, step time: 0.2522\n",
      "197/281, train_loss: 0.0877, step time: 0.2537\n",
      "198/281, train_loss: 0.2642, step time: 0.2516\n",
      "199/281, train_loss: 0.0797, step time: 0.2531\n",
      "200/281, train_loss: 0.0780, step time: 0.2569\n",
      "201/281, train_loss: 0.0735, step time: 0.2541\n",
      "202/281, train_loss: 0.2256, step time: 0.2510\n",
      "203/281, train_loss: 0.0754, step time: 0.2514\n",
      "204/281, train_loss: 0.0778, step time: 0.2544\n",
      "205/281, train_loss: 0.0777, step time: 0.2548\n",
      "206/281, train_loss: 0.1147, step time: 0.2586\n",
      "207/281, train_loss: 0.0633, step time: 0.2506\n",
      "208/281, train_loss: 0.0760, step time: 0.2532\n",
      "209/281, train_loss: 0.2551, step time: 0.2490\n",
      "210/281, train_loss: 0.0941, step time: 0.2554\n",
      "211/281, train_loss: 0.0702, step time: 0.2563\n",
      "212/281, train_loss: 0.1193, step time: 0.2538\n",
      "213/281, train_loss: 0.0728, step time: 0.2521\n",
      "214/281, train_loss: 0.0847, step time: 0.2522\n",
      "215/281, train_loss: 0.0671, step time: 0.2549\n",
      "216/281, train_loss: 0.1091, step time: 0.2555\n",
      "217/281, train_loss: 0.1031, step time: 0.2505\n",
      "218/281, train_loss: 0.0603, step time: 0.2524\n",
      "219/281, train_loss: 0.0847, step time: 0.2544\n",
      "220/281, train_loss: 0.1307, step time: 0.2539\n",
      "221/281, train_loss: 0.0869, step time: 0.2561\n",
      "222/281, train_loss: 0.0486, step time: 0.2507\n",
      "223/281, train_loss: 0.0574, step time: 0.2573\n",
      "224/281, train_loss: 0.2411, step time: 0.2570\n",
      "225/281, train_loss: 0.1495, step time: 0.2523\n",
      "226/281, train_loss: 0.0616, step time: 0.2558\n",
      "227/281, train_loss: 0.0896, step time: 0.2522\n",
      "228/281, train_loss: 0.2226, step time: 0.2507\n",
      "229/281, train_loss: 0.0561, step time: 0.2495\n",
      "230/281, train_loss: 0.0698, step time: 0.2520\n",
      "231/281, train_loss: 0.0672, step time: 0.2532\n",
      "232/281, train_loss: 0.0706, step time: 0.2510\n",
      "233/281, train_loss: 0.0639, step time: 0.2579\n",
      "234/281, train_loss: 0.2468, step time: 0.2576\n",
      "235/281, train_loss: 0.1408, step time: 0.2518\n",
      "236/281, train_loss: 0.0825, step time: 0.2535\n",
      "237/281, train_loss: 0.0866, step time: 0.2543\n",
      "238/281, train_loss: 0.0553, step time: 0.2574\n",
      "239/281, train_loss: 0.0466, step time: 0.2562\n",
      "240/281, train_loss: 0.0667, step time: 0.2566\n",
      "241/281, train_loss: 0.1356, step time: 0.2525\n",
      "242/281, train_loss: 0.0955, step time: 0.2564\n",
      "243/281, train_loss: 0.0712, step time: 0.2593\n",
      "244/281, train_loss: 0.0487, step time: 0.2582\n",
      "245/281, train_loss: 0.0930, step time: 0.2541\n",
      "246/281, train_loss: 0.3879, step time: 0.2550\n",
      "247/281, train_loss: 0.0557, step time: 0.2517\n",
      "248/281, train_loss: 0.1244, step time: 0.2503\n",
      "249/281, train_loss: 0.0722, step time: 0.2592\n",
      "250/281, train_loss: 0.0761, step time: 0.2593\n",
      "251/281, train_loss: 0.0933, step time: 0.2562\n",
      "252/281, train_loss: 0.0978, step time: 0.2541\n",
      "253/281, train_loss: 0.0575, step time: 0.2608\n",
      "254/281, train_loss: 0.2581, step time: 0.2579\n",
      "255/281, train_loss: 0.0666, step time: 0.2588\n",
      "256/281, train_loss: 0.2514, step time: 0.2593\n",
      "257/281, train_loss: 0.2335, step time: 0.2528\n",
      "258/281, train_loss: 0.0747, step time: 0.2538\n",
      "259/281, train_loss: 0.0775, step time: 0.2555\n",
      "260/281, train_loss: 0.0624, step time: 0.2581\n",
      "261/281, train_loss: 0.0888, step time: 0.2585\n",
      "262/281, train_loss: 0.2454, step time: 0.2561\n",
      "263/281, train_loss: 0.1022, step time: 0.2535\n",
      "264/281, train_loss: 0.0804, step time: 0.2557\n",
      "265/281, train_loss: 0.0710, step time: 0.2599\n",
      "266/281, train_loss: 0.0845, step time: 0.2540\n",
      "267/281, train_loss: 0.0835, step time: 0.2531\n",
      "268/281, train_loss: 0.0632, step time: 0.2525\n",
      "269/281, train_loss: 0.0700, step time: 0.2669\n",
      "270/281, train_loss: 0.0946, step time: 0.2564\n",
      "271/281, train_loss: 0.2316, step time: 0.2574\n",
      "272/281, train_loss: 0.0561, step time: 0.2543\n",
      "273/281, train_loss: 0.0771, step time: 0.2590\n",
      "274/281, train_loss: 0.0716, step time: 0.2587\n",
      "275/281, train_loss: 0.0585, step time: 0.2611\n",
      "276/281, train_loss: 0.0908, step time: 0.2702\n",
      "277/281, train_loss: 0.0643, step time: 0.2571\n",
      "278/281, train_loss: 0.0665, step time: 0.2560\n",
      "279/281, train_loss: 0.2451, step time: 0.2579\n",
      "280/281, train_loss: 0.0686, step time: 0.2570\n",
      "281/281, train_loss: 0.0860, step time: 0.2553\n",
      "282/281, train_loss: 0.4093, step time: 0.1522\n",
      "epoch 71 average loss: 0.1075\n",
      "current epoch: 71 current mean dice: 0.8861 tc: 0.8803 wt: 0.9184 et: 0.8684\n",
      "best mean dice: 0.8878 at epoch: 68\n",
      "time consuming of epoch 71 is: 376.9289\n",
      "----------\n",
      "epoch 72/200\n",
      "1/281, train_loss: 0.1106, step time: 0.2687\n",
      "2/281, train_loss: 0.3725, step time: 0.2653\n",
      "3/281, train_loss: 0.0704, step time: 0.2628\n",
      "4/281, train_loss: 0.0998, step time: 0.2526\n",
      "5/281, train_loss: 0.0689, step time: 0.2618\n",
      "6/281, train_loss: 0.0914, step time: 0.2519\n",
      "7/281, train_loss: 0.0593, step time: 0.2514\n",
      "8/281, train_loss: 0.0814, step time: 0.2572\n",
      "9/281, train_loss: 0.0574, step time: 0.2554\n",
      "10/281, train_loss: 0.0575, step time: 0.2562\n",
      "11/281, train_loss: 0.0615, step time: 0.2615\n",
      "12/281, train_loss: 0.0652, step time: 0.2547\n",
      "13/281, train_loss: 0.1146, step time: 0.2533\n",
      "14/281, train_loss: 0.0958, step time: 0.2579\n",
      "15/281, train_loss: 0.0800, step time: 0.2561\n",
      "16/281, train_loss: 0.3929, step time: 0.2583\n",
      "17/281, train_loss: 0.0714, step time: 0.2514\n",
      "18/281, train_loss: 0.1040, step time: 0.2543\n",
      "19/281, train_loss: 0.0949, step time: 0.2600\n",
      "20/281, train_loss: 0.1611, step time: 0.2584\n",
      "21/281, train_loss: 0.0830, step time: 0.2571\n",
      "22/281, train_loss: 0.1044, step time: 0.2564\n",
      "23/281, train_loss: 0.0788, step time: 0.2560\n",
      "24/281, train_loss: 0.0881, step time: 0.2545\n",
      "25/281, train_loss: 0.2510, step time: 0.2627\n",
      "26/281, train_loss: 0.1545, step time: 0.2635\n",
      "27/281, train_loss: 0.0684, step time: 0.2589\n",
      "28/281, train_loss: 0.0843, step time: 0.2557\n",
      "29/281, train_loss: 0.2471, step time: 0.2668\n",
      "30/281, train_loss: 0.2470, step time: 0.2590\n",
      "31/281, train_loss: 0.0751, step time: 0.2586\n",
      "32/281, train_loss: 0.0703, step time: 0.2564\n",
      "33/281, train_loss: 0.2472, step time: 0.2585\n",
      "34/281, train_loss: 0.1132, step time: 0.2570\n",
      "35/281, train_loss: 0.0986, step time: 0.2547\n",
      "36/281, train_loss: 0.1150, step time: 0.2519\n",
      "37/281, train_loss: 0.2535, step time: 0.2530\n",
      "38/281, train_loss: 0.1129, step time: 0.2560\n",
      "39/281, train_loss: 0.0627, step time: 0.2548\n",
      "40/281, train_loss: 0.0647, step time: 0.2630\n",
      "41/281, train_loss: 0.2244, step time: 0.2521\n",
      "42/281, train_loss: 0.0531, step time: 0.2557\n",
      "43/281, train_loss: 0.0482, step time: 0.2559\n",
      "44/281, train_loss: 0.0604, step time: 0.2562\n",
      "45/281, train_loss: 0.2475, step time: 0.2542\n",
      "46/281, train_loss: 0.0589, step time: 0.2536\n",
      "47/281, train_loss: 0.0709, step time: 0.2579\n",
      "48/281, train_loss: 0.0639, step time: 0.2565\n",
      "49/281, train_loss: 0.1486, step time: 0.2540\n",
      "50/281, train_loss: 0.0775, step time: 0.2579\n",
      "51/281, train_loss: 0.0554, step time: 0.2600\n",
      "52/281, train_loss: 0.0707, step time: 0.2602\n",
      "53/281, train_loss: 0.0707, step time: 0.2635\n",
      "54/281, train_loss: 0.1059, step time: 0.2574\n",
      "55/281, train_loss: 0.2801, step time: 0.2598\n",
      "56/281, train_loss: 0.0814, step time: 0.2564\n",
      "57/281, train_loss: 0.0579, step time: 0.2496\n",
      "58/281, train_loss: 0.0977, step time: 0.2614\n",
      "59/281, train_loss: 0.0481, step time: 0.2527\n",
      "60/281, train_loss: 0.0973, step time: 0.2514\n",
      "61/281, train_loss: 0.1086, step time: 0.2557\n",
      "62/281, train_loss: 0.0746, step time: 0.2538\n",
      "63/281, train_loss: 0.0944, step time: 0.2520\n",
      "64/281, train_loss: 0.0828, step time: 0.2552\n",
      "65/281, train_loss: 0.0794, step time: 0.2574\n",
      "66/281, train_loss: 0.1101, step time: 0.2588\n",
      "67/281, train_loss: 0.0618, step time: 0.2536\n",
      "68/281, train_loss: 0.0699, step time: 0.2574\n",
      "69/281, train_loss: 0.1014, step time: 0.2606\n",
      "70/281, train_loss: 0.2368, step time: 0.2538\n",
      "71/281, train_loss: 0.2187, step time: 0.2543\n",
      "72/281, train_loss: 0.0811, step time: 0.2587\n",
      "73/281, train_loss: 0.0950, step time: 0.2636\n",
      "74/281, train_loss: 0.0975, step time: 0.2551\n",
      "75/281, train_loss: 0.2095, step time: 0.2590\n",
      "76/281, train_loss: 0.0748, step time: 0.2545\n",
      "77/281, train_loss: 0.0583, step time: 0.2574\n",
      "78/281, train_loss: 0.0762, step time: 0.2545\n",
      "79/281, train_loss: 0.0980, step time: 0.2581\n",
      "80/281, train_loss: 0.0984, step time: 0.2546\n",
      "81/281, train_loss: 0.2706, step time: 0.2571\n",
      "82/281, train_loss: 0.0654, step time: 0.2505\n",
      "83/281, train_loss: 0.0854, step time: 0.2525\n",
      "84/281, train_loss: 0.0651, step time: 0.2593\n",
      "85/281, train_loss: 0.2352, step time: 0.2574\n",
      "86/281, train_loss: 0.1047, step time: 0.2579\n",
      "87/281, train_loss: 0.0759, step time: 0.2578\n",
      "88/281, train_loss: 0.0977, step time: 0.2568\n",
      "89/281, train_loss: 0.0592, step time: 0.2639\n",
      "90/281, train_loss: 0.0773, step time: 0.2574\n",
      "91/281, train_loss: 0.2848, step time: 0.2561\n",
      "92/281, train_loss: 0.1087, step time: 0.2544\n",
      "93/281, train_loss: 0.0738, step time: 0.2580\n",
      "94/281, train_loss: 0.0664, step time: 0.2572\n",
      "95/281, train_loss: 0.2050, step time: 0.2578\n",
      "96/281, train_loss: 0.2203, step time: 0.2497\n",
      "97/281, train_loss: 0.0812, step time: 0.2464\n",
      "98/281, train_loss: 0.0538, step time: 0.2467\n",
      "99/281, train_loss: 0.0314, step time: 0.2576\n",
      "100/281, train_loss: 0.0685, step time: 0.2598\n",
      "101/281, train_loss: 0.0796, step time: 0.2512\n",
      "102/281, train_loss: 0.0646, step time: 0.2508\n",
      "103/281, train_loss: 0.0782, step time: 0.2543\n",
      "104/281, train_loss: 0.0795, step time: 0.2519\n",
      "105/281, train_loss: 0.1107, step time: 0.2517\n",
      "106/281, train_loss: 0.0847, step time: 0.2545\n",
      "107/281, train_loss: 0.1269, step time: 0.2494\n",
      "108/281, train_loss: 0.0479, step time: 0.2525\n",
      "109/281, train_loss: 0.0726, step time: 0.2545\n",
      "110/281, train_loss: 0.0708, step time: 0.2537\n",
      "111/281, train_loss: 0.2537, step time: 0.2548\n",
      "112/281, train_loss: 0.0807, step time: 0.2559\n",
      "113/281, train_loss: 0.1053, step time: 0.2563\n",
      "114/281, train_loss: 0.0753, step time: 0.2514\n",
      "115/281, train_loss: 0.1121, step time: 0.2576\n",
      "116/281, train_loss: 0.0662, step time: 0.2594\n",
      "117/281, train_loss: 0.1657, step time: 0.2584\n",
      "118/281, train_loss: 0.0790, step time: 0.2539\n",
      "119/281, train_loss: 0.0817, step time: 0.2572\n",
      "120/281, train_loss: 0.0582, step time: 0.2676\n",
      "121/281, train_loss: 0.0801, step time: 0.2945\n",
      "122/281, train_loss: 0.0800, step time: 0.2597\n",
      "123/281, train_loss: 0.0757, step time: 0.2547\n",
      "124/281, train_loss: 0.0858, step time: 0.2573\n",
      "125/281, train_loss: 0.0940, step time: 0.2583\n",
      "126/281, train_loss: 0.1415, step time: 0.2582\n",
      "127/281, train_loss: 0.0899, step time: 0.2576\n",
      "128/281, train_loss: 0.0396, step time: 0.2553\n",
      "129/281, train_loss: 0.0713, step time: 0.2521\n",
      "130/281, train_loss: 0.0846, step time: 0.2511\n",
      "131/281, train_loss: 0.0695, step time: 0.2550\n",
      "132/281, train_loss: 0.2335, step time: 0.2553\n",
      "133/281, train_loss: 0.0521, step time: 0.2565\n",
      "134/281, train_loss: 0.2553, step time: 0.2571\n",
      "135/281, train_loss: 0.0574, step time: 0.2524\n",
      "136/281, train_loss: 0.0629, step time: 0.2570\n",
      "137/281, train_loss: 0.2349, step time: 0.2584\n",
      "138/281, train_loss: 0.0632, step time: 0.2571\n",
      "139/281, train_loss: 0.0745, step time: 0.2485\n",
      "140/281, train_loss: 0.0643, step time: 0.2576\n",
      "141/281, train_loss: 0.0622, step time: 0.2537\n",
      "142/281, train_loss: 0.0915, step time: 0.2496\n",
      "143/281, train_loss: 0.1628, step time: 0.2527\n",
      "144/281, train_loss: 0.0954, step time: 0.2589\n",
      "145/281, train_loss: 0.2395, step time: 0.2655\n",
      "146/281, train_loss: 0.0890, step time: 0.2715\n",
      "147/281, train_loss: 0.0761, step time: 0.2560\n",
      "148/281, train_loss: 0.0447, step time: 0.2585\n",
      "149/281, train_loss: 0.2481, step time: 0.2552\n",
      "150/281, train_loss: 0.1008, step time: 0.2577\n",
      "151/281, train_loss: 0.1278, step time: 0.2562\n",
      "152/281, train_loss: 0.1197, step time: 0.2525\n",
      "153/281, train_loss: 0.1241, step time: 0.2557\n",
      "154/281, train_loss: 0.0809, step time: 0.2543\n",
      "155/281, train_loss: 0.0642, step time: 0.2504\n",
      "156/281, train_loss: 0.0471, step time: 0.2467\n",
      "157/281, train_loss: 0.0963, step time: 0.2573\n",
      "158/281, train_loss: 0.0590, step time: 0.2583\n",
      "159/281, train_loss: 0.0732, step time: 0.2502\n",
      "160/281, train_loss: 0.2511, step time: 0.2517\n",
      "161/281, train_loss: 0.0657, step time: 0.2573\n",
      "162/281, train_loss: 0.0497, step time: 0.2564\n",
      "163/281, train_loss: 0.0587, step time: 0.2544\n",
      "164/281, train_loss: 0.0781, step time: 0.2501\n",
      "165/281, train_loss: 0.0809, step time: 0.2565\n",
      "166/281, train_loss: 0.0865, step time: 0.2548\n",
      "167/281, train_loss: 0.0828, step time: 0.2539\n",
      "168/281, train_loss: 0.1032, step time: 0.2502\n",
      "169/281, train_loss: 0.0567, step time: 0.2779\n",
      "170/281, train_loss: 0.0576, step time: 0.2562\n",
      "171/281, train_loss: 0.0580, step time: 0.2532\n",
      "172/281, train_loss: 0.0863, step time: 0.2576\n",
      "173/281, train_loss: 0.0998, step time: 0.2548\n",
      "174/281, train_loss: 0.0824, step time: 0.2555\n",
      "175/281, train_loss: 0.0585, step time: 0.2588\n",
      "176/281, train_loss: 0.2100, step time: 0.2575\n",
      "177/281, train_loss: 0.0678, step time: 0.2584\n",
      "178/281, train_loss: 0.1063, step time: 0.2533\n",
      "179/281, train_loss: 0.2176, step time: 0.2562\n",
      "180/281, train_loss: 0.0713, step time: 0.2527\n",
      "181/281, train_loss: 0.0583, step time: 0.2577\n",
      "182/281, train_loss: 0.1197, step time: 0.2595\n",
      "183/281, train_loss: 0.0575, step time: 0.2586\n",
      "184/281, train_loss: 0.0939, step time: 0.2570\n",
      "185/281, train_loss: 0.2505, step time: 0.2581\n",
      "186/281, train_loss: 0.0547, step time: 0.2537\n",
      "187/281, train_loss: 0.0906, step time: 0.2582\n",
      "188/281, train_loss: 0.0752, step time: 0.2586\n",
      "189/281, train_loss: 0.0728, step time: 0.2579\n",
      "190/281, train_loss: 0.0718, step time: 0.2592\n",
      "191/281, train_loss: 0.1195, step time: 0.2521\n",
      "192/281, train_loss: 0.2176, step time: 0.2554\n",
      "193/281, train_loss: 0.1000, step time: 0.2601\n",
      "194/281, train_loss: 0.1563, step time: 0.2571\n",
      "195/281, train_loss: 0.0880, step time: 0.2565\n",
      "196/281, train_loss: 0.1508, step time: 0.2580\n",
      "197/281, train_loss: 0.2253, step time: 0.2537\n",
      "198/281, train_loss: 0.0680, step time: 0.2551\n",
      "199/281, train_loss: 0.0921, step time: 0.2570\n",
      "200/281, train_loss: 0.0939, step time: 0.2562\n",
      "201/281, train_loss: 0.0880, step time: 0.2532\n",
      "202/281, train_loss: 0.1114, step time: 0.2577\n",
      "203/281, train_loss: 0.2234, step time: 0.2585\n",
      "204/281, train_loss: 0.2329, step time: 0.2578\n",
      "205/281, train_loss: 0.0773, step time: 0.2549\n",
      "206/281, train_loss: 0.0957, step time: 0.2535\n",
      "207/281, train_loss: 0.2389, step time: 0.2530\n",
      "208/281, train_loss: 0.0911, step time: 0.2556\n",
      "209/281, train_loss: 0.0921, step time: 0.2607\n",
      "210/281, train_loss: 0.0526, step time: 0.2561\n",
      "211/281, train_loss: 0.0907, step time: 0.2556\n",
      "212/281, train_loss: 0.0766, step time: 0.2568\n",
      "213/281, train_loss: 0.0894, step time: 0.2531\n",
      "214/281, train_loss: 0.0694, step time: 0.2487\n",
      "215/281, train_loss: 0.0665, step time: 0.2533\n",
      "216/281, train_loss: 0.0810, step time: 0.2573\n",
      "217/281, train_loss: 0.1033, step time: 0.2491\n",
      "218/281, train_loss: 0.0874, step time: 0.2543\n",
      "219/281, train_loss: 0.0908, step time: 0.2506\n",
      "220/281, train_loss: 0.0572, step time: 0.2534\n",
      "221/281, train_loss: 0.0437, step time: 0.2543\n",
      "222/281, train_loss: 0.0753, step time: 0.2543\n",
      "223/281, train_loss: 0.0842, step time: 0.2504\n",
      "224/281, train_loss: 0.0500, step time: 0.2571\n",
      "225/281, train_loss: 0.1018, step time: 0.2578\n",
      "226/281, train_loss: 0.0670, step time: 0.2580\n",
      "227/281, train_loss: 0.2274, step time: 0.2561\n",
      "228/281, train_loss: 0.0873, step time: 0.2572\n",
      "229/281, train_loss: 0.0643, step time: 0.2510\n",
      "230/281, train_loss: 0.2239, step time: 0.2584\n",
      "231/281, train_loss: 0.0637, step time: 0.2580\n",
      "232/281, train_loss: 0.0686, step time: 0.2515\n",
      "233/281, train_loss: 0.0445, step time: 0.2532\n",
      "234/281, train_loss: 0.0743, step time: 0.2560\n",
      "235/281, train_loss: 0.0759, step time: 0.2508\n",
      "236/281, train_loss: 0.1123, step time: 0.2565\n",
      "237/281, train_loss: 0.0736, step time: 0.2546\n",
      "238/281, train_loss: 0.0625, step time: 0.2516\n",
      "239/281, train_loss: 0.0793, step time: 0.2520\n",
      "240/281, train_loss: 0.0600, step time: 0.2516\n",
      "241/281, train_loss: 0.1231, step time: 0.2552\n",
      "242/281, train_loss: 0.1109, step time: 0.2560\n",
      "243/281, train_loss: 0.0840, step time: 0.2555\n",
      "244/281, train_loss: 0.0778, step time: 0.2526\n",
      "245/281, train_loss: 0.2207, step time: 0.2511\n",
      "246/281, train_loss: 0.0667, step time: 0.2575\n",
      "247/281, train_loss: 0.1404, step time: 0.2544\n",
      "248/281, train_loss: 0.1055, step time: 0.2532\n",
      "249/281, train_loss: 0.0693, step time: 0.2477\n",
      "250/281, train_loss: 0.1185, step time: 0.2570\n",
      "251/281, train_loss: 0.1053, step time: 0.2559\n",
      "252/281, train_loss: 0.0963, step time: 0.2519\n",
      "253/281, train_loss: 0.0687, step time: 0.2561\n",
      "254/281, train_loss: 0.0502, step time: 0.2566\n",
      "255/281, train_loss: 0.0569, step time: 0.2579\n",
      "256/281, train_loss: 0.2317, step time: 0.2521\n",
      "257/281, train_loss: 0.2511, step time: 0.2505\n",
      "258/281, train_loss: 0.0525, step time: 0.2502\n",
      "259/281, train_loss: 0.0957, step time: 0.2499\n",
      "260/281, train_loss: 0.0633, step time: 0.2510\n",
      "261/281, train_loss: 0.0983, step time: 0.2459\n",
      "262/281, train_loss: 0.1012, step time: 0.2602\n",
      "263/281, train_loss: 0.2282, step time: 0.2514\n",
      "264/281, train_loss: 0.2569, step time: 0.2604\n",
      "265/281, train_loss: 0.0497, step time: 0.2545\n",
      "266/281, train_loss: 0.0854, step time: 0.2494\n",
      "267/281, train_loss: 0.0800, step time: 0.2470\n",
      "268/281, train_loss: 0.0554, step time: 0.2540\n",
      "269/281, train_loss: 0.1028, step time: 0.2548\n",
      "270/281, train_loss: 0.0710, step time: 0.2558\n",
      "271/281, train_loss: 0.0804, step time: 0.2550\n",
      "272/281, train_loss: 0.0878, step time: 0.2495\n",
      "273/281, train_loss: 0.2378, step time: 0.2482\n",
      "274/281, train_loss: 0.0973, step time: 0.2577\n",
      "275/281, train_loss: 0.0906, step time: 0.2478\n",
      "276/281, train_loss: 0.0663, step time: 0.2528\n",
      "277/281, train_loss: 0.0556, step time: 0.2579\n",
      "278/281, train_loss: 0.0589, step time: 0.2568\n",
      "279/281, train_loss: 0.2342, step time: 0.2559\n",
      "280/281, train_loss: 0.0739, step time: 0.2546\n",
      "281/281, train_loss: 0.0988, step time: 0.2550\n",
      "282/281, train_loss: 0.0398, step time: 0.1530\n",
      "epoch 72 average loss: 0.1064\n",
      "current epoch: 72 current mean dice: 0.8868 tc: 0.8788 wt: 0.9191 et: 0.8715\n",
      "best mean dice: 0.8878 at epoch: 68\n",
      "time consuming of epoch 72 is: 414.4888\n",
      "----------\n",
      "epoch 73/200\n",
      "1/281, train_loss: 0.1000, step time: 0.2525\n",
      "2/281, train_loss: 0.1166, step time: 0.2632\n",
      "3/281, train_loss: 0.0955, step time: 0.2586\n",
      "4/281, train_loss: 0.1013, step time: 0.2588\n",
      "5/281, train_loss: 0.0785, step time: 0.2493\n",
      "6/281, train_loss: 0.1139, step time: 0.2538\n",
      "7/281, train_loss: 0.0825, step time: 0.2552\n",
      "8/281, train_loss: 0.0774, step time: 0.2601\n",
      "9/281, train_loss: 0.0997, step time: 0.2545\n",
      "10/281, train_loss: 0.0578, step time: 0.2556\n",
      "11/281, train_loss: 0.1033, step time: 0.2535\n",
      "12/281, train_loss: 0.2114, step time: 0.2516\n",
      "13/281, train_loss: 0.0619, step time: 0.2537\n",
      "14/281, train_loss: 0.2307, step time: 0.2509\n",
      "15/281, train_loss: 0.0683, step time: 0.2531\n",
      "16/281, train_loss: 0.0785, step time: 0.2550\n",
      "17/281, train_loss: 0.2327, step time: 0.2564\n",
      "18/281, train_loss: 0.0922, step time: 0.2502\n",
      "19/281, train_loss: 0.1042, step time: 0.2524\n",
      "20/281, train_loss: 0.0896, step time: 0.2503\n",
      "21/281, train_loss: 0.0573, step time: 0.2522\n",
      "22/281, train_loss: 0.0786, step time: 0.2570\n",
      "23/281, train_loss: 0.1038, step time: 0.2568\n",
      "24/281, train_loss: 0.0917, step time: 0.2548\n",
      "25/281, train_loss: 0.0639, step time: 0.2591\n",
      "26/281, train_loss: 0.2537, step time: 0.2598\n",
      "27/281, train_loss: 0.2480, step time: 0.2580\n",
      "28/281, train_loss: 0.0752, step time: 0.2609\n",
      "29/281, train_loss: 0.0676, step time: 0.2586\n",
      "30/281, train_loss: 0.0951, step time: 0.2568\n",
      "31/281, train_loss: 0.0870, step time: 0.2551\n",
      "32/281, train_loss: 0.0930, step time: 0.2535\n",
      "33/281, train_loss: 0.0946, step time: 0.2486\n",
      "34/281, train_loss: 0.1097, step time: 0.2511\n",
      "35/281, train_loss: 0.0841, step time: 0.2506\n",
      "36/281, train_loss: 0.0633, step time: 0.2513\n",
      "37/281, train_loss: 0.0561, step time: 0.2527\n",
      "38/281, train_loss: 0.0892, step time: 0.2486\n",
      "39/281, train_loss: 0.1158, step time: 0.2590\n",
      "40/281, train_loss: 0.2594, step time: 0.2511\n",
      "41/281, train_loss: 0.0534, step time: 0.2486\n",
      "42/281, train_loss: 0.0573, step time: 0.2528\n",
      "43/281, train_loss: 0.0741, step time: 0.2540\n",
      "44/281, train_loss: 0.0384, step time: 0.2527\n",
      "45/281, train_loss: 0.0920, step time: 0.2523\n",
      "46/281, train_loss: 0.2378, step time: 0.2499\n",
      "47/281, train_loss: 0.1264, step time: 0.2513\n",
      "48/281, train_loss: 0.0842, step time: 0.2498\n",
      "49/281, train_loss: 0.0694, step time: 0.2559\n",
      "50/281, train_loss: 0.0804, step time: 0.2530\n",
      "51/281, train_loss: 0.0683, step time: 0.2501\n",
      "52/281, train_loss: 0.2670, step time: 0.2537\n",
      "53/281, train_loss: 0.0481, step time: 0.2562\n",
      "54/281, train_loss: 0.1013, step time: 0.2527\n",
      "55/281, train_loss: 0.0495, step time: 0.2582\n",
      "56/281, train_loss: 0.0500, step time: 0.2572\n",
      "57/281, train_loss: 0.1023, step time: 0.2526\n",
      "58/281, train_loss: 0.0828, step time: 0.2538\n",
      "59/281, train_loss: 0.0854, step time: 0.2503\n",
      "60/281, train_loss: 0.2278, step time: 0.2525\n",
      "61/281, train_loss: 0.0789, step time: 0.2501\n",
      "62/281, train_loss: 0.0755, step time: 0.2471\n",
      "63/281, train_loss: 0.0719, step time: 0.2519\n",
      "64/281, train_loss: 0.0883, step time: 0.2524\n",
      "65/281, train_loss: 0.0728, step time: 0.2583\n",
      "66/281, train_loss: 0.0858, step time: 0.2537\n",
      "67/281, train_loss: 0.0589, step time: 0.2556\n",
      "68/281, train_loss: 0.1230, step time: 0.2483\n",
      "69/281, train_loss: 0.0919, step time: 0.2496\n",
      "70/281, train_loss: 0.0833, step time: 0.2493\n",
      "71/281, train_loss: 0.0713, step time: 0.2524\n",
      "72/281, train_loss: 0.0504, step time: 0.2536\n",
      "73/281, train_loss: 0.0960, step time: 0.2571\n",
      "74/281, train_loss: 0.0537, step time: 0.2540\n",
      "75/281, train_loss: 0.1415, step time: 0.2515\n",
      "76/281, train_loss: 0.0788, step time: 0.2546\n",
      "77/281, train_loss: 0.0831, step time: 0.2499\n",
      "78/281, train_loss: 0.4052, step time: 0.2522\n",
      "79/281, train_loss: 0.0601, step time: 0.2580\n",
      "80/281, train_loss: 0.0826, step time: 0.2495\n",
      "81/281, train_loss: 0.0933, step time: 0.2550\n",
      "82/281, train_loss: 0.0909, step time: 0.2533\n",
      "83/281, train_loss: 0.0780, step time: 0.2539\n",
      "84/281, train_loss: 0.0808, step time: 0.2523\n",
      "85/281, train_loss: 0.0465, step time: 0.2507\n",
      "86/281, train_loss: 0.0785, step time: 0.2452\n",
      "87/281, train_loss: 0.0929, step time: 0.2542\n",
      "88/281, train_loss: 0.2333, step time: 0.2479\n",
      "89/281, train_loss: 0.0631, step time: 0.2598\n",
      "90/281, train_loss: 0.0676, step time: 0.2557\n",
      "91/281, train_loss: 0.0571, step time: 0.2600\n",
      "92/281, train_loss: 0.0697, step time: 0.2604\n",
      "93/281, train_loss: 0.0995, step time: 0.2544\n",
      "94/281, train_loss: 0.0444, step time: 0.2533\n",
      "95/281, train_loss: 0.0592, step time: 0.2573\n",
      "96/281, train_loss: 0.0645, step time: 0.2559\n",
      "97/281, train_loss: 0.0831, step time: 0.2532\n",
      "98/281, train_loss: 0.0544, step time: 0.2564\n",
      "99/281, train_loss: 0.0604, step time: 0.2553\n",
      "100/281, train_loss: 0.0891, step time: 0.2491\n",
      "101/281, train_loss: 0.0467, step time: 0.2518\n",
      "102/281, train_loss: 0.0735, step time: 0.2539\n",
      "103/281, train_loss: 0.1066, step time: 0.2557\n",
      "104/281, train_loss: 0.0452, step time: 0.2561\n",
      "105/281, train_loss: 0.0584, step time: 0.2538\n",
      "106/281, train_loss: 0.1296, step time: 0.2503\n",
      "107/281, train_loss: 0.0762, step time: 0.2526\n",
      "108/281, train_loss: 0.1073, step time: 0.2466\n",
      "109/281, train_loss: 0.2266, step time: 0.2514\n",
      "110/281, train_loss: 0.0631, step time: 0.2476\n",
      "111/281, train_loss: 0.0830, step time: 0.2475\n",
      "112/281, train_loss: 0.2554, step time: 0.2490\n",
      "113/281, train_loss: 0.0806, step time: 0.2472\n",
      "114/281, train_loss: 0.0947, step time: 0.2486\n",
      "115/281, train_loss: 0.0894, step time: 0.2527\n",
      "116/281, train_loss: 0.0541, step time: 0.2474\n",
      "117/281, train_loss: 0.2292, step time: 0.2443\n",
      "118/281, train_loss: 0.0659, step time: 0.2517\n",
      "119/281, train_loss: 0.0805, step time: 0.2460\n",
      "120/281, train_loss: 0.2829, step time: 0.2516\n",
      "121/281, train_loss: 0.0733, step time: 0.2451\n",
      "122/281, train_loss: 0.0976, step time: 0.2554\n",
      "123/281, train_loss: 0.0848, step time: 0.2502\n",
      "124/281, train_loss: 0.0736, step time: 0.2522\n",
      "125/281, train_loss: 0.0752, step time: 0.2529\n",
      "126/281, train_loss: 0.1643, step time: 0.2297\n",
      "127/281, train_loss: 0.0932, step time: 0.2532\n",
      "128/281, train_loss: 0.0924, step time: 0.2529\n",
      "129/281, train_loss: 0.0608, step time: 0.2520\n",
      "130/281, train_loss: 0.0730, step time: 0.2561\n",
      "131/281, train_loss: 0.0567, step time: 0.2594\n",
      "132/281, train_loss: 0.0930, step time: 0.2578\n",
      "133/281, train_loss: 0.0665, step time: 0.2505\n",
      "134/281, train_loss: 0.0661, step time: 0.2614\n",
      "135/281, train_loss: 0.0633, step time: 0.2506\n",
      "136/281, train_loss: 0.0861, step time: 0.2495\n",
      "137/281, train_loss: 0.0699, step time: 0.2493\n",
      "138/281, train_loss: 0.2214, step time: 0.2540\n",
      "139/281, train_loss: 0.0814, step time: 0.2545\n",
      "140/281, train_loss: 0.0748, step time: 0.2517\n",
      "141/281, train_loss: 0.0414, step time: 0.2509\n",
      "142/281, train_loss: 0.0714, step time: 0.2475\n",
      "143/281, train_loss: 0.0732, step time: 0.2778\n",
      "144/281, train_loss: 0.0754, step time: 0.2554\n",
      "145/281, train_loss: 0.0719, step time: 0.2589\n",
      "146/281, train_loss: 0.0933, step time: 0.2546\n",
      "147/281, train_loss: 0.0762, step time: 0.2520\n",
      "148/281, train_loss: 0.0484, step time: 0.2596\n",
      "149/281, train_loss: 0.0758, step time: 0.2530\n",
      "150/281, train_loss: 0.0851, step time: 0.2524\n",
      "151/281, train_loss: 0.0795, step time: 0.2531\n",
      "152/281, train_loss: 0.0490, step time: 0.2517\n",
      "153/281, train_loss: 0.3874, step time: 0.2545\n",
      "154/281, train_loss: 0.0806, step time: 0.2610\n",
      "155/281, train_loss: 0.0628, step time: 0.2544\n",
      "156/281, train_loss: 0.0747, step time: 0.2517\n",
      "157/281, train_loss: 0.0998, step time: 0.2519\n",
      "158/281, train_loss: 0.0720, step time: 0.2556\n",
      "159/281, train_loss: 0.2322, step time: 0.2532\n",
      "160/281, train_loss: 0.1154, step time: 0.2534\n",
      "161/281, train_loss: 0.1116, step time: 0.2552\n",
      "162/281, train_loss: 0.0825, step time: 0.2534\n",
      "163/281, train_loss: 0.2444, step time: 0.2536\n",
      "164/281, train_loss: 0.1983, step time: 0.2457\n",
      "165/281, train_loss: 0.1030, step time: 0.2525\n",
      "166/281, train_loss: 0.1365, step time: 0.2535\n",
      "167/281, train_loss: 0.1211, step time: 0.2543\n",
      "168/281, train_loss: 0.0671, step time: 0.2456\n",
      "169/281, train_loss: 0.1203, step time: 0.2465\n",
      "170/281, train_loss: 0.1088, step time: 0.2489\n",
      "171/281, train_loss: 0.0742, step time: 0.2466\n",
      "172/281, train_loss: 0.0698, step time: 0.2492\n",
      "173/281, train_loss: 0.1045, step time: 0.2500\n",
      "174/281, train_loss: 0.2255, step time: 0.2551\n",
      "175/281, train_loss: 0.0591, step time: 0.2543\n",
      "176/281, train_loss: 0.1060, step time: 0.2495\n",
      "177/281, train_loss: 0.0802, step time: 0.2466\n",
      "178/281, train_loss: 0.0710, step time: 0.2545\n",
      "179/281, train_loss: 0.2604, step time: 0.2519\n",
      "180/281, train_loss: 0.0510, step time: 0.2479\n",
      "181/281, train_loss: 0.0551, step time: 0.2450\n",
      "182/281, train_loss: 0.1118, step time: 0.2529\n",
      "183/281, train_loss: 0.0536, step time: 0.2560\n",
      "184/281, train_loss: 0.0785, step time: 0.2490\n",
      "185/281, train_loss: 0.0572, step time: 0.2505\n",
      "186/281, train_loss: 0.2312, step time: 0.2500\n",
      "187/281, train_loss: 0.0674, step time: 0.2499\n",
      "188/281, train_loss: 0.0720, step time: 0.2532\n",
      "189/281, train_loss: 0.0903, step time: 0.2575\n",
      "190/281, train_loss: 0.0549, step time: 0.2487\n",
      "191/281, train_loss: 0.1513, step time: 0.2514\n",
      "192/281, train_loss: 0.0791, step time: 0.2588\n",
      "193/281, train_loss: 0.0506, step time: 0.2536\n",
      "194/281, train_loss: 0.0438, step time: 0.2517\n",
      "195/281, train_loss: 0.0956, step time: 0.2515\n",
      "196/281, train_loss: 0.0542, step time: 0.2425\n",
      "197/281, train_loss: 0.2355, step time: 0.2488\n",
      "198/281, train_loss: 0.0708, step time: 0.2467\n",
      "199/281, train_loss: 0.0765, step time: 0.2449\n",
      "200/281, train_loss: 0.2085, step time: 0.2267\n",
      "201/281, train_loss: 0.2493, step time: 0.2564\n",
      "202/281, train_loss: 0.0431, step time: 0.2497\n",
      "203/281, train_loss: 0.0736, step time: 0.2506\n",
      "204/281, train_loss: 0.2162, step time: 0.2533\n",
      "205/281, train_loss: 0.0558, step time: 0.2466\n",
      "206/281, train_loss: 0.0771, step time: 0.2479\n",
      "207/281, train_loss: 0.0856, step time: 0.2488\n",
      "208/281, train_loss: 0.0700, step time: 0.2492\n",
      "209/281, train_loss: 0.2224, step time: 0.2536\n",
      "210/281, train_loss: 0.0548, step time: 0.2518\n",
      "211/281, train_loss: 0.2468, step time: 0.2534\n",
      "212/281, train_loss: 0.2282, step time: 0.2505\n",
      "213/281, train_loss: 0.2324, step time: 0.2530\n",
      "214/281, train_loss: 0.0735, step time: 0.2468\n",
      "215/281, train_loss: 0.0988, step time: 0.2508\n",
      "216/281, train_loss: 0.1539, step time: 0.2576\n",
      "217/281, train_loss: 0.0927, step time: 0.2527\n",
      "218/281, train_loss: 0.0652, step time: 0.2557\n",
      "219/281, train_loss: 0.0806, step time: 0.2447\n",
      "220/281, train_loss: 0.0643, step time: 0.2495\n",
      "221/281, train_loss: 0.0493, step time: 0.2484\n",
      "222/281, train_loss: 0.2453, step time: 0.2473\n",
      "223/281, train_loss: 0.0392, step time: 0.2545\n",
      "224/281, train_loss: 0.0935, step time: 0.2528\n",
      "225/281, train_loss: 0.0865, step time: 0.2478\n",
      "226/281, train_loss: 0.0840, step time: 0.2469\n",
      "227/281, train_loss: 0.2308, step time: 0.2473\n",
      "228/281, train_loss: 0.0902, step time: 0.2509\n",
      "229/281, train_loss: 0.0677, step time: 0.2538\n",
      "230/281, train_loss: 0.0976, step time: 0.2541\n",
      "231/281, train_loss: 0.2203, step time: 0.2475\n",
      "232/281, train_loss: 0.2428, step time: 0.2512\n",
      "233/281, train_loss: 0.0820, step time: 0.2509\n",
      "234/281, train_loss: 0.0624, step time: 0.2433\n",
      "235/281, train_loss: 0.0886, step time: 0.2442\n",
      "236/281, train_loss: 0.1285, step time: 0.2482\n",
      "237/281, train_loss: 0.0665, step time: 0.2491\n",
      "238/281, train_loss: 0.0456, step time: 0.2493\n",
      "239/281, train_loss: 0.2628, step time: 0.2468\n",
      "240/281, train_loss: 0.0795, step time: 0.2528\n",
      "241/281, train_loss: 0.0841, step time: 0.2499\n",
      "242/281, train_loss: 0.0802, step time: 0.2507\n",
      "243/281, train_loss: 0.2139, step time: 0.2536\n",
      "244/281, train_loss: 0.0908, step time: 0.2517\n",
      "245/281, train_loss: 0.0722, step time: 0.2523\n",
      "246/281, train_loss: 0.0817, step time: 0.2480\n",
      "247/281, train_loss: 0.0738, step time: 0.2491\n",
      "248/281, train_loss: 0.1138, step time: 0.2470\n",
      "249/281, train_loss: 0.0835, step time: 0.2485\n",
      "250/281, train_loss: 0.0602, step time: 0.2506\n",
      "251/281, train_loss: 0.0935, step time: 0.2518\n",
      "252/281, train_loss: 0.0558, step time: 0.2469\n",
      "253/281, train_loss: 0.0561, step time: 0.2537\n",
      "254/281, train_loss: 0.0778, step time: 0.2513\n",
      "255/281, train_loss: 0.0794, step time: 0.2531\n",
      "256/281, train_loss: 0.0852, step time: 0.2570\n",
      "257/281, train_loss: 0.2467, step time: 0.2489\n",
      "258/281, train_loss: 0.4153, step time: 0.2478\n",
      "259/281, train_loss: 0.1255, step time: 0.2468\n",
      "260/281, train_loss: 0.1103, step time: 0.2768\n",
      "261/281, train_loss: 0.0899, step time: 0.2530\n",
      "262/281, train_loss: 0.0750, step time: 0.2502\n",
      "263/281, train_loss: 0.2233, step time: 0.2472\n",
      "264/281, train_loss: 0.0721, step time: 0.2463\n",
      "265/281, train_loss: 0.0979, step time: 0.2481\n",
      "266/281, train_loss: 0.1060, step time: 0.2515\n",
      "267/281, train_loss: 0.0764, step time: 0.2508\n",
      "268/281, train_loss: 0.0740, step time: 0.2515\n",
      "269/281, train_loss: 0.1230, step time: 0.2537\n",
      "270/281, train_loss: 0.0774, step time: 0.2555\n",
      "271/281, train_loss: 0.0945, step time: 0.2518\n",
      "272/281, train_loss: 0.0853, step time: 0.2529\n",
      "273/281, train_loss: 0.0878, step time: 0.2470\n",
      "274/281, train_loss: 0.0868, step time: 0.2512\n",
      "275/281, train_loss: 0.0782, step time: 0.2530\n",
      "276/281, train_loss: 0.0706, step time: 0.2544\n",
      "277/281, train_loss: 0.0876, step time: 0.2533\n",
      "278/281, train_loss: 0.2495, step time: 0.2551\n",
      "279/281, train_loss: 0.0841, step time: 0.2543\n",
      "280/281, train_loss: 0.1008, step time: 0.2551\n",
      "281/281, train_loss: 0.0552, step time: 0.2521\n",
      "282/281, train_loss: 0.3719, step time: 0.1508\n",
      "epoch 73 average loss: 0.1059\n",
      "current epoch: 73 current mean dice: 0.8857 tc: 0.8757 wt: 0.9191 et: 0.8716\n",
      "best mean dice: 0.8878 at epoch: 68\n",
      "time consuming of epoch 73 is: 386.6541\n",
      "----------\n",
      "epoch 74/200\n",
      "1/281, train_loss: 0.2235, step time: 0.2454\n",
      "2/281, train_loss: 0.1127, step time: 0.2445\n",
      "3/281, train_loss: 0.2473, step time: 0.2494\n",
      "4/281, train_loss: 0.0828, step time: 0.2490\n",
      "5/281, train_loss: 0.0580, step time: 0.2507\n",
      "6/281, train_loss: 0.0704, step time: 0.2539\n",
      "7/281, train_loss: 0.0616, step time: 0.2518\n",
      "8/281, train_loss: 0.1066, step time: 0.2512\n",
      "9/281, train_loss: 0.1357, step time: 0.2494\n",
      "10/281, train_loss: 0.0668, step time: 0.2540\n",
      "11/281, train_loss: 0.0893, step time: 0.2471\n",
      "12/281, train_loss: 0.2326, step time: 0.2519\n",
      "13/281, train_loss: 0.0908, step time: 0.2563\n",
      "14/281, train_loss: 0.0965, step time: 0.2554\n",
      "15/281, train_loss: 0.0659, step time: 0.2544\n",
      "16/281, train_loss: 0.0723, step time: 0.2466\n",
      "17/281, train_loss: 0.0798, step time: 0.2428\n",
      "18/281, train_loss: 0.0571, step time: 0.2427\n",
      "19/281, train_loss: 0.0647, step time: 0.2469\n",
      "20/281, train_loss: 0.1538, step time: 0.2550\n",
      "21/281, train_loss: 0.1246, step time: 0.2546\n",
      "22/281, train_loss: 0.0802, step time: 0.2484\n",
      "23/281, train_loss: 0.1059, step time: 0.2554\n",
      "24/281, train_loss: 0.1064, step time: 0.2564\n",
      "25/281, train_loss: 0.1106, step time: 0.2572\n",
      "26/281, train_loss: 0.0748, step time: 0.2636\n",
      "27/281, train_loss: 0.0500, step time: 0.2579\n",
      "28/281, train_loss: 0.0903, step time: 0.2610\n",
      "29/281, train_loss: 0.0814, step time: 0.2567\n",
      "30/281, train_loss: 0.0652, step time: 0.2535\n",
      "31/281, train_loss: 0.0844, step time: 0.2500\n",
      "32/281, train_loss: 0.1436, step time: 0.2558\n",
      "33/281, train_loss: 0.0630, step time: 0.2584\n",
      "34/281, train_loss: 0.1304, step time: 0.2593\n",
      "35/281, train_loss: 0.0403, step time: 0.2582\n",
      "36/281, train_loss: 0.0869, step time: 0.2535\n",
      "37/281, train_loss: 0.0692, step time: 0.2691\n",
      "38/281, train_loss: 0.0946, step time: 0.2550\n",
      "39/281, train_loss: 0.0820, step time: 0.2693\n",
      "40/281, train_loss: 0.0871, step time: 0.2565\n",
      "41/281, train_loss: 0.1074, step time: 0.2596\n",
      "42/281, train_loss: 0.0697, step time: 0.2621\n",
      "43/281, train_loss: 0.0539, step time: 0.2576\n",
      "44/281, train_loss: 0.1112, step time: 0.2594\n",
      "45/281, train_loss: 0.1072, step time: 0.2614\n",
      "46/281, train_loss: 0.0851, step time: 0.2566\n",
      "47/281, train_loss: 0.1119, step time: 0.2553\n",
      "48/281, train_loss: 0.0660, step time: 0.2546\n",
      "49/281, train_loss: 0.2318, step time: 0.2586\n",
      "50/281, train_loss: 0.0845, step time: 0.2566\n",
      "51/281, train_loss: 0.0754, step time: 0.2505\n",
      "52/281, train_loss: 0.0589, step time: 0.2569\n",
      "53/281, train_loss: 0.1245, step time: 0.2568\n",
      "54/281, train_loss: 0.0945, step time: 0.2545\n",
      "55/281, train_loss: 0.2363, step time: 0.2604\n",
      "56/281, train_loss: 0.0738, step time: 0.2524\n",
      "57/281, train_loss: 0.1058, step time: 0.2554\n",
      "58/281, train_loss: 0.0754, step time: 0.2540\n",
      "59/281, train_loss: 0.0574, step time: 0.2535\n",
      "60/281, train_loss: 0.0725, step time: 0.2513\n",
      "61/281, train_loss: 0.0710, step time: 0.2535\n",
      "62/281, train_loss: 0.0987, step time: 0.2552\n",
      "63/281, train_loss: 0.0656, step time: 0.2563\n",
      "64/281, train_loss: 0.1125, step time: 0.2551\n",
      "65/281, train_loss: 0.0682, step time: 0.2587\n",
      "66/281, train_loss: 0.0788, step time: 0.2561\n",
      "67/281, train_loss: 0.0827, step time: 0.2562\n",
      "68/281, train_loss: 0.0763, step time: 0.2595\n",
      "69/281, train_loss: 0.0841, step time: 0.2607\n",
      "70/281, train_loss: 0.0550, step time: 0.2633\n",
      "71/281, train_loss: 0.0899, step time: 0.2575\n",
      "72/281, train_loss: 0.0947, step time: 0.2541\n",
      "73/281, train_loss: 0.3862, step time: 0.2633\n",
      "74/281, train_loss: 0.0752, step time: 0.2595\n",
      "75/281, train_loss: 0.0951, step time: 0.2543\n",
      "76/281, train_loss: 0.2552, step time: 0.2545\n",
      "77/281, train_loss: 0.0708, step time: 0.2532\n",
      "78/281, train_loss: 0.0870, step time: 0.2566\n",
      "79/281, train_loss: 0.0939, step time: 0.2573\n",
      "80/281, train_loss: 0.0554, step time: 0.2584\n",
      "81/281, train_loss: 0.0944, step time: 0.2550\n",
      "82/281, train_loss: 0.0681, step time: 0.2561\n",
      "83/281, train_loss: 0.0500, step time: 0.2517\n",
      "84/281, train_loss: 0.2217, step time: 0.2597\n",
      "85/281, train_loss: 0.0765, step time: 0.2563\n",
      "86/281, train_loss: 0.0889, step time: 0.2586\n",
      "87/281, train_loss: 0.0931, step time: 0.2550\n",
      "88/281, train_loss: 0.1505, step time: 0.2548\n",
      "89/281, train_loss: 0.2729, step time: 0.2560\n",
      "90/281, train_loss: 0.0991, step time: 0.2531\n",
      "91/281, train_loss: 0.0925, step time: 0.2536\n",
      "92/281, train_loss: 0.0670, step time: 0.2587\n",
      "93/281, train_loss: 0.0652, step time: 0.2556\n",
      "94/281, train_loss: 0.0827, step time: 0.2497\n",
      "95/281, train_loss: 0.2221, step time: 0.2548\n",
      "96/281, train_loss: 0.0618, step time: 0.2560\n",
      "97/281, train_loss: 0.1106, step time: 0.2537\n",
      "98/281, train_loss: 0.1174, step time: 0.2557\n",
      "99/281, train_loss: 0.0829, step time: 0.2601\n",
      "100/281, train_loss: 0.2488, step time: 0.2878\n",
      "101/281, train_loss: 0.0720, step time: 0.2594\n",
      "102/281, train_loss: 0.1034, step time: 0.2575\n",
      "103/281, train_loss: 0.0704, step time: 0.2598\n",
      "104/281, train_loss: 0.0793, step time: 0.2577\n",
      "105/281, train_loss: 0.2434, step time: 0.2538\n",
      "106/281, train_loss: 0.0573, step time: 0.2675\n",
      "107/281, train_loss: 0.2436, step time: 0.2610\n",
      "108/281, train_loss: 0.1105, step time: 0.2516\n",
      "109/281, train_loss: 0.0851, step time: 0.2547\n",
      "110/281, train_loss: 0.0793, step time: 0.2495\n",
      "111/281, train_loss: 0.0851, step time: 0.2555\n",
      "112/281, train_loss: 0.1265, step time: 0.2579\n",
      "113/281, train_loss: 0.1023, step time: 0.2614\n",
      "114/281, train_loss: 0.0996, step time: 0.2625\n",
      "115/281, train_loss: 0.0706, step time: 0.2578\n",
      "116/281, train_loss: 0.0645, step time: 0.2573\n",
      "117/281, train_loss: 0.0606, step time: 0.2639\n",
      "118/281, train_loss: 0.0625, step time: 0.2631\n",
      "119/281, train_loss: 0.0958, step time: 0.2556\n",
      "120/281, train_loss: 0.1166, step time: 0.2584\n",
      "121/281, train_loss: 0.0838, step time: 0.2609\n",
      "122/281, train_loss: 0.0894, step time: 0.2590\n",
      "123/281, train_loss: 0.0880, step time: 0.2591\n",
      "124/281, train_loss: 0.0948, step time: 0.2618\n",
      "125/281, train_loss: 0.0707, step time: 0.2585\n",
      "126/281, train_loss: 0.0792, step time: 0.2604\n",
      "127/281, train_loss: 0.0396, step time: 0.2543\n",
      "128/281, train_loss: 0.0805, step time: 0.2569\n",
      "129/281, train_loss: 0.0628, step time: 0.2575\n",
      "130/281, train_loss: 0.0788, step time: 0.2563\n",
      "131/281, train_loss: 0.0501, step time: 0.2550\n",
      "132/281, train_loss: 0.0544, step time: 0.2585\n",
      "133/281, train_loss: 0.2093, step time: 0.2526\n",
      "134/281, train_loss: 0.2600, step time: 0.2507\n",
      "135/281, train_loss: 0.0579, step time: 0.2533\n",
      "136/281, train_loss: 0.0835, step time: 0.2545\n",
      "137/281, train_loss: 0.0948, step time: 0.2582\n",
      "138/281, train_loss: 0.0828, step time: 0.2474\n",
      "139/281, train_loss: 0.2168, step time: 0.2566\n",
      "140/281, train_loss: 0.0742, step time: 0.2551\n",
      "141/281, train_loss: 0.0970, step time: 0.2614\n",
      "142/281, train_loss: 0.2232, step time: 0.2598\n",
      "143/281, train_loss: 0.0513, step time: 0.2564\n",
      "144/281, train_loss: 0.2227, step time: 0.2576\n",
      "145/281, train_loss: 0.0676, step time: 0.2497\n",
      "146/281, train_loss: 0.1119, step time: 0.2543\n",
      "147/281, train_loss: 0.2126, step time: 0.2519\n",
      "148/281, train_loss: 0.0667, step time: 0.2537\n",
      "149/281, train_loss: 0.2401, step time: 0.2562\n",
      "150/281, train_loss: 0.0629, step time: 0.2551\n",
      "151/281, train_loss: 0.0809, step time: 0.2512\n",
      "152/281, train_loss: 0.2271, step time: 0.2587\n",
      "153/281, train_loss: 0.0662, step time: 0.2512\n",
      "154/281, train_loss: 0.0996, step time: 0.2522\n",
      "155/281, train_loss: 0.2312, step time: 0.2519\n",
      "156/281, train_loss: 0.1491, step time: 0.2572\n",
      "157/281, train_loss: 0.0787, step time: 0.2598\n",
      "158/281, train_loss: 0.0510, step time: 0.2614\n",
      "159/281, train_loss: 0.0628, step time: 0.2557\n",
      "160/281, train_loss: 0.0646, step time: 0.2573\n",
      "161/281, train_loss: 0.1273, step time: 0.2565\n",
      "162/281, train_loss: 0.0685, step time: 0.2516\n",
      "163/281, train_loss: 0.2818, step time: 0.2576\n",
      "164/281, train_loss: 0.0930, step time: 0.2526\n",
      "165/281, train_loss: 0.2075, step time: 0.2544\n",
      "166/281, train_loss: 0.0759, step time: 0.2691\n",
      "167/281, train_loss: 0.0748, step time: 0.2585\n",
      "168/281, train_loss: 0.0768, step time: 0.2578\n",
      "169/281, train_loss: 0.0785, step time: 0.2620\n",
      "170/281, train_loss: 0.0821, step time: 0.2750\n",
      "171/281, train_loss: 0.0865, step time: 0.2568\n",
      "172/281, train_loss: 0.0688, step time: 0.2578\n",
      "173/281, train_loss: 0.0953, step time: 0.2527\n",
      "174/281, train_loss: 0.2585, step time: 0.2610\n",
      "175/281, train_loss: 0.1290, step time: 0.2556\n",
      "176/281, train_loss: 0.0974, step time: 0.2537\n",
      "177/281, train_loss: 0.0806, step time: 0.2523\n",
      "178/281, train_loss: 0.1075, step time: 0.2518\n",
      "179/281, train_loss: 0.0410, step time: 0.2505\n",
      "180/281, train_loss: 0.0619, step time: 0.2505\n",
      "181/281, train_loss: 0.0855, step time: 0.2578\n",
      "182/281, train_loss: 0.0661, step time: 0.2560\n",
      "183/281, train_loss: 0.0865, step time: 0.2578\n",
      "184/281, train_loss: 0.0577, step time: 0.2592\n",
      "185/281, train_loss: 0.1027, step time: 0.2562\n",
      "186/281, train_loss: 0.0868, step time: 0.2586\n",
      "187/281, train_loss: 0.0974, step time: 0.2585\n",
      "188/281, train_loss: 0.1030, step time: 0.2570\n",
      "189/281, train_loss: 0.1042, step time: 0.2545\n",
      "190/281, train_loss: 0.0756, step time: 0.2586\n",
      "191/281, train_loss: 0.0983, step time: 0.2550\n",
      "192/281, train_loss: 0.1055, step time: 0.2518\n",
      "193/281, train_loss: 0.0860, step time: 0.2562\n",
      "194/281, train_loss: 0.0891, step time: 0.2583\n",
      "195/281, train_loss: 0.0997, step time: 0.2557\n",
      "196/281, train_loss: 0.1077, step time: 0.2546\n",
      "197/281, train_loss: 0.0987, step time: 0.2566\n",
      "198/281, train_loss: 0.0638, step time: 0.2593\n",
      "199/281, train_loss: 0.0682, step time: 0.2568\n",
      "200/281, train_loss: 0.1017, step time: 0.2604\n",
      "201/281, train_loss: 0.0946, step time: 0.2553\n",
      "202/281, train_loss: 0.0662, step time: 0.2566\n",
      "203/281, train_loss: 0.0896, step time: 0.2586\n",
      "204/281, train_loss: 0.0606, step time: 0.2583\n",
      "205/281, train_loss: 0.0913, step time: 0.2565\n",
      "206/281, train_loss: 0.2214, step time: 0.2611\n",
      "207/281, train_loss: 0.0990, step time: 0.2570\n",
      "208/281, train_loss: 0.0563, step time: 0.2582\n",
      "209/281, train_loss: 0.2161, step time: 0.2570\n",
      "210/281, train_loss: 0.0356, step time: 0.2591\n",
      "211/281, train_loss: 0.0975, step time: 0.2574\n",
      "212/281, train_loss: 0.0590, step time: 0.2545\n",
      "213/281, train_loss: 0.0770, step time: 0.2566\n",
      "214/281, train_loss: 0.0703, step time: 0.2585\n",
      "215/281, train_loss: 0.0812, step time: 0.2581\n",
      "216/281, train_loss: 0.0842, step time: 0.2579\n",
      "217/281, train_loss: 0.0652, step time: 0.2519\n",
      "218/281, train_loss: 0.0762, step time: 0.2566\n",
      "219/281, train_loss: 0.0665, step time: 0.2527\n",
      "220/281, train_loss: 0.0687, step time: 0.2515\n",
      "221/281, train_loss: 0.0567, step time: 0.2566\n",
      "222/281, train_loss: 0.0545, step time: 0.2566\n",
      "223/281, train_loss: 0.0699, step time: 0.2586\n",
      "224/281, train_loss: 0.0588, step time: 0.2576\n",
      "225/281, train_loss: 0.0659, step time: 0.2592\n",
      "226/281, train_loss: 0.0849, step time: 0.2522\n",
      "227/281, train_loss: 0.2574, step time: 0.2525\n",
      "228/281, train_loss: 0.0740, step time: 0.2586\n",
      "229/281, train_loss: 0.0723, step time: 0.2576\n",
      "230/281, train_loss: 0.2696, step time: 0.2592\n",
      "231/281, train_loss: 0.2082, step time: 0.2495\n",
      "232/281, train_loss: 0.0901, step time: 0.2511\n",
      "233/281, train_loss: 0.0874, step time: 0.2572\n",
      "234/281, train_loss: 0.0649, step time: 0.2536\n",
      "235/281, train_loss: 0.0694, step time: 0.2531\n",
      "236/281, train_loss: 0.2209, step time: 0.2545\n",
      "237/281, train_loss: 0.0591, step time: 0.2477\n",
      "238/281, train_loss: 0.1186, step time: 0.2536\n",
      "239/281, train_loss: 0.2625, step time: 0.2550\n",
      "240/281, train_loss: 0.0669, step time: 0.2559\n",
      "241/281, train_loss: 0.0623, step time: 0.2541\n",
      "242/281, train_loss: 0.1651, step time: 0.2525\n",
      "243/281, train_loss: 0.0782, step time: 0.2548\n",
      "244/281, train_loss: 0.0997, step time: 0.2531\n",
      "245/281, train_loss: 0.0605, step time: 0.2596\n",
      "246/281, train_loss: 0.0661, step time: 0.2588\n",
      "247/281, train_loss: 0.0677, step time: 0.2559\n",
      "248/281, train_loss: 0.0740, step time: 0.2568\n",
      "249/281, train_loss: 0.0592, step time: 0.2572\n",
      "250/281, train_loss: 0.0709, step time: 0.2562\n",
      "251/281, train_loss: 0.4123, step time: 0.2517\n",
      "252/281, train_loss: 0.0914, step time: 0.2575\n",
      "253/281, train_loss: 0.0834, step time: 0.2566\n",
      "254/281, train_loss: 0.2506, step time: 0.2562\n",
      "255/281, train_loss: 0.0976, step time: 0.2574\n",
      "256/281, train_loss: 0.1242, step time: 0.2518\n",
      "257/281, train_loss: 0.2618, step time: 0.2577\n",
      "258/281, train_loss: 0.1262, step time: 0.2583\n",
      "259/281, train_loss: 0.0644, step time: 0.2546\n",
      "260/281, train_loss: 0.1249, step time: 0.2485\n",
      "261/281, train_loss: 0.0794, step time: 0.2551\n",
      "262/281, train_loss: 0.0384, step time: 0.2575\n",
      "263/281, train_loss: 0.0683, step time: 0.2575\n",
      "264/281, train_loss: 0.2099, step time: 0.2591\n",
      "265/281, train_loss: 0.2124, step time: 0.2631\n",
      "266/281, train_loss: 0.0433, step time: 0.2602\n",
      "267/281, train_loss: 0.2240, step time: 0.2541\n",
      "268/281, train_loss: 0.3115, step time: 0.2572\n",
      "269/281, train_loss: 0.2563, step time: 0.2558\n",
      "270/281, train_loss: 0.0939, step time: 0.2507\n",
      "271/281, train_loss: 0.0810, step time: 0.2508\n",
      "272/281, train_loss: 0.0722, step time: 0.2524\n",
      "273/281, train_loss: 0.0759, step time: 0.2575\n",
      "274/281, train_loss: 0.2306, step time: 0.2557\n",
      "275/281, train_loss: 0.1043, step time: 0.2512\n",
      "276/281, train_loss: 0.0895, step time: 0.2500\n",
      "277/281, train_loss: 0.0503, step time: 0.2525\n",
      "278/281, train_loss: 0.0813, step time: 0.2568\n",
      "279/281, train_loss: 0.0900, step time: 0.2529\n",
      "280/281, train_loss: 0.0899, step time: 0.2572\n",
      "281/281, train_loss: 0.1062, step time: 0.2518\n",
      "282/281, train_loss: 0.0628, step time: 0.1503\n",
      "epoch 74 average loss: 0.1065\n",
      "current epoch: 74 current mean dice: 0.8877 tc: 0.8797 wt: 0.9193 et: 0.8737\n",
      "best mean dice: 0.8878 at epoch: 68\n",
      "time consuming of epoch 74 is: 382.8449\n",
      "----------\n",
      "epoch 75/200\n",
      "1/281, train_loss: 0.2241, step time: 0.2707\n",
      "2/281, train_loss: 0.3617, step time: 0.2649\n",
      "3/281, train_loss: 0.0793, step time: 0.2620\n",
      "4/281, train_loss: 0.0818, step time: 0.2558\n",
      "5/281, train_loss: 0.0469, step time: 0.2581\n",
      "6/281, train_loss: 0.0825, step time: 0.2607\n",
      "7/281, train_loss: 0.1133, step time: 0.2562\n",
      "8/281, train_loss: 0.1694, step time: 0.2630\n",
      "9/281, train_loss: 0.0673, step time: 0.2581\n",
      "10/281, train_loss: 0.1341, step time: 0.2575\n",
      "11/281, train_loss: 0.0728, step time: 0.2608\n",
      "12/281, train_loss: 0.0622, step time: 0.2553\n",
      "13/281, train_loss: 0.2260, step time: 0.2575\n",
      "14/281, train_loss: 0.0615, step time: 0.2581\n",
      "15/281, train_loss: 0.4256, step time: 0.2526\n",
      "16/281, train_loss: 0.0747, step time: 0.2605\n",
      "17/281, train_loss: 0.0648, step time: 0.2910\n",
      "18/281, train_loss: 0.1152, step time: 0.2571\n",
      "19/281, train_loss: 0.2211, step time: 0.2609\n",
      "20/281, train_loss: 0.0675, step time: 0.2551\n",
      "21/281, train_loss: 0.0757, step time: 0.2599\n",
      "22/281, train_loss: 0.0978, step time: 0.2584\n",
      "23/281, train_loss: 0.1126, step time: 0.2544\n",
      "24/281, train_loss: 0.0820, step time: 0.2585\n",
      "25/281, train_loss: 0.0819, step time: 0.2543\n",
      "26/281, train_loss: 0.0666, step time: 0.2560\n",
      "27/281, train_loss: 0.0538, step time: 0.2570\n",
      "28/281, train_loss: 0.0785, step time: 0.2512\n",
      "29/281, train_loss: 0.2503, step time: 0.2497\n",
      "30/281, train_loss: 0.2386, step time: 0.2565\n",
      "31/281, train_loss: 0.0755, step time: 0.2544\n",
      "32/281, train_loss: 0.0917, step time: 0.2596\n",
      "33/281, train_loss: 0.0674, step time: 0.2569\n",
      "34/281, train_loss: 0.0482, step time: 0.2526\n",
      "35/281, train_loss: 0.0990, step time: 0.2537\n",
      "36/281, train_loss: 0.0691, step time: 0.2506\n",
      "37/281, train_loss: 0.2512, step time: 0.2516\n",
      "38/281, train_loss: 0.2327, step time: 0.2500\n",
      "39/281, train_loss: 0.0517, step time: 0.2550\n",
      "40/281, train_loss: 0.0806, step time: 0.2593\n",
      "41/281, train_loss: 0.0814, step time: 0.2563\n",
      "42/281, train_loss: 0.0470, step time: 0.2551\n",
      "43/281, train_loss: 0.0695, step time: 0.2588\n",
      "44/281, train_loss: 0.0848, step time: 0.2541\n",
      "45/281, train_loss: 0.0639, step time: 0.2599\n",
      "46/281, train_loss: 0.0627, step time: 0.2604\n",
      "47/281, train_loss: 0.0675, step time: 0.2571\n",
      "48/281, train_loss: 0.0686, step time: 0.2559\n",
      "49/281, train_loss: 0.0675, step time: 0.2598\n",
      "50/281, train_loss: 0.2427, step time: 0.2572\n",
      "51/281, train_loss: 0.0740, step time: 0.2518\n",
      "52/281, train_loss: 0.0736, step time: 0.2550\n",
      "53/281, train_loss: 0.0461, step time: 0.2485\n",
      "54/281, train_loss: 0.1302, step time: 0.2484\n",
      "55/281, train_loss: 0.0944, step time: 0.2487\n",
      "56/281, train_loss: 0.0994, step time: 0.2516\n",
      "57/281, train_loss: 0.1087, step time: 0.2488\n",
      "58/281, train_loss: 0.0517, step time: 0.2516\n",
      "59/281, train_loss: 0.0622, step time: 0.2482\n",
      "60/281, train_loss: 0.2421, step time: 0.2469\n",
      "61/281, train_loss: 0.0704, step time: 0.2539\n",
      "62/281, train_loss: 0.0660, step time: 0.2574\n",
      "63/281, train_loss: 0.1193, step time: 0.2550\n",
      "64/281, train_loss: 0.0707, step time: 0.2550\n",
      "65/281, train_loss: 0.1144, step time: 0.2524\n",
      "66/281, train_loss: 0.2130, step time: 0.2538\n",
      "67/281, train_loss: 0.2258, step time: 0.2513\n",
      "68/281, train_loss: 0.1029, step time: 0.2502\n",
      "69/281, train_loss: 0.0599, step time: 0.2466\n",
      "70/281, train_loss: 0.1034, step time: 0.2540\n",
      "71/281, train_loss: 0.0805, step time: 0.2521\n",
      "72/281, train_loss: 0.1020, step time: 0.2564\n",
      "73/281, train_loss: 0.1165, step time: 0.2551\n",
      "74/281, train_loss: 0.0913, step time: 0.2647\n",
      "75/281, train_loss: 0.0770, step time: 0.2539\n",
      "76/281, train_loss: 0.0542, step time: 0.2517\n",
      "77/281, train_loss: 0.0918, step time: 0.2552\n",
      "78/281, train_loss: 0.2446, step time: 0.2580\n",
      "79/281, train_loss: 0.0604, step time: 0.2551\n",
      "80/281, train_loss: 0.0893, step time: 0.2552\n",
      "81/281, train_loss: 0.0885, step time: 0.2555\n",
      "82/281, train_loss: 0.0746, step time: 0.2564\n",
      "83/281, train_loss: 0.0721, step time: 0.2592\n",
      "84/281, train_loss: 0.1198, step time: 0.2582\n",
      "85/281, train_loss: 0.0899, step time: 0.2611\n",
      "86/281, train_loss: 0.0952, step time: 0.2537\n",
      "87/281, train_loss: 0.0570, step time: 0.2533\n",
      "88/281, train_loss: 0.0769, step time: 0.2479\n",
      "89/281, train_loss: 0.0553, step time: 0.2554\n",
      "90/281, train_loss: 0.0695, step time: 0.2583\n",
      "91/281, train_loss: 0.0906, step time: 0.2573\n",
      "92/281, train_loss: 0.2343, step time: 0.2578\n",
      "93/281, train_loss: 0.0595, step time: 0.2552\n",
      "94/281, train_loss: 0.0507, step time: 0.2597\n",
      "95/281, train_loss: 0.1119, step time: 0.2594\n",
      "96/281, train_loss: 0.0818, step time: 0.2570\n",
      "97/281, train_loss: 0.0648, step time: 0.2531\n",
      "98/281, train_loss: 0.0809, step time: 0.2572\n",
      "99/281, train_loss: 0.0522, step time: 0.2575\n",
      "100/281, train_loss: 0.1052, step time: 0.2493\n",
      "101/281, train_loss: 0.0498, step time: 0.2535\n",
      "102/281, train_loss: 0.0813, step time: 0.2587\n",
      "103/281, train_loss: 0.0793, step time: 0.2596\n",
      "104/281, train_loss: 0.0717, step time: 0.2562\n",
      "105/281, train_loss: 0.0945, step time: 0.2603\n",
      "106/281, train_loss: 0.1029, step time: 0.2579\n",
      "107/281, train_loss: 0.0628, step time: 0.2525\n",
      "108/281, train_loss: 0.0931, step time: 0.2560\n",
      "109/281, train_loss: 0.0975, step time: 0.2552\n",
      "110/281, train_loss: 0.0987, step time: 0.2563\n",
      "111/281, train_loss: 0.0901, step time: 0.2519\n",
      "112/281, train_loss: 0.2295, step time: 0.2517\n",
      "113/281, train_loss: 0.0618, step time: 0.2554\n",
      "114/281, train_loss: 0.2666, step time: 0.2516\n",
      "115/281, train_loss: 0.0820, step time: 0.2508\n",
      "116/281, train_loss: 0.1004, step time: 0.2516\n",
      "117/281, train_loss: 0.0781, step time: 0.2469\n",
      "118/281, train_loss: 0.0483, step time: 0.2527\n",
      "119/281, train_loss: 0.2390, step time: 0.2516\n",
      "120/281, train_loss: 0.0711, step time: 0.2497\n",
      "121/281, train_loss: 0.1039, step time: 0.2480\n",
      "122/281, train_loss: 0.2106, step time: 0.2647\n",
      "123/281, train_loss: 0.0737, step time: 0.2551\n",
      "124/281, train_loss: 0.0954, step time: 0.2578\n",
      "125/281, train_loss: 0.0896, step time: 0.2564\n",
      "126/281, train_loss: 0.0724, step time: 0.2519\n",
      "127/281, train_loss: 0.0616, step time: 0.2579\n",
      "128/281, train_loss: 0.0601, step time: 0.2529\n",
      "129/281, train_loss: 0.0679, step time: 0.2534\n",
      "130/281, train_loss: 0.0451, step time: 0.2469\n",
      "131/281, train_loss: 0.2224, step time: 0.2531\n",
      "132/281, train_loss: 0.2531, step time: 0.2538\n",
      "133/281, train_loss: 0.0861, step time: 0.2541\n",
      "134/281, train_loss: 0.0504, step time: 0.2585\n",
      "135/281, train_loss: 0.0930, step time: 0.2602\n",
      "136/281, train_loss: 0.0688, step time: 0.2546\n",
      "137/281, train_loss: 0.0568, step time: 0.2516\n",
      "138/281, train_loss: 0.0513, step time: 0.2568\n",
      "139/281, train_loss: 0.0992, step time: 0.2525\n",
      "140/281, train_loss: 0.0808, step time: 0.2550\n",
      "141/281, train_loss: 0.0803, step time: 0.2596\n",
      "142/281, train_loss: 0.0669, step time: 0.2564\n",
      "143/281, train_loss: 0.0679, step time: 0.2558\n",
      "144/281, train_loss: 0.0710, step time: 0.2475\n",
      "145/281, train_loss: 0.0749, step time: 0.2529\n",
      "146/281, train_loss: 0.0718, step time: 0.2531\n",
      "147/281, train_loss: 0.0818, step time: 0.2540\n",
      "148/281, train_loss: 0.0841, step time: 0.2548\n",
      "149/281, train_loss: 0.2452, step time: 0.2505\n",
      "150/281, train_loss: 0.0530, step time: 0.2520\n",
      "151/281, train_loss: 0.0568, step time: 0.2581\n",
      "152/281, train_loss: 0.1324, step time: 0.2646\n",
      "153/281, train_loss: 0.0568, step time: 0.2527\n",
      "154/281, train_loss: 0.2516, step time: 0.2533\n",
      "155/281, train_loss: 0.0648, step time: 0.2497\n",
      "156/281, train_loss: 0.2496, step time: 0.2521\n",
      "157/281, train_loss: 0.0887, step time: 0.2565\n",
      "158/281, train_loss: 0.0648, step time: 0.2555\n",
      "159/281, train_loss: 0.0893, step time: 0.2574\n",
      "160/281, train_loss: 0.0958, step time: 0.2590\n",
      "161/281, train_loss: 0.2723, step time: 0.2569\n",
      "162/281, train_loss: 0.0789, step time: 0.2607\n",
      "163/281, train_loss: 0.0392, step time: 0.2596\n",
      "164/281, train_loss: 0.2216, step time: 0.2553\n",
      "165/281, train_loss: 0.0923, step time: 0.2539\n",
      "166/281, train_loss: 0.0677, step time: 0.2526\n",
      "167/281, train_loss: 0.2193, step time: 0.2520\n",
      "168/281, train_loss: 0.0681, step time: 0.2483\n",
      "169/281, train_loss: 0.1078, step time: 0.2492\n",
      "170/281, train_loss: 0.0901, step time: 0.2541\n",
      "171/281, train_loss: 0.2383, step time: 0.2552\n",
      "172/281, train_loss: 0.1069, step time: 0.2519\n",
      "173/281, train_loss: 0.0966, step time: 0.2494\n",
      "174/281, train_loss: 0.0988, step time: 0.2565\n",
      "175/281, train_loss: 0.0948, step time: 0.2479\n",
      "176/281, train_loss: 0.0573, step time: 0.2504\n",
      "177/281, train_loss: 0.0917, step time: 0.2502\n",
      "178/281, train_loss: 0.0657, step time: 0.2460\n",
      "179/281, train_loss: 0.0927, step time: 0.2570\n",
      "180/281, train_loss: 0.0841, step time: 0.2472\n",
      "181/281, train_loss: 0.0721, step time: 0.2452\n",
      "182/281, train_loss: 0.2208, step time: 0.2490\n",
      "183/281, train_loss: 0.0552, step time: 0.2524\n",
      "184/281, train_loss: 0.0590, step time: 0.2530\n",
      "185/281, train_loss: 0.2326, step time: 0.2490\n",
      "186/281, train_loss: 0.2414, step time: 0.2509\n",
      "187/281, train_loss: 0.0746, step time: 0.2559\n",
      "188/281, train_loss: 0.0617, step time: 0.2538\n",
      "189/281, train_loss: 0.0591, step time: 0.2518\n",
      "190/281, train_loss: 0.2594, step time: 0.2630\n",
      "191/281, train_loss: 0.1231, step time: 0.2522\n",
      "192/281, train_loss: 0.0627, step time: 0.2518\n",
      "193/281, train_loss: 0.0820, step time: 0.2494\n",
      "194/281, train_loss: 0.1032, step time: 0.2477\n",
      "195/281, train_loss: 0.0524, step time: 0.2511\n",
      "196/281, train_loss: 0.0517, step time: 0.2533\n",
      "197/281, train_loss: 0.0942, step time: 0.2477\n",
      "198/281, train_loss: 0.0840, step time: 0.2484\n",
      "199/281, train_loss: 0.0815, step time: 0.2502\n",
      "200/281, train_loss: 0.1092, step time: 0.2502\n",
      "201/281, train_loss: 0.1059, step time: 0.2536\n",
      "202/281, train_loss: 0.0874, step time: 0.2524\n",
      "203/281, train_loss: 0.0985, step time: 0.2489\n",
      "204/281, train_loss: 0.0889, step time: 0.2507\n",
      "205/281, train_loss: 0.1074, step time: 0.2518\n",
      "206/281, train_loss: 0.0604, step time: 0.2499\n",
      "207/281, train_loss: 0.0850, step time: 0.2513\n",
      "208/281, train_loss: 0.0892, step time: 0.2502\n",
      "209/281, train_loss: 0.0881, step time: 0.2507\n",
      "210/281, train_loss: 0.0629, step time: 0.2521\n",
      "211/281, train_loss: 0.2310, step time: 0.2448\n",
      "212/281, train_loss: 0.1101, step time: 0.2488\n",
      "213/281, train_loss: 0.1227, step time: 0.2505\n",
      "214/281, train_loss: 0.0501, step time: 0.2502\n",
      "215/281, train_loss: 0.0654, step time: 0.2516\n",
      "216/281, train_loss: 0.0661, step time: 0.2472\n",
      "217/281, train_loss: 0.0809, step time: 0.2454\n",
      "218/281, train_loss: 0.0566, step time: 0.2497\n",
      "219/281, train_loss: 0.0949, step time: 0.2482\n",
      "220/281, train_loss: 0.0668, step time: 0.2524\n",
      "221/281, train_loss: 0.1129, step time: 0.2473\n",
      "222/281, train_loss: 0.1048, step time: 0.2500\n",
      "223/281, train_loss: 0.0611, step time: 0.2726\n",
      "224/281, train_loss: 0.0566, step time: 0.2523\n",
      "225/281, train_loss: 0.2314, step time: 0.2465\n",
      "226/281, train_loss: 0.1004, step time: 0.2479\n",
      "227/281, train_loss: 0.2290, step time: 0.2478\n",
      "228/281, train_loss: 0.0802, step time: 0.2531\n",
      "229/281, train_loss: 0.1539, step time: 0.2529\n",
      "230/281, train_loss: 0.0655, step time: 0.2494\n",
      "231/281, train_loss: 0.1374, step time: 0.2496\n",
      "232/281, train_loss: 0.0495, step time: 0.2543\n",
      "233/281, train_loss: 0.0571, step time: 0.2533\n",
      "234/281, train_loss: 0.0711, step time: 0.2542\n",
      "235/281, train_loss: 0.0611, step time: 0.2493\n",
      "236/281, train_loss: 0.0483, step time: 0.2506\n",
      "237/281, train_loss: 0.0649, step time: 0.2536\n",
      "238/281, train_loss: 0.2256, step time: 0.2450\n",
      "239/281, train_loss: 0.0452, step time: 0.2497\n",
      "240/281, train_loss: 0.2628, step time: 0.2492\n",
      "241/281, train_loss: 0.2571, step time: 0.2493\n",
      "242/281, train_loss: 0.0873, step time: 0.2466\n",
      "243/281, train_loss: 0.0819, step time: 0.2562\n",
      "244/281, train_loss: 0.1071, step time: 0.2512\n",
      "245/281, train_loss: 0.0988, step time: 0.2525\n",
      "246/281, train_loss: 0.0732, step time: 0.2542\n",
      "247/281, train_loss: 0.2421, step time: 0.2521\n",
      "248/281, train_loss: 0.0953, step time: 0.2488\n",
      "249/281, train_loss: 0.0643, step time: 0.2480\n",
      "250/281, train_loss: 0.0731, step time: 0.2542\n",
      "251/281, train_loss: 0.0705, step time: 0.2485\n",
      "252/281, train_loss: 0.2708, step time: 0.2454\n",
      "253/281, train_loss: 0.0833, step time: 0.2490\n",
      "254/281, train_loss: 0.0931, step time: 0.2523\n",
      "255/281, train_loss: 0.0596, step time: 0.2531\n",
      "256/281, train_loss: 0.0821, step time: 0.2509\n",
      "257/281, train_loss: 0.0990, step time: 0.2486\n",
      "258/281, train_loss: 0.1114, step time: 0.2509\n",
      "259/281, train_loss: 0.0720, step time: 0.2481\n",
      "260/281, train_loss: 0.0764, step time: 0.2458\n",
      "261/281, train_loss: 0.0852, step time: 0.2482\n",
      "262/281, train_loss: 0.1014, step time: 0.2454\n",
      "263/281, train_loss: 0.0755, step time: 0.2445\n",
      "264/281, train_loss: 0.0778, step time: 0.2438\n",
      "265/281, train_loss: 0.0654, step time: 0.2508\n",
      "266/281, train_loss: 0.1005, step time: 0.2457\n",
      "267/281, train_loss: 0.0852, step time: 0.2487\n",
      "268/281, train_loss: 0.0628, step time: 0.2519\n",
      "269/281, train_loss: 0.0439, step time: 0.2483\n",
      "270/281, train_loss: 0.0980, step time: 0.2449\n",
      "271/281, train_loss: 0.0909, step time: 0.2521\n",
      "272/281, train_loss: 0.0888, step time: 0.2562\n",
      "273/281, train_loss: 0.0707, step time: 0.2522\n",
      "274/281, train_loss: 0.0846, step time: 0.2533\n",
      "275/281, train_loss: 0.0894, step time: 0.2448\n",
      "276/281, train_loss: 0.2629, step time: 0.2466\n",
      "277/281, train_loss: 0.1143, step time: 0.2477\n",
      "278/281, train_loss: 0.0733, step time: 0.2518\n",
      "279/281, train_loss: 0.1027, step time: 0.2486\n",
      "280/281, train_loss: 0.2411, step time: 0.2441\n",
      "281/281, train_loss: 0.1002, step time: 0.2422\n",
      "282/281, train_loss: 0.1173, step time: 0.1461\n",
      "epoch 75 average loss: 0.1053\n",
      "current epoch: 75 current mean dice: 0.8840 tc: 0.8819 wt: 0.9069 et: 0.8718\n",
      "best mean dice: 0.8878 at epoch: 68\n",
      "time consuming of epoch 75 is: 385.6317\n",
      "----------\n",
      "epoch 76/200\n",
      "1/281, train_loss: 0.0813, step time: 0.2535\n",
      "2/281, train_loss: 0.1223, step time: 0.2432\n",
      "3/281, train_loss: 0.0617, step time: 0.2421\n",
      "4/281, train_loss: 0.2195, step time: 0.2482\n",
      "5/281, train_loss: 0.1070, step time: 0.2533\n",
      "6/281, train_loss: 0.2358, step time: 0.2495\n",
      "7/281, train_loss: 0.0591, step time: 0.2472\n",
      "8/281, train_loss: 0.1061, step time: 0.2452\n",
      "9/281, train_loss: 0.0983, step time: 0.2551\n",
      "10/281, train_loss: 0.0616, step time: 0.2593\n",
      "11/281, train_loss: 0.0691, step time: 0.2606\n",
      "12/281, train_loss: 0.0591, step time: 0.2586\n",
      "13/281, train_loss: 0.0787, step time: 0.2451\n",
      "14/281, train_loss: 0.0788, step time: 0.2496\n",
      "15/281, train_loss: 0.0839, step time: 0.2509\n",
      "16/281, train_loss: 0.2631, step time: 0.2509\n",
      "17/281, train_loss: 0.2470, step time: 0.2499\n",
      "18/281, train_loss: 0.1077, step time: 0.2503\n",
      "19/281, train_loss: 0.0661, step time: 0.2512\n",
      "20/281, train_loss: 0.2207, step time: 0.2462\n",
      "21/281, train_loss: 0.0822, step time: 0.2491\n",
      "22/281, train_loss: 0.0818, step time: 0.2519\n",
      "23/281, train_loss: 0.1208, step time: 0.2483\n",
      "24/281, train_loss: 0.0861, step time: 0.2514\n",
      "25/281, train_loss: 0.1229, step time: 0.2539\n",
      "26/281, train_loss: 0.2352, step time: 0.2533\n",
      "27/281, train_loss: 0.2139, step time: 0.2491\n",
      "28/281, train_loss: 0.0772, step time: 0.2510\n",
      "29/281, train_loss: 0.0854, step time: 0.2611\n",
      "30/281, train_loss: 0.0547, step time: 0.2685\n",
      "31/281, train_loss: 0.0819, step time: 0.2541\n",
      "32/281, train_loss: 0.0850, step time: 0.2567\n",
      "33/281, train_loss: 0.2571, step time: 0.2533\n",
      "34/281, train_loss: 0.2430, step time: 0.2580\n",
      "35/281, train_loss: 0.0777, step time: 0.2557\n",
      "36/281, train_loss: 0.0509, step time: 0.2598\n",
      "37/281, train_loss: 0.0759, step time: 0.2581\n",
      "38/281, train_loss: 0.0748, step time: 0.2545\n",
      "39/281, train_loss: 0.0644, step time: 0.2468\n",
      "40/281, train_loss: 0.0768, step time: 0.2477\n",
      "41/281, train_loss: 0.0860, step time: 0.2531\n",
      "42/281, train_loss: 0.0687, step time: 0.2522\n",
      "43/281, train_loss: 0.0741, step time: 0.2456\n",
      "44/281, train_loss: 0.1023, step time: 0.2441\n",
      "45/281, train_loss: 0.0941, step time: 0.2525\n",
      "46/281, train_loss: 0.2460, step time: 0.2517\n",
      "47/281, train_loss: 0.0764, step time: 0.2468\n",
      "48/281, train_loss: 0.0489, step time: 0.2531\n",
      "49/281, train_loss: 0.0579, step time: 0.2510\n",
      "50/281, train_loss: 0.0868, step time: 0.2551\n",
      "51/281, train_loss: 0.0709, step time: 0.2560\n",
      "52/281, train_loss: 0.0574, step time: 0.2530\n",
      "53/281, train_loss: 0.1043, step time: 0.2524\n",
      "54/281, train_loss: 0.2158, step time: 0.2543\n",
      "55/281, train_loss: 0.1035, step time: 0.2574\n",
      "56/281, train_loss: 0.2332, step time: 0.2569\n",
      "57/281, train_loss: 0.0700, step time: 0.2511\n",
      "58/281, train_loss: 0.0587, step time: 0.2533\n",
      "59/281, train_loss: 0.1003, step time: 0.2568\n",
      "60/281, train_loss: 0.1136, step time: 0.2571\n",
      "61/281, train_loss: 0.2230, step time: 0.2528\n",
      "62/281, train_loss: 0.2233, step time: 0.2527\n",
      "63/281, train_loss: 0.0702, step time: 0.2535\n",
      "64/281, train_loss: 0.0622, step time: 0.2491\n",
      "65/281, train_loss: 0.0969, step time: 0.2533\n",
      "66/281, train_loss: 0.0645, step time: 0.2519\n",
      "67/281, train_loss: 0.0689, step time: 0.2566\n",
      "68/281, train_loss: 0.0840, step time: 0.2617\n",
      "69/281, train_loss: 0.0606, step time: 0.2505\n",
      "70/281, train_loss: 0.0926, step time: 0.2498\n",
      "71/281, train_loss: 0.0725, step time: 0.2516\n",
      "72/281, train_loss: 0.1145, step time: 0.2544\n",
      "73/281, train_loss: 0.0779, step time: 0.2502\n",
      "74/281, train_loss: 0.0389, step time: 0.2531\n",
      "75/281, train_loss: 0.0752, step time: 0.2556\n",
      "76/281, train_loss: 0.0859, step time: 0.2483\n",
      "77/281, train_loss: 0.0749, step time: 0.2480\n",
      "78/281, train_loss: 0.0811, step time: 0.2741\n",
      "79/281, train_loss: 0.2341, step time: 0.2538\n",
      "80/281, train_loss: 0.0745, step time: 0.2452\n",
      "81/281, train_loss: 0.0986, step time: 0.2479\n",
      "82/281, train_loss: 0.1185, step time: 0.2507\n",
      "83/281, train_loss: 0.0625, step time: 0.2498\n",
      "84/281, train_loss: 0.2238, step time: 0.2542\n",
      "85/281, train_loss: 0.0481, step time: 0.2458\n",
      "86/281, train_loss: 0.1071, step time: 0.2469\n",
      "87/281, train_loss: 0.0732, step time: 0.2498\n",
      "88/281, train_loss: 0.0982, step time: 0.2450\n",
      "89/281, train_loss: 0.1176, step time: 0.2464\n",
      "90/281, train_loss: 0.2826, step time: 0.2501\n",
      "91/281, train_loss: 0.0626, step time: 0.2530\n",
      "92/281, train_loss: 0.0770, step time: 0.2531\n",
      "93/281, train_loss: 0.0967, step time: 0.2520\n",
      "94/281, train_loss: 0.2390, step time: 0.2475\n",
      "95/281, train_loss: 0.1033, step time: 0.2516\n",
      "96/281, train_loss: 0.0803, step time: 0.2471\n",
      "97/281, train_loss: 0.0526, step time: 0.2459\n",
      "98/281, train_loss: 0.2061, step time: 0.2510\n",
      "99/281, train_loss: 0.0767, step time: 0.2556\n",
      "100/281, train_loss: 0.0881, step time: 0.2734\n",
      "101/281, train_loss: 0.1035, step time: 0.2821\n",
      "102/281, train_loss: 0.0774, step time: 0.2487\n",
      "103/281, train_loss: 0.0858, step time: 0.2482\n",
      "104/281, train_loss: 0.1069, step time: 0.2490\n",
      "105/281, train_loss: 0.0994, step time: 0.2540\n",
      "106/281, train_loss: 0.2245, step time: 0.2581\n",
      "107/281, train_loss: 0.0766, step time: 0.2548\n",
      "108/281, train_loss: 0.1140, step time: 0.2527\n",
      "109/281, train_loss: 0.2377, step time: 0.2514\n",
      "110/281, train_loss: 0.0651, step time: 0.2539\n",
      "111/281, train_loss: 0.0822, step time: 0.2539\n",
      "112/281, train_loss: 0.0564, step time: 0.2534\n",
      "113/281, train_loss: 0.0771, step time: 0.2536\n",
      "114/281, train_loss: 0.0366, step time: 0.2517\n",
      "115/281, train_loss: 0.0593, step time: 0.2482\n",
      "116/281, train_loss: 0.0818, step time: 0.2482\n",
      "117/281, train_loss: 0.0622, step time: 0.2530\n",
      "118/281, train_loss: 0.0778, step time: 0.2563\n",
      "119/281, train_loss: 0.0820, step time: 0.2600\n",
      "120/281, train_loss: 0.0842, step time: 0.2573\n",
      "121/281, train_loss: 0.2524, step time: 0.2562\n",
      "122/281, train_loss: 0.0493, step time: 0.2554\n",
      "123/281, train_loss: 0.0790, step time: 0.2563\n",
      "124/281, train_loss: 0.2702, step time: 0.2550\n",
      "125/281, train_loss: 0.0773, step time: 0.2483\n",
      "126/281, train_loss: 0.0682, step time: 0.2544\n",
      "127/281, train_loss: 0.1035, step time: 0.2514\n",
      "128/281, train_loss: 0.0327, step time: 0.2639\n",
      "129/281, train_loss: 0.0530, step time: 0.2533\n",
      "130/281, train_loss: 0.1532, step time: 0.2605\n",
      "131/281, train_loss: 0.1029, step time: 0.2597\n",
      "132/281, train_loss: 0.2129, step time: 0.2626\n",
      "133/281, train_loss: 0.0739, step time: 0.2549\n",
      "134/281, train_loss: 0.0824, step time: 0.2594\n",
      "135/281, train_loss: 0.0758, step time: 0.2525\n",
      "136/281, train_loss: 0.1073, step time: 0.2540\n",
      "137/281, train_loss: 0.2219, step time: 0.2606\n",
      "138/281, train_loss: 0.1040, step time: 0.2557\n",
      "139/281, train_loss: 0.0539, step time: 0.2575\n",
      "140/281, train_loss: 0.2379, step time: 0.2566\n",
      "141/281, train_loss: 0.0882, step time: 0.2563\n",
      "142/281, train_loss: 0.0794, step time: 0.2584\n",
      "143/281, train_loss: 0.0537, step time: 0.2552\n",
      "144/281, train_loss: 0.0753, step time: 0.2586\n",
      "145/281, train_loss: 0.1016, step time: 0.2579\n",
      "146/281, train_loss: 0.3250, step time: 0.2519\n",
      "147/281, train_loss: 0.0846, step time: 0.2542\n",
      "148/281, train_loss: 0.0915, step time: 0.2569\n",
      "149/281, train_loss: 0.0946, step time: 0.2577\n",
      "150/281, train_loss: 0.0809, step time: 0.2544\n",
      "151/281, train_loss: 0.0568, step time: 0.2507\n",
      "152/281, train_loss: 0.0918, step time: 0.2567\n",
      "153/281, train_loss: 0.2305, step time: 0.2542\n",
      "154/281, train_loss: 0.0652, step time: 0.2531\n",
      "155/281, train_loss: 0.1013, step time: 0.2556\n",
      "156/281, train_loss: 0.0829, step time: 0.2524\n",
      "157/281, train_loss: 0.2338, step time: 0.2559\n",
      "158/281, train_loss: 0.0983, step time: 0.2602\n",
      "159/281, train_loss: 0.0578, step time: 0.2525\n",
      "160/281, train_loss: 0.0836, step time: 0.2499\n",
      "161/281, train_loss: 0.0740, step time: 0.2602\n",
      "162/281, train_loss: 0.0592, step time: 0.2564\n",
      "163/281, train_loss: 0.0765, step time: 0.2572\n",
      "164/281, train_loss: 0.0665, step time: 0.2524\n",
      "165/281, train_loss: 0.0824, step time: 0.2510\n",
      "166/281, train_loss: 0.0564, step time: 0.2576\n",
      "167/281, train_loss: 0.0712, step time: 0.2572\n",
      "168/281, train_loss: 0.1212, step time: 0.2529\n",
      "169/281, train_loss: 0.0819, step time: 0.2552\n",
      "170/281, train_loss: 0.2328, step time: 0.2645\n",
      "171/281, train_loss: 0.0547, step time: 0.2516\n",
      "172/281, train_loss: 0.0437, step time: 0.2524\n",
      "173/281, train_loss: 0.2485, step time: 0.2589\n",
      "174/281, train_loss: 0.2353, step time: 0.2583\n",
      "175/281, train_loss: 0.0727, step time: 0.2558\n",
      "176/281, train_loss: 0.2431, step time: 0.2551\n",
      "177/281, train_loss: 0.0807, step time: 0.2626\n",
      "178/281, train_loss: 0.0692, step time: 0.2579\n",
      "179/281, train_loss: 0.0378, step time: 0.2585\n",
      "180/281, train_loss: 0.1384, step time: 0.2585\n",
      "181/281, train_loss: 0.0900, step time: 0.2609\n",
      "182/281, train_loss: 0.0785, step time: 0.2585\n",
      "183/281, train_loss: 0.0649, step time: 0.2586\n",
      "184/281, train_loss: 0.0856, step time: 0.2573\n",
      "185/281, train_loss: 0.1115, step time: 0.2535\n",
      "186/281, train_loss: 0.0595, step time: 0.2571\n",
      "187/281, train_loss: 0.0819, step time: 0.2567\n",
      "188/281, train_loss: 0.0599, step time: 0.2575\n",
      "189/281, train_loss: 0.0656, step time: 0.2531\n",
      "190/281, train_loss: 0.0634, step time: 0.2532\n",
      "191/281, train_loss: 0.0512, step time: 0.2583\n",
      "192/281, train_loss: 0.1243, step time: 0.2555\n",
      "193/281, train_loss: 0.2400, step time: 0.2513\n",
      "194/281, train_loss: 0.0803, step time: 0.2502\n",
      "195/281, train_loss: 0.0635, step time: 0.2557\n",
      "196/281, train_loss: 0.0887, step time: 0.2541\n",
      "197/281, train_loss: 0.2355, step time: 0.2529\n",
      "198/281, train_loss: 0.1040, step time: 0.2554\n",
      "199/281, train_loss: 0.0777, step time: 0.2589\n",
      "200/281, train_loss: 0.0852, step time: 0.2568\n",
      "201/281, train_loss: 0.2550, step time: 0.2576\n",
      "202/281, train_loss: 0.0989, step time: 0.2573\n",
      "203/281, train_loss: 0.0554, step time: 0.2538\n",
      "204/281, train_loss: 0.0712, step time: 0.2551\n",
      "205/281, train_loss: 0.2427, step time: 0.2497\n",
      "206/281, train_loss: 0.0676, step time: 0.2540\n",
      "207/281, train_loss: 0.0498, step time: 0.2526\n",
      "208/281, train_loss: 0.0840, step time: 0.2430\n",
      "209/281, train_loss: 0.0759, step time: 0.2550\n",
      "210/281, train_loss: 0.0607, step time: 0.2688\n",
      "211/281, train_loss: 0.0906, step time: 0.2431\n",
      "212/281, train_loss: 0.2048, step time: 0.2488\n",
      "213/281, train_loss: 0.0664, step time: 0.2566\n",
      "214/281, train_loss: 0.0964, step time: 0.2523\n",
      "215/281, train_loss: 0.0934, step time: 0.2552\n",
      "216/281, train_loss: 0.0567, step time: 0.2506\n",
      "217/281, train_loss: 0.0526, step time: 0.2502\n",
      "218/281, train_loss: 0.0898, step time: 0.2743\n",
      "219/281, train_loss: 0.0903, step time: 0.2559\n",
      "220/281, train_loss: 0.0801, step time: 0.2465\n",
      "221/281, train_loss: 0.0673, step time: 0.2537\n",
      "222/281, train_loss: 0.0707, step time: 0.2553\n",
      "223/281, train_loss: 0.0608, step time: 0.2565\n",
      "224/281, train_loss: 0.1205, step time: 0.2570\n",
      "225/281, train_loss: 0.0635, step time: 0.2521\n",
      "226/281, train_loss: 0.0965, step time: 0.2531\n",
      "227/281, train_loss: 0.0596, step time: 0.2584\n",
      "228/281, train_loss: 0.0738, step time: 0.2503\n",
      "229/281, train_loss: 0.0967, step time: 0.2541\n",
      "230/281, train_loss: 0.0946, step time: 0.2588\n",
      "231/281, train_loss: 0.0584, step time: 0.2530\n",
      "232/281, train_loss: 0.0858, step time: 0.2553\n",
      "233/281, train_loss: 0.0557, step time: 0.2534\n",
      "234/281, train_loss: 0.0770, step time: 0.2546\n",
      "235/281, train_loss: 0.0702, step time: 0.2467\n",
      "236/281, train_loss: 0.0946, step time: 0.2503\n",
      "237/281, train_loss: 0.0924, step time: 0.2549\n",
      "238/281, train_loss: 0.0661, step time: 0.2496\n",
      "239/281, train_loss: 0.1128, step time: 0.2530\n",
      "240/281, train_loss: 0.0471, step time: 0.2501\n",
      "241/281, train_loss: 0.0889, step time: 0.2481\n",
      "242/281, train_loss: 0.0984, step time: 0.2494\n",
      "243/281, train_loss: 0.0782, step time: 0.2529\n",
      "244/281, train_loss: 0.0949, step time: 0.2498\n",
      "245/281, train_loss: 0.0614, step time: 0.2505\n",
      "246/281, train_loss: 0.0924, step time: 0.2493\n",
      "247/281, train_loss: 0.0680, step time: 0.2487\n",
      "248/281, train_loss: 0.0569, step time: 0.2551\n",
      "249/281, train_loss: 0.0461, step time: 0.2514\n",
      "250/281, train_loss: 0.0971, step time: 0.2559\n",
      "251/281, train_loss: 0.0688, step time: 0.2571\n",
      "252/281, train_loss: 0.0874, step time: 0.2573\n",
      "253/281, train_loss: 0.0814, step time: 0.2567\n",
      "254/281, train_loss: 0.0745, step time: 0.2523\n",
      "255/281, train_loss: 0.0733, step time: 0.2551\n",
      "256/281, train_loss: 0.0689, step time: 0.2506\n",
      "257/281, train_loss: 0.0968, step time: 0.2517\n",
      "258/281, train_loss: 0.0917, step time: 0.2531\n",
      "259/281, train_loss: 0.0663, step time: 0.2528\n",
      "260/281, train_loss: 0.0524, step time: 0.2493\n",
      "261/281, train_loss: 0.0524, step time: 0.2565\n",
      "262/281, train_loss: 0.0810, step time: 0.2507\n",
      "263/281, train_loss: 0.0846, step time: 0.2519\n",
      "264/281, train_loss: 0.0731, step time: 0.2466\n",
      "265/281, train_loss: 0.2454, step time: 0.2507\n",
      "266/281, train_loss: 0.2302, step time: 0.2485\n",
      "267/281, train_loss: 0.0628, step time: 0.2662\n",
      "268/281, train_loss: 0.0876, step time: 0.2532\n",
      "269/281, train_loss: 0.2070, step time: 0.2542\n",
      "270/281, train_loss: 0.0799, step time: 0.2537\n",
      "271/281, train_loss: 0.0717, step time: 0.2560\n",
      "272/281, train_loss: 0.0618, step time: 0.2513\n",
      "273/281, train_loss: 0.0924, step time: 0.2523\n",
      "274/281, train_loss: 0.0714, step time: 0.2490\n",
      "275/281, train_loss: 0.0746, step time: 0.2562\n",
      "276/281, train_loss: 0.0813, step time: 0.2524\n",
      "277/281, train_loss: 0.0540, step time: 0.2548\n",
      "278/281, train_loss: 0.2568, step time: 0.2499\n",
      "279/281, train_loss: 0.2310, step time: 0.2555\n",
      "280/281, train_loss: 0.1048, step time: 0.2493\n",
      "281/281, train_loss: 0.0617, step time: 0.2529\n",
      "282/281, train_loss: 0.0899, step time: 0.1521\n",
      "epoch 76 average loss: 0.1032\n",
      "current epoch: 76 current mean dice: 0.8868 tc: 0.8786 wt: 0.9208 et: 0.8702\n",
      "best mean dice: 0.8878 at epoch: 68\n",
      "time consuming of epoch 76 is: 392.4967\n",
      "----------\n",
      "epoch 77/200\n",
      "1/281, train_loss: 0.1114, step time: 0.2622\n",
      "2/281, train_loss: 0.2335, step time: 0.2564\n",
      "3/281, train_loss: 0.0850, step time: 0.2564\n",
      "4/281, train_loss: 0.1166, step time: 0.2531\n",
      "5/281, train_loss: 0.0485, step time: 0.2513\n",
      "6/281, train_loss: 0.1073, step time: 0.2564\n",
      "7/281, train_loss: 0.0857, step time: 0.2553\n",
      "8/281, train_loss: 0.2242, step time: 0.2486\n",
      "9/281, train_loss: 0.2374, step time: 0.2521\n",
      "10/281, train_loss: 0.0529, step time: 0.2573\n",
      "11/281, train_loss: 0.0859, step time: 0.2596\n",
      "12/281, train_loss: 0.2157, step time: 0.2561\n",
      "13/281, train_loss: 0.0511, step time: 0.2557\n",
      "14/281, train_loss: 0.0688, step time: 0.2575\n",
      "15/281, train_loss: 0.0818, step time: 0.2577\n",
      "16/281, train_loss: 0.1633, step time: 0.2591\n",
      "17/281, train_loss: 0.0758, step time: 0.2582\n",
      "18/281, train_loss: 0.0911, step time: 0.2595\n",
      "19/281, train_loss: 0.0705, step time: 0.2641\n",
      "20/281, train_loss: 0.2296, step time: 0.2582\n",
      "21/281, train_loss: 0.0659, step time: 0.2545\n",
      "22/281, train_loss: 0.0686, step time: 0.2570\n",
      "23/281, train_loss: 0.0806, step time: 0.2613\n",
      "24/281, train_loss: 0.0763, step time: 0.2483\n",
      "25/281, train_loss: 0.0502, step time: 0.2530\n",
      "26/281, train_loss: 0.0919, step time: 0.2581\n",
      "27/281, train_loss: 0.0926, step time: 0.2513\n",
      "28/281, train_loss: 0.1050, step time: 0.2574\n",
      "29/281, train_loss: 0.0669, step time: 0.2577\n",
      "30/281, train_loss: 0.2220, step time: 0.2593\n",
      "31/281, train_loss: 0.0820, step time: 0.2591\n",
      "32/281, train_loss: 0.2280, step time: 0.2615\n",
      "33/281, train_loss: 0.0976, step time: 0.2720\n",
      "34/281, train_loss: 0.0641, step time: 0.2510\n",
      "35/281, train_loss: 0.0816, step time: 0.2518\n",
      "36/281, train_loss: 0.0655, step time: 0.2543\n",
      "37/281, train_loss: 0.0589, step time: 0.2553\n",
      "38/281, train_loss: 0.0918, step time: 0.2551\n",
      "39/281, train_loss: 0.0744, step time: 0.2540\n",
      "40/281, train_loss: 0.0958, step time: 0.2636\n",
      "41/281, train_loss: 0.1275, step time: 0.2544\n",
      "42/281, train_loss: 0.2546, step time: 0.2616\n",
      "43/281, train_loss: 0.0787, step time: 0.2554\n",
      "44/281, train_loss: 0.0661, step time: 0.2546\n",
      "45/281, train_loss: 0.0744, step time: 0.2540\n",
      "46/281, train_loss: 0.0594, step time: 0.2599\n",
      "47/281, train_loss: 0.1024, step time: 0.2559\n",
      "48/281, train_loss: 0.1381, step time: 0.2560\n",
      "49/281, train_loss: 0.0510, step time: 0.2566\n",
      "50/281, train_loss: 0.0509, step time: 0.2538\n",
      "51/281, train_loss: 0.0886, step time: 0.2566\n",
      "52/281, train_loss: 0.1249, step time: 0.2537\n",
      "53/281, train_loss: 0.0635, step time: 0.2490\n",
      "54/281, train_loss: 0.0587, step time: 0.2601\n",
      "55/281, train_loss: 0.2565, step time: 0.2580\n",
      "56/281, train_loss: 0.0607, step time: 0.2525\n",
      "57/281, train_loss: 0.0691, step time: 0.2578\n",
      "58/281, train_loss: 0.0641, step time: 0.2540\n",
      "59/281, train_loss: 0.2236, step time: 0.2552\n",
      "60/281, train_loss: 0.0593, step time: 0.2551\n",
      "61/281, train_loss: 0.0605, step time: 0.2576\n",
      "62/281, train_loss: 0.2135, step time: 0.2798\n",
      "63/281, train_loss: 0.1148, step time: 0.2583\n",
      "64/281, train_loss: 0.0722, step time: 0.2567\n",
      "65/281, train_loss: 0.1135, step time: 0.2538\n",
      "66/281, train_loss: 0.1280, step time: 0.2630\n",
      "67/281, train_loss: 0.0551, step time: 0.2565\n",
      "68/281, train_loss: 0.0622, step time: 0.2576\n",
      "69/281, train_loss: 0.0927, step time: 0.2588\n",
      "70/281, train_loss: 0.1009, step time: 0.2572\n",
      "71/281, train_loss: 0.1093, step time: 0.2557\n",
      "72/281, train_loss: 0.0829, step time: 0.2693\n",
      "73/281, train_loss: 0.0641, step time: 0.2584\n",
      "74/281, train_loss: 0.0887, step time: 0.2573\n",
      "75/281, train_loss: 0.0737, step time: 0.2522\n",
      "76/281, train_loss: 0.0854, step time: 0.2591\n",
      "77/281, train_loss: 0.1538, step time: 0.2571\n",
      "78/281, train_loss: 0.0550, step time: 0.2550\n",
      "79/281, train_loss: 0.2047, step time: 0.2566\n",
      "80/281, train_loss: 0.1065, step time: 0.2570\n",
      "81/281, train_loss: 0.0646, step time: 0.2549\n",
      "82/281, train_loss: 0.0686, step time: 0.2574\n",
      "83/281, train_loss: 0.0799, step time: 0.2575\n",
      "84/281, train_loss: 0.0759, step time: 0.2595\n",
      "85/281, train_loss: 0.0577, step time: 0.2559\n",
      "86/281, train_loss: 0.0718, step time: 0.2526\n",
      "87/281, train_loss: 0.0793, step time: 0.2562\n",
      "88/281, train_loss: 0.0883, step time: 0.2575\n",
      "89/281, train_loss: 0.0658, step time: 0.2573\n",
      "90/281, train_loss: 0.0906, step time: 0.2528\n",
      "91/281, train_loss: 0.2780, step time: 0.2557\n",
      "92/281, train_loss: 0.0777, step time: 0.2592\n",
      "93/281, train_loss: 0.0851, step time: 0.2579\n",
      "94/281, train_loss: 0.0581, step time: 0.2532\n",
      "95/281, train_loss: 0.1355, step time: 0.2545\n",
      "96/281, train_loss: 0.0805, step time: 0.2580\n",
      "97/281, train_loss: 0.2340, step time: 0.2578\n",
      "98/281, train_loss: 0.0869, step time: 0.2813\n",
      "99/281, train_loss: 0.0669, step time: 0.2705\n",
      "100/281, train_loss: 0.1006, step time: 0.2521\n",
      "101/281, train_loss: 0.0878, step time: 0.2573\n",
      "102/281, train_loss: 0.2254, step time: 0.2536\n",
      "103/281, train_loss: 0.0723, step time: 0.2560\n",
      "104/281, train_loss: 0.0823, step time: 0.2586\n",
      "105/281, train_loss: 0.4252, step time: 0.2549\n",
      "106/281, train_loss: 0.0689, step time: 0.2521\n",
      "107/281, train_loss: 0.0476, step time: 0.2509\n",
      "108/281, train_loss: 0.2333, step time: 0.2487\n",
      "109/281, train_loss: 0.0684, step time: 0.2534\n",
      "110/281, train_loss: 0.0357, step time: 0.2545\n",
      "111/281, train_loss: 0.0820, step time: 0.2549\n",
      "112/281, train_loss: 0.2611, step time: 0.2554\n",
      "113/281, train_loss: 0.0993, step time: 0.2542\n",
      "114/281, train_loss: 0.0577, step time: 0.2548\n",
      "115/281, train_loss: 0.0623, step time: 0.2531\n",
      "116/281, train_loss: 0.0729, step time: 0.2551\n",
      "117/281, train_loss: 0.0388, step time: 0.2508\n",
      "118/281, train_loss: 0.0812, step time: 0.2502\n",
      "119/281, train_loss: 0.0930, step time: 0.2539\n",
      "120/281, train_loss: 0.0709, step time: 0.2461\n",
      "121/281, train_loss: 0.0848, step time: 0.2453\n",
      "122/281, train_loss: 0.0839, step time: 0.2573\n",
      "123/281, train_loss: 0.0539, step time: 0.2550\n",
      "124/281, train_loss: 0.2157, step time: 0.2542\n",
      "125/281, train_loss: 0.0809, step time: 0.2586\n",
      "126/281, train_loss: 0.0714, step time: 0.2557\n",
      "127/281, train_loss: 0.0771, step time: 0.2507\n",
      "128/281, train_loss: 0.1238, step time: 0.2512\n",
      "129/281, train_loss: 0.0630, step time: 0.2503\n",
      "130/281, train_loss: 0.2506, step time: 0.2498\n",
      "131/281, train_loss: 0.2171, step time: 0.2498\n",
      "132/281, train_loss: 0.0572, step time: 0.2531\n",
      "133/281, train_loss: 0.0640, step time: 0.2520\n",
      "134/281, train_loss: 0.0502, step time: 0.2518\n",
      "135/281, train_loss: 0.0732, step time: 0.2532\n",
      "136/281, train_loss: 0.0819, step time: 0.2580\n",
      "137/281, train_loss: 0.0509, step time: 0.2523\n",
      "138/281, train_loss: 0.0480, step time: 0.2525\n",
      "139/281, train_loss: 0.0583, step time: 0.2501\n",
      "140/281, train_loss: 0.0896, step time: 0.2554\n",
      "141/281, train_loss: 0.0746, step time: 0.2515\n",
      "142/281, train_loss: 0.0950, step time: 0.2538\n",
      "143/281, train_loss: 0.0810, step time: 0.2562\n",
      "144/281, train_loss: 0.0502, step time: 0.2577\n",
      "145/281, train_loss: 0.0899, step time: 0.2593\n",
      "146/281, train_loss: 0.1014, step time: 0.2645\n",
      "147/281, train_loss: 0.0802, step time: 0.2567\n",
      "148/281, train_loss: 0.0872, step time: 0.2510\n",
      "149/281, train_loss: 0.0836, step time: 0.2547\n",
      "150/281, train_loss: 0.2396, step time: 0.2594\n",
      "151/281, train_loss: 0.0974, step time: 0.2542\n",
      "152/281, train_loss: 0.1590, step time: 0.2491\n",
      "153/281, train_loss: 0.0857, step time: 0.2549\n",
      "154/281, train_loss: 0.2721, step time: 0.2523\n",
      "155/281, train_loss: 0.1032, step time: 0.2473\n",
      "156/281, train_loss: 0.2508, step time: 0.2517\n",
      "157/281, train_loss: 0.0411, step time: 0.2557\n",
      "158/281, train_loss: 0.0500, step time: 0.2550\n",
      "159/281, train_loss: 0.0708, step time: 0.2562\n",
      "160/281, train_loss: 0.0794, step time: 0.2536\n",
      "161/281, train_loss: 0.0624, step time: 0.2568\n",
      "162/281, train_loss: 0.0551, step time: 0.2571\n",
      "163/281, train_loss: 0.0580, step time: 0.2557\n",
      "164/281, train_loss: 0.0785, step time: 0.2556\n",
      "165/281, train_loss: 0.2431, step time: 0.2528\n",
      "166/281, train_loss: 0.0848, step time: 0.2549\n",
      "167/281, train_loss: 0.0702, step time: 0.2517\n",
      "168/281, train_loss: 0.0998, step time: 0.2577\n",
      "169/281, train_loss: 0.0789, step time: 0.2525\n",
      "170/281, train_loss: 0.0748, step time: 0.2458\n",
      "171/281, train_loss: 0.0774, step time: 0.2484\n",
      "172/281, train_loss: 0.2169, step time: 0.2558\n",
      "173/281, train_loss: 0.0668, step time: 0.2555\n",
      "174/281, train_loss: 0.2512, step time: 0.2541\n",
      "175/281, train_loss: 0.0614, step time: 0.2534\n",
      "176/281, train_loss: 0.0897, step time: 0.2571\n",
      "177/281, train_loss: 0.0897, step time: 0.2740\n",
      "178/281, train_loss: 0.0777, step time: 0.2751\n",
      "179/281, train_loss: 0.0720, step time: 0.2549\n",
      "180/281, train_loss: 0.0872, step time: 0.2552\n",
      "181/281, train_loss: 0.0859, step time: 0.2593\n",
      "182/281, train_loss: 0.0650, step time: 0.2566\n",
      "183/281, train_loss: 0.0518, step time: 0.2548\n",
      "184/281, train_loss: 0.0638, step time: 0.2503\n",
      "185/281, train_loss: 0.0833, step time: 0.2552\n",
      "186/281, train_loss: 0.0767, step time: 0.2505\n",
      "187/281, train_loss: 0.1181, step time: 0.2497\n",
      "188/281, train_loss: 0.0552, step time: 0.2545\n",
      "189/281, train_loss: 0.0961, step time: 0.2503\n",
      "190/281, train_loss: 0.0963, step time: 0.2491\n",
      "191/281, train_loss: 0.0860, step time: 0.2519\n",
      "192/281, train_loss: 0.1116, step time: 0.2551\n",
      "193/281, train_loss: 0.0731, step time: 0.2559\n",
      "194/281, train_loss: 0.0676, step time: 0.2530\n",
      "195/281, train_loss: 0.0529, step time: 0.2571\n",
      "196/281, train_loss: 0.2318, step time: 0.2507\n",
      "197/281, train_loss: 0.0448, step time: 0.2525\n",
      "198/281, train_loss: 0.2137, step time: 0.2526\n",
      "199/281, train_loss: 0.0448, step time: 0.2503\n",
      "200/281, train_loss: 0.0608, step time: 0.2558\n",
      "201/281, train_loss: 0.1022, step time: 0.2545\n",
      "202/281, train_loss: 0.0848, step time: 0.2562\n",
      "203/281, train_loss: 0.0578, step time: 0.2534\n",
      "204/281, train_loss: 0.0728, step time: 0.2537\n",
      "205/281, train_loss: 0.0641, step time: 0.2543\n",
      "206/281, train_loss: 0.0342, step time: 0.2554\n",
      "207/281, train_loss: 0.0646, step time: 0.2557\n",
      "208/281, train_loss: 0.1057, step time: 0.2579\n",
      "209/281, train_loss: 0.2751, step time: 0.2566\n",
      "210/281, train_loss: 0.0962, step time: 0.2547\n",
      "211/281, train_loss: 0.1962, step time: 0.2575\n",
      "212/281, train_loss: 0.0622, step time: 0.2574\n",
      "213/281, train_loss: 0.0805, step time: 0.2545\n",
      "214/281, train_loss: 0.0655, step time: 0.2566\n",
      "215/281, train_loss: 0.0697, step time: 0.2534\n",
      "216/281, train_loss: 0.2397, step time: 0.2526\n",
      "217/281, train_loss: 0.0657, step time: 0.2534\n",
      "218/281, train_loss: 0.2543, step time: 0.2475\n",
      "219/281, train_loss: 0.0436, step time: 0.2543\n",
      "220/281, train_loss: 0.0600, step time: 0.2512\n",
      "221/281, train_loss: 0.0771, step time: 0.2482\n",
      "222/281, train_loss: 0.1389, step time: 0.2533\n",
      "223/281, train_loss: 0.0650, step time: 0.2595\n",
      "224/281, train_loss: 0.0809, step time: 0.2501\n",
      "225/281, train_loss: 0.0600, step time: 0.2506\n",
      "226/281, train_loss: 0.0807, step time: 0.2508\n",
      "227/281, train_loss: 0.0894, step time: 0.2484\n",
      "228/281, train_loss: 0.0830, step time: 0.2492\n",
      "229/281, train_loss: 0.0882, step time: 0.2463\n",
      "230/281, train_loss: 0.0664, step time: 0.2479\n",
      "231/281, train_loss: 0.0713, step time: 0.2513\n",
      "232/281, train_loss: 0.0583, step time: 0.2518\n",
      "233/281, train_loss: 0.0551, step time: 0.2560\n",
      "234/281, train_loss: 0.0686, step time: 0.2486\n",
      "235/281, train_loss: 0.0808, step time: 0.2522\n",
      "236/281, train_loss: 0.0873, step time: 0.2490\n",
      "237/281, train_loss: 0.0792, step time: 0.2488\n",
      "238/281, train_loss: 0.2641, step time: 0.2523\n",
      "239/281, train_loss: 0.0816, step time: 0.2521\n",
      "240/281, train_loss: 0.0861, step time: 0.2487\n",
      "241/281, train_loss: 0.0566, step time: 0.2506\n",
      "242/281, train_loss: 0.0726, step time: 0.2531\n",
      "243/281, train_loss: 0.0362, step time: 0.2512\n",
      "244/281, train_loss: 0.0393, step time: 0.2476\n",
      "245/281, train_loss: 0.0892, step time: 0.2536\n",
      "246/281, train_loss: 0.2292, step time: 0.2540\n",
      "247/281, train_loss: 0.1127, step time: 0.2532\n",
      "248/281, train_loss: 0.0808, step time: 0.2549\n",
      "249/281, train_loss: 0.0665, step time: 0.2523\n",
      "250/281, train_loss: 0.0644, step time: 0.2574\n",
      "251/281, train_loss: 0.0620, step time: 0.2543\n",
      "252/281, train_loss: 0.0487, step time: 0.2527\n",
      "253/281, train_loss: 0.0946, step time: 0.2499\n",
      "254/281, train_loss: 0.0926, step time: 0.2522\n",
      "255/281, train_loss: 0.0681, step time: 0.2518\n",
      "256/281, train_loss: 0.2086, step time: 0.2528\n",
      "257/281, train_loss: 0.2147, step time: 0.2536\n",
      "258/281, train_loss: 0.0398, step time: 0.2513\n",
      "259/281, train_loss: 0.0671, step time: 0.2541\n",
      "260/281, train_loss: 0.0874, step time: 0.2522\n",
      "261/281, train_loss: 0.0546, step time: 0.2540\n",
      "262/281, train_loss: 0.0552, step time: 0.2515\n",
      "263/281, train_loss: 0.0784, step time: 0.2499\n",
      "264/281, train_loss: 0.0968, step time: 0.2489\n",
      "265/281, train_loss: 0.0514, step time: 0.2490\n",
      "266/281, train_loss: 0.2244, step time: 0.2478\n",
      "267/281, train_loss: 0.0884, step time: 0.2458\n",
      "268/281, train_loss: 0.0594, step time: 0.2533\n",
      "269/281, train_loss: 0.0811, step time: 0.2550\n",
      "270/281, train_loss: 0.3916, step time: 0.2569\n",
      "271/281, train_loss: 0.0892, step time: 0.2563\n",
      "272/281, train_loss: 0.0845, step time: 0.2490\n",
      "273/281, train_loss: 0.0400, step time: 0.2483\n",
      "274/281, train_loss: 0.0679, step time: 0.2501\n",
      "275/281, train_loss: 0.0714, step time: 0.2524\n",
      "276/281, train_loss: 0.0730, step time: 0.2541\n",
      "277/281, train_loss: 0.2167, step time: 0.2494\n",
      "278/281, train_loss: 0.0648, step time: 0.2512\n",
      "279/281, train_loss: 0.0823, step time: 0.2572\n",
      "280/281, train_loss: 0.2296, step time: 0.2577\n",
      "281/281, train_loss: 0.0616, step time: 0.2479\n",
      "282/281, train_loss: 0.3887, step time: 0.1506\n",
      "epoch 77 average loss: 0.1022\n",
      "saved new best metric model\n",
      "current epoch: 77 current mean dice: 0.8904 tc: 0.8842 wt: 0.9178 et: 0.8786\n",
      "best mean dice: 0.8904 at epoch: 77\n",
      "time consuming of epoch 77 is: 370.8480\n",
      "----------\n",
      "epoch 78/200\n",
      "1/281, train_loss: 0.0563, step time: 0.2541\n",
      "2/281, train_loss: 0.0633, step time: 0.2459\n",
      "3/281, train_loss: 0.2401, step time: 0.2457\n",
      "4/281, train_loss: 0.1011, step time: 0.2498\n",
      "5/281, train_loss: 0.0507, step time: 0.2506\n",
      "6/281, train_loss: 0.0987, step time: 0.2611\n",
      "7/281, train_loss: 0.0610, step time: 0.2583\n",
      "8/281, train_loss: 0.1043, step time: 0.2507\n",
      "9/281, train_loss: 0.0586, step time: 0.2475\n",
      "10/281, train_loss: 0.2123, step time: 0.2488\n",
      "11/281, train_loss: 0.0718, step time: 0.2563\n",
      "12/281, train_loss: 0.2158, step time: 0.2502\n",
      "13/281, train_loss: 0.0700, step time: 0.2500\n",
      "14/281, train_loss: 0.0975, step time: 0.2519\n",
      "15/281, train_loss: 0.1065, step time: 0.2524\n",
      "16/281, train_loss: 0.0593, step time: 0.2512\n",
      "17/281, train_loss: 0.1218, step time: 0.2508\n",
      "18/281, train_loss: 0.2199, step time: 0.2534\n",
      "19/281, train_loss: 0.2326, step time: 0.2510\n",
      "20/281, train_loss: 0.2251, step time: 0.2456\n",
      "21/281, train_loss: 0.0735, step time: 0.2433\n",
      "22/281, train_loss: 0.0728, step time: 0.2539\n",
      "23/281, train_loss: 0.0992, step time: 0.2512\n",
      "24/281, train_loss: 0.2753, step time: 0.2519\n",
      "25/281, train_loss: 0.0464, step time: 0.2501\n",
      "26/281, train_loss: 0.0806, step time: 0.2501\n",
      "27/281, train_loss: 0.2229, step time: 0.2480\n",
      "28/281, train_loss: 0.2376, step time: 0.2487\n",
      "29/281, train_loss: 0.0624, step time: 0.2476\n",
      "30/281, train_loss: 0.0910, step time: 0.2539\n",
      "31/281, train_loss: 0.0659, step time: 0.2521\n",
      "32/281, train_loss: 0.0634, step time: 0.2496\n",
      "33/281, train_loss: 0.0895, step time: 0.2496\n",
      "34/281, train_loss: 0.0708, step time: 0.2478\n",
      "35/281, train_loss: 0.0572, step time: 0.2494\n",
      "36/281, train_loss: 0.0707, step time: 0.2535\n",
      "37/281, train_loss: 0.0714, step time: 0.2481\n",
      "38/281, train_loss: 0.0969, step time: 0.2494\n",
      "39/281, train_loss: 0.2555, step time: 0.2511\n",
      "40/281, train_loss: 0.0816, step time: 0.2460\n",
      "41/281, train_loss: 0.0876, step time: 0.2458\n",
      "42/281, train_loss: 0.0480, step time: 0.2479\n",
      "43/281, train_loss: 0.2200, step time: 0.2516\n",
      "44/281, train_loss: 0.0855, step time: 0.2526\n",
      "45/281, train_loss: 0.0487, step time: 0.2436\n",
      "46/281, train_loss: 0.0688, step time: 0.2448\n",
      "47/281, train_loss: 0.0899, step time: 0.2547\n",
      "48/281, train_loss: 0.0495, step time: 0.2542\n",
      "49/281, train_loss: 0.0579, step time: 0.2482\n",
      "50/281, train_loss: 0.0745, step time: 0.2424\n",
      "51/281, train_loss: 0.0554, step time: 0.2508\n",
      "52/281, train_loss: 0.2314, step time: 0.2524\n",
      "53/281, train_loss: 0.0534, step time: 0.2549\n",
      "54/281, train_loss: 0.0564, step time: 0.2511\n",
      "55/281, train_loss: 0.0572, step time: 0.2534\n",
      "56/281, train_loss: 0.1169, step time: 0.2570\n",
      "57/281, train_loss: 0.0500, step time: 0.2544\n",
      "58/281, train_loss: 0.1076, step time: 0.2543\n",
      "59/281, train_loss: 0.0413, step time: 0.2489\n",
      "60/281, train_loss: 0.0848, step time: 0.2486\n",
      "61/281, train_loss: 0.0611, step time: 0.2522\n",
      "62/281, train_loss: 0.0881, step time: 0.2582\n",
      "63/281, train_loss: 0.2175, step time: 0.2674\n",
      "64/281, train_loss: 0.2239, step time: 0.2564\n",
      "65/281, train_loss: 0.0675, step time: 0.2523\n",
      "66/281, train_loss: 0.1032, step time: 0.2533\n",
      "67/281, train_loss: 0.0606, step time: 0.2476\n",
      "68/281, train_loss: 0.0437, step time: 0.2493\n",
      "69/281, train_loss: 0.0670, step time: 0.2511\n",
      "70/281, train_loss: 0.2599, step time: 0.2513\n",
      "71/281, train_loss: 0.2384, step time: 0.2465\n",
      "72/281, train_loss: 0.0466, step time: 0.2440\n",
      "73/281, train_loss: 0.0927, step time: 0.2468\n",
      "74/281, train_loss: 0.0902, step time: 0.2518\n",
      "75/281, train_loss: 0.1088, step time: 0.2509\n",
      "76/281, train_loss: 0.0684, step time: 0.2528\n",
      "77/281, train_loss: 0.0437, step time: 0.2530\n",
      "78/281, train_loss: 0.0472, step time: 0.2519\n",
      "79/281, train_loss: 0.0720, step time: 0.2518\n",
      "80/281, train_loss: 0.0723, step time: 0.2534\n",
      "81/281, train_loss: 0.0589, step time: 0.2485\n",
      "82/281, train_loss: 0.0704, step time: 0.2579\n",
      "83/281, train_loss: 0.0658, step time: 0.2546\n",
      "84/281, train_loss: 0.0750, step time: 0.2552\n",
      "85/281, train_loss: 0.0818, step time: 0.2524\n",
      "86/281, train_loss: 0.0441, step time: 0.2491\n",
      "87/281, train_loss: 0.0798, step time: 0.2529\n",
      "88/281, train_loss: 0.0597, step time: 0.2517\n",
      "89/281, train_loss: 0.0905, step time: 0.2544\n",
      "90/281, train_loss: 0.1105, step time: 0.2495\n",
      "91/281, train_loss: 0.0665, step time: 0.2491\n",
      "92/281, train_loss: 0.0887, step time: 0.2468\n",
      "93/281, train_loss: 0.0897, step time: 0.2472\n",
      "94/281, train_loss: 0.2315, step time: 0.2564\n",
      "95/281, train_loss: 0.0832, step time: 0.2484\n",
      "96/281, train_loss: 0.0831, step time: 0.2455\n",
      "97/281, train_loss: 0.2203, step time: 0.2433\n",
      "98/281, train_loss: 0.0580, step time: 0.2503\n",
      "99/281, train_loss: 0.0330, step time: 0.2527\n",
      "100/281, train_loss: 0.2308, step time: 0.2506\n",
      "101/281, train_loss: 0.0997, step time: 0.2515\n",
      "102/281, train_loss: 0.2260, step time: 0.2499\n",
      "103/281, train_loss: 0.0472, step time: 0.2513\n",
      "104/281, train_loss: 0.0636, step time: 0.2525\n",
      "105/281, train_loss: 0.0812, step time: 0.2541\n",
      "106/281, train_loss: 0.0525, step time: 0.2544\n",
      "107/281, train_loss: 0.0792, step time: 0.2522\n",
      "108/281, train_loss: 0.0564, step time: 0.2506\n",
      "109/281, train_loss: 0.0695, step time: 0.2470\n",
      "110/281, train_loss: 0.0677, step time: 0.2666\n",
      "111/281, train_loss: 0.0632, step time: 0.2531\n",
      "112/281, train_loss: 0.0965, step time: 0.2546\n",
      "113/281, train_loss: 0.0502, step time: 0.2541\n",
      "114/281, train_loss: 0.0698, step time: 0.2569\n",
      "115/281, train_loss: 0.0734, step time: 0.2564\n",
      "116/281, train_loss: 0.1172, step time: 0.2561\n",
      "117/281, train_loss: 0.0806, step time: 0.2492\n",
      "118/281, train_loss: 0.0500, step time: 0.2480\n",
      "119/281, train_loss: 0.0620, step time: 0.2495\n",
      "120/281, train_loss: 0.2209, step time: 0.2432\n",
      "121/281, train_loss: 0.0680, step time: 0.2425\n",
      "122/281, train_loss: 0.0977, step time: 0.2452\n",
      "123/281, train_loss: 0.0881, step time: 0.2512\n",
      "124/281, train_loss: 0.0913, step time: 0.2436\n",
      "125/281, train_loss: 0.0609, step time: 0.2439\n",
      "126/281, train_loss: 0.2244, step time: 0.2442\n",
      "127/281, train_loss: 0.2470, step time: 0.2672\n",
      "128/281, train_loss: 0.0987, step time: 0.2669\n",
      "129/281, train_loss: 0.0422, step time: 0.2504\n",
      "130/281, train_loss: 0.2501, step time: 0.2563\n",
      "131/281, train_loss: 0.0758, step time: 0.2532\n",
      "132/281, train_loss: 0.0612, step time: 0.2489\n",
      "133/281, train_loss: 0.0892, step time: 0.2518\n",
      "134/281, train_loss: 0.2332, step time: 0.2540\n",
      "135/281, train_loss: 0.0963, step time: 0.2519\n",
      "136/281, train_loss: 0.2542, step time: 0.2490\n",
      "137/281, train_loss: 0.0525, step time: 0.2478\n",
      "138/281, train_loss: 0.0573, step time: 0.2526\n",
      "139/281, train_loss: 0.1002, step time: 0.2571\n",
      "140/281, train_loss: 0.2306, step time: 0.2512\n",
      "141/281, train_loss: 0.0708, step time: 0.2551\n",
      "142/281, train_loss: 0.0591, step time: 0.2519\n",
      "143/281, train_loss: 0.1105, step time: 0.2545\n",
      "144/281, train_loss: 0.0965, step time: 0.2464\n",
      "145/281, train_loss: 0.1102, step time: 0.2502\n",
      "146/281, train_loss: 0.1029, step time: 0.2533\n",
      "147/281, train_loss: 0.0722, step time: 0.2492\n",
      "148/281, train_loss: 0.0738, step time: 0.2507\n",
      "149/281, train_loss: 0.0538, step time: 0.2562\n",
      "150/281, train_loss: 0.0751, step time: 0.2535\n",
      "151/281, train_loss: 0.0654, step time: 0.2581\n",
      "152/281, train_loss: 0.2548, step time: 0.2602\n",
      "153/281, train_loss: 0.1165, step time: 0.2532\n",
      "154/281, train_loss: 0.0932, step time: 0.2569\n",
      "155/281, train_loss: 0.0763, step time: 0.2520\n",
      "156/281, train_loss: 0.0708, step time: 0.2499\n",
      "157/281, train_loss: 0.0700, step time: 0.2539\n",
      "158/281, train_loss: 0.0484, step time: 0.2598\n",
      "159/281, train_loss: 0.0547, step time: 0.2650\n",
      "160/281, train_loss: 0.2436, step time: 0.2571\n",
      "161/281, train_loss: 0.1098, step time: 0.2518\n",
      "162/281, train_loss: 0.0952, step time: 0.2614\n",
      "163/281, train_loss: 0.0964, step time: 0.2534\n",
      "164/281, train_loss: 0.1083, step time: 0.2499\n",
      "165/281, train_loss: 0.0803, step time: 0.2514\n",
      "166/281, train_loss: 0.0859, step time: 0.2544\n",
      "167/281, train_loss: 0.0544, step time: 0.2531\n",
      "168/281, train_loss: 0.0977, step time: 0.2623\n",
      "169/281, train_loss: 0.1460, step time: 0.2542\n",
      "170/281, train_loss: 0.1432, step time: 0.2560\n",
      "171/281, train_loss: 0.0398, step time: 0.2523\n",
      "172/281, train_loss: 0.0769, step time: 0.2582\n",
      "173/281, train_loss: 0.0550, step time: 0.2594\n",
      "174/281, train_loss: 0.1280, step time: 0.2523\n",
      "175/281, train_loss: 0.0914, step time: 0.2534\n",
      "176/281, train_loss: 0.0549, step time: 0.2515\n",
      "177/281, train_loss: 0.0561, step time: 0.2556\n",
      "178/281, train_loss: 0.0390, step time: 0.2588\n",
      "179/281, train_loss: 0.0595, step time: 0.2538\n",
      "180/281, train_loss: 0.0795, step time: 0.2570\n",
      "181/281, train_loss: 0.0945, step time: 0.2609\n",
      "182/281, train_loss: 0.2519, step time: 0.2567\n",
      "183/281, train_loss: 0.0721, step time: 0.2565\n",
      "184/281, train_loss: 0.0537, step time: 0.2538\n",
      "185/281, train_loss: 0.0767, step time: 0.2598\n",
      "186/281, train_loss: 0.0938, step time: 0.2544\n",
      "187/281, train_loss: 0.0866, step time: 0.2532\n",
      "188/281, train_loss: 0.0754, step time: 0.2603\n",
      "189/281, train_loss: 0.0737, step time: 0.2558\n",
      "190/281, train_loss: 0.1154, step time: 0.2586\n",
      "191/281, train_loss: 0.0622, step time: 0.2579\n",
      "192/281, train_loss: 0.2676, step time: 0.2533\n",
      "193/281, train_loss: 0.2464, step time: 0.2586\n",
      "194/281, train_loss: 0.2677, step time: 0.2579\n",
      "195/281, train_loss: 0.0622, step time: 0.2560\n",
      "196/281, train_loss: 0.2851, step time: 0.2580\n",
      "197/281, train_loss: 0.2423, step time: 0.2611\n",
      "198/281, train_loss: 0.0625, step time: 0.2589\n",
      "199/281, train_loss: 0.0966, step time: 0.2560\n",
      "200/281, train_loss: 0.0804, step time: 0.2546\n",
      "201/281, train_loss: 0.1105, step time: 0.2558\n",
      "202/281, train_loss: 0.0660, step time: 0.2569\n",
      "203/281, train_loss: 0.0694, step time: 0.2599\n",
      "204/281, train_loss: 0.0699, step time: 0.2594\n",
      "205/281, train_loss: 0.1157, step time: 0.2557\n",
      "206/281, train_loss: 0.1433, step time: 0.2528\n",
      "207/281, train_loss: 0.1077, step time: 0.2557\n",
      "208/281, train_loss: 0.0989, step time: 0.2507\n",
      "209/281, train_loss: 0.1085, step time: 0.2535\n",
      "210/281, train_loss: 0.2830, step time: 0.2544\n",
      "211/281, train_loss: 0.0708, step time: 0.2562\n",
      "212/281, train_loss: 0.0696, step time: 0.2546\n",
      "213/281, train_loss: 0.1024, step time: 0.2529\n",
      "214/281, train_loss: 0.0833, step time: 0.2563\n",
      "215/281, train_loss: 0.0540, step time: 0.2581\n",
      "216/281, train_loss: 0.0703, step time: 0.2577\n",
      "217/281, train_loss: 0.0735, step time: 0.2525\n",
      "218/281, train_loss: 0.0547, step time: 0.2593\n",
      "219/281, train_loss: 0.0525, step time: 0.2579\n",
      "220/281, train_loss: 0.0676, step time: 0.2551\n",
      "221/281, train_loss: 0.1084, step time: 0.2532\n",
      "222/281, train_loss: 0.0696, step time: 0.2542\n",
      "223/281, train_loss: 0.1021, step time: 0.2530\n",
      "224/281, train_loss: 0.0515, step time: 0.2581\n",
      "225/281, train_loss: 0.2379, step time: 0.2558\n",
      "226/281, train_loss: 0.0666, step time: 0.2499\n",
      "227/281, train_loss: 0.2275, step time: 0.2527\n",
      "228/281, train_loss: 0.0633, step time: 0.2589\n",
      "229/281, train_loss: 0.0935, step time: 0.2566\n",
      "230/281, train_loss: 0.2183, step time: 0.2531\n",
      "231/281, train_loss: 0.0527, step time: 0.2536\n",
      "232/281, train_loss: 0.0759, step time: 0.2561\n",
      "233/281, train_loss: 0.0945, step time: 0.2575\n",
      "234/281, train_loss: 0.0607, step time: 0.2509\n",
      "235/281, train_loss: 0.0952, step time: 0.2530\n",
      "236/281, train_loss: 0.0772, step time: 0.2531\n",
      "237/281, train_loss: 0.0877, step time: 0.2522\n",
      "238/281, train_loss: 0.0949, step time: 0.2506\n",
      "239/281, train_loss: 0.0698, step time: 0.2520\n",
      "240/281, train_loss: 0.0706, step time: 0.2515\n",
      "241/281, train_loss: 0.0567, step time: 0.2514\n",
      "242/281, train_loss: 0.0712, step time: 0.2520\n",
      "243/281, train_loss: 0.1057, step time: 0.2497\n",
      "244/281, train_loss: 0.0945, step time: 0.2501\n",
      "245/281, train_loss: 0.0613, step time: 0.2515\n",
      "246/281, train_loss: 0.1065, step time: 0.2506\n",
      "247/281, train_loss: 0.1143, step time: 0.2482\n",
      "248/281, train_loss: 0.0687, step time: 0.2589\n",
      "249/281, train_loss: 0.0595, step time: 0.2498\n",
      "250/281, train_loss: 0.0734, step time: 0.2531\n",
      "251/281, train_loss: 0.0566, step time: 0.2587\n",
      "252/281, train_loss: 0.1101, step time: 0.2548\n",
      "253/281, train_loss: 0.0956, step time: 0.2562\n",
      "254/281, train_loss: 0.0612, step time: 0.2744\n",
      "255/281, train_loss: 0.0637, step time: 0.2569\n",
      "256/281, train_loss: 0.0631, step time: 0.2541\n",
      "257/281, train_loss: 0.0855, step time: 0.2529\n",
      "258/281, train_loss: 0.0719, step time: 0.2506\n",
      "259/281, train_loss: 0.0455, step time: 0.2502\n",
      "260/281, train_loss: 0.0764, step time: 0.2556\n",
      "261/281, train_loss: 0.0594, step time: 0.2536\n",
      "262/281, train_loss: 0.0824, step time: 0.2541\n",
      "263/281, train_loss: 0.0463, step time: 0.2582\n",
      "264/281, train_loss: 0.0716, step time: 0.2548\n",
      "265/281, train_loss: 0.0933, step time: 0.2526\n",
      "266/281, train_loss: 0.0737, step time: 0.2602\n",
      "267/281, train_loss: 0.0508, step time: 0.2735\n",
      "268/281, train_loss: 0.1070, step time: 0.2623\n",
      "269/281, train_loss: 0.2419, step time: 0.2572\n",
      "270/281, train_loss: 0.1226, step time: 0.2530\n",
      "271/281, train_loss: 0.0595, step time: 0.2576\n",
      "272/281, train_loss: 0.0720, step time: 0.2568\n",
      "273/281, train_loss: 0.1179, step time: 0.2574\n",
      "274/281, train_loss: 0.0782, step time: 0.2597\n",
      "275/281, train_loss: 0.2215, step time: 0.2550\n",
      "276/281, train_loss: 0.0796, step time: 0.2596\n",
      "277/281, train_loss: 0.0521, step time: 0.2569\n",
      "278/281, train_loss: 0.2591, step time: 0.2553\n",
      "279/281, train_loss: 0.0922, step time: 0.2521\n",
      "280/281, train_loss: 0.0872, step time: 0.2570\n",
      "281/281, train_loss: 0.2795, step time: 0.2511\n",
      "282/281, train_loss: 0.1101, step time: 0.1486\n",
      "epoch 78 average loss: 0.1019\n",
      "current epoch: 78 current mean dice: 0.8890 tc: 0.8827 wt: 0.9191 et: 0.8749\n",
      "best mean dice: 0.8904 at epoch: 77\n",
      "time consuming of epoch 78 is: 396.1189\n",
      "----------\n",
      "epoch 79/200\n",
      "1/281, train_loss: 0.0512, step time: 0.2637\n",
      "2/281, train_loss: 0.0632, step time: 0.2586\n",
      "3/281, train_loss: 0.0718, step time: 0.2664\n",
      "4/281, train_loss: 0.0501, step time: 0.2664\n",
      "5/281, train_loss: 0.0579, step time: 0.2631\n",
      "6/281, train_loss: 0.0695, step time: 0.2575\n",
      "7/281, train_loss: 0.2175, step time: 0.2606\n",
      "8/281, train_loss: 0.0639, step time: 0.2703\n",
      "9/281, train_loss: 0.0794, step time: 0.2623\n",
      "10/281, train_loss: 0.0893, step time: 0.2599\n",
      "11/281, train_loss: 0.0786, step time: 0.2572\n",
      "12/281, train_loss: 0.0684, step time: 0.2630\n",
      "13/281, train_loss: 0.0757, step time: 0.2559\n",
      "14/281, train_loss: 0.0524, step time: 0.2571\n",
      "15/281, train_loss: 0.0683, step time: 0.2681\n",
      "16/281, train_loss: 0.0442, step time: 0.2654\n",
      "17/281, train_loss: 0.0726, step time: 0.2643\n",
      "18/281, train_loss: 0.0909, step time: 0.2661\n",
      "19/281, train_loss: 0.0873, step time: 0.2649\n",
      "20/281, train_loss: 0.2171, step time: 0.2585\n",
      "21/281, train_loss: 0.0699, step time: 0.2657\n",
      "22/281, train_loss: 0.0785, step time: 0.2652\n",
      "23/281, train_loss: 0.1085, step time: 0.2592\n",
      "24/281, train_loss: 0.2375, step time: 0.2594\n",
      "25/281, train_loss: 0.0371, step time: 0.2680\n",
      "26/281, train_loss: 0.0709, step time: 0.2543\n",
      "27/281, train_loss: 0.0558, step time: 0.2656\n",
      "28/281, train_loss: 0.2235, step time: 0.2800\n",
      "29/281, train_loss: 0.0775, step time: 0.2596\n",
      "30/281, train_loss: 0.0553, step time: 0.2702\n",
      "31/281, train_loss: 0.2740, step time: 0.2604\n",
      "32/281, train_loss: 0.0433, step time: 0.2569\n",
      "33/281, train_loss: 0.0531, step time: 0.2590\n",
      "34/281, train_loss: 0.0984, step time: 0.2567\n",
      "35/281, train_loss: 0.2375, step time: 0.2553\n",
      "36/281, train_loss: 0.0566, step time: 0.2543\n",
      "37/281, train_loss: 0.0489, step time: 0.2644\n",
      "38/281, train_loss: 0.0717, step time: 0.2584\n",
      "39/281, train_loss: 0.0760, step time: 0.2570\n",
      "40/281, train_loss: 0.0884, step time: 0.2503\n",
      "41/281, train_loss: 0.0894, step time: 0.2553\n",
      "42/281, train_loss: 0.0508, step time: 0.2533\n",
      "43/281, train_loss: 0.0474, step time: 0.2657\n",
      "44/281, train_loss: 0.0616, step time: 0.2621\n",
      "45/281, train_loss: 0.2787, step time: 0.2598\n",
      "46/281, train_loss: 0.2641, step time: 0.2598\n",
      "47/281, train_loss: 0.0658, step time: 0.2563\n",
      "48/281, train_loss: 0.0484, step time: 0.2607\n",
      "49/281, train_loss: 0.0816, step time: 0.2464\n",
      "50/281, train_loss: 0.2329, step time: 0.2517\n",
      "51/281, train_loss: 0.0491, step time: 0.2520\n",
      "52/281, train_loss: 0.2797, step time: 0.2573\n",
      "53/281, train_loss: 0.1238, step time: 0.2533\n",
      "54/281, train_loss: 0.0930, step time: 0.2558\n",
      "55/281, train_loss: 0.2437, step time: 0.2500\n",
      "56/281, train_loss: 0.2197, step time: 0.2493\n",
      "57/281, train_loss: 0.1031, step time: 0.2477\n",
      "58/281, train_loss: 0.0799, step time: 0.2567\n",
      "59/281, train_loss: 0.0578, step time: 0.2542\n",
      "60/281, train_loss: 0.0566, step time: 0.2526\n",
      "61/281, train_loss: 0.0578, step time: 0.2479\n",
      "62/281, train_loss: 0.0821, step time: 0.2632\n",
      "63/281, train_loss: 0.1156, step time: 0.2514\n",
      "64/281, train_loss: 0.0772, step time: 0.2519\n",
      "65/281, train_loss: 0.0552, step time: 0.2559\n",
      "66/281, train_loss: 0.0700, step time: 0.2556\n",
      "67/281, train_loss: 0.0852, step time: 0.2636\n",
      "68/281, train_loss: 0.0808, step time: 0.2566\n",
      "69/281, train_loss: 0.0764, step time: 0.2554\n",
      "70/281, train_loss: 0.0569, step time: 0.2547\n",
      "71/281, train_loss: 0.0695, step time: 0.2586\n",
      "72/281, train_loss: 0.0444, step time: 0.2510\n",
      "73/281, train_loss: 0.0621, step time: 0.2544\n",
      "74/281, train_loss: 0.0978, step time: 0.2557\n",
      "75/281, train_loss: 0.0811, step time: 0.2551\n",
      "76/281, train_loss: 0.0779, step time: 0.2591\n",
      "77/281, train_loss: 0.0598, step time: 0.2566\n",
      "78/281, train_loss: 0.1212, step time: 0.2545\n",
      "79/281, train_loss: 0.0546, step time: 0.2574\n",
      "80/281, train_loss: 0.0796, step time: 0.2656\n",
      "81/281, train_loss: 0.2234, step time: 0.2578\n",
      "82/281, train_loss: 0.0781, step time: 0.2559\n",
      "83/281, train_loss: 0.0886, step time: 0.2517\n",
      "84/281, train_loss: 0.0920, step time: 0.2611\n",
      "85/281, train_loss: 0.0610, step time: 0.2545\n",
      "86/281, train_loss: 0.0782, step time: 0.2674\n",
      "87/281, train_loss: 0.0405, step time: 0.2633\n",
      "88/281, train_loss: 0.0479, step time: 0.2533\n",
      "89/281, train_loss: 0.1293, step time: 0.2569\n",
      "90/281, train_loss: 0.1154, step time: 0.2639\n",
      "91/281, train_loss: 0.1042, step time: 0.2526\n",
      "92/281, train_loss: 0.0527, step time: 0.2548\n",
      "93/281, train_loss: 0.0590, step time: 0.2516\n",
      "94/281, train_loss: 0.2294, step time: 0.2524\n",
      "95/281, train_loss: 0.2401, step time: 0.2559\n",
      "96/281, train_loss: 0.2242, step time: 0.2547\n",
      "97/281, train_loss: 0.1118, step time: 0.2696\n",
      "98/281, train_loss: 0.0546, step time: 0.2545\n",
      "99/281, train_loss: 0.0931, step time: 0.2556\n",
      "100/281, train_loss: 0.0750, step time: 0.2527\n",
      "101/281, train_loss: 0.0640, step time: 0.2571\n",
      "102/281, train_loss: 0.2218, step time: 0.2580\n",
      "103/281, train_loss: 0.2272, step time: 0.2596\n",
      "104/281, train_loss: 0.1934, step time: 0.2801\n",
      "105/281, train_loss: 0.0719, step time: 0.2525\n",
      "106/281, train_loss: 0.0625, step time: 0.2512\n",
      "107/281, train_loss: 0.0757, step time: 0.2535\n",
      "108/281, train_loss: 0.0519, step time: 0.2713\n",
      "109/281, train_loss: 0.0649, step time: 0.2687\n",
      "110/281, train_loss: 0.0675, step time: 0.2538\n",
      "111/281, train_loss: 0.2511, step time: 0.2469\n",
      "112/281, train_loss: 0.3960, step time: 0.2511\n",
      "113/281, train_loss: 0.0864, step time: 0.2500\n",
      "114/281, train_loss: 0.0765, step time: 0.2563\n",
      "115/281, train_loss: 0.0881, step time: 0.2500\n",
      "116/281, train_loss: 0.0716, step time: 0.2528\n",
      "117/281, train_loss: 0.0907, step time: 0.2532\n",
      "118/281, train_loss: 0.0764, step time: 0.2505\n",
      "119/281, train_loss: 0.2086, step time: 0.2494\n",
      "120/281, train_loss: 0.0564, step time: 0.2492\n",
      "121/281, train_loss: 0.0854, step time: 0.2570\n",
      "122/281, train_loss: 0.0802, step time: 0.2599\n",
      "123/281, train_loss: 0.0810, step time: 0.2512\n",
      "124/281, train_loss: 0.0737, step time: 0.2637\n",
      "125/281, train_loss: 0.2869, step time: 0.2569\n",
      "126/281, train_loss: 0.2542, step time: 0.2567\n",
      "127/281, train_loss: 0.0751, step time: 0.2506\n",
      "128/281, train_loss: 0.0636, step time: 0.2553\n",
      "129/281, train_loss: 0.0428, step time: 0.2567\n",
      "130/281, train_loss: 0.0564, step time: 0.2496\n",
      "131/281, train_loss: 0.0791, step time: 0.2536\n",
      "132/281, train_loss: 0.0603, step time: 0.2531\n",
      "133/281, train_loss: 0.0656, step time: 0.2539\n",
      "134/281, train_loss: 0.0672, step time: 0.2605\n",
      "135/281, train_loss: 0.2182, step time: 0.2556\n",
      "136/281, train_loss: 0.0753, step time: 0.2575\n",
      "137/281, train_loss: 0.1068, step time: 0.2547\n",
      "138/281, train_loss: 0.0780, step time: 0.2620\n",
      "139/281, train_loss: 0.0865, step time: 0.2613\n",
      "140/281, train_loss: 0.2487, step time: 0.2569\n",
      "141/281, train_loss: 0.0440, step time: 0.2574\n",
      "142/281, train_loss: 0.0676, step time: 0.2555\n",
      "143/281, train_loss: 0.0793, step time: 0.2542\n",
      "144/281, train_loss: 0.0720, step time: 0.2574\n",
      "145/281, train_loss: 0.0787, step time: 0.2498\n",
      "146/281, train_loss: 0.2037, step time: 0.2537\n",
      "147/281, train_loss: 0.0589, step time: 0.2512\n",
      "148/281, train_loss: 0.0442, step time: 0.2533\n",
      "149/281, train_loss: 0.1147, step time: 0.2573\n",
      "150/281, train_loss: 0.0360, step time: 0.2543\n",
      "151/281, train_loss: 0.0665, step time: 0.2515\n",
      "152/281, train_loss: 0.2302, step time: 0.2556\n",
      "153/281, train_loss: 0.2929, step time: 0.2579\n",
      "154/281, train_loss: 0.2102, step time: 0.2533\n",
      "155/281, train_loss: 0.2409, step time: 0.2480\n",
      "156/281, train_loss: 0.0461, step time: 0.2550\n",
      "157/281, train_loss: 0.0473, step time: 0.2519\n",
      "158/281, train_loss: 0.0652, step time: 0.2566\n",
      "159/281, train_loss: 0.0672, step time: 0.2725\n",
      "160/281, train_loss: 0.0800, step time: 0.2562\n",
      "161/281, train_loss: 0.0701, step time: 0.2594\n",
      "162/281, train_loss: 0.0701, step time: 0.2668\n",
      "163/281, train_loss: 0.0844, step time: 0.2528\n",
      "164/281, train_loss: 0.0619, step time: 0.2519\n",
      "165/281, train_loss: 0.2106, step time: 0.2617\n",
      "166/281, train_loss: 0.1098, step time: 0.2454\n",
      "167/281, train_loss: 0.0653, step time: 0.2557\n",
      "168/281, train_loss: 0.1057, step time: 0.2580\n",
      "169/281, train_loss: 0.1286, step time: 0.2606\n",
      "170/281, train_loss: 0.0466, step time: 0.2529\n",
      "171/281, train_loss: 0.1320, step time: 0.2490\n",
      "172/281, train_loss: 0.0739, step time: 0.2491\n",
      "173/281, train_loss: 0.0732, step time: 0.2449\n",
      "174/281, train_loss: 0.0856, step time: 0.2478\n",
      "175/281, train_loss: 0.0574, step time: 0.2522\n",
      "176/281, train_loss: 0.1106, step time: 0.2431\n",
      "177/281, train_loss: 0.0670, step time: 0.2478\n",
      "178/281, train_loss: 0.0844, step time: 0.2470\n",
      "179/281, train_loss: 0.0446, step time: 0.2462\n",
      "180/281, train_loss: 0.1419, step time: 0.2482\n",
      "181/281, train_loss: 0.1219, step time: 0.2499\n",
      "182/281, train_loss: 0.0556, step time: 0.2511\n",
      "183/281, train_loss: 0.2518, step time: 0.2654\n",
      "184/281, train_loss: 0.1020, step time: 0.2584\n",
      "185/281, train_loss: 0.2560, step time: 0.2518\n",
      "186/281, train_loss: 0.2151, step time: 0.2551\n",
      "187/281, train_loss: 0.0713, step time: 0.2432\n",
      "188/281, train_loss: 0.0912, step time: 0.2468\n",
      "189/281, train_loss: 0.2699, step time: 0.2522\n",
      "190/281, train_loss: 0.0624, step time: 0.2462\n",
      "191/281, train_loss: 0.0670, step time: 0.2507\n",
      "192/281, train_loss: 0.0835, step time: 0.2480\n",
      "193/281, train_loss: 0.0773, step time: 0.2485\n",
      "194/281, train_loss: 0.0664, step time: 0.2680\n",
      "195/281, train_loss: 0.0733, step time: 0.2466\n",
      "196/281, train_loss: 0.0730, step time: 0.2473\n",
      "197/281, train_loss: 0.0836, step time: 0.2494\n",
      "198/281, train_loss: 0.0808, step time: 0.2484\n",
      "199/281, train_loss: 0.0530, step time: 0.2501\n",
      "200/281, train_loss: 0.0696, step time: 0.2497\n",
      "201/281, train_loss: 0.0985, step time: 0.2448\n",
      "202/281, train_loss: 0.0789, step time: 0.2543\n",
      "203/281, train_loss: 0.0854, step time: 0.2465\n",
      "204/281, train_loss: 0.1148, step time: 0.2561\n",
      "205/281, train_loss: 0.0936, step time: 0.2509\n",
      "206/281, train_loss: 0.0501, step time: 0.2493\n",
      "207/281, train_loss: 0.1021, step time: 0.2431\n",
      "208/281, train_loss: 0.1005, step time: 0.2475\n",
      "209/281, train_loss: 0.1077, step time: 0.2533\n",
      "210/281, train_loss: 0.0597, step time: 0.2487\n",
      "211/281, train_loss: 0.0750, step time: 0.2527\n",
      "212/281, train_loss: 0.0876, step time: 0.2540\n",
      "213/281, train_loss: 0.0910, step time: 0.2542\n",
      "214/281, train_loss: 0.0693, step time: 0.2520\n",
      "215/281, train_loss: 0.0825, step time: 0.2563\n",
      "216/281, train_loss: 0.1045, step time: 0.2508\n",
      "217/281, train_loss: 0.0746, step time: 0.2541\n",
      "218/281, train_loss: 0.2256, step time: 0.2498\n",
      "219/281, train_loss: 0.0444, step time: 0.2500\n",
      "220/281, train_loss: 0.1105, step time: 0.2515\n",
      "221/281, train_loss: 0.0438, step time: 0.2535\n",
      "222/281, train_loss: 0.2799, step time: 0.2541\n",
      "223/281, train_loss: 0.0955, step time: 0.2552\n",
      "224/281, train_loss: 0.0673, step time: 0.2530\n",
      "225/281, train_loss: 0.0696, step time: 0.2494\n",
      "226/281, train_loss: 0.0630, step time: 0.2501\n",
      "227/281, train_loss: 0.1179, step time: 0.2490\n",
      "228/281, train_loss: 0.0701, step time: 0.2420\n",
      "229/281, train_loss: 0.0896, step time: 0.2451\n",
      "230/281, train_loss: 0.0579, step time: 0.2451\n",
      "231/281, train_loss: 0.0886, step time: 0.2516\n",
      "232/281, train_loss: 0.0678, step time: 0.2513\n",
      "233/281, train_loss: 0.0999, step time: 0.2452\n",
      "234/281, train_loss: 0.0903, step time: 0.2450\n",
      "235/281, train_loss: 0.0760, step time: 0.2484\n",
      "236/281, train_loss: 0.0712, step time: 0.2572\n",
      "237/281, train_loss: 0.0673, step time: 0.2536\n",
      "238/281, train_loss: 0.1011, step time: 0.2440\n",
      "239/281, train_loss: 0.0780, step time: 0.2431\n",
      "240/281, train_loss: 0.2467, step time: 0.2501\n",
      "241/281, train_loss: 0.0661, step time: 0.2455\n",
      "242/281, train_loss: 0.3096, step time: 0.2517\n",
      "243/281, train_loss: 0.0738, step time: 0.2452\n",
      "244/281, train_loss: 0.0687, step time: 0.2503\n",
      "245/281, train_loss: 0.0456, step time: 0.2544\n",
      "246/281, train_loss: 0.0694, step time: 0.2512\n",
      "247/281, train_loss: 0.0662, step time: 0.2534\n",
      "248/281, train_loss: 0.0763, step time: 0.2505\n",
      "249/281, train_loss: 0.0589, step time: 0.2527\n",
      "250/281, train_loss: 0.0866, step time: 0.2481\n",
      "251/281, train_loss: 0.0771, step time: 0.2513\n",
      "252/281, train_loss: 0.0750, step time: 0.2533\n",
      "253/281, train_loss: 0.0909, step time: 0.2535\n",
      "254/281, train_loss: 0.2553, step time: 0.2491\n",
      "255/281, train_loss: 0.1377, step time: 0.2483\n",
      "256/281, train_loss: 0.2203, step time: 0.2496\n",
      "257/281, train_loss: 0.1062, step time: 0.2505\n",
      "258/281, train_loss: 0.0722, step time: 0.2528\n",
      "259/281, train_loss: 0.0838, step time: 0.2489\n",
      "260/281, train_loss: 0.0673, step time: 0.2472\n",
      "261/281, train_loss: 0.0369, step time: 0.2503\n",
      "262/281, train_loss: 0.0815, step time: 0.2497\n",
      "263/281, train_loss: 0.0854, step time: 0.2491\n",
      "264/281, train_loss: 0.0932, step time: 0.2502\n",
      "265/281, train_loss: 0.0519, step time: 0.2548\n",
      "266/281, train_loss: 0.0752, step time: 0.2525\n",
      "267/281, train_loss: 0.0481, step time: 0.2535\n",
      "268/281, train_loss: 0.0835, step time: 0.2499\n",
      "269/281, train_loss: 0.0688, step time: 0.2503\n",
      "270/281, train_loss: 0.0824, step time: 0.2470\n",
      "271/281, train_loss: 0.0709, step time: 0.2440\n",
      "272/281, train_loss: 0.0579, step time: 0.2495\n",
      "273/281, train_loss: 0.0686, step time: 0.2532\n",
      "274/281, train_loss: 0.0934, step time: 0.2514\n",
      "275/281, train_loss: 0.0494, step time: 0.2535\n",
      "276/281, train_loss: 0.0916, step time: 0.2504\n",
      "277/281, train_loss: 0.2653, step time: 0.2523\n",
      "278/281, train_loss: 0.0930, step time: 0.2458\n",
      "279/281, train_loss: 0.1273, step time: 0.2498\n",
      "280/281, train_loss: 0.0668, step time: 0.2523\n",
      "281/281, train_loss: 0.1074, step time: 0.2516\n",
      "282/281, train_loss: 0.0590, step time: 0.1465\n",
      "epoch 79 average loss: 0.1014\n",
      "current epoch: 79 current mean dice: 0.8902 tc: 0.8835 wt: 0.9219 et: 0.8746\n",
      "best mean dice: 0.8904 at epoch: 77\n",
      "time consuming of epoch 79 is: 356.3628\n",
      "----------\n",
      "epoch 80/200\n",
      "1/281, train_loss: 0.1127, step time: 0.2604\n",
      "2/281, train_loss: 0.0631, step time: 0.2587\n",
      "3/281, train_loss: 0.0542, step time: 0.2562\n",
      "4/281, train_loss: 0.0708, step time: 0.2521\n",
      "5/281, train_loss: 0.0982, step time: 0.2515\n",
      "6/281, train_loss: 0.0689, step time: 0.2508\n",
      "7/281, train_loss: 0.0786, step time: 0.2526\n",
      "8/281, train_loss: 0.2477, step time: 0.2538\n",
      "9/281, train_loss: 0.2225, step time: 0.2466\n",
      "10/281, train_loss: 0.1314, step time: 0.2557\n",
      "11/281, train_loss: 0.0725, step time: 0.2561\n",
      "12/281, train_loss: 0.0731, step time: 0.2504\n",
      "13/281, train_loss: 0.1419, step time: 0.2474\n",
      "14/281, train_loss: 0.0893, step time: 0.2598\n",
      "15/281, train_loss: 0.0634, step time: 0.2513\n",
      "16/281, train_loss: 0.0620, step time: 0.2520\n",
      "17/281, train_loss: 0.1043, step time: 0.2539\n",
      "18/281, train_loss: 0.0723, step time: 0.2562\n",
      "19/281, train_loss: 0.1138, step time: 0.2503\n",
      "20/281, train_loss: 0.0522, step time: 0.2468\n",
      "21/281, train_loss: 0.2128, step time: 0.2603\n",
      "22/281, train_loss: 0.0893, step time: 0.2511\n",
      "23/281, train_loss: 0.0716, step time: 0.2535\n",
      "24/281, train_loss: 0.0661, step time: 0.2549\n",
      "25/281, train_loss: 0.0657, step time: 0.2564\n",
      "26/281, train_loss: 0.0641, step time: 0.2507\n",
      "27/281, train_loss: 0.0425, step time: 0.2544\n",
      "28/281, train_loss: 0.0836, step time: 0.2459\n",
      "29/281, train_loss: 0.1156, step time: 0.2490\n",
      "30/281, train_loss: 0.0857, step time: 0.2579\n",
      "31/281, train_loss: 0.0827, step time: 0.2577\n",
      "32/281, train_loss: 0.0836, step time: 0.2530\n",
      "33/281, train_loss: 0.0357, step time: 0.2545\n",
      "34/281, train_loss: 0.1021, step time: 0.2518\n",
      "35/281, train_loss: 0.3965, step time: 0.2513\n",
      "36/281, train_loss: 0.0377, step time: 0.2560\n",
      "37/281, train_loss: 0.0467, step time: 0.2572\n",
      "38/281, train_loss: 0.0948, step time: 0.2596\n",
      "39/281, train_loss: 0.0850, step time: 0.2492\n",
      "40/281, train_loss: 0.2503, step time: 0.2568\n",
      "41/281, train_loss: 0.0729, step time: 0.2537\n",
      "42/281, train_loss: 0.1041, step time: 0.2516\n",
      "43/281, train_loss: 0.0823, step time: 0.2555\n",
      "44/281, train_loss: 0.2296, step time: 0.2484\n",
      "45/281, train_loss: 0.0733, step time: 0.2558\n",
      "46/281, train_loss: 0.2367, step time: 0.2496\n",
      "47/281, train_loss: 0.0937, step time: 0.2559\n",
      "48/281, train_loss: 0.0637, step time: 0.2529\n",
      "49/281, train_loss: 0.0827, step time: 0.2461\n",
      "50/281, train_loss: 0.1059, step time: 0.2534\n",
      "51/281, train_loss: 0.0698, step time: 0.2534\n",
      "52/281, train_loss: 0.0923, step time: 0.2486\n",
      "53/281, train_loss: 0.0509, step time: 0.2537\n",
      "54/281, train_loss: 0.0782, step time: 0.2515\n",
      "55/281, train_loss: 0.0919, step time: 0.2575\n",
      "56/281, train_loss: 0.0552, step time: 0.2570\n",
      "57/281, train_loss: 0.0487, step time: 0.2539\n",
      "58/281, train_loss: 0.2856, step time: 0.2561\n",
      "59/281, train_loss: 0.2170, step time: 0.2558\n",
      "60/281, train_loss: 0.0863, step time: 0.2540\n",
      "61/281, train_loss: 0.0369, step time: 0.2484\n",
      "62/281, train_loss: 0.0473, step time: 0.2554\n",
      "63/281, train_loss: 0.0670, step time: 0.2533\n",
      "64/281, train_loss: 0.0819, step time: 0.2570\n",
      "65/281, train_loss: 0.0651, step time: 0.2515\n",
      "66/281, train_loss: 0.0447, step time: 0.2541\n",
      "67/281, train_loss: 0.0802, step time: 0.2620\n",
      "68/281, train_loss: 0.0606, step time: 0.2565\n",
      "69/281, train_loss: 0.0547, step time: 0.2584\n",
      "70/281, train_loss: 0.2448, step time: 0.2616\n",
      "71/281, train_loss: 0.0955, step time: 0.2550\n",
      "72/281, train_loss: 0.2309, step time: 0.2557\n",
      "73/281, train_loss: 0.2497, step time: 0.2551\n",
      "74/281, train_loss: 0.1406, step time: 0.2554\n",
      "75/281, train_loss: 0.0714, step time: 0.2590\n",
      "76/281, train_loss: 0.0703, step time: 0.2570\n",
      "77/281, train_loss: 0.0912, step time: 0.2580\n",
      "78/281, train_loss: 0.2161, step time: 0.2558\n",
      "79/281, train_loss: 0.0826, step time: 0.2555\n",
      "80/281, train_loss: 0.2500, step time: 0.2545\n",
      "81/281, train_loss: 0.1587, step time: 0.2553\n",
      "82/281, train_loss: 0.0428, step time: 0.2555\n",
      "83/281, train_loss: 0.2298, step time: 0.2579\n",
      "84/281, train_loss: 0.0619, step time: 0.2544\n",
      "85/281, train_loss: 0.0778, step time: 0.2590\n",
      "86/281, train_loss: 0.2180, step time: 0.2614\n",
      "87/281, train_loss: 0.1134, step time: 0.2527\n",
      "88/281, train_loss: 0.1022, step time: 0.2487\n",
      "89/281, train_loss: 0.0698, step time: 0.2546\n",
      "90/281, train_loss: 0.0518, step time: 0.2520\n",
      "91/281, train_loss: 0.0860, step time: 0.2667\n",
      "92/281, train_loss: 0.0955, step time: 0.2628\n",
      "93/281, train_loss: 0.0614, step time: 0.2551\n",
      "94/281, train_loss: 0.0515, step time: 0.2560\n",
      "95/281, train_loss: 0.2382, step time: 0.2537\n",
      "96/281, train_loss: 0.0844, step time: 0.2503\n",
      "97/281, train_loss: 0.0943, step time: 0.2530\n",
      "98/281, train_loss: 0.0525, step time: 0.2559\n",
      "99/281, train_loss: 0.2161, step time: 0.2570\n",
      "100/281, train_loss: 0.0499, step time: 0.2562\n",
      "101/281, train_loss: 0.0808, step time: 0.2499\n",
      "102/281, train_loss: 0.0639, step time: 0.2516\n",
      "103/281, train_loss: 0.1031, step time: 0.2511\n",
      "104/281, train_loss: 0.2209, step time: 0.2509\n",
      "105/281, train_loss: 0.0794, step time: 0.2541\n",
      "106/281, train_loss: 0.0734, step time: 0.2557\n",
      "107/281, train_loss: 0.0627, step time: 0.2583\n",
      "108/281, train_loss: 0.0827, step time: 0.2572\n",
      "109/281, train_loss: 0.1124, step time: 0.2582\n",
      "110/281, train_loss: 0.1151, step time: 0.2579\n",
      "111/281, train_loss: 0.0759, step time: 0.2531\n",
      "112/281, train_loss: 0.0802, step time: 0.2498\n",
      "113/281, train_loss: 0.0735, step time: 0.2523\n",
      "114/281, train_loss: 0.0729, step time: 0.2492\n",
      "115/281, train_loss: 0.0827, step time: 0.2547\n",
      "116/281, train_loss: 0.0782, step time: 0.2542\n",
      "117/281, train_loss: 0.0994, step time: 0.2454\n",
      "118/281, train_loss: 0.0675, step time: 0.2469\n",
      "119/281, train_loss: 0.0539, step time: 0.2553\n",
      "120/281, train_loss: 0.0752, step time: 0.2556\n",
      "121/281, train_loss: 0.0441, step time: 0.2527\n",
      "122/281, train_loss: 0.0879, step time: 0.2495\n",
      "123/281, train_loss: 0.0646, step time: 0.2542\n",
      "124/281, train_loss: 0.0459, step time: 0.2540\n",
      "125/281, train_loss: 0.0593, step time: 0.2547\n",
      "126/281, train_loss: 0.0699, step time: 0.2519\n",
      "127/281, train_loss: 0.0868, step time: 0.2524\n",
      "128/281, train_loss: 0.0628, step time: 0.2483\n",
      "129/281, train_loss: 0.0905, step time: 0.2446\n",
      "130/281, train_loss: 0.1219, step time: 0.2503\n",
      "131/281, train_loss: 0.0851, step time: 0.2672\n",
      "132/281, train_loss: 0.0577, step time: 0.2519\n",
      "133/281, train_loss: 0.0848, step time: 0.2533\n",
      "134/281, train_loss: 0.0488, step time: 0.2498\n",
      "135/281, train_loss: 0.1240, step time: 0.2524\n",
      "136/281, train_loss: 0.0672, step time: 0.2533\n",
      "137/281, train_loss: 0.0938, step time: 0.2496\n",
      "138/281, train_loss: 0.0858, step time: 0.2583\n",
      "139/281, train_loss: 0.0980, step time: 0.2537\n",
      "140/281, train_loss: 0.0890, step time: 0.2559\n",
      "141/281, train_loss: 0.0671, step time: 0.2547\n",
      "142/281, train_loss: 0.2235, step time: 0.2590\n",
      "143/281, train_loss: 0.0590, step time: 0.2548\n",
      "144/281, train_loss: 0.0983, step time: 0.2591\n",
      "145/281, train_loss: 0.2277, step time: 0.2566\n",
      "146/281, train_loss: 0.0576, step time: 0.2565\n",
      "147/281, train_loss: 0.0620, step time: 0.2543\n",
      "148/281, train_loss: 0.0793, step time: 0.2677\n",
      "149/281, train_loss: 0.2379, step time: 0.2570\n",
      "150/281, train_loss: 0.0885, step time: 0.2530\n",
      "151/281, train_loss: 0.0715, step time: 0.2557\n",
      "152/281, train_loss: 0.0924, step time: 0.2566\n",
      "153/281, train_loss: 0.0935, step time: 0.2583\n",
      "154/281, train_loss: 0.0795, step time: 0.2595\n",
      "155/281, train_loss: 0.0463, step time: 0.2512\n",
      "156/281, train_loss: 0.1171, step time: 0.2541\n",
      "157/281, train_loss: 0.0728, step time: 0.2576\n",
      "158/281, train_loss: 0.0894, step time: 0.2583\n",
      "159/281, train_loss: 0.0692, step time: 0.2613\n",
      "160/281, train_loss: 0.0493, step time: 0.2562\n",
      "161/281, train_loss: 0.2403, step time: 0.2505\n",
      "162/281, train_loss: 0.1061, step time: 0.2579\n",
      "163/281, train_loss: 0.0705, step time: 0.2539\n",
      "164/281, train_loss: 0.0980, step time: 0.2577\n",
      "165/281, train_loss: 0.0762, step time: 0.2578\n",
      "166/281, train_loss: 0.1082, step time: 0.2541\n",
      "167/281, train_loss: 0.0527, step time: 0.2565\n",
      "168/281, train_loss: 0.1378, step time: 0.2546\n",
      "169/281, train_loss: 0.0601, step time: 0.2559\n",
      "170/281, train_loss: 0.0715, step time: 0.2564\n",
      "171/281, train_loss: 0.0852, step time: 0.2625\n",
      "172/281, train_loss: 0.0677, step time: 0.2548\n",
      "173/281, train_loss: 0.2317, step time: 0.2514\n",
      "174/281, train_loss: 0.0808, step time: 0.2527\n",
      "175/281, train_loss: 0.2872, step time: 0.2591\n",
      "176/281, train_loss: 0.0674, step time: 0.2571\n",
      "177/281, train_loss: 0.0678, step time: 0.2592\n",
      "178/281, train_loss: 0.0455, step time: 0.2582\n",
      "179/281, train_loss: 0.1342, step time: 0.2555\n",
      "180/281, train_loss: 0.0756, step time: 0.2626\n",
      "181/281, train_loss: 0.0640, step time: 0.2565\n",
      "182/281, train_loss: 0.0868, step time: 0.2568\n",
      "183/281, train_loss: 0.0602, step time: 0.2601\n",
      "184/281, train_loss: 0.0570, step time: 0.2532\n",
      "185/281, train_loss: 0.0680, step time: 0.2549\n",
      "186/281, train_loss: 0.2286, step time: 0.2541\n",
      "187/281, train_loss: 0.0986, step time: 0.2531\n",
      "188/281, train_loss: 0.0611, step time: 0.2540\n",
      "189/281, train_loss: 0.0468, step time: 0.2512\n",
      "190/281, train_loss: 0.0726, step time: 0.2578\n",
      "191/281, train_loss: 0.1001, step time: 0.2528\n",
      "192/281, train_loss: 0.0977, step time: 0.2493\n",
      "193/281, train_loss: 0.1102, step time: 0.2501\n",
      "194/281, train_loss: 0.0937, step time: 0.2543\n",
      "195/281, train_loss: 0.0992, step time: 0.2553\n",
      "196/281, train_loss: 0.0716, step time: 0.2578\n",
      "197/281, train_loss: 0.2700, step time: 0.2525\n",
      "198/281, train_loss: 0.0464, step time: 0.2493\n",
      "199/281, train_loss: 0.0680, step time: 0.2574\n",
      "200/281, train_loss: 0.0571, step time: 0.2541\n",
      "201/281, train_loss: 0.2424, step time: 0.2511\n",
      "202/281, train_loss: 0.0606, step time: 0.2549\n",
      "203/281, train_loss: 0.2494, step time: 0.2515\n",
      "204/281, train_loss: 0.0684, step time: 0.2486\n",
      "205/281, train_loss: 0.0650, step time: 0.2516\n",
      "206/281, train_loss: 0.0790, step time: 0.2546\n",
      "207/281, train_loss: 0.0691, step time: 0.2522\n",
      "208/281, train_loss: 0.0776, step time: 0.2451\n",
      "209/281, train_loss: 0.2388, step time: 0.2529\n",
      "210/281, train_loss: 0.0606, step time: 0.2518\n",
      "211/281, train_loss: 0.0775, step time: 0.2530\n",
      "212/281, train_loss: 0.0697, step time: 0.2528\n",
      "213/281, train_loss: 0.0651, step time: 0.2521\n",
      "214/281, train_loss: 0.2225, step time: 0.2514\n",
      "215/281, train_loss: 0.0601, step time: 0.2548\n",
      "216/281, train_loss: 0.0573, step time: 0.2540\n",
      "217/281, train_loss: 0.0818, step time: 0.2587\n",
      "218/281, train_loss: 0.0647, step time: 0.2548\n",
      "219/281, train_loss: 0.0878, step time: 0.2566\n",
      "220/281, train_loss: 0.0934, step time: 0.2507\n",
      "221/281, train_loss: 0.1194, step time: 0.2547\n",
      "222/281, train_loss: 0.0500, step time: 0.2607\n",
      "223/281, train_loss: 0.0852, step time: 0.2559\n",
      "224/281, train_loss: 0.0659, step time: 0.2576\n",
      "225/281, train_loss: 0.0718, step time: 0.2589\n",
      "226/281, train_loss: 0.0521, step time: 0.2528\n",
      "227/281, train_loss: 0.0686, step time: 0.2532\n",
      "228/281, train_loss: 0.0676, step time: 0.2502\n",
      "229/281, train_loss: 0.1130, step time: 0.2554\n",
      "230/281, train_loss: 0.0595, step time: 0.2529\n",
      "231/281, train_loss: 0.0586, step time: 0.2568\n",
      "232/281, train_loss: 0.0779, step time: 0.2574\n",
      "233/281, train_loss: 0.2223, step time: 0.2529\n",
      "234/281, train_loss: 0.0442, step time: 0.2519\n",
      "235/281, train_loss: 0.2226, step time: 0.2572\n",
      "236/281, train_loss: 0.0987, step time: 0.2543\n",
      "237/281, train_loss: 0.0792, step time: 0.2559\n",
      "238/281, train_loss: 0.2259, step time: 0.2574\n",
      "239/281, train_loss: 0.0588, step time: 0.2596\n",
      "240/281, train_loss: 0.2018, step time: 0.2590\n",
      "241/281, train_loss: 0.0391, step time: 0.2618\n",
      "242/281, train_loss: 0.1385, step time: 0.2563\n",
      "243/281, train_loss: 0.2077, step time: 0.2529\n",
      "244/281, train_loss: 0.0621, step time: 0.2566\n",
      "245/281, train_loss: 0.0716, step time: 0.2545\n",
      "246/281, train_loss: 0.2323, step time: 0.2589\n",
      "247/281, train_loss: 0.0547, step time: 0.2539\n",
      "248/281, train_loss: 0.0645, step time: 0.2596\n",
      "249/281, train_loss: 0.1178, step time: 0.2564\n",
      "250/281, train_loss: 0.0644, step time: 0.2600\n",
      "251/281, train_loss: 0.0617, step time: 0.2611\n",
      "252/281, train_loss: 0.0983, step time: 0.2741\n",
      "253/281, train_loss: 0.0421, step time: 0.2548\n",
      "254/281, train_loss: 0.0638, step time: 0.2577\n",
      "255/281, train_loss: 0.0873, step time: 0.2557\n",
      "256/281, train_loss: 0.0728, step time: 0.2589\n",
      "257/281, train_loss: 0.0605, step time: 0.2583\n",
      "258/281, train_loss: 0.2445, step time: 0.2570\n",
      "259/281, train_loss: 0.3947, step time: 0.2556\n",
      "260/281, train_loss: 0.0850, step time: 0.2581\n",
      "261/281, train_loss: 0.0881, step time: 0.2611\n",
      "262/281, train_loss: 0.0832, step time: 0.2538\n",
      "263/281, train_loss: 0.2132, step time: 0.2595\n",
      "264/281, train_loss: 0.0954, step time: 0.2599\n",
      "265/281, train_loss: 0.0621, step time: 0.2586\n",
      "266/281, train_loss: 0.0750, step time: 0.2562\n",
      "267/281, train_loss: 0.0733, step time: 0.2572\n",
      "268/281, train_loss: 0.0933, step time: 0.2572\n",
      "269/281, train_loss: 0.0811, step time: 0.2576\n",
      "270/281, train_loss: 0.2501, step time: 0.2658\n",
      "271/281, train_loss: 0.1218, step time: 0.2618\n",
      "272/281, train_loss: 0.0674, step time: 0.2634\n",
      "273/281, train_loss: 0.0838, step time: 0.2608\n",
      "274/281, train_loss: 0.0509, step time: 0.2580\n",
      "275/281, train_loss: 0.0636, step time: 0.2546\n",
      "276/281, train_loss: 0.0628, step time: 0.2580\n",
      "277/281, train_loss: 0.0821, step time: 0.2533\n",
      "278/281, train_loss: 0.0605, step time: 0.2501\n",
      "279/281, train_loss: 0.0884, step time: 0.2564\n",
      "280/281, train_loss: 0.0725, step time: 0.2563\n",
      "281/281, train_loss: 0.0594, step time: 0.2504\n",
      "282/281, train_loss: 0.0609, step time: 0.1501\n",
      "epoch 80 average loss: 0.1010\n",
      "current epoch: 80 current mean dice: 0.8894 tc: 0.8854 wt: 0.9180 et: 0.8740\n",
      "best mean dice: 0.8904 at epoch: 77\n",
      "time consuming of epoch 80 is: 402.7047\n",
      "----------\n",
      "epoch 81/200\n",
      "1/281, train_loss: 0.0510, step time: 0.2667\n",
      "2/281, train_loss: 0.0738, step time: 0.2554\n",
      "3/281, train_loss: 0.0429, step time: 0.2558\n",
      "4/281, train_loss: 0.0731, step time: 0.2522\n",
      "5/281, train_loss: 0.0511, step time: 0.2558\n",
      "6/281, train_loss: 0.0551, step time: 0.2586\n",
      "7/281, train_loss: 0.0805, step time: 0.2559\n",
      "8/281, train_loss: 0.0877, step time: 0.2578\n",
      "9/281, train_loss: 0.2594, step time: 0.2572\n",
      "10/281, train_loss: 0.0776, step time: 0.2622\n",
      "11/281, train_loss: 0.0663, step time: 0.2537\n",
      "12/281, train_loss: 0.0850, step time: 0.2541\n",
      "13/281, train_loss: 0.0850, step time: 0.2494\n",
      "14/281, train_loss: 0.0869, step time: 0.2517\n",
      "15/281, train_loss: 0.0853, step time: 0.2498\n",
      "16/281, train_loss: 0.0658, step time: 0.2515\n",
      "17/281, train_loss: 0.0609, step time: 0.2543\n",
      "18/281, train_loss: 0.0680, step time: 0.2557\n",
      "19/281, train_loss: 0.0590, step time: 0.2554\n",
      "20/281, train_loss: 0.0911, step time: 0.2658\n",
      "21/281, train_loss: 0.0632, step time: 0.2532\n",
      "22/281, train_loss: 0.1286, step time: 0.2564\n",
      "23/281, train_loss: 0.2166, step time: 0.2510\n",
      "24/281, train_loss: 0.0968, step time: 0.2575\n",
      "25/281, train_loss: 0.0530, step time: 0.2556\n",
      "26/281, train_loss: 0.0770, step time: 0.2530\n",
      "27/281, train_loss: 0.0919, step time: 0.2585\n",
      "28/281, train_loss: 0.0585, step time: 0.2578\n",
      "29/281, train_loss: 0.1410, step time: 0.2545\n",
      "30/281, train_loss: 0.0890, step time: 0.2579\n",
      "31/281, train_loss: 0.0975, step time: 0.2539\n",
      "32/281, train_loss: 0.0853, step time: 0.2662\n",
      "33/281, train_loss: 0.0517, step time: 0.2571\n",
      "34/281, train_loss: 0.0988, step time: 0.2551\n",
      "35/281, train_loss: 0.2668, step time: 0.2593\n",
      "36/281, train_loss: 0.0437, step time: 0.2538\n",
      "37/281, train_loss: 0.0810, step time: 0.2535\n",
      "38/281, train_loss: 0.0767, step time: 0.2634\n",
      "39/281, train_loss: 0.0896, step time: 0.2577\n",
      "40/281, train_loss: 0.2721, step time: 0.2581\n",
      "41/281, train_loss: 0.0412, step time: 0.2549\n",
      "42/281, train_loss: 0.0730, step time: 0.2594\n",
      "43/281, train_loss: 0.0690, step time: 0.2630\n",
      "44/281, train_loss: 0.0797, step time: 0.2540\n",
      "45/281, train_loss: 0.0563, step time: 0.2500\n",
      "46/281, train_loss: 0.0915, step time: 0.2545\n",
      "47/281, train_loss: 0.0726, step time: 0.2505\n",
      "48/281, train_loss: 0.2419, step time: 0.2530\n",
      "49/281, train_loss: 0.0880, step time: 0.2475\n",
      "50/281, train_loss: 0.0712, step time: 0.2524\n",
      "51/281, train_loss: 0.0710, step time: 0.2567\n",
      "52/281, train_loss: 0.0654, step time: 0.2557\n",
      "53/281, train_loss: 0.0635, step time: 0.2598\n",
      "54/281, train_loss: 0.0693, step time: 0.2519\n",
      "55/281, train_loss: 0.0920, step time: 0.2559\n",
      "56/281, train_loss: 0.0643, step time: 0.2512\n",
      "57/281, train_loss: 0.0451, step time: 0.2547\n",
      "58/281, train_loss: 0.0502, step time: 0.2536\n",
      "59/281, train_loss: 0.0955, step time: 0.2553\n",
      "60/281, train_loss: 0.0712, step time: 0.2537\n",
      "61/281, train_loss: 0.2197, step time: 0.2507\n",
      "62/281, train_loss: 0.0875, step time: 0.2494\n",
      "63/281, train_loss: 0.0833, step time: 0.2516\n",
      "64/281, train_loss: 0.0731, step time: 0.2553\n",
      "65/281, train_loss: 0.0831, step time: 0.2556\n",
      "66/281, train_loss: 0.0867, step time: 0.2477\n",
      "67/281, train_loss: 0.0797, step time: 0.2669\n",
      "68/281, train_loss: 0.0884, step time: 0.2567\n",
      "69/281, train_loss: 0.0768, step time: 0.2511\n",
      "70/281, train_loss: 0.0881, step time: 0.2527\n",
      "71/281, train_loss: 0.2376, step time: 0.2539\n",
      "72/281, train_loss: 0.0779, step time: 0.2559\n",
      "73/281, train_loss: 0.0929, step time: 0.2496\n",
      "74/281, train_loss: 0.0692, step time: 0.2542\n",
      "75/281, train_loss: 0.0626, step time: 0.2577\n",
      "76/281, train_loss: 0.0953, step time: 0.2534\n",
      "77/281, train_loss: 0.0829, step time: 0.2528\n",
      "78/281, train_loss: 0.0561, step time: 0.2521\n",
      "79/281, train_loss: 0.0855, step time: 0.2569\n",
      "80/281, train_loss: 0.0921, step time: 0.2551\n",
      "81/281, train_loss: 0.0835, step time: 0.2552\n",
      "82/281, train_loss: 0.2388, step time: 0.2617\n",
      "83/281, train_loss: 0.0687, step time: 0.2576\n",
      "84/281, train_loss: 0.0568, step time: 0.2548\n",
      "85/281, train_loss: 0.0670, step time: 0.2546\n",
      "86/281, train_loss: 0.3710, step time: 0.2559\n",
      "87/281, train_loss: 0.0705, step time: 0.2546\n",
      "88/281, train_loss: 0.0724, step time: 0.2505\n",
      "89/281, train_loss: 0.0726, step time: 0.2570\n",
      "90/281, train_loss: 0.0913, step time: 0.2543\n",
      "91/281, train_loss: 0.0797, step time: 0.2471\n",
      "92/281, train_loss: 0.0760, step time: 0.2591\n",
      "93/281, train_loss: 0.0958, step time: 0.2610\n",
      "94/281, train_loss: 0.0627, step time: 0.2573\n",
      "95/281, train_loss: 0.1228, step time: 0.2581\n",
      "96/281, train_loss: 0.1159, step time: 0.2558\n",
      "97/281, train_loss: 0.0848, step time: 0.2573\n",
      "98/281, train_loss: 0.0490, step time: 0.2570\n",
      "99/281, train_loss: 0.0399, step time: 0.2509\n",
      "100/281, train_loss: 0.2644, step time: 0.2685\n",
      "101/281, train_loss: 0.0640, step time: 0.2556\n",
      "102/281, train_loss: 0.0911, step time: 0.2574\n",
      "103/281, train_loss: 0.0405, step time: 0.2577\n",
      "104/281, train_loss: 0.0533, step time: 0.2521\n",
      "105/281, train_loss: 0.0581, step time: 0.2512\n",
      "106/281, train_loss: 0.0772, step time: 0.2502\n",
      "107/281, train_loss: 0.0797, step time: 0.2520\n",
      "108/281, train_loss: 0.0584, step time: 0.2624\n",
      "109/281, train_loss: 0.0838, step time: 0.2502\n",
      "110/281, train_loss: 0.3708, step time: 0.2509\n",
      "111/281, train_loss: 0.0695, step time: 0.2547\n",
      "112/281, train_loss: 0.0825, step time: 0.2608\n",
      "113/281, train_loss: 0.0785, step time: 0.2531\n",
      "114/281, train_loss: 0.2273, step time: 0.2540\n",
      "115/281, train_loss: 0.0813, step time: 0.2562\n",
      "116/281, train_loss: 0.0688, step time: 0.2597\n",
      "117/281, train_loss: 0.0586, step time: 0.2523\n",
      "118/281, train_loss: 0.0883, step time: 0.2496\n",
      "119/281, train_loss: 0.2195, step time: 0.2540\n",
      "120/281, train_loss: 0.1072, step time: 0.2530\n",
      "121/281, train_loss: 0.2185, step time: 0.2519\n",
      "122/281, train_loss: 0.1270, step time: 0.2498\n",
      "123/281, train_loss: 0.2337, step time: 0.2519\n",
      "124/281, train_loss: 0.0828, step time: 0.2515\n",
      "125/281, train_loss: 0.0923, step time: 0.2592\n",
      "126/281, train_loss: 0.0854, step time: 0.2535\n",
      "127/281, train_loss: 0.2200, step time: 0.2516\n",
      "128/281, train_loss: 0.0912, step time: 0.2536\n",
      "129/281, train_loss: 0.0603, step time: 0.2549\n",
      "130/281, train_loss: 0.0713, step time: 0.2581\n",
      "131/281, train_loss: 0.0726, step time: 0.2569\n",
      "132/281, train_loss: 0.0590, step time: 0.2537\n",
      "133/281, train_loss: 0.0799, step time: 0.2544\n",
      "134/281, train_loss: 0.0904, step time: 0.2562\n",
      "135/281, train_loss: 0.0521, step time: 0.2567\n",
      "136/281, train_loss: 0.0770, step time: 0.2523\n",
      "137/281, train_loss: 0.0468, step time: 0.2585\n",
      "138/281, train_loss: 0.2092, step time: 0.2545\n",
      "139/281, train_loss: 0.2611, step time: 0.2553\n",
      "140/281, train_loss: 0.0643, step time: 0.2540\n",
      "141/281, train_loss: 0.2213, step time: 0.2496\n",
      "142/281, train_loss: 0.1169, step time: 0.2550\n",
      "143/281, train_loss: 0.0883, step time: 0.2546\n",
      "144/281, train_loss: 0.1066, step time: 0.2538\n",
      "145/281, train_loss: 0.1198, step time: 0.2545\n",
      "146/281, train_loss: 0.0683, step time: 0.2550\n",
      "147/281, train_loss: 0.0530, step time: 0.2522\n",
      "148/281, train_loss: 0.0702, step time: 0.2508\n",
      "149/281, train_loss: 0.0383, step time: 0.2555\n",
      "150/281, train_loss: 0.0462, step time: 0.2523\n",
      "151/281, train_loss: 0.0404, step time: 0.2520\n",
      "152/281, train_loss: 0.0781, step time: 0.2546\n",
      "153/281, train_loss: 0.0512, step time: 0.2559\n",
      "154/281, train_loss: 0.0464, step time: 0.2515\n",
      "155/281, train_loss: 0.2503, step time: 0.2519\n",
      "156/281, train_loss: 0.2245, step time: 0.2534\n",
      "157/281, train_loss: 0.0547, step time: 0.2507\n",
      "158/281, train_loss: 0.0643, step time: 0.2527\n",
      "159/281, train_loss: 0.0950, step time: 0.2514\n",
      "160/281, train_loss: 0.0851, step time: 0.2525\n",
      "161/281, train_loss: 0.0923, step time: 0.2578\n",
      "162/281, train_loss: 0.0903, step time: 0.2465\n",
      "163/281, train_loss: 0.2394, step time: 0.2483\n",
      "164/281, train_loss: 0.0494, step time: 0.2479\n",
      "165/281, train_loss: 0.0832, step time: 0.2478\n",
      "166/281, train_loss: 0.0754, step time: 0.2555\n",
      "167/281, train_loss: 0.0616, step time: 0.2639\n",
      "168/281, train_loss: 0.0610, step time: 0.2529\n",
      "169/281, train_loss: 0.0516, step time: 0.2552\n",
      "170/281, train_loss: 0.0648, step time: 0.2545\n",
      "171/281, train_loss: 0.0589, step time: 0.2523\n",
      "172/281, train_loss: 0.0763, step time: 0.2512\n",
      "173/281, train_loss: 0.0686, step time: 0.2557\n",
      "174/281, train_loss: 0.1090, step time: 0.2546\n",
      "175/281, train_loss: 0.0723, step time: 0.2518\n",
      "176/281, train_loss: 0.0837, step time: 0.2505\n",
      "177/281, train_loss: 0.2405, step time: 0.2556\n",
      "178/281, train_loss: 0.0368, step time: 0.2566\n",
      "179/281, train_loss: 0.2591, step time: 0.2511\n",
      "180/281, train_loss: 0.0650, step time: 0.2517\n",
      "181/281, train_loss: 0.0660, step time: 0.2513\n",
      "182/281, train_loss: 0.1021, step time: 0.2559\n",
      "183/281, train_loss: 0.0418, step time: 0.2469\n",
      "184/281, train_loss: 0.0527, step time: 0.2503\n",
      "185/281, train_loss: 0.0533, step time: 0.2482\n",
      "186/281, train_loss: 0.0684, step time: 0.2525\n",
      "187/281, train_loss: 0.2361, step time: 0.2474\n",
      "188/281, train_loss: 0.0749, step time: 0.2515\n",
      "189/281, train_loss: 0.2167, step time: 0.2551\n",
      "190/281, train_loss: 0.1372, step time: 0.2497\n",
      "191/281, train_loss: 0.0807, step time: 0.2530\n",
      "192/281, train_loss: 0.0666, step time: 0.2472\n",
      "193/281, train_loss: 0.0611, step time: 0.2518\n",
      "194/281, train_loss: 0.1146, step time: 0.2565\n",
      "195/281, train_loss: 0.0620, step time: 0.2535\n",
      "196/281, train_loss: 0.2074, step time: 0.2528\n",
      "197/281, train_loss: 0.0637, step time: 0.2517\n",
      "198/281, train_loss: 0.0877, step time: 0.2563\n",
      "199/281, train_loss: 0.0635, step time: 0.2538\n",
      "200/281, train_loss: 0.0602, step time: 0.2563\n",
      "201/281, train_loss: 0.0889, step time: 0.2497\n",
      "202/281, train_loss: 0.1204, step time: 0.2532\n",
      "203/281, train_loss: 0.0492, step time: 0.2461\n",
      "204/281, train_loss: 0.0596, step time: 0.2494\n",
      "205/281, train_loss: 0.0855, step time: 0.2503\n",
      "206/281, train_loss: 0.0774, step time: 0.2549\n",
      "207/281, train_loss: 0.0805, step time: 0.2509\n",
      "208/281, train_loss: 0.2449, step time: 0.2466\n",
      "209/281, train_loss: 0.2527, step time: 0.2543\n",
      "210/281, train_loss: 0.0512, step time: 0.2532\n",
      "211/281, train_loss: 0.0600, step time: 0.2469\n",
      "212/281, train_loss: 0.0949, step time: 0.2499\n",
      "213/281, train_loss: 0.0772, step time: 0.2524\n",
      "214/281, train_loss: 0.0594, step time: 0.2535\n",
      "215/281, train_loss: 0.2193, step time: 0.2437\n",
      "216/281, train_loss: 0.1082, step time: 0.2508\n",
      "217/281, train_loss: 0.1223, step time: 0.2468\n",
      "218/281, train_loss: 0.0591, step time: 0.2470\n",
      "219/281, train_loss: 0.1341, step time: 0.2488\n",
      "220/281, train_loss: 0.0713, step time: 0.2528\n",
      "221/281, train_loss: 0.0606, step time: 0.2472\n",
      "222/281, train_loss: 0.0690, step time: 0.2499\n",
      "223/281, train_loss: 0.0662, step time: 0.2516\n",
      "224/281, train_loss: 0.0433, step time: 0.2506\n",
      "225/281, train_loss: 0.3190, step time: 0.2493\n",
      "226/281, train_loss: 0.0720, step time: 0.2506\n",
      "227/281, train_loss: 0.0685, step time: 0.2507\n",
      "228/281, train_loss: 0.0839, step time: 0.2451\n",
      "229/281, train_loss: 0.1031, step time: 0.2443\n",
      "230/281, train_loss: 0.0697, step time: 0.2556\n",
      "231/281, train_loss: 0.1090, step time: 0.2556\n",
      "232/281, train_loss: 0.0708, step time: 0.2455\n",
      "233/281, train_loss: 0.0900, step time: 0.2477\n",
      "234/281, train_loss: 0.0629, step time: 0.2465\n",
      "235/281, train_loss: 0.1014, step time: 0.2495\n",
      "236/281, train_loss: 0.1260, step time: 0.2494\n",
      "237/281, train_loss: 0.0729, step time: 0.2511\n",
      "238/281, train_loss: 0.0588, step time: 0.2471\n",
      "239/281, train_loss: 0.0811, step time: 0.2478\n",
      "240/281, train_loss: 0.0680, step time: 0.2510\n",
      "241/281, train_loss: 0.0518, step time: 0.2507\n",
      "242/281, train_loss: 0.2414, step time: 0.2467\n",
      "243/281, train_loss: 0.2274, step time: 0.2715\n",
      "244/281, train_loss: 0.0863, step time: 0.2687\n",
      "245/281, train_loss: 0.0804, step time: 0.2531\n",
      "246/281, train_loss: 0.0757, step time: 0.2460\n",
      "247/281, train_loss: 0.1301, step time: 0.2487\n",
      "248/281, train_loss: 0.2444, step time: 0.2521\n",
      "249/281, train_loss: 0.2251, step time: 0.2482\n",
      "250/281, train_loss: 0.0743, step time: 0.2512\n",
      "251/281, train_loss: 0.0442, step time: 0.2473\n",
      "252/281, train_loss: 0.0756, step time: 0.2544\n",
      "253/281, train_loss: 0.2048, step time: 0.2563\n",
      "254/281, train_loss: 0.0864, step time: 0.2491\n",
      "255/281, train_loss: 0.0701, step time: 0.2472\n",
      "256/281, train_loss: 0.0690, step time: 0.2507\n",
      "257/281, train_loss: 0.0967, step time: 0.2543\n",
      "258/281, train_loss: 0.0769, step time: 0.2539\n",
      "259/281, train_loss: 0.0509, step time: 0.2481\n",
      "260/281, train_loss: 0.2549, step time: 0.2488\n",
      "261/281, train_loss: 0.2336, step time: 0.2525\n",
      "262/281, train_loss: 0.2221, step time: 0.2519\n",
      "263/281, train_loss: 0.0626, step time: 0.2466\n",
      "264/281, train_loss: 0.0949, step time: 0.2475\n",
      "265/281, train_loss: 0.0632, step time: 0.2493\n",
      "266/281, train_loss: 0.2674, step time: 0.2520\n",
      "267/281, train_loss: 0.1033, step time: 0.2516\n",
      "268/281, train_loss: 0.0889, step time: 0.2475\n",
      "269/281, train_loss: 0.0767, step time: 0.2501\n",
      "270/281, train_loss: 0.0624, step time: 0.2588\n",
      "271/281, train_loss: 0.0405, step time: 0.2832\n",
      "272/281, train_loss: 0.0885, step time: 0.2551\n",
      "273/281, train_loss: 0.0835, step time: 0.2527\n",
      "274/281, train_loss: 0.0376, step time: 0.2526\n",
      "275/281, train_loss: 0.0703, step time: 0.2455\n",
      "276/281, train_loss: 0.2455, step time: 0.2465\n",
      "277/281, train_loss: 0.0822, step time: 0.2499\n",
      "278/281, train_loss: 0.0815, step time: 0.2466\n",
      "279/281, train_loss: 0.0786, step time: 0.2505\n",
      "280/281, train_loss: 0.0665, step time: 0.2452\n",
      "281/281, train_loss: 0.0766, step time: 0.2433\n",
      "282/281, train_loss: 0.0663, step time: 0.1492\n",
      "epoch 81 average loss: 0.1002\n",
      "saved new best metric model\n",
      "current epoch: 81 current mean dice: 0.8918 tc: 0.8845 wt: 0.9233 et: 0.8773\n",
      "best mean dice: 0.8918 at epoch: 81\n",
      "time consuming of epoch 81 is: 372.7753\n",
      "----------\n",
      "epoch 82/200\n",
      "1/281, train_loss: 0.0584, step time: 0.2600\n",
      "2/281, train_loss: 0.0503, step time: 0.2554\n",
      "3/281, train_loss: 0.0713, step time: 0.2581\n",
      "4/281, train_loss: 0.0898, step time: 0.2830\n",
      "5/281, train_loss: 0.0819, step time: 0.2504\n",
      "6/281, train_loss: 0.2253, step time: 0.2471\n",
      "7/281, train_loss: 0.1191, step time: 0.2541\n",
      "8/281, train_loss: 0.2118, step time: 0.2536\n",
      "9/281, train_loss: 0.0637, step time: 0.2509\n",
      "10/281, train_loss: 0.0770, step time: 0.2487\n",
      "11/281, train_loss: 0.0618, step time: 0.2449\n",
      "12/281, train_loss: 0.0614, step time: 0.2486\n",
      "13/281, train_loss: 0.0808, step time: 0.2531\n",
      "14/281, train_loss: 0.0582, step time: 0.2517\n",
      "15/281, train_loss: 0.0892, step time: 0.2540\n",
      "16/281, train_loss: 0.0605, step time: 0.2542\n",
      "17/281, train_loss: 0.0899, step time: 0.2514\n",
      "18/281, train_loss: 0.2479, step time: 0.2509\n",
      "19/281, train_loss: 0.2247, step time: 0.2543\n",
      "20/281, train_loss: 0.2058, step time: 0.2527\n",
      "21/281, train_loss: 0.0824, step time: 0.2545\n",
      "22/281, train_loss: 0.0742, step time: 0.2545\n",
      "23/281, train_loss: 0.0605, step time: 0.2547\n",
      "24/281, train_loss: 0.0881, step time: 0.2580\n",
      "25/281, train_loss: 0.0631, step time: 0.2548\n",
      "26/281, train_loss: 0.0922, step time: 0.2546\n",
      "27/281, train_loss: 0.0581, step time: 0.2540\n",
      "28/281, train_loss: 0.0412, step time: 0.2588\n",
      "29/281, train_loss: 0.1129, step time: 0.2548\n",
      "30/281, train_loss: 0.0706, step time: 0.2548\n",
      "31/281, train_loss: 0.0745, step time: 0.2546\n",
      "32/281, train_loss: 0.2366, step time: 0.2636\n",
      "33/281, train_loss: 0.0535, step time: 0.2545\n",
      "34/281, train_loss: 0.1131, step time: 0.2497\n",
      "35/281, train_loss: 0.0714, step time: 0.2647\n",
      "36/281, train_loss: 0.2246, step time: 0.2562\n",
      "37/281, train_loss: 0.0910, step time: 0.2536\n",
      "38/281, train_loss: 0.0631, step time: 0.2530\n",
      "39/281, train_loss: 0.0488, step time: 0.2492\n",
      "40/281, train_loss: 0.0668, step time: 0.2514\n",
      "41/281, train_loss: 0.1240, step time: 0.2514\n",
      "42/281, train_loss: 0.0872, step time: 0.2484\n",
      "43/281, train_loss: 0.0949, step time: 0.2555\n",
      "44/281, train_loss: 0.0862, step time: 0.2490\n",
      "45/281, train_loss: 0.0628, step time: 0.2539\n",
      "46/281, train_loss: 0.1020, step time: 0.2534\n",
      "47/281, train_loss: 0.0995, step time: 0.2515\n",
      "48/281, train_loss: 0.0568, step time: 0.2533\n",
      "49/281, train_loss: 0.1058, step time: 0.2476\n",
      "50/281, train_loss: 0.0503, step time: 0.2459\n",
      "51/281, train_loss: 0.0618, step time: 0.2461\n",
      "52/281, train_loss: 0.0873, step time: 0.2580\n",
      "53/281, train_loss: 0.0683, step time: 0.2558\n",
      "54/281, train_loss: 0.0740, step time: 0.2570\n",
      "55/281, train_loss: 0.2106, step time: 0.2576\n",
      "56/281, train_loss: 0.0716, step time: 0.2539\n",
      "57/281, train_loss: 0.0966, step time: 0.2503\n",
      "58/281, train_loss: 0.0928, step time: 0.2495\n",
      "59/281, train_loss: 0.1040, step time: 0.2469\n",
      "60/281, train_loss: 0.0960, step time: 0.2534\n",
      "61/281, train_loss: 0.0737, step time: 0.2537\n",
      "62/281, train_loss: 0.0663, step time: 0.2521\n",
      "63/281, train_loss: 0.0632, step time: 0.2530\n",
      "64/281, train_loss: 0.0807, step time: 0.2525\n",
      "65/281, train_loss: 0.0802, step time: 0.2549\n",
      "66/281, train_loss: 0.0779, step time: 0.2564\n",
      "67/281, train_loss: 0.0454, step time: 0.2503\n",
      "68/281, train_loss: 0.0737, step time: 0.2555\n",
      "69/281, train_loss: 0.2398, step time: 0.2483\n",
      "70/281, train_loss: 0.1342, step time: 0.2555\n",
      "71/281, train_loss: 0.0631, step time: 0.2502\n",
      "72/281, train_loss: 0.2334, step time: 0.2561\n",
      "73/281, train_loss: 0.0712, step time: 0.2590\n",
      "74/281, train_loss: 0.0963, step time: 0.2503\n",
      "75/281, train_loss: 0.0700, step time: 0.2487\n",
      "76/281, train_loss: 0.2333, step time: 0.2543\n",
      "77/281, train_loss: 0.1087, step time: 0.2490\n",
      "78/281, train_loss: 0.2467, step time: 0.2574\n",
      "79/281, train_loss: 0.0723, step time: 0.2562\n",
      "80/281, train_loss: 0.0620, step time: 0.2495\n",
      "81/281, train_loss: 0.0567, step time: 0.2547\n",
      "82/281, train_loss: 0.2516, step time: 0.2495\n",
      "83/281, train_loss: 0.0467, step time: 0.2514\n",
      "84/281, train_loss: 0.0505, step time: 0.2559\n",
      "85/281, train_loss: 0.0901, step time: 0.2553\n",
      "86/281, train_loss: 0.1038, step time: 0.2581\n",
      "87/281, train_loss: 0.0854, step time: 0.2536\n",
      "88/281, train_loss: 0.1092, step time: 0.2503\n",
      "89/281, train_loss: 0.0427, step time: 0.2541\n",
      "90/281, train_loss: 0.2080, step time: 0.2530\n",
      "91/281, train_loss: 0.0734, step time: 0.2583\n",
      "92/281, train_loss: 0.0823, step time: 0.2498\n",
      "93/281, train_loss: 0.1070, step time: 0.2528\n",
      "94/281, train_loss: 0.0908, step time: 0.2524\n",
      "95/281, train_loss: 0.0981, step time: 0.2579\n",
      "96/281, train_loss: 0.0863, step time: 0.2550\n",
      "97/281, train_loss: 0.0756, step time: 0.2584\n",
      "98/281, train_loss: 0.2244, step time: 0.2587\n",
      "99/281, train_loss: 0.0677, step time: 0.2540\n",
      "100/281, train_loss: 0.0619, step time: 0.2556\n",
      "101/281, train_loss: 0.0967, step time: 0.2490\n",
      "102/281, train_loss: 0.0583, step time: 0.2584\n",
      "103/281, train_loss: 0.0949, step time: 0.2599\n",
      "104/281, train_loss: 0.2305, step time: 0.2500\n",
      "105/281, train_loss: 0.0472, step time: 0.2522\n",
      "106/281, train_loss: 0.0450, step time: 0.2533\n",
      "107/281, train_loss: 0.1025, step time: 0.2554\n",
      "108/281, train_loss: 0.0904, step time: 0.2581\n",
      "109/281, train_loss: 0.0741, step time: 0.2564\n",
      "110/281, train_loss: 0.2506, step time: 0.2525\n",
      "111/281, train_loss: 0.0899, step time: 0.2568\n",
      "112/281, train_loss: 0.0931, step time: 0.2580\n",
      "113/281, train_loss: 0.0796, step time: 0.2594\n",
      "114/281, train_loss: 0.0736, step time: 0.2561\n",
      "115/281, train_loss: 0.1100, step time: 0.2557\n",
      "116/281, train_loss: 0.0939, step time: 0.2537\n",
      "117/281, train_loss: 0.0485, step time: 0.2561\n",
      "118/281, train_loss: 0.0522, step time: 0.2591\n",
      "119/281, train_loss: 0.0660, step time: 0.2578\n",
      "120/281, train_loss: 0.2276, step time: 0.2593\n",
      "121/281, train_loss: 0.0841, step time: 0.2557\n",
      "122/281, train_loss: 0.1140, step time: 0.2595\n",
      "123/281, train_loss: 0.0786, step time: 0.2543\n",
      "124/281, train_loss: 0.0978, step time: 0.2581\n",
      "125/281, train_loss: 0.0790, step time: 0.2560\n",
      "126/281, train_loss: 0.0802, step time: 0.2533\n",
      "127/281, train_loss: 0.2180, step time: 0.2556\n",
      "128/281, train_loss: 0.0758, step time: 0.2556\n",
      "129/281, train_loss: 0.0839, step time: 0.2542\n",
      "130/281, train_loss: 0.0735, step time: 0.2593\n",
      "131/281, train_loss: 0.0791, step time: 0.2581\n",
      "132/281, train_loss: 0.0575, step time: 0.2502\n",
      "133/281, train_loss: 0.1211, step time: 0.2521\n",
      "134/281, train_loss: 0.0662, step time: 0.2559\n",
      "135/281, train_loss: 0.0659, step time: 0.2577\n",
      "136/281, train_loss: 0.0921, step time: 0.2566\n",
      "137/281, train_loss: 0.0693, step time: 0.2568\n",
      "138/281, train_loss: 0.0507, step time: 0.2597\n",
      "139/281, train_loss: 0.0953, step time: 0.2558\n",
      "140/281, train_loss: 0.0781, step time: 0.2540\n",
      "141/281, train_loss: 0.0781, step time: 0.2528\n",
      "142/281, train_loss: 0.1129, step time: 0.2520\n",
      "143/281, train_loss: 0.1092, step time: 0.2540\n",
      "144/281, train_loss: 0.0521, step time: 0.2573\n",
      "145/281, train_loss: 0.1224, step time: 0.2624\n",
      "146/281, train_loss: 0.0528, step time: 0.2553\n",
      "147/281, train_loss: 0.2194, step time: 0.2576\n",
      "148/281, train_loss: 0.0588, step time: 0.2575\n",
      "149/281, train_loss: 0.0599, step time: 0.2511\n",
      "150/281, train_loss: 0.1362, step time: 0.2721\n",
      "151/281, train_loss: 0.0720, step time: 0.2599\n",
      "152/281, train_loss: 0.0861, step time: 0.2514\n",
      "153/281, train_loss: 0.0628, step time: 0.2541\n",
      "154/281, train_loss: 0.2930, step time: 0.2550\n",
      "155/281, train_loss: 0.0746, step time: 0.2522\n",
      "156/281, train_loss: 0.0629, step time: 0.2512\n",
      "157/281, train_loss: 0.0961, step time: 0.2558\n",
      "158/281, train_loss: 0.2221, step time: 0.2585\n",
      "159/281, train_loss: 0.0790, step time: 0.2513\n",
      "160/281, train_loss: 0.1435, step time: 0.2520\n",
      "161/281, train_loss: 0.1328, step time: 0.2519\n",
      "162/281, train_loss: 0.0664, step time: 0.2599\n",
      "163/281, train_loss: 0.0820, step time: 0.2596\n",
      "164/281, train_loss: 0.0699, step time: 0.2560\n",
      "165/281, train_loss: 0.2624, step time: 0.2551\n",
      "166/281, train_loss: 0.0616, step time: 0.2547\n",
      "167/281, train_loss: 0.2601, step time: 0.2527\n",
      "168/281, train_loss: 0.0575, step time: 0.2527\n",
      "169/281, train_loss: 0.0776, step time: 0.2536\n",
      "170/281, train_loss: 0.1058, step time: 0.2545\n",
      "171/281, train_loss: 0.0509, step time: 0.2550\n",
      "172/281, train_loss: 0.0764, step time: 0.2565\n",
      "173/281, train_loss: 0.2393, step time: 0.2562\n",
      "174/281, train_loss: 0.0901, step time: 0.2536\n",
      "175/281, train_loss: 0.2381, step time: 0.2543\n",
      "176/281, train_loss: 0.1071, step time: 0.2555\n",
      "177/281, train_loss: 0.0932, step time: 0.2506\n",
      "178/281, train_loss: 0.0748, step time: 0.2574\n",
      "179/281, train_loss: 0.2270, step time: 0.2538\n",
      "180/281, train_loss: 0.0827, step time: 0.2546\n",
      "181/281, train_loss: 0.0531, step time: 0.2554\n",
      "182/281, train_loss: 0.0921, step time: 0.2554\n",
      "183/281, train_loss: 0.0785, step time: 0.2572\n",
      "184/281, train_loss: 0.0597, step time: 0.2501\n",
      "185/281, train_loss: 0.0635, step time: 0.2511\n",
      "186/281, train_loss: 0.0699, step time: 0.2545\n",
      "187/281, train_loss: 0.2529, step time: 0.2545\n",
      "188/281, train_loss: 0.1318, step time: 0.2576\n",
      "189/281, train_loss: 0.0528, step time: 0.2531\n",
      "190/281, train_loss: 0.0690, step time: 0.2588\n",
      "191/281, train_loss: 0.0650, step time: 0.2570\n",
      "192/281, train_loss: 0.0519, step time: 0.2551\n",
      "193/281, train_loss: 0.0754, step time: 0.2538\n",
      "194/281, train_loss: 0.0763, step time: 0.2535\n",
      "195/281, train_loss: 0.0588, step time: 0.2614\n",
      "196/281, train_loss: 0.0695, step time: 0.2520\n",
      "197/281, train_loss: 0.1074, step time: 0.2569\n",
      "198/281, train_loss: 0.2435, step time: 0.2585\n",
      "199/281, train_loss: 0.0907, step time: 0.2574\n",
      "200/281, train_loss: 0.0412, step time: 0.2534\n",
      "201/281, train_loss: 0.0803, step time: 0.2519\n",
      "202/281, train_loss: 0.0704, step time: 0.2523\n",
      "203/281, train_loss: 0.0667, step time: 0.2708\n",
      "204/281, train_loss: 0.2492, step time: 0.2563\n",
      "205/281, train_loss: 0.0593, step time: 0.2592\n",
      "206/281, train_loss: 0.0969, step time: 0.2573\n",
      "207/281, train_loss: 0.0527, step time: 0.2578\n",
      "208/281, train_loss: 0.0962, step time: 0.2567\n",
      "209/281, train_loss: 0.2270, step time: 0.2547\n",
      "210/281, train_loss: 0.0464, step time: 0.2561\n",
      "211/281, train_loss: 0.2480, step time: 0.2581\n",
      "212/281, train_loss: 0.0503, step time: 0.2545\n",
      "213/281, train_loss: 0.0677, step time: 0.2566\n",
      "214/281, train_loss: 0.0884, step time: 0.2511\n",
      "215/281, train_loss: 0.0647, step time: 0.2505\n",
      "216/281, train_loss: 0.0443, step time: 0.2506\n",
      "217/281, train_loss: 0.0609, step time: 0.2588\n",
      "218/281, train_loss: 0.1082, step time: 0.2520\n",
      "219/281, train_loss: 0.0511, step time: 0.2600\n",
      "220/281, train_loss: 0.0741, step time: 0.2522\n",
      "221/281, train_loss: 0.1091, step time: 0.2602\n",
      "222/281, train_loss: 0.0545, step time: 0.2569\n",
      "223/281, train_loss: 0.0382, step time: 0.2579\n",
      "224/281, train_loss: 0.0859, step time: 0.2572\n",
      "225/281, train_loss: 0.0766, step time: 0.2593\n",
      "226/281, train_loss: 0.1039, step time: 0.2575\n",
      "227/281, train_loss: 0.0940, step time: 0.2566\n",
      "228/281, train_loss: 0.0851, step time: 0.2476\n",
      "229/281, train_loss: 0.0906, step time: 0.2588\n",
      "230/281, train_loss: 0.0622, step time: 0.2528\n",
      "231/281, train_loss: 0.0789, step time: 0.2518\n",
      "232/281, train_loss: 0.0646, step time: 0.2520\n",
      "233/281, train_loss: 0.0626, step time: 0.2471\n",
      "234/281, train_loss: 0.0606, step time: 0.2479\n",
      "235/281, train_loss: 0.0943, step time: 0.2485\n",
      "236/281, train_loss: 0.0436, step time: 0.2533\n",
      "237/281, train_loss: 0.0539, step time: 0.2492\n",
      "238/281, train_loss: 0.1229, step time: 0.2510\n",
      "239/281, train_loss: 0.0908, step time: 0.2558\n",
      "240/281, train_loss: 0.0525, step time: 0.2552\n",
      "241/281, train_loss: 0.0579, step time: 0.2557\n",
      "242/281, train_loss: 0.0864, step time: 0.2535\n",
      "243/281, train_loss: 0.0667, step time: 0.2466\n",
      "244/281, train_loss: 0.0883, step time: 0.2562\n",
      "245/281, train_loss: 0.0922, step time: 0.2570\n",
      "246/281, train_loss: 0.2294, step time: 0.2569\n",
      "247/281, train_loss: 0.1020, step time: 0.2507\n",
      "248/281, train_loss: 0.0983, step time: 0.2550\n",
      "249/281, train_loss: 0.0541, step time: 0.2524\n",
      "250/281, train_loss: 0.1349, step time: 0.2506\n",
      "251/281, train_loss: 0.0701, step time: 0.2518\n",
      "252/281, train_loss: 0.0511, step time: 0.2591\n",
      "253/281, train_loss: 0.0978, step time: 0.2586\n",
      "254/281, train_loss: 0.2459, step time: 0.2563\n",
      "255/281, train_loss: 0.0797, step time: 0.2526\n",
      "256/281, train_loss: 0.3977, step time: 0.2588\n",
      "257/281, train_loss: 0.0891, step time: 0.2580\n",
      "258/281, train_loss: 0.2132, step time: 0.2557\n",
      "259/281, train_loss: 0.0922, step time: 0.2568\n",
      "260/281, train_loss: 0.2667, step time: 0.2598\n",
      "261/281, train_loss: 0.2056, step time: 0.2567\n",
      "262/281, train_loss: 0.0758, step time: 0.2542\n",
      "263/281, train_loss: 0.0487, step time: 0.2572\n",
      "264/281, train_loss: 0.0765, step time: 0.2565\n",
      "265/281, train_loss: 0.1035, step time: 0.2561\n",
      "266/281, train_loss: 0.0783, step time: 0.2590\n",
      "267/281, train_loss: 0.0775, step time: 0.2532\n",
      "268/281, train_loss: 0.2448, step time: 0.2571\n",
      "269/281, train_loss: 0.0704, step time: 0.2577\n",
      "270/281, train_loss: 0.0597, step time: 0.2573\n",
      "271/281, train_loss: 0.0633, step time: 0.2560\n",
      "272/281, train_loss: 0.0517, step time: 0.2585\n",
      "273/281, train_loss: 0.0878, step time: 0.2559\n",
      "274/281, train_loss: 0.0934, step time: 0.2606\n",
      "275/281, train_loss: 0.0688, step time: 0.2565\n",
      "276/281, train_loss: 0.0750, step time: 0.2589\n",
      "277/281, train_loss: 0.2378, step time: 0.2588\n",
      "278/281, train_loss: 0.0713, step time: 0.2562\n",
      "279/281, train_loss: 0.0596, step time: 0.2523\n",
      "280/281, train_loss: 0.2194, step time: 0.2512\n",
      "281/281, train_loss: 0.0608, step time: 0.2543\n",
      "282/281, train_loss: 0.3995, step time: 0.1524\n",
      "epoch 82 average loss: 0.1023\n",
      "current epoch: 82 current mean dice: 0.8901 tc: 0.8797 wt: 0.9217 et: 0.8784\n",
      "best mean dice: 0.8918 at epoch: 81\n",
      "time consuming of epoch 82 is: 436.4582\n",
      "----------\n",
      "epoch 83/200\n",
      "1/281, train_loss: 0.0935, step time: 0.2649\n",
      "2/281, train_loss: 0.0638, step time: 0.2543\n",
      "3/281, train_loss: 0.2253, step time: 0.2528\n",
      "4/281, train_loss: 0.0736, step time: 0.2523\n",
      "5/281, train_loss: 0.0815, step time: 0.2525\n",
      "6/281, train_loss: 0.0527, step time: 0.2495\n",
      "7/281, train_loss: 0.0585, step time: 0.2534\n",
      "8/281, train_loss: 0.0771, step time: 0.2506\n",
      "9/281, train_loss: 0.0648, step time: 0.2590\n",
      "10/281, train_loss: 0.0981, step time: 0.2642\n",
      "11/281, train_loss: 0.1153, step time: 0.2621\n",
      "12/281, train_loss: 0.0574, step time: 0.2569\n",
      "13/281, train_loss: 0.0906, step time: 0.2583\n",
      "14/281, train_loss: 0.0567, step time: 0.2539\n",
      "15/281, train_loss: 0.0999, step time: 0.2497\n",
      "16/281, train_loss: 0.0862, step time: 0.2575\n",
      "17/281, train_loss: 0.0588, step time: 0.2564\n",
      "18/281, train_loss: 0.0488, step time: 0.2504\n",
      "19/281, train_loss: 0.1033, step time: 0.2615\n",
      "20/281, train_loss: 0.2706, step time: 0.2591\n",
      "21/281, train_loss: 0.2167, step time: 0.2582\n",
      "22/281, train_loss: 0.0464, step time: 0.2573\n",
      "23/281, train_loss: 0.2292, step time: 0.2620\n",
      "24/281, train_loss: 0.0821, step time: 0.2591\n",
      "25/281, train_loss: 0.0663, step time: 0.2568\n",
      "26/281, train_loss: 0.0830, step time: 0.2585\n",
      "27/281, train_loss: 0.0421, step time: 0.2504\n",
      "28/281, train_loss: 0.0599, step time: 0.2511\n",
      "29/281, train_loss: 0.0964, step time: 0.2508\n",
      "30/281, train_loss: 0.0607, step time: 0.2525\n",
      "31/281, train_loss: 0.0718, step time: 0.2478\n",
      "32/281, train_loss: 0.0431, step time: 0.2505\n",
      "33/281, train_loss: 0.0711, step time: 0.2505\n",
      "34/281, train_loss: 0.0943, step time: 0.2586\n",
      "35/281, train_loss: 0.0720, step time: 0.2554\n",
      "36/281, train_loss: 0.1120, step time: 0.2587\n",
      "37/281, train_loss: 0.0854, step time: 0.2650\n",
      "38/281, train_loss: 0.0829, step time: 0.2590\n",
      "39/281, train_loss: 0.1260, step time: 0.2643\n",
      "40/281, train_loss: 0.0587, step time: 0.2789\n",
      "41/281, train_loss: 0.0558, step time: 0.2566\n",
      "42/281, train_loss: 0.0536, step time: 0.2562\n",
      "43/281, train_loss: 0.0962, step time: 0.2585\n",
      "44/281, train_loss: 0.0780, step time: 0.2592\n",
      "45/281, train_loss: 0.0771, step time: 0.2584\n",
      "46/281, train_loss: 0.0595, step time: 0.2545\n",
      "47/281, train_loss: 0.0375, step time: 0.2601\n",
      "48/281, train_loss: 0.2404, step time: 0.2532\n",
      "49/281, train_loss: 0.0632, step time: 0.2530\n",
      "50/281, train_loss: 0.0445, step time: 0.2507\n",
      "51/281, train_loss: 0.1064, step time: 0.2510\n",
      "52/281, train_loss: 0.0858, step time: 0.2534\n",
      "53/281, train_loss: 0.0744, step time: 0.2461\n",
      "54/281, train_loss: 0.0728, step time: 0.2514\n",
      "55/281, train_loss: 0.0591, step time: 0.2517\n",
      "56/281, train_loss: 0.2299, step time: 0.2532\n",
      "57/281, train_loss: 0.0678, step time: 0.2484\n",
      "58/281, train_loss: 0.0981, step time: 0.2479\n",
      "59/281, train_loss: 0.0837, step time: 0.2522\n",
      "60/281, train_loss: 0.0757, step time: 0.2511\n",
      "61/281, train_loss: 0.0429, step time: 0.2535\n",
      "62/281, train_loss: 0.0726, step time: 0.2573\n",
      "63/281, train_loss: 0.2416, step time: 0.2571\n",
      "64/281, train_loss: 0.0550, step time: 0.2578\n",
      "65/281, train_loss: 0.0539, step time: 0.2522\n",
      "66/281, train_loss: 0.1028, step time: 0.2547\n",
      "67/281, train_loss: 0.0907, step time: 0.2532\n",
      "68/281, train_loss: 0.0531, step time: 0.2511\n",
      "69/281, train_loss: 0.0881, step time: 0.2552\n",
      "70/281, train_loss: 0.0715, step time: 0.2565\n",
      "71/281, train_loss: 0.0695, step time: 0.2590\n",
      "72/281, train_loss: 0.0592, step time: 0.2552\n",
      "73/281, train_loss: 0.2037, step time: 0.2532\n",
      "74/281, train_loss: 0.0646, step time: 0.2577\n",
      "75/281, train_loss: 0.0654, step time: 0.2580\n",
      "76/281, train_loss: 0.0719, step time: 0.2535\n",
      "77/281, train_loss: 0.0800, step time: 0.2577\n",
      "78/281, train_loss: 0.1337, step time: 0.2487\n",
      "79/281, train_loss: 0.2449, step time: 0.2487\n",
      "80/281, train_loss: 0.0802, step time: 0.2607\n",
      "81/281, train_loss: 0.1190, step time: 0.2552\n",
      "82/281, train_loss: 0.0795, step time: 0.2538\n",
      "83/281, train_loss: 0.0666, step time: 0.2553\n",
      "84/281, train_loss: 0.0669, step time: 0.2528\n",
      "85/281, train_loss: 0.0771, step time: 0.2535\n",
      "86/281, train_loss: 0.0965, step time: 0.2563\n",
      "87/281, train_loss: 0.0848, step time: 0.2593\n",
      "88/281, train_loss: 0.0458, step time: 0.2527\n",
      "89/281, train_loss: 0.0863, step time: 0.2559\n",
      "90/281, train_loss: 0.2180, step time: 0.2569\n",
      "91/281, train_loss: 0.0901, step time: 0.2577\n",
      "92/281, train_loss: 0.0393, step time: 0.2579\n",
      "93/281, train_loss: 0.0976, step time: 0.2502\n",
      "94/281, train_loss: 0.0594, step time: 0.2600\n",
      "95/281, train_loss: 0.0970, step time: 0.2569\n",
      "96/281, train_loss: 0.0951, step time: 0.2529\n",
      "97/281, train_loss: 0.0540, step time: 0.2535\n",
      "98/281, train_loss: 0.0851, step time: 0.2562\n",
      "99/281, train_loss: 0.0454, step time: 0.2580\n",
      "100/281, train_loss: 0.0707, step time: 0.2600\n",
      "101/281, train_loss: 0.0745, step time: 0.2617\n",
      "102/281, train_loss: 0.0609, step time: 0.2553\n",
      "103/281, train_loss: 0.0509, step time: 0.2558\n",
      "104/281, train_loss: 0.0767, step time: 0.2571\n",
      "105/281, train_loss: 0.2160, step time: 0.2537\n",
      "106/281, train_loss: 0.2351, step time: 0.2506\n",
      "107/281, train_loss: 0.2364, step time: 0.2514\n",
      "108/281, train_loss: 0.0728, step time: 0.2529\n",
      "109/281, train_loss: 0.0396, step time: 0.2511\n",
      "110/281, train_loss: 0.0894, step time: 0.2564\n",
      "111/281, train_loss: 0.0705, step time: 0.2559\n",
      "112/281, train_loss: 0.0739, step time: 0.2571\n",
      "113/281, train_loss: 0.1067, step time: 0.2529\n",
      "114/281, train_loss: 0.0791, step time: 0.2541\n",
      "115/281, train_loss: 0.1100, step time: 0.2556\n",
      "116/281, train_loss: 0.0968, step time: 0.2569\n",
      "117/281, train_loss: 0.0885, step time: 0.2540\n",
      "118/281, train_loss: 0.0529, step time: 0.2561\n",
      "119/281, train_loss: 0.0723, step time: 0.2566\n",
      "120/281, train_loss: 0.0462, step time: 0.2571\n",
      "121/281, train_loss: 0.0577, step time: 0.2541\n",
      "122/281, train_loss: 0.0523, step time: 0.2527\n",
      "123/281, train_loss: 0.0954, step time: 0.2525\n",
      "124/281, train_loss: 0.2238, step time: 0.2513\n",
      "125/281, train_loss: 0.2497, step time: 0.2559\n",
      "126/281, train_loss: 0.0765, step time: 0.2517\n",
      "127/281, train_loss: 0.0710, step time: 0.2552\n",
      "128/281, train_loss: 0.0837, step time: 0.2563\n",
      "129/281, train_loss: 0.0642, step time: 0.2573\n",
      "130/281, train_loss: 0.1232, step time: 0.2568\n",
      "131/281, train_loss: 0.0343, step time: 0.2510\n",
      "132/281, train_loss: 0.0717, step time: 0.2485\n",
      "133/281, train_loss: 0.1233, step time: 0.2574\n",
      "134/281, train_loss: 0.0530, step time: 0.2562\n",
      "135/281, train_loss: 0.1060, step time: 0.2558\n",
      "136/281, train_loss: 0.0810, step time: 0.2556\n",
      "137/281, train_loss: 0.0845, step time: 0.2523\n",
      "138/281, train_loss: 0.0506, step time: 0.2531\n",
      "139/281, train_loss: 0.0980, step time: 0.2572\n",
      "140/281, train_loss: 0.2105, step time: 0.2509\n",
      "141/281, train_loss: 0.0659, step time: 0.2545\n",
      "142/281, train_loss: 0.0747, step time: 0.2503\n",
      "143/281, train_loss: 0.0650, step time: 0.2519\n",
      "144/281, train_loss: 0.0601, step time: 0.2508\n",
      "145/281, train_loss: 0.0302, step time: 0.2515\n",
      "146/281, train_loss: 0.0629, step time: 0.2541\n",
      "147/281, train_loss: 0.2554, step time: 0.2529\n",
      "148/281, train_loss: 0.0786, step time: 0.2534\n",
      "149/281, train_loss: 0.0648, step time: 0.2547\n",
      "150/281, train_loss: 0.0695, step time: 0.2621\n",
      "151/281, train_loss: 0.0317, step time: 0.2503\n",
      "152/281, train_loss: 0.0670, step time: 0.2498\n",
      "153/281, train_loss: 0.2083, step time: 0.2558\n",
      "154/281, train_loss: 0.0784, step time: 0.2539\n",
      "155/281, train_loss: 0.0408, step time: 0.2496\n",
      "156/281, train_loss: 0.0655, step time: 0.2516\n",
      "157/281, train_loss: 0.0882, step time: 0.2508\n",
      "158/281, train_loss: 0.0833, step time: 0.2534\n",
      "159/281, train_loss: 0.2306, step time: 0.2574\n",
      "160/281, train_loss: 0.0838, step time: 0.2634\n",
      "161/281, train_loss: 0.1062, step time: 0.2511\n",
      "162/281, train_loss: 0.0889, step time: 0.2568\n",
      "163/281, train_loss: 0.0656, step time: 0.2527\n",
      "164/281, train_loss: 0.1087, step time: 0.2507\n",
      "165/281, train_loss: 0.2215, step time: 0.2530\n",
      "166/281, train_loss: 0.0832, step time: 0.2584\n",
      "167/281, train_loss: 0.0918, step time: 0.2571\n",
      "168/281, train_loss: 0.0665, step time: 0.2601\n",
      "169/281, train_loss: 0.1289, step time: 0.2542\n",
      "170/281, train_loss: 0.0492, step time: 0.2567\n",
      "171/281, train_loss: 0.2353, step time: 0.2547\n",
      "172/281, train_loss: 0.0839, step time: 0.2523\n",
      "173/281, train_loss: 0.0665, step time: 0.2646\n",
      "174/281, train_loss: 0.0737, step time: 0.2600\n",
      "175/281, train_loss: 0.2064, step time: 0.2603\n",
      "176/281, train_loss: 0.0666, step time: 0.2608\n",
      "177/281, train_loss: 0.0602, step time: 0.2643\n",
      "178/281, train_loss: 0.0808, step time: 0.2578\n",
      "179/281, train_loss: 0.0552, step time: 0.2600\n",
      "180/281, train_loss: 0.0944, step time: 0.2516\n",
      "181/281, train_loss: 0.0825, step time: 0.2531\n",
      "182/281, train_loss: 0.0942, step time: 0.2539\n",
      "183/281, train_loss: 0.0859, step time: 0.2547\n",
      "184/281, train_loss: 0.0927, step time: 0.2589\n",
      "185/281, train_loss: 0.0751, step time: 0.2574\n",
      "186/281, train_loss: 0.2295, step time: 0.2500\n",
      "187/281, train_loss: 0.0696, step time: 0.2555\n",
      "188/281, train_loss: 0.0982, step time: 0.2574\n",
      "189/281, train_loss: 0.0907, step time: 0.2567\n",
      "190/281, train_loss: 0.4446, step time: 0.2501\n",
      "191/281, train_loss: 0.0775, step time: 0.2492\n",
      "192/281, train_loss: 0.0779, step time: 0.2719\n",
      "193/281, train_loss: 0.0936, step time: 0.2612\n",
      "194/281, train_loss: 0.0483, step time: 0.2548\n",
      "195/281, train_loss: 0.0779, step time: 0.2515\n",
      "196/281, train_loss: 0.0561, step time: 0.2550\n",
      "197/281, train_loss: 0.0603, step time: 0.2539\n",
      "198/281, train_loss: 0.0725, step time: 0.2577\n",
      "199/281, train_loss: 0.2443, step time: 0.2593\n",
      "200/281, train_loss: 0.2204, step time: 0.2615\n",
      "201/281, train_loss: 0.0866, step time: 0.2820\n",
      "202/281, train_loss: 0.1028, step time: 0.2529\n",
      "203/281, train_loss: 0.0803, step time: 0.2479\n",
      "204/281, train_loss: 0.0650, step time: 0.2551\n",
      "205/281, train_loss: 0.0733, step time: 0.2533\n",
      "206/281, train_loss: 0.0836, step time: 0.2556\n",
      "207/281, train_loss: 0.1249, step time: 0.2563\n",
      "208/281, train_loss: 0.0536, step time: 0.2564\n",
      "209/281, train_loss: 0.2424, step time: 0.2510\n",
      "210/281, train_loss: 0.2382, step time: 0.2535\n",
      "211/281, train_loss: 0.0754, step time: 0.2517\n",
      "212/281, train_loss: 0.0863, step time: 0.2472\n",
      "213/281, train_loss: 0.0447, step time: 0.2536\n",
      "214/281, train_loss: 0.0700, step time: 0.2519\n",
      "215/281, train_loss: 0.0739, step time: 0.2545\n",
      "216/281, train_loss: 0.0962, step time: 0.2590\n",
      "217/281, train_loss: 0.0638, step time: 0.2553\n",
      "218/281, train_loss: 0.0940, step time: 0.2543\n",
      "219/281, train_loss: 0.0623, step time: 0.2522\n",
      "220/281, train_loss: 0.0603, step time: 0.2548\n",
      "221/281, train_loss: 0.0799, step time: 0.2542\n",
      "222/281, train_loss: 0.0618, step time: 0.2569\n",
      "223/281, train_loss: 0.0542, step time: 0.2492\n",
      "224/281, train_loss: 0.3895, step time: 0.2531\n",
      "225/281, train_loss: 0.0629, step time: 0.2528\n",
      "226/281, train_loss: 0.2204, step time: 0.2551\n",
      "227/281, train_loss: 0.2604, step time: 0.2511\n",
      "228/281, train_loss: 0.0979, step time: 0.2501\n",
      "229/281, train_loss: 0.0916, step time: 0.2473\n",
      "230/281, train_loss: 0.2398, step time: 0.2496\n",
      "231/281, train_loss: 0.1109, step time: 0.2505\n",
      "232/281, train_loss: 0.2500, step time: 0.2511\n",
      "233/281, train_loss: 0.0646, step time: 0.2513\n",
      "234/281, train_loss: 0.0419, step time: 0.2524\n",
      "235/281, train_loss: 0.0712, step time: 0.2493\n",
      "236/281, train_loss: 0.0567, step time: 0.2518\n",
      "237/281, train_loss: 0.0542, step time: 0.2542\n",
      "238/281, train_loss: 0.2516, step time: 0.2537\n",
      "239/281, train_loss: 0.0988, step time: 0.2484\n",
      "240/281, train_loss: 0.0899, step time: 0.2517\n",
      "241/281, train_loss: 0.0809, step time: 0.2538\n",
      "242/281, train_loss: 0.0312, step time: 0.2522\n",
      "243/281, train_loss: 0.0818, step time: 0.2550\n",
      "244/281, train_loss: 0.0627, step time: 0.2521\n",
      "245/281, train_loss: 0.2092, step time: 0.2526\n",
      "246/281, train_loss: 0.0805, step time: 0.2483\n",
      "247/281, train_loss: 0.2376, step time: 0.2464\n",
      "248/281, train_loss: 0.2239, step time: 0.2466\n",
      "249/281, train_loss: 0.0652, step time: 0.2534\n",
      "250/281, train_loss: 0.0631, step time: 0.2473\n",
      "251/281, train_loss: 0.0760, step time: 0.2537\n",
      "252/281, train_loss: 0.1382, step time: 0.2506\n",
      "253/281, train_loss: 0.0598, step time: 0.2470\n",
      "254/281, train_loss: 0.2279, step time: 0.2481\n",
      "255/281, train_loss: 0.1059, step time: 0.2497\n",
      "256/281, train_loss: 0.0711, step time: 0.2536\n",
      "257/281, train_loss: 0.0750, step time: 0.2528\n",
      "258/281, train_loss: 0.0615, step time: 0.2481\n",
      "259/281, train_loss: 0.0662, step time: 0.2479\n",
      "260/281, train_loss: 0.0826, step time: 0.2455\n",
      "261/281, train_loss: 0.0592, step time: 0.2425\n",
      "262/281, train_loss: 0.1098, step time: 0.2460\n",
      "263/281, train_loss: 0.0470, step time: 0.2478\n",
      "264/281, train_loss: 0.0713, step time: 0.2482\n",
      "265/281, train_loss: 0.1161, step time: 0.2462\n",
      "266/281, train_loss: 0.0668, step time: 0.2452\n",
      "267/281, train_loss: 0.0564, step time: 0.2507\n",
      "268/281, train_loss: 0.0733, step time: 0.2507\n",
      "269/281, train_loss: 0.2527, step time: 0.2492\n",
      "270/281, train_loss: 0.0491, step time: 0.2446\n",
      "271/281, train_loss: 0.2604, step time: 0.2468\n",
      "272/281, train_loss: 0.0696, step time: 0.2478\n",
      "273/281, train_loss: 0.0544, step time: 0.2455\n",
      "274/281, train_loss: 0.2482, step time: 0.2411\n",
      "275/281, train_loss: 0.0633, step time: 0.2493\n",
      "276/281, train_loss: 0.0693, step time: 0.2448\n",
      "277/281, train_loss: 0.1070, step time: 0.2508\n",
      "278/281, train_loss: 0.0782, step time: 0.2446\n",
      "279/281, train_loss: 0.0426, step time: 0.2576\n",
      "280/281, train_loss: 0.0344, step time: 0.2510\n",
      "281/281, train_loss: 0.0705, step time: 0.2456\n",
      "282/281, train_loss: 0.0600, step time: 0.1478\n",
      "epoch 83 average loss: 0.0990\n",
      "current epoch: 83 current mean dice: 0.8909 tc: 0.8830 wt: 0.9217 et: 0.8769\n",
      "best mean dice: 0.8918 at epoch: 81\n",
      "time consuming of epoch 83 is: 357.2840\n",
      "----------\n",
      "epoch 84/200\n",
      "1/281, train_loss: 0.2407, step time: 0.2627\n",
      "2/281, train_loss: 0.0613, step time: 0.2582\n",
      "3/281, train_loss: 0.0520, step time: 0.2595\n",
      "4/281, train_loss: 0.2147, step time: 0.2559\n",
      "5/281, train_loss: 0.0553, step time: 0.2508\n",
      "6/281, train_loss: 0.2567, step time: 0.2611\n",
      "7/281, train_loss: 0.0634, step time: 0.2519\n",
      "8/281, train_loss: 0.0823, step time: 0.2509\n",
      "9/281, train_loss: 0.0775, step time: 0.2725\n",
      "10/281, train_loss: 0.0706, step time: 0.2586\n",
      "11/281, train_loss: 0.0511, step time: 0.2543\n",
      "12/281, train_loss: 0.0516, step time: 0.2539\n",
      "13/281, train_loss: 0.0502, step time: 0.2570\n",
      "14/281, train_loss: 0.0553, step time: 0.2560\n",
      "15/281, train_loss: 0.0314, step time: 0.2511\n",
      "16/281, train_loss: 0.0892, step time: 0.2503\n",
      "17/281, train_loss: 0.0948, step time: 0.2542\n",
      "18/281, train_loss: 0.0494, step time: 0.2637\n",
      "19/281, train_loss: 0.1184, step time: 0.2456\n",
      "20/281, train_loss: 0.0710, step time: 0.2446\n",
      "21/281, train_loss: 0.2090, step time: 0.2504\n",
      "22/281, train_loss: 0.0723, step time: 0.2560\n",
      "23/281, train_loss: 0.0477, step time: 0.2568\n",
      "24/281, train_loss: 0.0411, step time: 0.2800\n",
      "25/281, train_loss: 0.0839, step time: 0.2679\n",
      "26/281, train_loss: 0.1281, step time: 0.2494\n",
      "27/281, train_loss: 0.2665, step time: 0.2481\n",
      "28/281, train_loss: 0.0467, step time: 0.2537\n",
      "29/281, train_loss: 0.0631, step time: 0.2563\n",
      "30/281, train_loss: 0.0383, step time: 0.2494\n",
      "31/281, train_loss: 0.0860, step time: 0.2541\n",
      "32/281, train_loss: 0.0417, step time: 0.2533\n",
      "33/281, train_loss: 0.2330, step time: 0.2592\n",
      "34/281, train_loss: 0.0546, step time: 0.2501\n",
      "35/281, train_loss: 0.0748, step time: 0.2506\n",
      "36/281, train_loss: 0.1101, step time: 0.2672\n",
      "37/281, train_loss: 0.2179, step time: 0.2537\n",
      "38/281, train_loss: 0.0727, step time: 0.2534\n",
      "39/281, train_loss: 0.1177, step time: 0.2462\n",
      "40/281, train_loss: 0.1022, step time: 0.2480\n",
      "41/281, train_loss: 0.0620, step time: 0.2492\n",
      "42/281, train_loss: 0.0652, step time: 0.2529\n",
      "43/281, train_loss: 0.1041, step time: 0.2586\n",
      "44/281, train_loss: 0.0718, step time: 0.2535\n",
      "45/281, train_loss: 0.0728, step time: 0.2456\n",
      "46/281, train_loss: 0.0682, step time: 0.2510\n",
      "47/281, train_loss: 0.0649, step time: 0.2590\n",
      "48/281, train_loss: 0.0961, step time: 0.2805\n",
      "49/281, train_loss: 0.0902, step time: 0.2551\n",
      "50/281, train_loss: 0.0717, step time: 0.2535\n",
      "51/281, train_loss: 0.0975, step time: 0.2541\n",
      "52/281, train_loss: 0.0541, step time: 0.2525\n",
      "53/281, train_loss: 0.0637, step time: 0.2488\n",
      "54/281, train_loss: 0.2329, step time: 0.2577\n",
      "55/281, train_loss: 0.0726, step time: 0.2535\n",
      "56/281, train_loss: 0.0401, step time: 0.2541\n",
      "57/281, train_loss: 0.0959, step time: 0.2510\n",
      "58/281, train_loss: 0.0575, step time: 0.2523\n",
      "59/281, train_loss: 0.2318, step time: 0.2599\n",
      "60/281, train_loss: 0.0727, step time: 0.2516\n",
      "61/281, train_loss: 0.1312, step time: 0.2572\n",
      "62/281, train_loss: 0.0655, step time: 0.2532\n",
      "63/281, train_loss: 0.0680, step time: 0.2537\n",
      "64/281, train_loss: 0.0681, step time: 0.2513\n",
      "65/281, train_loss: 0.1051, step time: 0.2522\n",
      "66/281, train_loss: 0.2756, step time: 0.2553\n",
      "67/281, train_loss: 0.0661, step time: 0.2491\n",
      "68/281, train_loss: 0.0760, step time: 0.2452\n",
      "69/281, train_loss: 0.0852, step time: 0.2495\n",
      "70/281, train_loss: 0.0874, step time: 0.2534\n",
      "71/281, train_loss: 0.1039, step time: 0.2501\n",
      "72/281, train_loss: 0.1243, step time: 0.2515\n",
      "73/281, train_loss: 0.0533, step time: 0.2475\n",
      "74/281, train_loss: 0.2306, step time: 0.2489\n",
      "75/281, train_loss: 0.0555, step time: 0.2548\n",
      "76/281, train_loss: 0.0663, step time: 0.2537\n",
      "77/281, train_loss: 0.2473, step time: 0.2568\n",
      "78/281, train_loss: 0.2154, step time: 0.2521\n",
      "79/281, train_loss: 0.0748, step time: 0.2527\n",
      "80/281, train_loss: 0.0565, step time: 0.2484\n",
      "81/281, train_loss: 0.0538, step time: 0.2582\n",
      "82/281, train_loss: 0.0714, step time: 0.2513\n",
      "83/281, train_loss: 0.0520, step time: 0.2498\n",
      "84/281, train_loss: 0.2229, step time: 0.2508\n",
      "85/281, train_loss: 0.0630, step time: 0.2502\n",
      "86/281, train_loss: 0.0823, step time: 0.2511\n",
      "87/281, train_loss: 0.0809, step time: 0.2482\n",
      "88/281, train_loss: 0.2203, step time: 0.2596\n",
      "89/281, train_loss: 0.0680, step time: 0.2533\n",
      "90/281, train_loss: 0.0746, step time: 0.2481\n",
      "91/281, train_loss: 0.0662, step time: 0.2547\n",
      "92/281, train_loss: 0.0614, step time: 0.2498\n",
      "93/281, train_loss: 0.1063, step time: 0.2512\n",
      "94/281, train_loss: 0.0343, step time: 0.2540\n",
      "95/281, train_loss: 0.0679, step time: 0.2503\n",
      "96/281, train_loss: 0.0531, step time: 0.2537\n",
      "97/281, train_loss: 0.0815, step time: 0.2534\n",
      "98/281, train_loss: 0.0691, step time: 0.2519\n",
      "99/281, train_loss: 0.1224, step time: 0.2552\n",
      "100/281, train_loss: 0.0488, step time: 0.2518\n",
      "101/281, train_loss: 0.1073, step time: 0.2480\n",
      "102/281, train_loss: 0.0516, step time: 0.2511\n",
      "103/281, train_loss: 0.0613, step time: 0.2557\n",
      "104/281, train_loss: 0.0855, step time: 0.2484\n",
      "105/281, train_loss: 0.2343, step time: 0.2497\n",
      "106/281, train_loss: 0.2315, step time: 0.2561\n",
      "107/281, train_loss: 0.2253, step time: 0.2549\n",
      "108/281, train_loss: 0.0536, step time: 0.2569\n",
      "109/281, train_loss: 0.0728, step time: 0.2607\n",
      "110/281, train_loss: 0.0615, step time: 0.2555\n",
      "111/281, train_loss: 0.0894, step time: 0.2587\n",
      "112/281, train_loss: 0.0728, step time: 0.2520\n",
      "113/281, train_loss: 0.0884, step time: 0.2492\n",
      "114/281, train_loss: 0.0864, step time: 0.2493\n",
      "115/281, train_loss: 0.2267, step time: 0.2582\n",
      "116/281, train_loss: 0.0744, step time: 0.2590\n",
      "117/281, train_loss: 0.0354, step time: 0.2512\n",
      "118/281, train_loss: 0.0821, step time: 0.2542\n",
      "119/281, train_loss: 0.0731, step time: 0.2561\n",
      "120/281, train_loss: 0.1200, step time: 0.2532\n",
      "121/281, train_loss: 0.0826, step time: 0.2502\n",
      "122/281, train_loss: 0.0703, step time: 0.2550\n",
      "123/281, train_loss: 0.0878, step time: 0.2517\n",
      "124/281, train_loss: 0.0652, step time: 0.2482\n",
      "125/281, train_loss: 0.2301, step time: 0.2592\n",
      "126/281, train_loss: 0.0790, step time: 0.2552\n",
      "127/281, train_loss: 0.0624, step time: 0.2522\n",
      "128/281, train_loss: 0.0720, step time: 0.2473\n",
      "129/281, train_loss: 0.0617, step time: 0.2560\n",
      "130/281, train_loss: 0.2307, step time: 0.2528\n",
      "131/281, train_loss: 0.0813, step time: 0.2546\n",
      "132/281, train_loss: 0.2440, step time: 0.2528\n",
      "133/281, train_loss: 0.0667, step time: 0.2526\n",
      "134/281, train_loss: 0.0849, step time: 0.2516\n",
      "135/281, train_loss: 0.0887, step time: 0.2529\n",
      "136/281, train_loss: 0.0760, step time: 0.2456\n",
      "137/281, train_loss: 0.2299, step time: 0.2539\n",
      "138/281, train_loss: 0.0642, step time: 0.2598\n",
      "139/281, train_loss: 0.0898, step time: 0.2516\n",
      "140/281, train_loss: 0.0905, step time: 0.2545\n",
      "141/281, train_loss: 0.0972, step time: 0.2464\n",
      "142/281, train_loss: 0.0927, step time: 0.2591\n",
      "143/281, train_loss: 0.0681, step time: 0.2581\n",
      "144/281, train_loss: 0.2516, step time: 0.2529\n",
      "145/281, train_loss: 0.0754, step time: 0.2577\n",
      "146/281, train_loss: 0.0838, step time: 0.2532\n",
      "147/281, train_loss: 0.0886, step time: 0.2502\n",
      "148/281, train_loss: 0.0700, step time: 0.2501\n",
      "149/281, train_loss: 0.0858, step time: 0.2549\n",
      "150/281, train_loss: 0.0822, step time: 0.2482\n",
      "151/281, train_loss: 0.2085, step time: 0.2495\n",
      "152/281, train_loss: 0.2456, step time: 0.2523\n",
      "153/281, train_loss: 0.0950, step time: 0.2519\n",
      "154/281, train_loss: 0.2674, step time: 0.2514\n",
      "155/281, train_loss: 0.0650, step time: 0.2501\n",
      "156/281, train_loss: 0.0746, step time: 0.2514\n",
      "157/281, train_loss: 0.0754, step time: 0.2526\n",
      "158/281, train_loss: 0.2423, step time: 0.2551\n",
      "159/281, train_loss: 0.0853, step time: 0.2498\n",
      "160/281, train_loss: 0.0737, step time: 0.2466\n",
      "161/281, train_loss: 0.0638, step time: 0.2468\n",
      "162/281, train_loss: 0.2511, step time: 0.2580\n",
      "163/281, train_loss: 0.0835, step time: 0.2501\n",
      "164/281, train_loss: 0.0748, step time: 0.2434\n",
      "165/281, train_loss: 0.0575, step time: 0.2427\n",
      "166/281, train_loss: 0.0901, step time: 0.2514\n",
      "167/281, train_loss: 0.1331, step time: 0.2535\n",
      "168/281, train_loss: 0.0750, step time: 0.2503\n",
      "169/281, train_loss: 0.0718, step time: 0.2551\n",
      "170/281, train_loss: 0.0658, step time: 0.2500\n",
      "171/281, train_loss: 0.0774, step time: 0.2556\n",
      "172/281, train_loss: 0.0678, step time: 0.2506\n",
      "173/281, train_loss: 0.0628, step time: 0.2517\n",
      "174/281, train_loss: 0.0946, step time: 0.2475\n",
      "175/281, train_loss: 0.0565, step time: 0.2457\n",
      "176/281, train_loss: 0.0767, step time: 0.2475\n",
      "177/281, train_loss: 0.0989, step time: 0.2513\n",
      "178/281, train_loss: 0.0737, step time: 0.2470\n",
      "179/281, train_loss: 0.1003, step time: 0.2530\n",
      "180/281, train_loss: 0.1180, step time: 0.2541\n",
      "181/281, train_loss: 0.0646, step time: 0.2438\n",
      "182/281, train_loss: 0.0603, step time: 0.2505\n",
      "183/281, train_loss: 0.0679, step time: 0.2498\n",
      "184/281, train_loss: 0.0920, step time: 0.2473\n",
      "185/281, train_loss: 0.2253, step time: 0.2537\n",
      "186/281, train_loss: 0.0707, step time: 0.2485\n",
      "187/281, train_loss: 0.0857, step time: 0.2479\n",
      "188/281, train_loss: 0.2361, step time: 0.2545\n",
      "189/281, train_loss: 0.0365, step time: 0.2504\n",
      "190/281, train_loss: 0.0444, step time: 0.2513\n",
      "191/281, train_loss: 0.0673, step time: 0.2543\n",
      "192/281, train_loss: 0.0537, step time: 0.2612\n",
      "193/281, train_loss: 0.2027, step time: 0.2600\n",
      "194/281, train_loss: 0.0814, step time: 0.2514\n",
      "195/281, train_loss: 0.0894, step time: 0.2547\n",
      "196/281, train_loss: 0.0762, step time: 0.2566\n",
      "197/281, train_loss: 0.0794, step time: 0.2533\n",
      "198/281, train_loss: 0.1035, step time: 0.2526\n",
      "199/281, train_loss: 0.0415, step time: 0.2485\n",
      "200/281, train_loss: 0.0938, step time: 0.2517\n",
      "201/281, train_loss: 0.0750, step time: 0.2478\n",
      "202/281, train_loss: 0.0513, step time: 0.2530\n",
      "203/281, train_loss: 0.0740, step time: 0.2519\n",
      "204/281, train_loss: 0.0819, step time: 0.2734\n",
      "205/281, train_loss: 0.1146, step time: 0.2489\n",
      "206/281, train_loss: 0.0738, step time: 0.2500\n",
      "207/281, train_loss: 0.0537, step time: 0.2534\n",
      "208/281, train_loss: 0.0686, step time: 0.2573\n",
      "209/281, train_loss: 0.0907, step time: 0.2530\n",
      "210/281, train_loss: 0.0678, step time: 0.2510\n",
      "211/281, train_loss: 0.0965, step time: 0.2522\n",
      "212/281, train_loss: 0.2414, step time: 0.2549\n",
      "213/281, train_loss: 0.0774, step time: 0.2519\n",
      "214/281, train_loss: 0.0897, step time: 0.2454\n",
      "215/281, train_loss: 0.0805, step time: 0.2451\n",
      "216/281, train_loss: 0.0963, step time: 0.2507\n",
      "217/281, train_loss: 0.0946, step time: 0.2555\n",
      "218/281, train_loss: 0.0722, step time: 0.2478\n",
      "219/281, train_loss: 0.1256, step time: 0.2503\n",
      "220/281, train_loss: 0.0497, step time: 0.2467\n",
      "221/281, train_loss: 0.0795, step time: 0.2491\n",
      "222/281, train_loss: 0.2253, step time: 0.2564\n",
      "223/281, train_loss: 0.1050, step time: 0.2525\n",
      "224/281, train_loss: 0.2354, step time: 0.2511\n",
      "225/281, train_loss: 0.2205, step time: 0.2544\n",
      "226/281, train_loss: 0.0612, step time: 0.2493\n",
      "227/281, train_loss: 0.0871, step time: 0.2479\n",
      "228/281, train_loss: 0.0921, step time: 0.2481\n",
      "229/281, train_loss: 0.0842, step time: 0.2526\n",
      "230/281, train_loss: 0.0418, step time: 0.2504\n",
      "231/281, train_loss: 0.0629, step time: 0.2511\n",
      "232/281, train_loss: 0.2461, step time: 0.2526\n",
      "233/281, train_loss: 0.1219, step time: 0.2498\n",
      "234/281, train_loss: 0.0619, step time: 0.2531\n",
      "235/281, train_loss: 0.0445, step time: 0.2543\n",
      "236/281, train_loss: 0.1011, step time: 0.2572\n",
      "237/281, train_loss: 0.2227, step time: 0.2512\n",
      "238/281, train_loss: 0.0521, step time: 0.2481\n",
      "239/281, train_loss: 0.0648, step time: 0.2564\n",
      "240/281, train_loss: 0.0573, step time: 0.2553\n",
      "241/281, train_loss: 0.2866, step time: 0.2549\n",
      "242/281, train_loss: 0.0594, step time: 0.2562\n",
      "243/281, train_loss: 0.0864, step time: 0.2492\n",
      "244/281, train_loss: 0.0907, step time: 0.2488\n",
      "245/281, train_loss: 0.2406, step time: 0.2534\n",
      "246/281, train_loss: 0.0760, step time: 0.2525\n",
      "247/281, train_loss: 0.0871, step time: 0.2531\n",
      "248/281, train_loss: 0.0637, step time: 0.2556\n",
      "249/281, train_loss: 0.0485, step time: 0.2475\n",
      "250/281, train_loss: 0.0951, step time: 0.2515\n",
      "251/281, train_loss: 0.0471, step time: 0.2503\n",
      "252/281, train_loss: 0.0890, step time: 0.2522\n",
      "253/281, train_loss: 0.0795, step time: 0.2530\n",
      "254/281, train_loss: 0.0678, step time: 0.2534\n",
      "255/281, train_loss: 0.1062, step time: 0.2506\n",
      "256/281, train_loss: 0.1034, step time: 0.2521\n",
      "257/281, train_loss: 0.0786, step time: 0.2561\n",
      "258/281, train_loss: 0.0886, step time: 0.2501\n",
      "259/281, train_loss: 0.0626, step time: 0.2499\n",
      "260/281, train_loss: 0.0503, step time: 0.2499\n",
      "261/281, train_loss: 0.0778, step time: 0.2470\n",
      "262/281, train_loss: 0.2286, step time: 0.2528\n",
      "263/281, train_loss: 0.2390, step time: 0.2494\n",
      "264/281, train_loss: 0.0961, step time: 0.2508\n",
      "265/281, train_loss: 0.1106, step time: 0.2477\n",
      "266/281, train_loss: 0.0378, step time: 0.2500\n",
      "267/281, train_loss: 0.0702, step time: 0.2495\n",
      "268/281, train_loss: 0.0730, step time: 0.2479\n",
      "269/281, train_loss: 0.0600, step time: 0.2533\n",
      "270/281, train_loss: 0.1087, step time: 0.2565\n",
      "271/281, train_loss: 0.0864, step time: 0.2520\n",
      "272/281, train_loss: 0.0626, step time: 0.2512\n",
      "273/281, train_loss: 0.0615, step time: 0.2557\n",
      "274/281, train_loss: 0.0925, step time: 0.2499\n",
      "275/281, train_loss: 0.2106, step time: 0.2544\n",
      "276/281, train_loss: 0.0676, step time: 0.2502\n",
      "277/281, train_loss: 0.0868, step time: 0.2505\n",
      "278/281, train_loss: 0.2622, step time: 0.2489\n",
      "279/281, train_loss: 0.0956, step time: 0.2524\n",
      "280/281, train_loss: 0.0488, step time: 0.2501\n",
      "281/281, train_loss: 0.0726, step time: 0.2522\n",
      "282/281, train_loss: 0.1344, step time: 0.1521\n",
      "epoch 84 average loss: 0.1007\n",
      "current epoch: 84 current mean dice: 0.8852 tc: 0.8711 wt: 0.9188 et: 0.8753\n",
      "best mean dice: 0.8918 at epoch: 81\n",
      "time consuming of epoch 84 is: 369.5860\n",
      "----------\n",
      "epoch 85/200\n",
      "1/281, train_loss: 0.2391, step time: 0.2609\n",
      "2/281, train_loss: 0.0781, step time: 0.2608\n",
      "3/281, train_loss: 0.0446, step time: 0.2572\n",
      "4/281, train_loss: 0.1364, step time: 0.2588\n",
      "5/281, train_loss: 0.0664, step time: 0.2584\n",
      "6/281, train_loss: 0.0695, step time: 0.2605\n",
      "7/281, train_loss: 0.0566, step time: 0.2503\n",
      "8/281, train_loss: 0.0558, step time: 0.2552\n",
      "9/281, train_loss: 0.0787, step time: 0.2602\n",
      "10/281, train_loss: 0.2386, step time: 0.2590\n",
      "11/281, train_loss: 0.0895, step time: 0.2606\n",
      "12/281, train_loss: 0.0695, step time: 0.2597\n",
      "13/281, train_loss: 0.0927, step time: 0.2556\n",
      "14/281, train_loss: 0.1301, step time: 0.2568\n",
      "15/281, train_loss: 0.0692, step time: 0.2529\n",
      "16/281, train_loss: 0.0753, step time: 0.2584\n",
      "17/281, train_loss: 0.1238, step time: 0.2552\n",
      "18/281, train_loss: 0.0508, step time: 0.2569\n",
      "19/281, train_loss: 0.0843, step time: 0.2509\n",
      "20/281, train_loss: 0.0823, step time: 0.2566\n",
      "21/281, train_loss: 0.0696, step time: 0.2572\n",
      "22/281, train_loss: 0.2313, step time: 0.2566\n",
      "23/281, train_loss: 0.2420, step time: 0.2578\n",
      "24/281, train_loss: 0.0448, step time: 0.2591\n",
      "25/281, train_loss: 0.1029, step time: 0.2553\n",
      "26/281, train_loss: 0.2172, step time: 0.2580\n",
      "27/281, train_loss: 0.0977, step time: 0.2556\n",
      "28/281, train_loss: 0.0895, step time: 0.2554\n",
      "29/281, train_loss: 0.0986, step time: 0.2545\n",
      "30/281, train_loss: 0.2205, step time: 0.2534\n",
      "31/281, train_loss: 0.2241, step time: 0.2554\n",
      "32/281, train_loss: 0.4086, step time: 0.2488\n",
      "33/281, train_loss: 0.0641, step time: 0.2587\n",
      "34/281, train_loss: 0.0660, step time: 0.2585\n",
      "35/281, train_loss: 0.1953, step time: 0.2541\n",
      "36/281, train_loss: 0.1083, step time: 0.2547\n",
      "37/281, train_loss: 0.0648, step time: 0.2545\n",
      "38/281, train_loss: 0.0771, step time: 0.2529\n",
      "39/281, train_loss: 0.0693, step time: 0.2600\n",
      "40/281, train_loss: 0.0709, step time: 0.2595\n",
      "41/281, train_loss: 0.0767, step time: 0.2615\n",
      "42/281, train_loss: 0.0668, step time: 0.2563\n",
      "43/281, train_loss: 0.0854, step time: 0.2600\n",
      "44/281, train_loss: 0.0712, step time: 0.2580\n",
      "45/281, train_loss: 0.0583, step time: 0.2499\n",
      "46/281, train_loss: 0.0779, step time: 0.2507\n",
      "47/281, train_loss: 0.1064, step time: 0.2588\n",
      "48/281, train_loss: 0.1072, step time: 0.2579\n",
      "49/281, train_loss: 0.0858, step time: 0.2619\n",
      "50/281, train_loss: 0.3912, step time: 0.2656\n",
      "51/281, train_loss: 0.0791, step time: 0.2527\n",
      "52/281, train_loss: 0.0699, step time: 0.2566\n",
      "53/281, train_loss: 0.2580, step time: 0.2596\n",
      "54/281, train_loss: 0.0672, step time: 0.2540\n",
      "55/281, train_loss: 0.0820, step time: 0.2678\n",
      "56/281, train_loss: 0.0937, step time: 0.2586\n",
      "57/281, train_loss: 0.2274, step time: 0.2568\n",
      "58/281, train_loss: 0.0794, step time: 0.2595\n",
      "59/281, train_loss: 0.1043, step time: 0.2573\n",
      "60/281, train_loss: 0.2645, step time: 0.2564\n",
      "61/281, train_loss: 0.2077, step time: 0.2676\n",
      "62/281, train_loss: 0.2274, step time: 0.2612\n",
      "63/281, train_loss: 0.0683, step time: 0.2556\n",
      "64/281, train_loss: 0.0655, step time: 0.2544\n",
      "65/281, train_loss: 0.0770, step time: 0.2601\n",
      "66/281, train_loss: 0.0737, step time: 0.2584\n",
      "67/281, train_loss: 0.2568, step time: 0.2575\n",
      "68/281, train_loss: 0.0729, step time: 0.2583\n",
      "69/281, train_loss: 0.0690, step time: 0.2524\n",
      "70/281, train_loss: 0.0677, step time: 0.2512\n",
      "71/281, train_loss: 0.0502, step time: 0.2493\n",
      "72/281, train_loss: 0.0789, step time: 0.2573\n",
      "73/281, train_loss: 0.0756, step time: 0.2592\n",
      "74/281, train_loss: 0.2613, step time: 0.2533\n",
      "75/281, train_loss: 0.1322, step time: 0.2588\n",
      "76/281, train_loss: 0.2104, step time: 0.2570\n",
      "77/281, train_loss: 0.0598, step time: 0.2583\n",
      "78/281, train_loss: 0.1336, step time: 0.2528\n",
      "79/281, train_loss: 0.0542, step time: 0.2517\n",
      "80/281, train_loss: 0.2313, step time: 0.2556\n",
      "81/281, train_loss: 0.0696, step time: 0.2539\n",
      "82/281, train_loss: 0.0830, step time: 0.2593\n",
      "83/281, train_loss: 0.0978, step time: 0.2554\n",
      "84/281, train_loss: 0.2300, step time: 0.2526\n",
      "85/281, train_loss: 0.0781, step time: 0.2473\n",
      "86/281, train_loss: 0.0441, step time: 0.2472\n",
      "87/281, train_loss: 0.0694, step time: 0.2488\n",
      "88/281, train_loss: 0.3937, step time: 0.2589\n",
      "89/281, train_loss: 0.0524, step time: 0.2589\n",
      "90/281, train_loss: 0.2347, step time: 0.2527\n",
      "91/281, train_loss: 0.0737, step time: 0.2490\n",
      "92/281, train_loss: 0.0847, step time: 0.2480\n",
      "93/281, train_loss: 0.0843, step time: 0.2539\n",
      "94/281, train_loss: 0.1118, step time: 0.2553\n",
      "95/281, train_loss: 0.0657, step time: 0.2504\n",
      "96/281, train_loss: 0.0553, step time: 0.2556\n",
      "97/281, train_loss: 0.0596, step time: 0.2505\n",
      "98/281, train_loss: 0.0497, step time: 0.2527\n",
      "99/281, train_loss: 0.0746, step time: 0.2585\n",
      "100/281, train_loss: 0.1243, step time: 0.2604\n",
      "101/281, train_loss: 0.0814, step time: 0.2528\n",
      "102/281, train_loss: 0.2388, step time: 0.2540\n",
      "103/281, train_loss: 0.0735, step time: 0.2489\n",
      "104/281, train_loss: 0.0745, step time: 0.2533\n",
      "105/281, train_loss: 0.0790, step time: 0.2607\n",
      "106/281, train_loss: 0.0773, step time: 0.2552\n",
      "107/281, train_loss: 0.0708, step time: 0.2516\n",
      "108/281, train_loss: 0.0731, step time: 0.2544\n",
      "109/281, train_loss: 0.1145, step time: 0.2588\n",
      "110/281, train_loss: 0.0908, step time: 0.2533\n",
      "111/281, train_loss: 0.0787, step time: 0.2489\n",
      "112/281, train_loss: 0.0780, step time: 0.2538\n",
      "113/281, train_loss: 0.2355, step time: 0.2572\n",
      "114/281, train_loss: 0.0701, step time: 0.2568\n",
      "115/281, train_loss: 0.0695, step time: 0.2488\n",
      "116/281, train_loss: 0.0685, step time: 0.2573\n",
      "117/281, train_loss: 0.0915, step time: 0.2569\n",
      "118/281, train_loss: 0.0395, step time: 0.2499\n",
      "119/281, train_loss: 0.0554, step time: 0.2529\n",
      "120/281, train_loss: 0.0753, step time: 0.2556\n",
      "121/281, train_loss: 0.0405, step time: 0.2523\n",
      "122/281, train_loss: 0.0699, step time: 0.2584\n",
      "123/281, train_loss: 0.1299, step time: 0.2580\n",
      "124/281, train_loss: 0.2292, step time: 0.2581\n",
      "125/281, train_loss: 0.0795, step time: 0.2565\n",
      "126/281, train_loss: 0.0887, step time: 0.2536\n",
      "127/281, train_loss: 0.2208, step time: 0.2502\n",
      "128/281, train_loss: 0.0555, step time: 0.2522\n",
      "129/281, train_loss: 0.0887, step time: 0.2562\n",
      "130/281, train_loss: 0.2751, step time: 0.2577\n",
      "131/281, train_loss: 0.0368, step time: 0.2517\n",
      "132/281, train_loss: 0.0833, step time: 0.2627\n",
      "133/281, train_loss: 0.0231, step time: 0.2513\n",
      "134/281, train_loss: 0.0935, step time: 0.2543\n",
      "135/281, train_loss: 0.0617, step time: 0.2546\n",
      "136/281, train_loss: 0.0638, step time: 0.2727\n",
      "137/281, train_loss: 0.0589, step time: 0.2568\n",
      "138/281, train_loss: 0.0596, step time: 0.2542\n",
      "139/281, train_loss: 0.0991, step time: 0.2517\n",
      "140/281, train_loss: 0.1190, step time: 0.2514\n",
      "141/281, train_loss: 0.0897, step time: 0.2496\n",
      "142/281, train_loss: 0.0578, step time: 0.2540\n",
      "143/281, train_loss: 0.0402, step time: 0.2547\n",
      "144/281, train_loss: 0.0871, step time: 0.2555\n",
      "145/281, train_loss: 0.0805, step time: 0.2516\n",
      "146/281, train_loss: 0.0720, step time: 0.2575\n",
      "147/281, train_loss: 0.0553, step time: 0.2534\n",
      "148/281, train_loss: 0.2262, step time: 0.2580\n",
      "149/281, train_loss: 0.1280, step time: 0.2549\n",
      "150/281, train_loss: 0.1044, step time: 0.2524\n",
      "151/281, train_loss: 0.0425, step time: 0.2528\n",
      "152/281, train_loss: 0.0614, step time: 0.2512\n",
      "153/281, train_loss: 0.0679, step time: 0.2522\n",
      "154/281, train_loss: 0.2456, step time: 0.2476\n",
      "155/281, train_loss: 0.0640, step time: 0.2474\n",
      "156/281, train_loss: 0.0726, step time: 0.2520\n",
      "157/281, train_loss: 0.0775, step time: 0.2553\n",
      "158/281, train_loss: 0.2415, step time: 0.2586\n",
      "159/281, train_loss: 0.0843, step time: 0.2544\n",
      "160/281, train_loss: 0.0882, step time: 0.2499\n",
      "161/281, train_loss: 0.1002, step time: 0.2512\n",
      "162/281, train_loss: 0.0688, step time: 0.2485\n",
      "163/281, train_loss: 0.0469, step time: 0.2522\n",
      "164/281, train_loss: 0.0432, step time: 0.2519\n",
      "165/281, train_loss: 0.0462, step time: 0.2518\n",
      "166/281, train_loss: 0.2476, step time: 0.2587\n",
      "167/281, train_loss: 0.0644, step time: 0.2570\n",
      "168/281, train_loss: 0.0802, step time: 0.2547\n",
      "169/281, train_loss: 0.0746, step time: 0.2559\n",
      "170/281, train_loss: 0.0379, step time: 0.2500\n",
      "171/281, train_loss: 0.0749, step time: 0.2531\n",
      "172/281, train_loss: 0.0681, step time: 0.2612\n",
      "173/281, train_loss: 0.0947, step time: 0.2574\n",
      "174/281, train_loss: 0.0933, step time: 0.2515\n",
      "175/281, train_loss: 0.0934, step time: 0.2519\n",
      "176/281, train_loss: 0.0844, step time: 0.2531\n",
      "177/281, train_loss: 0.0430, step time: 0.2633\n",
      "178/281, train_loss: 0.0797, step time: 0.2592\n",
      "179/281, train_loss: 0.0678, step time: 0.2564\n",
      "180/281, train_loss: 0.0982, step time: 0.2580\n",
      "181/281, train_loss: 0.0857, step time: 0.2580\n",
      "182/281, train_loss: 0.0963, step time: 0.2551\n",
      "183/281, train_loss: 0.0556, step time: 0.2579\n",
      "184/281, train_loss: 0.0810, step time: 0.2561\n",
      "185/281, train_loss: 0.0795, step time: 0.2565\n",
      "186/281, train_loss: 0.0537, step time: 0.2557\n",
      "187/281, train_loss: 0.0522, step time: 0.2561\n",
      "188/281, train_loss: 0.1059, step time: 0.2626\n",
      "189/281, train_loss: 0.0565, step time: 0.2573\n",
      "190/281, train_loss: 0.1061, step time: 0.2557\n",
      "191/281, train_loss: 0.0904, step time: 0.2580\n",
      "192/281, train_loss: 0.0590, step time: 0.2597\n",
      "193/281, train_loss: 0.0553, step time: 0.2550\n",
      "194/281, train_loss: 0.0624, step time: 0.2572\n",
      "195/281, train_loss: 0.0603, step time: 0.2561\n",
      "196/281, train_loss: 0.0722, step time: 0.2583\n",
      "197/281, train_loss: 0.0693, step time: 0.2605\n",
      "198/281, train_loss: 0.0803, step time: 0.2632\n",
      "199/281, train_loss: 0.0611, step time: 0.2603\n",
      "200/281, train_loss: 0.0684, step time: 0.2614\n",
      "201/281, train_loss: 0.0892, step time: 0.2510\n",
      "202/281, train_loss: 0.0936, step time: 0.2490\n",
      "203/281, train_loss: 0.0702, step time: 0.2482\n",
      "204/281, train_loss: 0.0652, step time: 0.2579\n",
      "205/281, train_loss: 0.0754, step time: 0.2591\n",
      "206/281, train_loss: 0.0892, step time: 0.2575\n",
      "207/281, train_loss: 0.0808, step time: 0.2585\n",
      "208/281, train_loss: 0.0627, step time: 0.2519\n",
      "209/281, train_loss: 0.0888, step time: 0.2582\n",
      "210/281, train_loss: 0.0794, step time: 0.2596\n",
      "211/281, train_loss: 0.0663, step time: 0.2586\n",
      "212/281, train_loss: 0.1201, step time: 0.2558\n",
      "213/281, train_loss: 0.2422, step time: 0.2489\n",
      "214/281, train_loss: 0.2316, step time: 0.2497\n",
      "215/281, train_loss: 0.0611, step time: 0.2517\n",
      "216/281, train_loss: 0.0739, step time: 0.2590\n",
      "217/281, train_loss: 0.0668, step time: 0.2515\n",
      "218/281, train_loss: 0.0742, step time: 0.2451\n",
      "219/281, train_loss: 0.0461, step time: 0.2494\n",
      "220/281, train_loss: 0.2163, step time: 0.2535\n",
      "221/281, train_loss: 0.1053, step time: 0.2546\n",
      "222/281, train_loss: 0.0573, step time: 0.2503\n",
      "223/281, train_loss: 0.0855, step time: 0.2501\n",
      "224/281, train_loss: 0.0758, step time: 0.2567\n",
      "225/281, train_loss: 0.0895, step time: 0.2569\n",
      "226/281, train_loss: 0.0676, step time: 0.2546\n",
      "227/281, train_loss: 0.0479, step time: 0.2526\n",
      "228/281, train_loss: 0.0798, step time: 0.2582\n",
      "229/281, train_loss: 0.2236, step time: 0.2576\n",
      "230/281, train_loss: 0.0755, step time: 0.2519\n",
      "231/281, train_loss: 0.0675, step time: 0.2516\n",
      "232/281, train_loss: 0.0743, step time: 0.2591\n",
      "233/281, train_loss: 0.0434, step time: 0.2514\n",
      "234/281, train_loss: 0.0479, step time: 0.2517\n",
      "235/281, train_loss: 0.0437, step time: 0.2521\n",
      "236/281, train_loss: 0.0634, step time: 0.2625\n",
      "237/281, train_loss: 0.2314, step time: 0.2579\n",
      "238/281, train_loss: 0.0678, step time: 0.2584\n",
      "239/281, train_loss: 0.0854, step time: 0.2560\n",
      "240/281, train_loss: 0.0857, step time: 0.2581\n",
      "241/281, train_loss: 0.0743, step time: 0.2556\n",
      "242/281, train_loss: 0.0866, step time: 0.2529\n",
      "243/281, train_loss: 0.0585, step time: 0.2511\n",
      "244/281, train_loss: 0.2242, step time: 0.2582\n",
      "245/281, train_loss: 0.0541, step time: 0.2531\n",
      "246/281, train_loss: 0.2282, step time: 0.2531\n",
      "247/281, train_loss: 0.0781, step time: 0.2559\n",
      "248/281, train_loss: 0.0651, step time: 0.2580\n",
      "249/281, train_loss: 0.0612, step time: 0.2581\n",
      "250/281, train_loss: 0.0525, step time: 0.2623\n",
      "251/281, train_loss: 0.0762, step time: 0.2511\n",
      "252/281, train_loss: 0.0440, step time: 0.2555\n",
      "253/281, train_loss: 0.0868, step time: 0.2549\n",
      "254/281, train_loss: 0.0550, step time: 0.2576\n",
      "255/281, train_loss: 0.0877, step time: 0.2563\n",
      "256/281, train_loss: 0.2285, step time: 0.2560\n",
      "257/281, train_loss: 0.0499, step time: 0.2566\n",
      "258/281, train_loss: 0.0618, step time: 0.2531\n",
      "259/281, train_loss: 0.0521, step time: 0.2566\n",
      "260/281, train_loss: 0.0873, step time: 0.2561\n",
      "261/281, train_loss: 0.0717, step time: 0.2509\n",
      "262/281, train_loss: 0.2478, step time: 0.2508\n",
      "263/281, train_loss: 0.0478, step time: 0.2524\n",
      "264/281, train_loss: 0.0646, step time: 0.2567\n",
      "265/281, train_loss: 0.0656, step time: 0.2533\n",
      "266/281, train_loss: 0.1286, step time: 0.2565\n",
      "267/281, train_loss: 0.0534, step time: 0.2527\n",
      "268/281, train_loss: 0.0530, step time: 0.2592\n",
      "269/281, train_loss: 0.0426, step time: 0.2591\n",
      "270/281, train_loss: 0.0923, step time: 0.2593\n",
      "271/281, train_loss: 0.0766, step time: 0.2555\n",
      "272/281, train_loss: 0.0848, step time: 0.2543\n",
      "273/281, train_loss: 0.0700, step time: 0.2519\n",
      "274/281, train_loss: 0.0951, step time: 0.2555\n",
      "275/281, train_loss: 0.0718, step time: 0.2532\n",
      "276/281, train_loss: 0.0737, step time: 0.2550\n",
      "277/281, train_loss: 0.0400, step time: 0.2542\n",
      "278/281, train_loss: 0.0802, step time: 0.2580\n",
      "279/281, train_loss: 0.0617, step time: 0.2585\n",
      "280/281, train_loss: 0.0825, step time: 0.2578\n",
      "281/281, train_loss: 0.0589, step time: 0.2559\n",
      "282/281, train_loss: 0.1101, step time: 0.1545\n",
      "epoch 85 average loss: 0.0990\n",
      "current epoch: 85 current mean dice: 0.8918 tc: 0.8849 wt: 0.9217 et: 0.8768\n",
      "best mean dice: 0.8918 at epoch: 81\n",
      "time consuming of epoch 85 is: 380.7846\n",
      "----------\n",
      "epoch 86/200\n",
      "1/281, train_loss: 0.0781, step time: 0.2602\n",
      "2/281, train_loss: 0.0702, step time: 0.2513\n",
      "3/281, train_loss: 0.0655, step time: 0.2616\n",
      "4/281, train_loss: 0.0691, step time: 0.2528\n",
      "5/281, train_loss: 0.0743, step time: 0.2495\n",
      "6/281, train_loss: 0.0668, step time: 0.2496\n",
      "7/281, train_loss: 0.2206, step time: 0.2513\n",
      "8/281, train_loss: 0.0716, step time: 0.2493\n",
      "9/281, train_loss: 0.0822, step time: 0.2503\n",
      "10/281, train_loss: 0.0683, step time: 0.2464\n",
      "11/281, train_loss: 0.2106, step time: 0.2448\n",
      "12/281, train_loss: 0.0569, step time: 0.2461\n",
      "13/281, train_loss: 0.0700, step time: 0.2509\n",
      "14/281, train_loss: 0.0963, step time: 0.2623\n",
      "15/281, train_loss: 0.2426, step time: 0.2659\n",
      "16/281, train_loss: 0.0745, step time: 0.2491\n",
      "17/281, train_loss: 0.0501, step time: 0.2472\n",
      "18/281, train_loss: 0.0739, step time: 0.2474\n",
      "19/281, train_loss: 0.0411, step time: 0.2455\n",
      "20/281, train_loss: 0.0598, step time: 0.2502\n",
      "21/281, train_loss: 0.0856, step time: 0.2519\n",
      "22/281, train_loss: 0.0807, step time: 0.2601\n",
      "23/281, train_loss: 0.2310, step time: 0.2535\n",
      "24/281, train_loss: 0.2335, step time: 0.2594\n",
      "25/281, train_loss: 0.0946, step time: 0.2575\n",
      "26/281, train_loss: 0.0975, step time: 0.2552\n",
      "27/281, train_loss: 0.3741, step time: 0.2582\n",
      "28/281, train_loss: 0.0471, step time: 0.2543\n",
      "29/281, train_loss: 0.2268, step time: 0.2481\n",
      "30/281, train_loss: 0.0788, step time: 0.2469\n",
      "31/281, train_loss: 0.0531, step time: 0.2488\n",
      "32/281, train_loss: 0.0554, step time: 0.2486\n",
      "33/281, train_loss: 0.0483, step time: 0.2530\n",
      "34/281, train_loss: 0.0685, step time: 0.2807\n",
      "35/281, train_loss: 0.0660, step time: 0.2573\n",
      "36/281, train_loss: 0.0661, step time: 0.2570\n",
      "37/281, train_loss: 0.2358, step time: 0.2492\n",
      "38/281, train_loss: 0.0889, step time: 0.2460\n",
      "39/281, train_loss: 0.0728, step time: 0.2435\n",
      "40/281, train_loss: 0.0590, step time: 0.2491\n",
      "41/281, train_loss: 0.0858, step time: 0.2534\n",
      "42/281, train_loss: 0.0610, step time: 0.2499\n",
      "43/281, train_loss: 0.0759, step time: 0.2521\n",
      "44/281, train_loss: 0.0598, step time: 0.2512\n",
      "45/281, train_loss: 0.1083, step time: 0.2572\n",
      "46/281, train_loss: 0.0709, step time: 0.2541\n",
      "47/281, train_loss: 0.0624, step time: 0.2507\n",
      "48/281, train_loss: 0.0672, step time: 0.2530\n",
      "49/281, train_loss: 0.0586, step time: 0.2535\n",
      "50/281, train_loss: 0.0705, step time: 0.2547\n",
      "51/281, train_loss: 0.0453, step time: 0.2498\n",
      "52/281, train_loss: 0.0891, step time: 0.2497\n",
      "53/281, train_loss: 0.0733, step time: 0.2579\n",
      "54/281, train_loss: 0.2415, step time: 0.2548\n",
      "55/281, train_loss: 0.2193, step time: 0.2577\n",
      "56/281, train_loss: 0.2329, step time: 0.2587\n",
      "57/281, train_loss: 0.0707, step time: 0.2570\n",
      "58/281, train_loss: 0.0664, step time: 0.2571\n",
      "59/281, train_loss: 0.0955, step time: 0.2527\n",
      "60/281, train_loss: 0.0681, step time: 0.2541\n",
      "61/281, train_loss: 0.0779, step time: 0.2530\n",
      "62/281, train_loss: 0.0729, step time: 0.2507\n",
      "63/281, train_loss: 0.0669, step time: 0.2600\n",
      "64/281, train_loss: 0.1084, step time: 0.2491\n",
      "65/281, train_loss: 0.0491, step time: 0.2524\n",
      "66/281, train_loss: 0.0980, step time: 0.2541\n",
      "67/281, train_loss: 0.0653, step time: 0.2597\n",
      "68/281, train_loss: 0.0566, step time: 0.2549\n",
      "69/281, train_loss: 0.2839, step time: 0.2638\n",
      "70/281, train_loss: 0.0428, step time: 0.2556\n",
      "71/281, train_loss: 0.0679, step time: 0.2595\n",
      "72/281, train_loss: 0.0891, step time: 0.2591\n",
      "73/281, train_loss: 0.0770, step time: 0.2563\n",
      "74/281, train_loss: 0.2412, step time: 0.2581\n",
      "75/281, train_loss: 0.0827, step time: 0.2547\n",
      "76/281, train_loss: 0.3889, step time: 0.2506\n",
      "77/281, train_loss: 0.0748, step time: 0.2564\n",
      "78/281, train_loss: 0.2092, step time: 0.2497\n",
      "79/281, train_loss: 0.0493, step time: 0.2589\n",
      "80/281, train_loss: 0.0632, step time: 0.2494\n",
      "81/281, train_loss: 0.0702, step time: 0.2509\n",
      "82/281, train_loss: 0.0467, step time: 0.2459\n",
      "83/281, train_loss: 0.0506, step time: 0.2519\n",
      "84/281, train_loss: 0.2282, step time: 0.2595\n",
      "85/281, train_loss: 0.0599, step time: 0.2518\n",
      "86/281, train_loss: 0.0560, step time: 0.2501\n",
      "87/281, train_loss: 0.0982, step time: 0.2508\n",
      "88/281, train_loss: 0.0852, step time: 0.2565\n",
      "89/281, train_loss: 0.0774, step time: 0.2556\n",
      "90/281, train_loss: 0.0336, step time: 0.2564\n",
      "91/281, train_loss: 0.0781, step time: 0.2533\n",
      "92/281, train_loss: 0.0868, step time: 0.2559\n",
      "93/281, train_loss: 0.1055, step time: 0.2523\n",
      "94/281, train_loss: 0.1157, step time: 0.2512\n",
      "95/281, train_loss: 0.0626, step time: 0.2552\n",
      "96/281, train_loss: 0.0774, step time: 0.2485\n",
      "97/281, train_loss: 0.0974, step time: 0.2573\n",
      "98/281, train_loss: 0.0707, step time: 0.2543\n",
      "99/281, train_loss: 0.0533, step time: 0.2539\n",
      "100/281, train_loss: 0.0517, step time: 0.2527\n",
      "101/281, train_loss: 0.0783, step time: 0.2541\n",
      "102/281, train_loss: 0.0502, step time: 0.2522\n",
      "103/281, train_loss: 0.0560, step time: 0.2563\n",
      "104/281, train_loss: 0.0700, step time: 0.2543\n",
      "105/281, train_loss: 0.0820, step time: 0.2525\n",
      "106/281, train_loss: 0.1244, step time: 0.2572\n",
      "107/281, train_loss: 0.0831, step time: 0.2587\n",
      "108/281, train_loss: 0.1052, step time: 0.2566\n",
      "109/281, train_loss: 0.0856, step time: 0.2541\n",
      "110/281, train_loss: 0.0571, step time: 0.2537\n",
      "111/281, train_loss: 0.0753, step time: 0.2538\n",
      "112/281, train_loss: 0.0561, step time: 0.2523\n",
      "113/281, train_loss: 0.0729, step time: 0.2522\n",
      "114/281, train_loss: 0.0646, step time: 0.2561\n",
      "115/281, train_loss: 0.0882, step time: 0.2569\n",
      "116/281, train_loss: 0.0686, step time: 0.2524\n",
      "117/281, train_loss: 0.0855, step time: 0.2528\n",
      "118/281, train_loss: 0.0449, step time: 0.2592\n",
      "119/281, train_loss: 0.0700, step time: 0.2568\n",
      "120/281, train_loss: 0.0751, step time: 0.2534\n",
      "121/281, train_loss: 0.0509, step time: 0.2516\n",
      "122/281, train_loss: 0.0548, step time: 0.2639\n",
      "123/281, train_loss: 0.0826, step time: 0.2599\n",
      "124/281, train_loss: 0.0779, step time: 0.2553\n",
      "125/281, train_loss: 0.1259, step time: 0.2497\n",
      "126/281, train_loss: 0.0938, step time: 0.2479\n",
      "127/281, train_loss: 0.0533, step time: 0.2487\n",
      "128/281, train_loss: 0.0674, step time: 0.2483\n",
      "129/281, train_loss: 0.2133, step time: 0.2461\n",
      "130/281, train_loss: 0.0568, step time: 0.2487\n",
      "131/281, train_loss: 0.1242, step time: 0.2490\n",
      "132/281, train_loss: 0.0926, step time: 0.2520\n",
      "133/281, train_loss: 0.0778, step time: 0.2549\n",
      "134/281, train_loss: 0.0441, step time: 0.2461\n",
      "135/281, train_loss: 0.0668, step time: 0.2447\n",
      "136/281, train_loss: 0.0704, step time: 0.2473\n",
      "137/281, train_loss: 0.0842, step time: 0.2493\n",
      "138/281, train_loss: 0.0495, step time: 0.2487\n",
      "139/281, train_loss: 0.0769, step time: 0.2528\n",
      "140/281, train_loss: 0.0585, step time: 0.2534\n",
      "141/281, train_loss: 0.0559, step time: 0.2483\n",
      "142/281, train_loss: 0.0652, step time: 0.2528\n",
      "143/281, train_loss: 0.0605, step time: 0.2491\n",
      "144/281, train_loss: 0.0559, step time: 0.2520\n",
      "145/281, train_loss: 0.0780, step time: 0.2462\n",
      "146/281, train_loss: 0.2488, step time: 0.2500\n",
      "147/281, train_loss: 0.0872, step time: 0.2470\n",
      "148/281, train_loss: 0.0499, step time: 0.2469\n",
      "149/281, train_loss: 0.0451, step time: 0.2477\n",
      "150/281, train_loss: 0.0524, step time: 0.2491\n",
      "151/281, train_loss: 0.0548, step time: 0.2484\n",
      "152/281, train_loss: 0.0790, step time: 0.2460\n",
      "153/281, train_loss: 0.0787, step time: 0.2483\n",
      "154/281, train_loss: 0.1232, step time: 0.2511\n",
      "155/281, train_loss: 0.0599, step time: 0.2527\n",
      "156/281, train_loss: 0.0689, step time: 0.2501\n",
      "157/281, train_loss: 0.0969, step time: 0.2514\n",
      "158/281, train_loss: 0.0752, step time: 0.2487\n",
      "159/281, train_loss: 0.0713, step time: 0.2489\n",
      "160/281, train_loss: 0.0744, step time: 0.2481\n",
      "161/281, train_loss: 0.2485, step time: 0.2461\n",
      "162/281, train_loss: 0.0748, step time: 0.2528\n",
      "163/281, train_loss: 0.1126, step time: 0.2568\n",
      "164/281, train_loss: 0.2331, step time: 0.2551\n",
      "165/281, train_loss: 0.0813, step time: 0.2520\n",
      "166/281, train_loss: 0.0603, step time: 0.2542\n",
      "167/281, train_loss: 0.2503, step time: 0.2546\n",
      "168/281, train_loss: 0.0817, step time: 0.2471\n",
      "169/281, train_loss: 0.0709, step time: 0.2520\n",
      "170/281, train_loss: 0.0600, step time: 0.2544\n",
      "171/281, train_loss: 0.0764, step time: 0.2554\n",
      "172/281, train_loss: 0.0803, step time: 0.2541\n",
      "173/281, train_loss: 0.0591, step time: 0.2542\n",
      "174/281, train_loss: 0.0577, step time: 0.2487\n",
      "175/281, train_loss: 0.0538, step time: 0.2465\n",
      "176/281, train_loss: 0.0682, step time: 0.2466\n",
      "177/281, train_loss: 0.2309, step time: 0.2463\n",
      "178/281, train_loss: 0.0596, step time: 0.2471\n",
      "179/281, train_loss: 0.0727, step time: 0.2506\n",
      "180/281, train_loss: 0.3881, step time: 0.2489\n",
      "181/281, train_loss: 0.0618, step time: 0.2525\n",
      "182/281, train_loss: 0.0652, step time: 0.2509\n",
      "183/281, train_loss: 0.0720, step time: 0.2509\n",
      "184/281, train_loss: 0.0787, step time: 0.2513\n",
      "185/281, train_loss: 0.0775, step time: 0.2474\n",
      "186/281, train_loss: 0.2639, step time: 0.2474\n",
      "187/281, train_loss: 0.0590, step time: 0.2667\n",
      "188/281, train_loss: 0.0722, step time: 0.2494\n",
      "189/281, train_loss: 0.1427, step time: 0.2490\n",
      "190/281, train_loss: 0.2378, step time: 0.2570\n",
      "191/281, train_loss: 0.0546, step time: 0.2522\n",
      "192/281, train_loss: 0.0835, step time: 0.2439\n",
      "193/281, train_loss: 0.0595, step time: 0.2455\n",
      "194/281, train_loss: 0.0781, step time: 0.2473\n",
      "195/281, train_loss: 0.0456, step time: 0.2459\n",
      "196/281, train_loss: 0.0455, step time: 0.2493\n",
      "197/281, train_loss: 0.2053, step time: 0.2446\n",
      "198/281, train_loss: 0.0677, step time: 0.2499\n",
      "199/281, train_loss: 0.0580, step time: 0.2460\n",
      "200/281, train_loss: 0.0685, step time: 0.2492\n",
      "201/281, train_loss: 0.0644, step time: 0.2525\n",
      "202/281, train_loss: 0.0567, step time: 0.2669\n",
      "203/281, train_loss: 0.0560, step time: 0.2498\n",
      "204/281, train_loss: 0.0485, step time: 0.2479\n",
      "205/281, train_loss: 0.1350, step time: 0.2501\n",
      "206/281, train_loss: 0.0555, step time: 0.2537\n",
      "207/281, train_loss: 0.0531, step time: 0.2523\n",
      "208/281, train_loss: 0.2156, step time: 0.2468\n",
      "209/281, train_loss: 0.0938, step time: 0.2474\n",
      "210/281, train_loss: 0.2291, step time: 0.2524\n",
      "211/281, train_loss: 0.1021, step time: 0.2530\n",
      "212/281, train_loss: 0.2245, step time: 0.2522\n",
      "213/281, train_loss: 0.1057, step time: 0.2514\n",
      "214/281, train_loss: 0.0659, step time: 0.2456\n",
      "215/281, train_loss: 0.0746, step time: 0.2444\n",
      "216/281, train_loss: 0.1035, step time: 0.2498\n",
      "217/281, train_loss: 0.2632, step time: 0.2834\n",
      "218/281, train_loss: 0.1063, step time: 0.2588\n",
      "219/281, train_loss: 0.1267, step time: 0.2534\n",
      "220/281, train_loss: 0.0947, step time: 0.2458\n",
      "221/281, train_loss: 0.0665, step time: 0.2430\n",
      "222/281, train_loss: 0.0663, step time: 0.2521\n",
      "223/281, train_loss: 0.3720, step time: 0.2518\n",
      "224/281, train_loss: 0.0628, step time: 0.2444\n",
      "225/281, train_loss: 0.0752, step time: 0.2449\n",
      "226/281, train_loss: 0.0998, step time: 0.2642\n",
      "227/281, train_loss: 0.1064, step time: 0.2487\n",
      "228/281, train_loss: 0.0596, step time: 0.2485\n",
      "229/281, train_loss: 0.0703, step time: 0.2471\n",
      "230/281, train_loss: 0.0746, step time: 0.2503\n",
      "231/281, train_loss: 0.0775, step time: 0.2531\n",
      "232/281, train_loss: 0.1275, step time: 0.2439\n",
      "233/281, train_loss: 0.0511, step time: 0.2493\n",
      "234/281, train_loss: 0.0758, step time: 0.2547\n",
      "235/281, train_loss: 0.0825, step time: 0.2504\n",
      "236/281, train_loss: 0.0711, step time: 0.2441\n",
      "237/281, train_loss: 0.0844, step time: 0.2438\n",
      "238/281, train_loss: 0.2171, step time: 0.2499\n",
      "239/281, train_loss: 0.0635, step time: 0.2488\n",
      "240/281, train_loss: 0.0900, step time: 0.2543\n",
      "241/281, train_loss: 0.0758, step time: 0.2468\n",
      "242/281, train_loss: 0.0894, step time: 0.2510\n",
      "243/281, train_loss: 0.0497, step time: 0.2488\n",
      "244/281, train_loss: 0.0699, step time: 0.2476\n",
      "245/281, train_loss: 0.0658, step time: 0.2493\n",
      "246/281, train_loss: 0.0608, step time: 0.2533\n",
      "247/281, train_loss: 0.0533, step time: 0.2495\n",
      "248/281, train_loss: 0.0770, step time: 0.2552\n",
      "249/281, train_loss: 0.0905, step time: 0.2500\n",
      "250/281, train_loss: 0.0652, step time: 0.2454\n",
      "251/281, train_loss: 0.0845, step time: 0.2480\n",
      "252/281, train_loss: 0.0725, step time: 0.2498\n",
      "253/281, train_loss: 0.0708, step time: 0.2527\n",
      "254/281, train_loss: 0.0685, step time: 0.2470\n",
      "255/281, train_loss: 0.0588, step time: 0.2484\n",
      "256/281, train_loss: 0.0592, step time: 0.2513\n",
      "257/281, train_loss: 0.0538, step time: 0.2515\n",
      "258/281, train_loss: 0.2406, step time: 0.2446\n",
      "259/281, train_loss: 0.0719, step time: 0.2498\n",
      "260/281, train_loss: 0.2523, step time: 0.2526\n",
      "261/281, train_loss: 0.0500, step time: 0.2493\n",
      "262/281, train_loss: 0.0780, step time: 0.2493\n",
      "263/281, train_loss: 0.2564, step time: 0.2485\n",
      "264/281, train_loss: 0.0375, step time: 0.2424\n",
      "265/281, train_loss: 0.0888, step time: 0.2453\n",
      "266/281, train_loss: 0.0664, step time: 0.2482\n",
      "267/281, train_loss: 0.0849, step time: 0.2441\n",
      "268/281, train_loss: 0.0969, step time: 0.2466\n",
      "269/281, train_loss: 0.1088, step time: 0.2467\n",
      "270/281, train_loss: 0.0806, step time: 0.2491\n",
      "271/281, train_loss: 0.2221, step time: 0.2452\n",
      "272/281, train_loss: 0.0775, step time: 0.2434\n",
      "273/281, train_loss: 0.1090, step time: 0.2444\n",
      "274/281, train_loss: 0.2190, step time: 0.2516\n",
      "275/281, train_loss: 0.2371, step time: 0.2465\n",
      "276/281, train_loss: 0.0635, step time: 0.2488\n",
      "277/281, train_loss: 0.0796, step time: 0.2460\n",
      "278/281, train_loss: 0.0699, step time: 0.2495\n",
      "279/281, train_loss: 0.2286, step time: 0.2474\n",
      "280/281, train_loss: 0.0749, step time: 0.2402\n",
      "281/281, train_loss: 0.0899, step time: 0.2421\n",
      "282/281, train_loss: 0.0680, step time: 0.1445\n",
      "epoch 86 average loss: 0.0975\n",
      "current epoch: 86 current mean dice: 0.8901 tc: 0.8828 wt: 0.9200 et: 0.8774\n",
      "best mean dice: 0.8918 at epoch: 81\n",
      "time consuming of epoch 86 is: 381.6613\n",
      "----------\n",
      "epoch 87/200\n",
      "1/281, train_loss: 0.2581, step time: 0.2630\n",
      "2/281, train_loss: 0.0629, step time: 0.2586\n",
      "3/281, train_loss: 0.0813, step time: 0.2614\n",
      "4/281, train_loss: 0.0841, step time: 0.2646\n",
      "5/281, train_loss: 0.0763, step time: 0.2515\n",
      "6/281, train_loss: 0.0849, step time: 0.2486\n",
      "7/281, train_loss: 0.3926, step time: 0.2485\n",
      "8/281, train_loss: 0.0756, step time: 0.2473\n",
      "9/281, train_loss: 0.0885, step time: 0.2515\n",
      "10/281, train_loss: 0.0494, step time: 0.2473\n",
      "11/281, train_loss: 0.2538, step time: 0.2534\n",
      "12/281, train_loss: 0.0789, step time: 0.2478\n",
      "13/281, train_loss: 0.4078, step time: 0.2533\n",
      "14/281, train_loss: 0.0757, step time: 0.2506\n",
      "15/281, train_loss: 0.0914, step time: 0.2607\n",
      "16/281, train_loss: 0.0512, step time: 0.2540\n",
      "17/281, train_loss: 0.0855, step time: 0.2576\n",
      "18/281, train_loss: 0.3976, step time: 0.2530\n",
      "19/281, train_loss: 0.0397, step time: 0.2562\n",
      "20/281, train_loss: 0.0790, step time: 0.2470\n",
      "21/281, train_loss: 0.0679, step time: 0.2527\n",
      "22/281, train_loss: 0.0652, step time: 0.2513\n",
      "23/281, train_loss: 0.0451, step time: 0.2480\n",
      "24/281, train_loss: 0.0463, step time: 0.2509\n",
      "25/281, train_loss: 0.0586, step time: 0.2505\n",
      "26/281, train_loss: 0.1010, step time: 0.2698\n",
      "27/281, train_loss: 0.0452, step time: 0.2501\n",
      "28/281, train_loss: 0.0708, step time: 0.2492\n",
      "29/281, train_loss: 0.0941, step time: 0.2571\n",
      "30/281, train_loss: 0.0449, step time: 0.2511\n",
      "31/281, train_loss: 0.0966, step time: 0.2595\n",
      "32/281, train_loss: 0.0899, step time: 0.2507\n",
      "33/281, train_loss: 0.0741, step time: 0.2520\n",
      "34/281, train_loss: 0.0983, step time: 0.2526\n",
      "35/281, train_loss: 0.0885, step time: 0.2549\n",
      "36/281, train_loss: 0.0607, step time: 0.2514\n",
      "37/281, train_loss: 0.2399, step time: 0.2455\n",
      "38/281, train_loss: 0.2209, step time: 0.2496\n",
      "39/281, train_loss: 0.0646, step time: 0.2517\n",
      "40/281, train_loss: 0.0980, step time: 0.2478\n",
      "41/281, train_loss: 0.0599, step time: 0.2489\n",
      "42/281, train_loss: 0.0936, step time: 0.2513\n",
      "43/281, train_loss: 0.2264, step time: 0.2525\n",
      "44/281, train_loss: 0.0531, step time: 0.2434\n",
      "45/281, train_loss: 0.0790, step time: 0.2467\n",
      "46/281, train_loss: 0.1141, step time: 0.2510\n",
      "47/281, train_loss: 0.0417, step time: 0.2481\n",
      "48/281, train_loss: 0.0895, step time: 0.2509\n",
      "49/281, train_loss: 0.0879, step time: 0.2498\n",
      "50/281, train_loss: 0.0748, step time: 0.2594\n",
      "51/281, train_loss: 0.0856, step time: 0.2598\n",
      "52/281, train_loss: 0.0694, step time: 0.2550\n",
      "53/281, train_loss: 0.0597, step time: 0.2528\n",
      "54/281, train_loss: 0.0590, step time: 0.2525\n",
      "55/281, train_loss: 0.0810, step time: 0.2541\n",
      "56/281, train_loss: 0.2431, step time: 0.2518\n",
      "57/281, train_loss: 0.0636, step time: 0.2555\n",
      "58/281, train_loss: 0.0940, step time: 0.2546\n",
      "59/281, train_loss: 0.2350, step time: 0.2670\n",
      "60/281, train_loss: 0.0826, step time: 0.2460\n",
      "61/281, train_loss: 0.0560, step time: 0.2470\n",
      "62/281, train_loss: 0.0696, step time: 0.2522\n",
      "63/281, train_loss: 0.0915, step time: 0.2605\n",
      "64/281, train_loss: 0.0714, step time: 0.2549\n",
      "65/281, train_loss: 0.0663, step time: 0.2545\n",
      "66/281, train_loss: 0.1202, step time: 0.2568\n",
      "67/281, train_loss: 0.0977, step time: 0.2493\n",
      "68/281, train_loss: 0.2040, step time: 0.2501\n",
      "69/281, train_loss: 0.0602, step time: 0.2494\n",
      "70/281, train_loss: 0.0690, step time: 0.2601\n",
      "71/281, train_loss: 0.0493, step time: 0.2560\n",
      "72/281, train_loss: 0.0617, step time: 0.2550\n",
      "73/281, train_loss: 0.0588, step time: 0.2496\n",
      "74/281, train_loss: 0.0877, step time: 0.2474\n",
      "75/281, train_loss: 0.0681, step time: 0.2482\n",
      "76/281, train_loss: 0.0606, step time: 0.2531\n",
      "77/281, train_loss: 0.1117, step time: 0.2609\n",
      "78/281, train_loss: 0.0596, step time: 0.2561\n",
      "79/281, train_loss: 0.0651, step time: 0.2506\n",
      "80/281, train_loss: 0.0641, step time: 0.2543\n",
      "81/281, train_loss: 0.2146, step time: 0.2526\n",
      "82/281, train_loss: 0.0742, step time: 0.2511\n",
      "83/281, train_loss: 0.2355, step time: 0.2548\n",
      "84/281, train_loss: 0.0539, step time: 0.2571\n",
      "85/281, train_loss: 0.0477, step time: 0.2554\n",
      "86/281, train_loss: 0.0613, step time: 0.2541\n",
      "87/281, train_loss: 0.2924, step time: 0.2523\n",
      "88/281, train_loss: 0.0971, step time: 0.2471\n",
      "89/281, train_loss: 0.0515, step time: 0.2501\n",
      "90/281, train_loss: 0.0622, step time: 0.2526\n",
      "91/281, train_loss: 0.0706, step time: 0.2608\n",
      "92/281, train_loss: 0.0787, step time: 0.2610\n",
      "93/281, train_loss: 0.0642, step time: 0.2545\n",
      "94/281, train_loss: 0.0450, step time: 0.2525\n",
      "95/281, train_loss: 0.0892, step time: 0.2527\n",
      "96/281, train_loss: 0.0588, step time: 0.2565\n",
      "97/281, train_loss: 0.0450, step time: 0.2502\n",
      "98/281, train_loss: 0.0738, step time: 0.2565\n",
      "99/281, train_loss: 0.1116, step time: 0.2564\n",
      "100/281, train_loss: 0.0748, step time: 0.2573\n",
      "101/281, train_loss: 0.0744, step time: 0.2505\n",
      "102/281, train_loss: 0.0599, step time: 0.2601\n",
      "103/281, train_loss: 0.0906, step time: 0.2542\n",
      "104/281, train_loss: 0.2134, step time: 0.2541\n",
      "105/281, train_loss: 0.0811, step time: 0.2526\n",
      "106/281, train_loss: 0.0554, step time: 0.2486\n",
      "107/281, train_loss: 0.0684, step time: 0.2531\n",
      "108/281, train_loss: 0.0564, step time: 0.2568\n",
      "109/281, train_loss: 0.0619, step time: 0.2547\n",
      "110/281, train_loss: 0.1265, step time: 0.2530\n",
      "111/281, train_loss: 0.0510, step time: 0.2539\n",
      "112/281, train_loss: 0.0621, step time: 0.2619\n",
      "113/281, train_loss: 0.0689, step time: 0.2566\n",
      "114/281, train_loss: 0.1071, step time: 0.2547\n",
      "115/281, train_loss: 0.0440, step time: 0.2542\n",
      "116/281, train_loss: 0.0724, step time: 0.2533\n",
      "117/281, train_loss: 0.2705, step time: 0.2484\n",
      "118/281, train_loss: 0.1246, step time: 0.2517\n",
      "119/281, train_loss: 0.1115, step time: 0.2540\n",
      "120/281, train_loss: 0.0628, step time: 0.2562\n",
      "121/281, train_loss: 0.1071, step time: 0.2593\n",
      "122/281, train_loss: 0.0498, step time: 0.2515\n",
      "123/281, train_loss: 0.0919, step time: 0.2556\n",
      "124/281, train_loss: 0.0904, step time: 0.2539\n",
      "125/281, train_loss: 0.1020, step time: 0.2497\n",
      "126/281, train_loss: 0.0755, step time: 0.2541\n",
      "127/281, train_loss: 0.1084, step time: 0.2555\n",
      "128/281, train_loss: 0.0834, step time: 0.2581\n",
      "129/281, train_loss: 0.2127, step time: 0.2574\n",
      "130/281, train_loss: 0.0752, step time: 0.2567\n",
      "131/281, train_loss: 0.2302, step time: 0.2550\n",
      "132/281, train_loss: 0.0969, step time: 0.2567\n",
      "133/281, train_loss: 0.0601, step time: 0.2556\n",
      "134/281, train_loss: 0.1033, step time: 0.2532\n",
      "135/281, train_loss: 0.2196, step time: 0.2509\n",
      "136/281, train_loss: 0.1452, step time: 0.2507\n",
      "137/281, train_loss: 0.0631, step time: 0.2589\n",
      "138/281, train_loss: 0.0924, step time: 0.2582\n",
      "139/281, train_loss: 0.0602, step time: 0.2542\n",
      "140/281, train_loss: 0.2429, step time: 0.2522\n",
      "141/281, train_loss: 0.0563, step time: 0.2543\n",
      "142/281, train_loss: 0.2301, step time: 0.2598\n",
      "143/281, train_loss: 0.0806, step time: 0.2578\n",
      "144/281, train_loss: 0.2637, step time: 0.2529\n",
      "145/281, train_loss: 0.0758, step time: 0.2554\n",
      "146/281, train_loss: 0.0305, step time: 0.2565\n",
      "147/281, train_loss: 0.0658, step time: 0.2592\n",
      "148/281, train_loss: 0.0411, step time: 0.2563\n",
      "149/281, train_loss: 0.0654, step time: 0.2549\n",
      "150/281, train_loss: 0.0472, step time: 0.2507\n",
      "151/281, train_loss: 0.2470, step time: 0.2519\n",
      "152/281, train_loss: 0.0834, step time: 0.2527\n",
      "153/281, train_loss: 0.0811, step time: 0.2520\n",
      "154/281, train_loss: 0.2435, step time: 0.2534\n",
      "155/281, train_loss: 0.2463, step time: 0.2542\n",
      "156/281, train_loss: 0.0938, step time: 0.2550\n",
      "157/281, train_loss: 0.0575, step time: 0.2515\n",
      "158/281, train_loss: 0.0607, step time: 0.2546\n",
      "159/281, train_loss: 0.0539, step time: 0.2582\n",
      "160/281, train_loss: 0.2145, step time: 0.2519\n",
      "161/281, train_loss: 0.0791, step time: 0.2492\n",
      "162/281, train_loss: 0.0368, step time: 0.2583\n",
      "163/281, train_loss: 0.0487, step time: 0.2586\n",
      "164/281, train_loss: 0.0900, step time: 0.2506\n",
      "165/281, train_loss: 0.0649, step time: 0.2512\n",
      "166/281, train_loss: 0.0673, step time: 0.2543\n",
      "167/281, train_loss: 0.0883, step time: 0.2553\n",
      "168/281, train_loss: 0.0810, step time: 0.2562\n",
      "169/281, train_loss: 0.0515, step time: 0.2559\n",
      "170/281, train_loss: 0.0728, step time: 0.2557\n",
      "171/281, train_loss: 0.0702, step time: 0.2525\n",
      "172/281, train_loss: 0.0603, step time: 0.2608\n",
      "173/281, train_loss: 0.0745, step time: 0.2564\n",
      "174/281, train_loss: 0.0548, step time: 0.2536\n",
      "175/281, train_loss: 0.0405, step time: 0.2575\n",
      "176/281, train_loss: 0.0504, step time: 0.2556\n",
      "177/281, train_loss: 0.0848, step time: 0.2567\n",
      "178/281, train_loss: 0.0714, step time: 0.2533\n",
      "179/281, train_loss: 0.0452, step time: 0.2550\n",
      "180/281, train_loss: 0.0855, step time: 0.2565\n",
      "181/281, train_loss: 0.0772, step time: 0.2604\n",
      "182/281, train_loss: 0.0451, step time: 0.2574\n",
      "183/281, train_loss: 0.0754, step time: 0.2525\n",
      "184/281, train_loss: 0.0795, step time: 0.2571\n",
      "185/281, train_loss: 0.0312, step time: 0.2550\n",
      "186/281, train_loss: 0.0629, step time: 0.2592\n",
      "187/281, train_loss: 0.0491, step time: 0.2620\n",
      "188/281, train_loss: 0.1011, step time: 0.2567\n",
      "189/281, train_loss: 0.0555, step time: 0.2513\n",
      "190/281, train_loss: 0.0803, step time: 0.2534\n",
      "191/281, train_loss: 0.0807, step time: 0.2621\n",
      "192/281, train_loss: 0.0581, step time: 0.2549\n",
      "193/281, train_loss: 0.0713, step time: 0.2515\n",
      "194/281, train_loss: 0.0824, step time: 0.2486\n",
      "195/281, train_loss: 0.0554, step time: 0.2511\n",
      "196/281, train_loss: 0.0632, step time: 0.2576\n",
      "197/281, train_loss: 0.0589, step time: 0.2586\n",
      "198/281, train_loss: 0.0685, step time: 0.2588\n",
      "199/281, train_loss: 0.0474, step time: 0.2516\n",
      "200/281, train_loss: 0.0641, step time: 0.2558\n",
      "201/281, train_loss: 0.0817, step time: 0.2500\n",
      "202/281, train_loss: 0.0982, step time: 0.2533\n",
      "203/281, train_loss: 0.1111, step time: 0.2571\n",
      "204/281, train_loss: 0.0693, step time: 0.2536\n",
      "205/281, train_loss: 0.0651, step time: 0.2560\n",
      "206/281, train_loss: 0.0570, step time: 0.2561\n",
      "207/281, train_loss: 0.0707, step time: 0.2541\n",
      "208/281, train_loss: 0.0595, step time: 0.2529\n",
      "209/281, train_loss: 0.0681, step time: 0.2495\n",
      "210/281, train_loss: 0.0651, step time: 0.2564\n",
      "211/281, train_loss: 0.2165, step time: 0.2574\n",
      "212/281, train_loss: 0.0937, step time: 0.2589\n",
      "213/281, train_loss: 0.0522, step time: 0.2575\n",
      "214/281, train_loss: 0.2121, step time: 0.2567\n",
      "215/281, train_loss: 0.0658, step time: 0.2604\n",
      "216/281, train_loss: 0.0885, step time: 0.2574\n",
      "217/281, train_loss: 0.0601, step time: 0.2561\n",
      "218/281, train_loss: 0.0491, step time: 0.2586\n",
      "219/281, train_loss: 0.0505, step time: 0.2553\n",
      "220/281, train_loss: 0.2328, step time: 0.2567\n",
      "221/281, train_loss: 0.0371, step time: 0.2571\n",
      "222/281, train_loss: 0.0866, step time: 0.2561\n",
      "223/281, train_loss: 0.0830, step time: 0.2521\n",
      "224/281, train_loss: 0.0392, step time: 0.2541\n",
      "225/281, train_loss: 0.0781, step time: 0.2566\n",
      "226/281, train_loss: 0.0945, step time: 0.2515\n",
      "227/281, train_loss: 0.0584, step time: 0.2508\n",
      "228/281, train_loss: 0.0914, step time: 0.2572\n",
      "229/281, train_loss: 0.0775, step time: 0.2570\n",
      "230/281, train_loss: 0.0610, step time: 0.2505\n",
      "231/281, train_loss: 0.2262, step time: 0.2511\n",
      "232/281, train_loss: 0.1047, step time: 0.2572\n",
      "233/281, train_loss: 0.0753, step time: 0.2576\n",
      "234/281, train_loss: 0.2238, step time: 0.2576\n",
      "235/281, train_loss: 0.0686, step time: 0.2588\n",
      "236/281, train_loss: 0.0727, step time: 0.2586\n",
      "237/281, train_loss: 0.0742, step time: 0.2563\n",
      "238/281, train_loss: 0.2592, step time: 0.2588\n",
      "239/281, train_loss: 0.0519, step time: 0.2508\n",
      "240/281, train_loss: 0.0492, step time: 0.2580\n",
      "241/281, train_loss: 0.0541, step time: 0.2554\n",
      "242/281, train_loss: 0.0736, step time: 0.2557\n",
      "243/281, train_loss: 0.2598, step time: 0.2593\n",
      "244/281, train_loss: 0.0629, step time: 0.2590\n",
      "245/281, train_loss: 0.0571, step time: 0.2594\n",
      "246/281, train_loss: 0.2314, step time: 0.2579\n",
      "247/281, train_loss: 0.0712, step time: 0.2511\n",
      "248/281, train_loss: 0.2400, step time: 0.2549\n",
      "249/281, train_loss: 0.0450, step time: 0.2554\n",
      "250/281, train_loss: 0.0582, step time: 0.2548\n",
      "251/281, train_loss: 0.0835, step time: 0.2577\n",
      "252/281, train_loss: 0.0746, step time: 0.2564\n",
      "253/281, train_loss: 0.0551, step time: 0.2568\n",
      "254/281, train_loss: 0.0480, step time: 0.2533\n",
      "255/281, train_loss: 0.0668, step time: 0.2593\n",
      "256/281, train_loss: 0.0801, step time: 0.2566\n",
      "257/281, train_loss: 0.0764, step time: 0.2530\n",
      "258/281, train_loss: 0.0804, step time: 0.2567\n",
      "259/281, train_loss: 0.2485, step time: 0.2556\n",
      "260/281, train_loss: 0.2290, step time: 0.2533\n",
      "261/281, train_loss: 0.1578, step time: 0.2533\n",
      "262/281, train_loss: 0.0618, step time: 0.2547\n",
      "263/281, train_loss: 0.2332, step time: 0.2558\n",
      "264/281, train_loss: 0.0521, step time: 0.2545\n",
      "265/281, train_loss: 0.0451, step time: 0.2562\n",
      "266/281, train_loss: 0.0826, step time: 0.2555\n",
      "267/281, train_loss: 0.0325, step time: 0.2584\n",
      "268/281, train_loss: 0.2243, step time: 0.2588\n",
      "269/281, train_loss: 0.0819, step time: 0.2569\n",
      "270/281, train_loss: 0.0624, step time: 0.2574\n",
      "271/281, train_loss: 0.0721, step time: 0.2583\n",
      "272/281, train_loss: 0.0687, step time: 0.2579\n",
      "273/281, train_loss: 0.2384, step time: 0.2569\n",
      "274/281, train_loss: 0.1139, step time: 0.2566\n",
      "275/281, train_loss: 0.0833, step time: 0.2510\n",
      "276/281, train_loss: 0.0475, step time: 0.2524\n",
      "277/281, train_loss: 0.0609, step time: 0.2508\n",
      "278/281, train_loss: 0.0670, step time: 0.2513\n",
      "279/281, train_loss: 0.0785, step time: 0.2530\n",
      "280/281, train_loss: 0.0674, step time: 0.2580\n",
      "281/281, train_loss: 0.0700, step time: 0.2590\n",
      "282/281, train_loss: 0.0489, step time: 0.1556\n",
      "epoch 87 average loss: 0.0966\n",
      "saved new best metric model\n",
      "current epoch: 87 current mean dice: 0.8939 tc: 0.8873 wt: 0.9217 et: 0.8820\n",
      "best mean dice: 0.8939 at epoch: 87\n",
      "time consuming of epoch 87 is: 405.6133\n",
      "----------\n",
      "epoch 88/200\n",
      "1/281, train_loss: 0.0450, step time: 0.2656\n",
      "2/281, train_loss: 0.0536, step time: 0.2489\n",
      "3/281, train_loss: 0.0829, step time: 0.2564\n",
      "4/281, train_loss: 0.0713, step time: 0.2515\n",
      "5/281, train_loss: 0.0630, step time: 0.2549\n",
      "6/281, train_loss: 0.0883, step time: 0.2526\n",
      "7/281, train_loss: 0.0790, step time: 0.2591\n",
      "8/281, train_loss: 0.0810, step time: 0.2591\n",
      "9/281, train_loss: 0.0556, step time: 0.2535\n",
      "10/281, train_loss: 0.0630, step time: 0.2497\n",
      "11/281, train_loss: 0.0864, step time: 0.2581\n",
      "12/281, train_loss: 0.0553, step time: 0.2618\n",
      "13/281, train_loss: 0.0898, step time: 0.2627\n",
      "14/281, train_loss: 0.0505, step time: 0.2649\n",
      "15/281, train_loss: 0.0712, step time: 0.2514\n",
      "16/281, train_loss: 0.0979, step time: 0.2847\n",
      "17/281, train_loss: 0.2399, step time: 0.2590\n",
      "18/281, train_loss: 0.0582, step time: 0.2507\n",
      "19/281, train_loss: 0.0711, step time: 0.2649\n",
      "20/281, train_loss: 0.3699, step time: 0.2493\n",
      "21/281, train_loss: 0.0863, step time: 0.2541\n",
      "22/281, train_loss: 0.0493, step time: 0.2518\n",
      "23/281, train_loss: 0.1069, step time: 0.2550\n",
      "24/281, train_loss: 0.0585, step time: 0.2582\n",
      "25/281, train_loss: 0.0502, step time: 0.2558\n",
      "26/281, train_loss: 0.0952, step time: 0.2565\n",
      "27/281, train_loss: 0.0751, step time: 0.2520\n",
      "28/281, train_loss: 0.1093, step time: 0.2591\n",
      "29/281, train_loss: 0.0757, step time: 0.2582\n",
      "30/281, train_loss: 0.0726, step time: 0.2639\n",
      "31/281, train_loss: 0.0887, step time: 0.2588\n",
      "32/281, train_loss: 0.2144, step time: 0.2587\n",
      "33/281, train_loss: 0.0737, step time: 0.2465\n",
      "34/281, train_loss: 0.0691, step time: 0.2467\n",
      "35/281, train_loss: 0.0497, step time: 0.2511\n",
      "36/281, train_loss: 0.0780, step time: 0.2496\n",
      "37/281, train_loss: 0.0707, step time: 0.2523\n",
      "38/281, train_loss: 0.0821, step time: 0.2507\n",
      "39/281, train_loss: 0.2342, step time: 0.2512\n",
      "40/281, train_loss: 0.0559, step time: 0.2555\n",
      "41/281, train_loss: 0.0934, step time: 0.2530\n",
      "42/281, train_loss: 0.2128, step time: 0.2536\n",
      "43/281, train_loss: 0.1145, step time: 0.2570\n",
      "44/281, train_loss: 0.0537, step time: 0.2583\n",
      "45/281, train_loss: 0.0698, step time: 0.2510\n",
      "46/281, train_loss: 0.0533, step time: 0.2483\n",
      "47/281, train_loss: 0.0619, step time: 0.2505\n",
      "48/281, train_loss: 0.2217, step time: 0.2554\n",
      "49/281, train_loss: 0.2253, step time: 0.2566\n",
      "50/281, train_loss: 0.2170, step time: 0.2502\n",
      "51/281, train_loss: 0.1321, step time: 0.2500\n",
      "52/281, train_loss: 0.0749, step time: 0.2504\n",
      "53/281, train_loss: 0.0471, step time: 0.2561\n",
      "54/281, train_loss: 0.0559, step time: 0.2549\n",
      "55/281, train_loss: 0.0794, step time: 0.2521\n",
      "56/281, train_loss: 0.0856, step time: 0.2514\n",
      "57/281, train_loss: 0.0648, step time: 0.2623\n",
      "58/281, train_loss: 0.0654, step time: 0.2592\n",
      "59/281, train_loss: 0.0391, step time: 0.2597\n",
      "60/281, train_loss: 0.0666, step time: 0.2613\n",
      "61/281, train_loss: 0.0851, step time: 0.2603\n",
      "62/281, train_loss: 0.0898, step time: 0.2583\n",
      "63/281, train_loss: 0.0920, step time: 0.2579\n",
      "64/281, train_loss: 0.0751, step time: 0.2520\n",
      "65/281, train_loss: 0.0799, step time: 0.2618\n",
      "66/281, train_loss: 0.0689, step time: 0.2557\n",
      "67/281, train_loss: 0.0758, step time: 0.2583\n",
      "68/281, train_loss: 0.0946, step time: 0.2603\n",
      "69/281, train_loss: 0.0509, step time: 0.2593\n",
      "70/281, train_loss: 0.0727, step time: 0.2591\n",
      "71/281, train_loss: 0.2291, step time: 0.2534\n",
      "72/281, train_loss: 0.0680, step time: 0.2551\n",
      "73/281, train_loss: 0.1153, step time: 0.2620\n",
      "74/281, train_loss: 0.1144, step time: 0.2570\n",
      "75/281, train_loss: 0.0762, step time: 0.2581\n",
      "76/281, train_loss: 0.0828, step time: 0.2555\n",
      "77/281, train_loss: 0.0645, step time: 0.2538\n",
      "78/281, train_loss: 0.2388, step time: 0.2577\n",
      "79/281, train_loss: 0.0659, step time: 0.2523\n",
      "80/281, train_loss: 0.0735, step time: 0.2562\n",
      "81/281, train_loss: 0.0511, step time: 0.2539\n",
      "82/281, train_loss: 0.0612, step time: 0.2582\n",
      "83/281, train_loss: 0.0641, step time: 0.2571\n",
      "84/281, train_loss: 0.0802, step time: 0.2543\n",
      "85/281, train_loss: 0.0777, step time: 0.2559\n",
      "86/281, train_loss: 0.3833, step time: 0.2549\n",
      "87/281, train_loss: 0.0816, step time: 0.2566\n",
      "88/281, train_loss: 0.0673, step time: 0.2570\n",
      "89/281, train_loss: 0.0771, step time: 0.2573\n",
      "90/281, train_loss: 0.1315, step time: 0.2561\n",
      "91/281, train_loss: 0.1022, step time: 0.2527\n",
      "92/281, train_loss: 0.0738, step time: 0.2465\n",
      "93/281, train_loss: 0.0512, step time: 0.2633\n",
      "94/281, train_loss: 0.0761, step time: 0.2706\n",
      "95/281, train_loss: 0.0775, step time: 0.2607\n",
      "96/281, train_loss: 0.0931, step time: 0.2615\n",
      "97/281, train_loss: 0.0750, step time: 0.2563\n",
      "98/281, train_loss: 0.0750, step time: 0.2554\n",
      "99/281, train_loss: 0.0785, step time: 0.2518\n",
      "100/281, train_loss: 0.0554, step time: 0.2502\n",
      "101/281, train_loss: 0.0734, step time: 0.2510\n",
      "102/281, train_loss: 0.0413, step time: 0.2514\n",
      "103/281, train_loss: 0.0384, step time: 0.2531\n",
      "104/281, train_loss: 0.0785, step time: 0.2539\n",
      "105/281, train_loss: 0.0719, step time: 0.2483\n",
      "106/281, train_loss: 0.0743, step time: 0.2505\n",
      "107/281, train_loss: 0.0507, step time: 0.2473\n",
      "108/281, train_loss: 0.0479, step time: 0.2475\n",
      "109/281, train_loss: 0.0503, step time: 0.2564\n",
      "110/281, train_loss: 0.0886, step time: 0.2541\n",
      "111/281, train_loss: 0.0611, step time: 0.2551\n",
      "112/281, train_loss: 0.0860, step time: 0.2539\n",
      "113/281, train_loss: 0.0596, step time: 0.2474\n",
      "114/281, train_loss: 0.0894, step time: 0.2473\n",
      "115/281, train_loss: 0.0720, step time: 0.2527\n",
      "116/281, train_loss: 0.2179, step time: 0.2520\n",
      "117/281, train_loss: 0.1014, step time: 0.2631\n",
      "118/281, train_loss: 0.2310, step time: 0.2565\n",
      "119/281, train_loss: 0.0875, step time: 0.2546\n",
      "120/281, train_loss: 0.0735, step time: 0.2503\n",
      "121/281, train_loss: 0.0664, step time: 0.2573\n",
      "122/281, train_loss: 0.0714, step time: 0.2547\n",
      "123/281, train_loss: 0.0901, step time: 0.2569\n",
      "124/281, train_loss: 0.0942, step time: 0.2523\n",
      "125/281, train_loss: 0.1294, step time: 0.2528\n",
      "126/281, train_loss: 0.0801, step time: 0.2580\n",
      "127/281, train_loss: 0.0334, step time: 0.2595\n",
      "128/281, train_loss: 0.0826, step time: 0.2566\n",
      "129/281, train_loss: 0.0965, step time: 0.2527\n",
      "130/281, train_loss: 0.0510, step time: 0.2496\n",
      "131/281, train_loss: 0.0487, step time: 0.2525\n",
      "132/281, train_loss: 0.0908, step time: 0.2518\n",
      "133/281, train_loss: 0.2379, step time: 0.2553\n",
      "134/281, train_loss: 0.0760, step time: 0.2532\n",
      "135/281, train_loss: 0.0925, step time: 0.2527\n",
      "136/281, train_loss: 0.0949, step time: 0.2602\n",
      "137/281, train_loss: 0.0732, step time: 0.2540\n",
      "138/281, train_loss: 0.0547, step time: 0.2517\n",
      "139/281, train_loss: 0.1048, step time: 0.2540\n",
      "140/281, train_loss: 0.2269, step time: 0.2503\n",
      "141/281, train_loss: 0.0454, step time: 0.2510\n",
      "142/281, train_loss: 0.0539, step time: 0.2534\n",
      "143/281, train_loss: 0.0504, step time: 0.2548\n",
      "144/281, train_loss: 0.0685, step time: 0.2513\n",
      "145/281, train_loss: 0.0575, step time: 0.2499\n",
      "146/281, train_loss: 0.0860, step time: 0.2547\n",
      "147/281, train_loss: 0.0288, step time: 0.2495\n",
      "148/281, train_loss: 0.0541, step time: 0.2530\n",
      "149/281, train_loss: 0.2328, step time: 0.2498\n",
      "150/281, train_loss: 0.0571, step time: 0.2644\n",
      "151/281, train_loss: 0.0575, step time: 0.2544\n",
      "152/281, train_loss: 0.2437, step time: 0.2547\n",
      "153/281, train_loss: 0.0521, step time: 0.2513\n",
      "154/281, train_loss: 0.0707, step time: 0.2554\n",
      "155/281, train_loss: 0.0546, step time: 0.2484\n",
      "156/281, train_loss: 0.0338, step time: 0.2545\n",
      "157/281, train_loss: 0.0739, step time: 0.2466\n",
      "158/281, train_loss: 0.0885, step time: 0.2538\n",
      "159/281, train_loss: 0.0469, step time: 0.2539\n",
      "160/281, train_loss: 0.0808, step time: 0.2560\n",
      "161/281, train_loss: 0.0620, step time: 0.2543\n",
      "162/281, train_loss: 0.1005, step time: 0.2605\n",
      "163/281, train_loss: 0.0531, step time: 0.2614\n",
      "164/281, train_loss: 0.2243, step time: 0.2555\n",
      "165/281, train_loss: 0.0488, step time: 0.2543\n",
      "166/281, train_loss: 0.2035, step time: 0.2488\n",
      "167/281, train_loss: 0.4043, step time: 0.2506\n",
      "168/281, train_loss: 0.1002, step time: 0.2465\n",
      "169/281, train_loss: 0.2337, step time: 0.2521\n",
      "170/281, train_loss: 0.2157, step time: 0.2542\n",
      "171/281, train_loss: 0.0554, step time: 0.2559\n",
      "172/281, train_loss: 0.0605, step time: 0.2522\n",
      "173/281, train_loss: 0.0590, step time: 0.2545\n",
      "174/281, train_loss: 0.0645, step time: 0.2504\n",
      "175/281, train_loss: 0.0606, step time: 0.2534\n",
      "176/281, train_loss: 0.0529, step time: 0.2503\n",
      "177/281, train_loss: 0.2326, step time: 0.2577\n",
      "178/281, train_loss: 0.1053, step time: 0.2507\n",
      "179/281, train_loss: 0.2392, step time: 0.2514\n",
      "180/281, train_loss: 0.0615, step time: 0.2578\n",
      "181/281, train_loss: 0.0697, step time: 0.2558\n",
      "182/281, train_loss: 0.0742, step time: 0.2587\n",
      "183/281, train_loss: 0.0579, step time: 0.2585\n",
      "184/281, train_loss: 0.0439, step time: 0.2571\n",
      "185/281, train_loss: 0.0543, step time: 0.2556\n",
      "186/281, train_loss: 0.0523, step time: 0.2548\n",
      "187/281, train_loss: 0.2355, step time: 0.2557\n",
      "188/281, train_loss: 0.0959, step time: 0.2559\n",
      "189/281, train_loss: 0.0842, step time: 0.2491\n",
      "190/281, train_loss: 0.3051, step time: 0.2554\n",
      "191/281, train_loss: 0.0742, step time: 0.2577\n",
      "192/281, train_loss: 0.0750, step time: 0.2603\n",
      "193/281, train_loss: 0.0491, step time: 0.2559\n",
      "194/281, train_loss: 0.0553, step time: 0.2549\n",
      "195/281, train_loss: 0.0704, step time: 0.2542\n",
      "196/281, train_loss: 0.0686, step time: 0.2521\n",
      "197/281, train_loss: 0.0669, step time: 0.2532\n",
      "198/281, train_loss: 0.0656, step time: 0.2524\n",
      "199/281, train_loss: 0.0485, step time: 0.2563\n",
      "200/281, train_loss: 0.1032, step time: 0.2570\n",
      "201/281, train_loss: 0.0618, step time: 0.2574\n",
      "202/281, train_loss: 0.0814, step time: 0.2524\n",
      "203/281, train_loss: 0.0454, step time: 0.2568\n",
      "204/281, train_loss: 0.0630, step time: 0.2535\n",
      "205/281, train_loss: 0.0662, step time: 0.2583\n",
      "206/281, train_loss: 0.0526, step time: 0.2596\n",
      "207/281, train_loss: 0.0576, step time: 0.2541\n",
      "208/281, train_loss: 0.0819, step time: 0.2519\n",
      "209/281, train_loss: 0.0452, step time: 0.2518\n",
      "210/281, train_loss: 0.0618, step time: 0.2548\n",
      "211/281, train_loss: 0.1202, step time: 0.2590\n",
      "212/281, train_loss: 0.2216, step time: 0.2542\n",
      "213/281, train_loss: 0.0590, step time: 0.2465\n",
      "214/281, train_loss: 0.0535, step time: 0.2503\n",
      "215/281, train_loss: 0.0425, step time: 0.2580\n",
      "216/281, train_loss: 0.0691, step time: 0.2514\n",
      "217/281, train_loss: 0.2390, step time: 0.2502\n",
      "218/281, train_loss: 0.2293, step time: 0.2480\n",
      "219/281, train_loss: 0.0665, step time: 0.2549\n",
      "220/281, train_loss: 0.2264, step time: 0.2542\n",
      "221/281, train_loss: 0.0685, step time: 0.2542\n",
      "222/281, train_loss: 0.0756, step time: 0.2552\n",
      "223/281, train_loss: 0.0764, step time: 0.2558\n",
      "224/281, train_loss: 0.0582, step time: 0.2483\n",
      "225/281, train_loss: 0.0490, step time: 0.2503\n",
      "226/281, train_loss: 0.0605, step time: 0.2475\n",
      "227/281, train_loss: 0.1187, step time: 0.2533\n",
      "228/281, train_loss: 0.0760, step time: 0.2541\n",
      "229/281, train_loss: 0.0366, step time: 0.2525\n",
      "230/281, train_loss: 0.2164, step time: 0.2512\n",
      "231/281, train_loss: 0.2366, step time: 0.2521\n",
      "232/281, train_loss: 0.0873, step time: 0.2534\n",
      "233/281, train_loss: 0.0822, step time: 0.2543\n",
      "234/281, train_loss: 0.0629, step time: 0.2531\n",
      "235/281, train_loss: 0.2208, step time: 0.2596\n",
      "236/281, train_loss: 0.0782, step time: 0.2537\n",
      "237/281, train_loss: 0.0797, step time: 0.2633\n",
      "238/281, train_loss: 0.1017, step time: 0.2536\n",
      "239/281, train_loss: 0.1033, step time: 0.2502\n",
      "240/281, train_loss: 0.0651, step time: 0.2518\n",
      "241/281, train_loss: 0.0704, step time: 0.2505\n",
      "242/281, train_loss: 0.0548, step time: 0.2514\n",
      "243/281, train_loss: 0.2176, step time: 0.2556\n",
      "244/281, train_loss: 0.0792, step time: 0.2531\n",
      "245/281, train_loss: 0.0526, step time: 0.2531\n",
      "246/281, train_loss: 0.0568, step time: 0.2514\n",
      "247/281, train_loss: 0.0599, step time: 0.2758\n",
      "248/281, train_loss: 0.0606, step time: 0.2570\n",
      "249/281, train_loss: 0.0603, step time: 0.2524\n",
      "250/281, train_loss: 0.0814, step time: 0.2490\n",
      "251/281, train_loss: 0.0537, step time: 0.2522\n",
      "252/281, train_loss: 0.0936, step time: 0.2534\n",
      "253/281, train_loss: 0.1021, step time: 0.2556\n",
      "254/281, train_loss: 0.0533, step time: 0.2526\n",
      "255/281, train_loss: 0.0913, step time: 0.2521\n",
      "256/281, train_loss: 0.2142, step time: 0.2541\n",
      "257/281, train_loss: 0.0545, step time: 0.2509\n",
      "258/281, train_loss: 0.0706, step time: 0.2513\n",
      "259/281, train_loss: 0.0457, step time: 0.2586\n",
      "260/281, train_loss: 0.0803, step time: 0.2546\n",
      "261/281, train_loss: 0.0610, step time: 0.2542\n",
      "262/281, train_loss: 0.0615, step time: 0.2547\n",
      "263/281, train_loss: 0.0717, step time: 0.2565\n",
      "264/281, train_loss: 0.2097, step time: 0.2539\n",
      "265/281, train_loss: 0.0488, step time: 0.2476\n",
      "266/281, train_loss: 0.0554, step time: 0.2504\n",
      "267/281, train_loss: 0.0834, step time: 0.2522\n",
      "268/281, train_loss: 0.2414, step time: 0.2510\n",
      "269/281, train_loss: 0.0716, step time: 0.2519\n",
      "270/281, train_loss: 0.0893, step time: 0.2544\n",
      "271/281, train_loss: 0.0788, step time: 0.2547\n",
      "272/281, train_loss: 0.0964, step time: 0.2540\n",
      "273/281, train_loss: 0.2425, step time: 0.2515\n",
      "274/281, train_loss: 0.0687, step time: 0.2513\n",
      "275/281, train_loss: 0.0789, step time: 0.2505\n",
      "276/281, train_loss: 0.0688, step time: 0.2585\n",
      "277/281, train_loss: 0.0596, step time: 0.2514\n",
      "278/281, train_loss: 0.0638, step time: 0.2502\n",
      "279/281, train_loss: 0.2453, step time: 0.2533\n",
      "280/281, train_loss: 0.0855, step time: 0.2524\n",
      "281/281, train_loss: 0.0626, step time: 0.2441\n",
      "282/281, train_loss: 0.3790, step time: 0.1462\n",
      "epoch 88 average loss: 0.0959\n",
      "saved new best metric model\n",
      "current epoch: 88 current mean dice: 0.8944 tc: 0.8856 wt: 0.9231 et: 0.8840\n",
      "best mean dice: 0.8944 at epoch: 88\n",
      "time consuming of epoch 88 is: 406.2855\n",
      "----------\n",
      "epoch 89/200\n",
      "1/281, train_loss: 0.0842, step time: 0.2512\n",
      "2/281, train_loss: 0.0976, step time: 0.2443\n",
      "3/281, train_loss: 0.0939, step time: 0.2438\n",
      "4/281, train_loss: 0.0737, step time: 0.2489\n",
      "5/281, train_loss: 0.0607, step time: 0.2448\n",
      "6/281, train_loss: 0.2290, step time: 0.2509\n",
      "7/281, train_loss: 0.0524, step time: 0.2459\n",
      "8/281, train_loss: 0.0671, step time: 0.2517\n",
      "9/281, train_loss: 0.0747, step time: 0.2516\n",
      "10/281, train_loss: 0.2155, step time: 0.2493\n",
      "11/281, train_loss: 0.2244, step time: 0.2470\n",
      "12/281, train_loss: 0.0714, step time: 0.2478\n",
      "13/281, train_loss: 0.0585, step time: 0.2444\n",
      "14/281, train_loss: 0.2242, step time: 0.2425\n",
      "15/281, train_loss: 0.0512, step time: 0.2469\n",
      "16/281, train_loss: 0.0989, step time: 0.2504\n",
      "17/281, train_loss: 0.0386, step time: 0.2479\n",
      "18/281, train_loss: 0.0573, step time: 0.2501\n",
      "19/281, train_loss: 0.0738, step time: 0.2537\n",
      "20/281, train_loss: 0.0836, step time: 0.2531\n",
      "21/281, train_loss: 0.0402, step time: 0.2561\n",
      "22/281, train_loss: 0.0569, step time: 0.2530\n",
      "23/281, train_loss: 0.0765, step time: 0.2513\n",
      "24/281, train_loss: 0.0427, step time: 0.2575\n",
      "25/281, train_loss: 0.2261, step time: 0.2472\n",
      "26/281, train_loss: 0.0880, step time: 0.2475\n",
      "27/281, train_loss: 0.1022, step time: 0.2437\n",
      "28/281, train_loss: 0.0364, step time: 0.2525\n",
      "29/281, train_loss: 0.0944, step time: 0.2532\n",
      "30/281, train_loss: 0.0987, step time: 0.2504\n",
      "31/281, train_loss: 0.0422, step time: 0.2495\n",
      "32/281, train_loss: 0.2260, step time: 0.2590\n",
      "33/281, train_loss: 0.0702, step time: 0.2534\n",
      "34/281, train_loss: 0.0779, step time: 0.2527\n",
      "35/281, train_loss: 0.0501, step time: 0.2500\n",
      "36/281, train_loss: 0.0987, step time: 0.2456\n",
      "37/281, train_loss: 0.0547, step time: 0.2479\n",
      "38/281, train_loss: 0.0501, step time: 0.2478\n",
      "39/281, train_loss: 0.2217, step time: 0.2540\n",
      "40/281, train_loss: 0.0905, step time: 0.2462\n",
      "41/281, train_loss: 0.1240, step time: 0.2554\n",
      "42/281, train_loss: 0.0477, step time: 0.2505\n",
      "43/281, train_loss: 0.0554, step time: 0.2476\n",
      "44/281, train_loss: 0.0630, step time: 0.2481\n",
      "45/281, train_loss: 0.0583, step time: 0.2510\n",
      "46/281, train_loss: 0.0618, step time: 0.2483\n",
      "47/281, train_loss: 0.0677, step time: 0.2503\n",
      "48/281, train_loss: 0.0627, step time: 0.2568\n",
      "49/281, train_loss: 0.2122, step time: 0.2495\n",
      "50/281, train_loss: 0.1222, step time: 0.2468\n",
      "51/281, train_loss: 0.0702, step time: 0.2521\n",
      "52/281, train_loss: 0.0807, step time: 0.2477\n",
      "53/281, train_loss: 0.0900, step time: 0.2437\n",
      "54/281, train_loss: 0.0486, step time: 0.2523\n",
      "55/281, train_loss: 0.0895, step time: 0.2550\n",
      "56/281, train_loss: 0.0472, step time: 0.2472\n",
      "57/281, train_loss: 0.0440, step time: 0.2476\n",
      "58/281, train_loss: 0.0524, step time: 0.2473\n",
      "59/281, train_loss: 0.0632, step time: 0.2418\n",
      "60/281, train_loss: 0.0743, step time: 0.2515\n",
      "61/281, train_loss: 0.0642, step time: 0.2521\n",
      "62/281, train_loss: 0.2424, step time: 0.2471\n",
      "63/281, train_loss: 0.2178, step time: 0.2519\n",
      "64/281, train_loss: 0.0840, step time: 0.2481\n",
      "65/281, train_loss: 0.0851, step time: 0.2539\n",
      "66/281, train_loss: 0.1230, step time: 0.2704\n",
      "67/281, train_loss: 0.0586, step time: 0.2515\n",
      "68/281, train_loss: 0.2559, step time: 0.2479\n",
      "69/281, train_loss: 0.0804, step time: 0.2503\n",
      "70/281, train_loss: 0.0680, step time: 0.2497\n",
      "71/281, train_loss: 0.2004, step time: 0.2514\n",
      "72/281, train_loss: 0.0852, step time: 0.2522\n",
      "73/281, train_loss: 0.0672, step time: 0.2542\n",
      "74/281, train_loss: 0.0885, step time: 0.2518\n",
      "75/281, train_loss: 0.0708, step time: 0.2544\n",
      "76/281, train_loss: 0.0704, step time: 0.2543\n",
      "77/281, train_loss: 0.0628, step time: 0.2489\n",
      "78/281, train_loss: 0.0910, step time: 0.2479\n",
      "79/281, train_loss: 0.0570, step time: 0.2487\n",
      "80/281, train_loss: 0.0617, step time: 0.2478\n",
      "81/281, train_loss: 0.1037, step time: 0.2682\n",
      "82/281, train_loss: 0.0634, step time: 0.2514\n",
      "83/281, train_loss: 0.2157, step time: 0.2503\n",
      "84/281, train_loss: 0.0894, step time: 0.2513\n",
      "85/281, train_loss: 0.0979, step time: 0.2486\n",
      "86/281, train_loss: 0.2079, step time: 0.2498\n",
      "87/281, train_loss: 0.0844, step time: 0.2555\n",
      "88/281, train_loss: 0.0546, step time: 0.2487\n",
      "89/281, train_loss: 0.0952, step time: 0.2526\n",
      "90/281, train_loss: 0.0528, step time: 0.2525\n",
      "91/281, train_loss: 0.0791, step time: 0.2563\n",
      "92/281, train_loss: 0.0702, step time: 0.2527\n",
      "93/281, train_loss: 0.0500, step time: 0.2536\n",
      "94/281, train_loss: 0.0581, step time: 0.2498\n",
      "95/281, train_loss: 0.0716, step time: 0.2572\n",
      "96/281, train_loss: 0.1099, step time: 0.2448\n",
      "97/281, train_loss: 0.1449, step time: 0.2766\n",
      "98/281, train_loss: 0.2788, step time: 0.2582\n",
      "99/281, train_loss: 0.0593, step time: 0.2539\n",
      "100/281, train_loss: 0.0504, step time: 0.2515\n",
      "101/281, train_loss: 0.0753, step time: 0.2512\n",
      "102/281, train_loss: 0.0566, step time: 0.2429\n",
      "103/281, train_loss: 0.0971, step time: 0.2449\n",
      "104/281, train_loss: 0.0769, step time: 0.2473\n",
      "105/281, train_loss: 0.0652, step time: 0.2561\n",
      "106/281, train_loss: 0.0754, step time: 0.2526\n",
      "107/281, train_loss: 0.0343, step time: 0.2527\n",
      "108/281, train_loss: 0.2053, step time: 0.2600\n",
      "109/281, train_loss: 0.0581, step time: 0.2528\n",
      "110/281, train_loss: 0.0615, step time: 0.2544\n",
      "111/281, train_loss: 0.1090, step time: 0.2547\n",
      "112/281, train_loss: 0.0709, step time: 0.2501\n",
      "113/281, train_loss: 0.2145, step time: 0.2541\n",
      "114/281, train_loss: 0.1003, step time: 0.2569\n",
      "115/281, train_loss: 0.0588, step time: 0.2454\n",
      "116/281, train_loss: 0.0712, step time: 0.2568\n",
      "117/281, train_loss: 0.0772, step time: 0.2545\n",
      "118/281, train_loss: 0.0521, step time: 0.2570\n",
      "119/281, train_loss: 0.0879, step time: 0.2536\n",
      "120/281, train_loss: 0.0790, step time: 0.2578\n",
      "121/281, train_loss: 0.0522, step time: 0.2517\n",
      "122/281, train_loss: 0.2160, step time: 0.2498\n",
      "123/281, train_loss: 0.0723, step time: 0.2580\n",
      "124/281, train_loss: 0.2187, step time: 0.2535\n",
      "125/281, train_loss: 0.0681, step time: 0.2498\n",
      "126/281, train_loss: 0.1148, step time: 0.2479\n",
      "127/281, train_loss: 0.2094, step time: 0.2538\n",
      "128/281, train_loss: 0.0682, step time: 0.2596\n",
      "129/281, train_loss: 0.0736, step time: 0.2524\n",
      "130/281, train_loss: 0.0473, step time: 0.2488\n",
      "131/281, train_loss: 0.0829, step time: 0.2599\n",
      "132/281, train_loss: 0.0571, step time: 0.2532\n",
      "133/281, train_loss: 0.0568, step time: 0.2525\n",
      "134/281, train_loss: 0.0902, step time: 0.2551\n",
      "135/281, train_loss: 0.0665, step time: 0.2547\n",
      "136/281, train_loss: 0.1024, step time: 0.2593\n",
      "137/281, train_loss: 0.2212, step time: 0.2572\n",
      "138/281, train_loss: 0.0463, step time: 0.2567\n",
      "139/281, train_loss: 0.0675, step time: 0.2496\n",
      "140/281, train_loss: 0.0893, step time: 0.2521\n",
      "141/281, train_loss: 0.2321, step time: 0.2580\n",
      "142/281, train_loss: 0.2503, step time: 0.2510\n",
      "143/281, train_loss: 0.0774, step time: 0.2516\n",
      "144/281, train_loss: 0.2356, step time: 0.2564\n",
      "145/281, train_loss: 0.0438, step time: 0.2530\n",
      "146/281, train_loss: 0.0543, step time: 0.2490\n",
      "147/281, train_loss: 0.0669, step time: 0.2546\n",
      "148/281, train_loss: 0.1157, step time: 0.2545\n",
      "149/281, train_loss: 0.2483, step time: 0.2548\n",
      "150/281, train_loss: 0.2176, step time: 0.2543\n",
      "151/281, train_loss: 0.1097, step time: 0.2491\n",
      "152/281, train_loss: 0.0769, step time: 0.2555\n",
      "153/281, train_loss: 0.2508, step time: 0.2509\n",
      "154/281, train_loss: 0.0762, step time: 0.2533\n",
      "155/281, train_loss: 0.1164, step time: 0.2534\n",
      "156/281, train_loss: 0.2372, step time: 0.2532\n",
      "157/281, train_loss: 0.0598, step time: 0.2555\n",
      "158/281, train_loss: 0.0782, step time: 0.2480\n",
      "159/281, train_loss: 0.0545, step time: 0.2510\n",
      "160/281, train_loss: 0.0475, step time: 0.2479\n",
      "161/281, train_loss: 0.0731, step time: 0.2497\n",
      "162/281, train_loss: 0.2520, step time: 0.2419\n",
      "163/281, train_loss: 0.0514, step time: 0.2459\n",
      "164/281, train_loss: 0.0551, step time: 0.2548\n",
      "165/281, train_loss: 0.0835, step time: 0.2540\n",
      "166/281, train_loss: 0.0559, step time: 0.2513\n",
      "167/281, train_loss: 0.0985, step time: 0.2531\n",
      "168/281, train_loss: 0.0402, step time: 0.2563\n",
      "169/281, train_loss: 0.0568, step time: 0.2520\n",
      "170/281, train_loss: 0.0671, step time: 0.2518\n",
      "171/281, train_loss: 0.0627, step time: 0.2478\n",
      "172/281, train_loss: 0.0556, step time: 0.2518\n",
      "173/281, train_loss: 0.2396, step time: 0.2565\n",
      "174/281, train_loss: 0.0621, step time: 0.2522\n",
      "175/281, train_loss: 0.0614, step time: 0.2553\n",
      "176/281, train_loss: 0.0903, step time: 0.2496\n",
      "177/281, train_loss: 0.0921, step time: 0.2488\n",
      "178/281, train_loss: 0.0647, step time: 0.2513\n",
      "179/281, train_loss: 0.0721, step time: 0.2554\n",
      "180/281, train_loss: 0.0740, step time: 0.2741\n",
      "181/281, train_loss: 0.0653, step time: 0.2586\n",
      "182/281, train_loss: 0.0668, step time: 0.2561\n",
      "183/281, train_loss: 0.0628, step time: 0.2595\n",
      "184/281, train_loss: 0.1162, step time: 0.2552\n",
      "185/281, train_loss: 0.0390, step time: 0.2546\n",
      "186/281, train_loss: 0.0745, step time: 0.2611\n",
      "187/281, train_loss: 0.0327, step time: 0.2588\n",
      "188/281, train_loss: 0.0599, step time: 0.2538\n",
      "189/281, train_loss: 0.0572, step time: 0.2535\n",
      "190/281, train_loss: 0.0891, step time: 0.2510\n",
      "191/281, train_loss: 0.0961, step time: 0.2554\n",
      "192/281, train_loss: 0.0703, step time: 0.2664\n",
      "193/281, train_loss: 0.0455, step time: 0.2560\n",
      "194/281, train_loss: 0.0844, step time: 0.2519\n",
      "195/281, train_loss: 0.0791, step time: 0.2508\n",
      "196/281, train_loss: 0.0889, step time: 0.2539\n",
      "197/281, train_loss: 0.0671, step time: 0.2609\n",
      "198/281, train_loss: 0.0666, step time: 0.2568\n",
      "199/281, train_loss: 0.0639, step time: 0.2516\n",
      "200/281, train_loss: 0.2589, step time: 0.2485\n",
      "201/281, train_loss: 0.0432, step time: 0.2564\n",
      "202/281, train_loss: 0.0766, step time: 0.2575\n",
      "203/281, train_loss: 0.0439, step time: 0.2530\n",
      "204/281, train_loss: 0.0933, step time: 0.2482\n",
      "205/281, train_loss: 0.0501, step time: 0.2548\n",
      "206/281, train_loss: 0.0599, step time: 0.2567\n",
      "207/281, train_loss: 0.0768, step time: 0.2555\n",
      "208/281, train_loss: 0.0604, step time: 0.2529\n",
      "209/281, train_loss: 0.0739, step time: 0.2580\n",
      "210/281, train_loss: 0.0815, step time: 0.2602\n",
      "211/281, train_loss: 0.0343, step time: 0.2574\n",
      "212/281, train_loss: 0.0507, step time: 0.2501\n",
      "213/281, train_loss: 0.0652, step time: 0.2572\n",
      "214/281, train_loss: 0.0899, step time: 0.2627\n",
      "215/281, train_loss: 0.0871, step time: 0.2548\n",
      "216/281, train_loss: 0.0723, step time: 0.2531\n",
      "217/281, train_loss: 0.2028, step time: 0.2571\n",
      "218/281, train_loss: 0.0747, step time: 0.2553\n",
      "219/281, train_loss: 0.0477, step time: 0.2595\n",
      "220/281, train_loss: 0.2380, step time: 0.2541\n",
      "221/281, train_loss: 0.0790, step time: 0.2577\n",
      "222/281, train_loss: 0.0703, step time: 0.2577\n",
      "223/281, train_loss: 0.0714, step time: 0.2562\n",
      "224/281, train_loss: 0.0860, step time: 0.2573\n",
      "225/281, train_loss: 0.0810, step time: 0.2545\n",
      "226/281, train_loss: 0.0869, step time: 0.2551\n",
      "227/281, train_loss: 0.0872, step time: 0.2549\n",
      "228/281, train_loss: 0.2239, step time: 0.2554\n",
      "229/281, train_loss: 0.0922, step time: 0.2516\n",
      "230/281, train_loss: 0.0608, step time: 0.2539\n",
      "231/281, train_loss: 0.0796, step time: 0.2526\n",
      "232/281, train_loss: 0.0747, step time: 0.2511\n",
      "233/281, train_loss: 0.0468, step time: 0.2569\n",
      "234/281, train_loss: 0.0572, step time: 0.2549\n",
      "235/281, train_loss: 0.0384, step time: 0.2566\n",
      "236/281, train_loss: 0.0669, step time: 0.2524\n",
      "237/281, train_loss: 0.0703, step time: 0.2546\n",
      "238/281, train_loss: 0.0640, step time: 0.2554\n",
      "239/281, train_loss: 0.2153, step time: 0.2545\n",
      "240/281, train_loss: 0.0753, step time: 0.2500\n",
      "241/281, train_loss: 0.0268, step time: 0.2540\n",
      "242/281, train_loss: 0.2453, step time: 0.2550\n",
      "243/281, train_loss: 0.1481, step time: 0.2543\n",
      "244/281, train_loss: 0.1067, step time: 0.2576\n",
      "245/281, train_loss: 0.0813, step time: 0.2574\n",
      "246/281, train_loss: 0.0511, step time: 0.2599\n",
      "247/281, train_loss: 0.0516, step time: 0.2581\n",
      "248/281, train_loss: 0.0933, step time: 0.2616\n",
      "249/281, train_loss: 0.0471, step time: 0.2564\n",
      "250/281, train_loss: 0.0774, step time: 0.2533\n",
      "251/281, train_loss: 0.0483, step time: 0.2519\n",
      "252/281, train_loss: 0.0637, step time: 0.2572\n",
      "253/281, train_loss: 0.0561, step time: 0.2720\n",
      "254/281, train_loss: 0.0460, step time: 0.2677\n",
      "255/281, train_loss: 0.2224, step time: 0.2566\n",
      "256/281, train_loss: 0.0845, step time: 0.2627\n",
      "257/281, train_loss: 0.0840, step time: 0.2584\n",
      "258/281, train_loss: 0.0596, step time: 0.2523\n",
      "259/281, train_loss: 0.0518, step time: 0.2582\n",
      "260/281, train_loss: 0.0836, step time: 0.2578\n",
      "261/281, train_loss: 0.0463, step time: 0.2533\n",
      "262/281, train_loss: 0.2215, step time: 0.2587\n",
      "263/281, train_loss: 0.0532, step time: 0.2580\n",
      "264/281, train_loss: 0.0664, step time: 0.2581\n",
      "265/281, train_loss: 0.0519, step time: 0.2598\n",
      "266/281, train_loss: 0.0588, step time: 0.2559\n",
      "267/281, train_loss: 0.0419, step time: 0.2532\n",
      "268/281, train_loss: 0.2489, step time: 0.2600\n",
      "269/281, train_loss: 0.0804, step time: 0.2562\n",
      "270/281, train_loss: 0.0608, step time: 0.2520\n",
      "271/281, train_loss: 0.0489, step time: 0.2490\n",
      "272/281, train_loss: 0.0860, step time: 0.2529\n",
      "273/281, train_loss: 0.0658, step time: 0.2729\n",
      "274/281, train_loss: 0.2362, step time: 0.2530\n",
      "275/281, train_loss: 0.2314, step time: 0.2512\n",
      "276/281, train_loss: 0.0815, step time: 0.2494\n",
      "277/281, train_loss: 0.0587, step time: 0.2604\n",
      "278/281, train_loss: 0.1196, step time: 0.2649\n",
      "279/281, train_loss: 0.0550, step time: 0.2598\n",
      "280/281, train_loss: 0.2319, step time: 0.2578\n",
      "281/281, train_loss: 0.2457, step time: 0.2568\n",
      "282/281, train_loss: 0.0656, step time: 0.1534\n",
      "epoch 89 average loss: 0.0951\n",
      "saved new best metric model\n",
      "current epoch: 89 current mean dice: 0.8956 tc: 0.8904 wt: 0.9236 et: 0.8818\n",
      "best mean dice: 0.8956 at epoch: 89\n",
      "time consuming of epoch 89 is: 395.0863\n",
      "----------\n",
      "epoch 90/200\n",
      "1/281, train_loss: 0.0749, step time: 0.2727\n",
      "2/281, train_loss: 0.1023, step time: 0.2586\n",
      "3/281, train_loss: 0.0682, step time: 0.2660\n",
      "4/281, train_loss: 0.0883, step time: 0.2690\n",
      "5/281, train_loss: 0.0659, step time: 0.2542\n",
      "6/281, train_loss: 0.0800, step time: 0.2545\n",
      "7/281, train_loss: 0.0541, step time: 0.2595\n",
      "8/281, train_loss: 0.0548, step time: 0.2607\n",
      "9/281, train_loss: 0.2066, step time: 0.2591\n",
      "10/281, train_loss: 0.2237, step time: 0.2576\n",
      "11/281, train_loss: 0.0993, step time: 0.2580\n",
      "12/281, train_loss: 0.0626, step time: 0.2622\n",
      "13/281, train_loss: 0.0808, step time: 0.2547\n",
      "14/281, train_loss: 0.0673, step time: 0.2533\n",
      "15/281, train_loss: 0.1096, step time: 0.2566\n",
      "16/281, train_loss: 0.0812, step time: 0.2541\n",
      "17/281, train_loss: 0.2058, step time: 0.2516\n",
      "18/281, train_loss: 0.2353, step time: 0.2579\n",
      "19/281, train_loss: 0.1160, step time: 0.2582\n",
      "20/281, train_loss: 0.0438, step time: 0.2563\n",
      "21/281, train_loss: 0.3821, step time: 0.2569\n",
      "22/281, train_loss: 0.0417, step time: 0.2624\n",
      "23/281, train_loss: 0.0913, step time: 0.2528\n",
      "24/281, train_loss: 0.0800, step time: 0.2571\n",
      "25/281, train_loss: 0.2115, step time: 0.2312\n",
      "26/281, train_loss: 0.0640, step time: 0.2522\n",
      "27/281, train_loss: 0.0495, step time: 0.2590\n",
      "28/281, train_loss: 0.1009, step time: 0.2566\n",
      "29/281, train_loss: 0.0545, step time: 0.2571\n",
      "30/281, train_loss: 0.0710, step time: 0.2591\n",
      "31/281, train_loss: 0.1091, step time: 0.2576\n",
      "32/281, train_loss: 0.0972, step time: 0.2643\n",
      "33/281, train_loss: 0.0714, step time: 0.2575\n",
      "34/281, train_loss: 0.1124, step time: 0.2666\n",
      "35/281, train_loss: 0.0637, step time: 0.2606\n",
      "36/281, train_loss: 0.0961, step time: 0.2576\n",
      "37/281, train_loss: 0.0850, step time: 0.2544\n",
      "38/281, train_loss: 0.0687, step time: 0.2635\n",
      "39/281, train_loss: 0.2510, step time: 0.2642\n",
      "40/281, train_loss: 0.0817, step time: 0.2588\n",
      "41/281, train_loss: 0.0791, step time: 0.2566\n",
      "42/281, train_loss: 0.0636, step time: 0.2581\n",
      "43/281, train_loss: 0.0993, step time: 0.2563\n",
      "44/281, train_loss: 0.0851, step time: 0.2594\n",
      "45/281, train_loss: 0.0769, step time: 0.2643\n",
      "46/281, train_loss: 0.0613, step time: 0.2552\n",
      "47/281, train_loss: 0.1010, step time: 0.2581\n",
      "48/281, train_loss: 0.0851, step time: 0.2492\n",
      "49/281, train_loss: 0.0392, step time: 0.2524\n",
      "50/281, train_loss: 0.0693, step time: 0.2538\n",
      "51/281, train_loss: 0.0354, step time: 0.2501\n",
      "52/281, train_loss: 0.0554, step time: 0.2517\n",
      "53/281, train_loss: 0.0680, step time: 0.2474\n",
      "54/281, train_loss: 0.0666, step time: 0.2522\n",
      "55/281, train_loss: 0.0766, step time: 0.2492\n",
      "56/281, train_loss: 0.0560, step time: 0.2488\n",
      "57/281, train_loss: 0.2623, step time: 0.2496\n",
      "58/281, train_loss: 0.0671, step time: 0.2495\n",
      "59/281, train_loss: 0.0644, step time: 0.2510\n",
      "60/281, train_loss: 0.0650, step time: 0.2521\n",
      "61/281, train_loss: 0.2404, step time: 0.2502\n",
      "62/281, train_loss: 0.2208, step time: 0.2512\n",
      "63/281, train_loss: 0.0653, step time: 0.2580\n",
      "64/281, train_loss: 0.0682, step time: 0.2493\n",
      "65/281, train_loss: 0.0780, step time: 0.2496\n",
      "66/281, train_loss: 0.2336, step time: 0.2502\n",
      "67/281, train_loss: 0.1213, step time: 0.2499\n",
      "68/281, train_loss: 0.2427, step time: 0.2544\n",
      "69/281, train_loss: 0.0658, step time: 0.2513\n",
      "70/281, train_loss: 0.0421, step time: 0.2473\n",
      "71/281, train_loss: 0.0675, step time: 0.2550\n",
      "72/281, train_loss: 0.0781, step time: 0.2552\n",
      "73/281, train_loss: 0.0638, step time: 0.2516\n",
      "74/281, train_loss: 0.0894, step time: 0.2497\n",
      "75/281, train_loss: 0.0531, step time: 0.2530\n",
      "76/281, train_loss: 0.0651, step time: 0.2528\n",
      "77/281, train_loss: 0.0825, step time: 0.2529\n",
      "78/281, train_loss: 0.0899, step time: 0.2554\n",
      "79/281, train_loss: 0.2441, step time: 0.2522\n",
      "80/281, train_loss: 0.0783, step time: 0.2540\n",
      "81/281, train_loss: 0.2063, step time: 0.2517\n",
      "82/281, train_loss: 0.0969, step time: 0.2492\n",
      "83/281, train_loss: 0.0601, step time: 0.2491\n",
      "84/281, train_loss: 0.0647, step time: 0.2434\n",
      "85/281, train_loss: 0.2440, step time: 0.2554\n",
      "86/281, train_loss: 0.0543, step time: 0.2565\n",
      "87/281, train_loss: 0.0519, step time: 0.2567\n",
      "88/281, train_loss: 0.0845, step time: 0.2531\n",
      "89/281, train_loss: 0.0790, step time: 0.2541\n",
      "90/281, train_loss: 0.0611, step time: 0.2510\n",
      "91/281, train_loss: 0.0957, step time: 0.2506\n",
      "92/281, train_loss: 0.2695, step time: 0.2517\n",
      "93/281, train_loss: 0.0542, step time: 0.2634\n",
      "94/281, train_loss: 0.0608, step time: 0.2550\n",
      "95/281, train_loss: 0.2254, step time: 0.2564\n",
      "96/281, train_loss: 0.0495, step time: 0.2566\n",
      "97/281, train_loss: 0.0543, step time: 0.2542\n",
      "98/281, train_loss: 0.0648, step time: 0.2529\n",
      "99/281, train_loss: 0.0727, step time: 0.2530\n",
      "100/281, train_loss: 0.0506, step time: 0.2542\n",
      "101/281, train_loss: 0.0800, step time: 0.2637\n",
      "102/281, train_loss: 0.0936, step time: 0.2639\n",
      "103/281, train_loss: 0.0870, step time: 0.2646\n",
      "104/281, train_loss: 0.0498, step time: 0.2562\n",
      "105/281, train_loss: 0.0587, step time: 0.2649\n",
      "106/281, train_loss: 0.2243, step time: 0.2551\n",
      "107/281, train_loss: 0.0773, step time: 0.2504\n",
      "108/281, train_loss: 0.0493, step time: 0.2548\n",
      "109/281, train_loss: 0.0901, step time: 0.2558\n",
      "110/281, train_loss: 0.0645, step time: 0.2541\n",
      "111/281, train_loss: 0.0552, step time: 0.2578\n",
      "112/281, train_loss: 0.0465, step time: 0.2575\n",
      "113/281, train_loss: 0.0846, step time: 0.2558\n",
      "114/281, train_loss: 0.0584, step time: 0.2563\n",
      "115/281, train_loss: 0.0690, step time: 0.2549\n",
      "116/281, train_loss: 0.2081, step time: 0.2596\n",
      "117/281, train_loss: 0.2226, step time: 0.2509\n",
      "118/281, train_loss: 0.0572, step time: 0.2559\n",
      "119/281, train_loss: 0.0638, step time: 0.2530\n",
      "120/281, train_loss: 0.0815, step time: 0.2598\n",
      "121/281, train_loss: 0.0510, step time: 0.2557\n",
      "122/281, train_loss: 0.0668, step time: 0.2557\n",
      "123/281, train_loss: 0.0905, step time: 0.2554\n",
      "124/281, train_loss: 0.2113, step time: 0.2557\n",
      "125/281, train_loss: 0.0578, step time: 0.2507\n",
      "126/281, train_loss: 0.0537, step time: 0.2517\n",
      "127/281, train_loss: 0.0671, step time: 0.2595\n",
      "128/281, train_loss: 0.0648, step time: 0.2566\n",
      "129/281, train_loss: 0.0604, step time: 0.2579\n",
      "130/281, train_loss: 0.0622, step time: 0.2576\n",
      "131/281, train_loss: 0.2374, step time: 0.2580\n",
      "132/281, train_loss: 0.0624, step time: 0.2579\n",
      "133/281, train_loss: 0.0730, step time: 0.2512\n",
      "134/281, train_loss: 0.0597, step time: 0.2556\n",
      "135/281, train_loss: 0.0766, step time: 0.2506\n",
      "136/281, train_loss: 0.0526, step time: 0.2480\n",
      "137/281, train_loss: 0.0521, step time: 0.2498\n",
      "138/281, train_loss: 0.1054, step time: 0.2497\n",
      "139/281, train_loss: 0.0572, step time: 0.2495\n",
      "140/281, train_loss: 0.0674, step time: 0.2479\n",
      "141/281, train_loss: 0.0674, step time: 0.2505\n",
      "142/281, train_loss: 0.0499, step time: 0.2509\n",
      "143/281, train_loss: 0.2047, step time: 0.2530\n",
      "144/281, train_loss: 0.1087, step time: 0.2466\n",
      "145/281, train_loss: 0.0840, step time: 0.2523\n",
      "146/281, train_loss: 0.0643, step time: 0.2491\n",
      "147/281, train_loss: 0.0601, step time: 0.2526\n",
      "148/281, train_loss: 0.0911, step time: 0.2545\n",
      "149/281, train_loss: 0.0596, step time: 0.2513\n",
      "150/281, train_loss: 0.0628, step time: 0.2510\n",
      "151/281, train_loss: 0.0741, step time: 0.2512\n",
      "152/281, train_loss: 0.0554, step time: 0.2499\n",
      "153/281, train_loss: 0.0740, step time: 0.2509\n",
      "154/281, train_loss: 0.0764, step time: 0.2551\n",
      "155/281, train_loss: 0.0479, step time: 0.2555\n",
      "156/281, train_loss: 0.2160, step time: 0.2556\n",
      "157/281, train_loss: 0.2151, step time: 0.2520\n",
      "158/281, train_loss: 0.0935, step time: 0.2530\n",
      "159/281, train_loss: 0.0896, step time: 0.2567\n",
      "160/281, train_loss: 0.0794, step time: 0.2544\n",
      "161/281, train_loss: 0.0573, step time: 0.2546\n",
      "162/281, train_loss: 0.0598, step time: 0.2506\n",
      "163/281, train_loss: 0.0431, step time: 0.2517\n",
      "164/281, train_loss: 0.0730, step time: 0.2546\n",
      "165/281, train_loss: 0.0773, step time: 0.2544\n",
      "166/281, train_loss: 0.0445, step time: 0.2540\n",
      "167/281, train_loss: 0.1181, step time: 0.2507\n",
      "168/281, train_loss: 0.0863, step time: 0.2549\n",
      "169/281, train_loss: 0.0547, step time: 0.2546\n",
      "170/281, train_loss: 0.0711, step time: 0.2581\n",
      "171/281, train_loss: 0.0913, step time: 0.2538\n",
      "172/281, train_loss: 0.0725, step time: 0.2489\n",
      "173/281, train_loss: 0.0384, step time: 0.2554\n",
      "174/281, train_loss: 0.0610, step time: 0.2534\n",
      "175/281, train_loss: 0.0539, step time: 0.2506\n",
      "176/281, train_loss: 0.0484, step time: 0.2477\n",
      "177/281, train_loss: 0.0687, step time: 0.2557\n",
      "178/281, train_loss: 0.1170, step time: 0.2587\n",
      "179/281, train_loss: 0.0745, step time: 0.2574\n",
      "180/281, train_loss: 0.0671, step time: 0.2533\n",
      "181/281, train_loss: 0.0592, step time: 0.2553\n",
      "182/281, train_loss: 0.0568, step time: 0.2505\n",
      "183/281, train_loss: 0.0840, step time: 0.2554\n",
      "184/281, train_loss: 0.0684, step time: 0.2511\n",
      "185/281, train_loss: 0.0656, step time: 0.2554\n",
      "186/281, train_loss: 0.2278, step time: 0.2538\n",
      "187/281, train_loss: 0.0514, step time: 0.2551\n",
      "188/281, train_loss: 0.0806, step time: 0.2533\n",
      "189/281, train_loss: 0.0955, step time: 0.2521\n",
      "190/281, train_loss: 0.0775, step time: 0.2521\n",
      "191/281, train_loss: 0.2286, step time: 0.2520\n",
      "192/281, train_loss: 0.0559, step time: 0.2548\n",
      "193/281, train_loss: 0.0726, step time: 0.2545\n",
      "194/281, train_loss: 0.0524, step time: 0.2551\n",
      "195/281, train_loss: 0.0782, step time: 0.2549\n",
      "196/281, train_loss: 0.0775, step time: 0.2501\n",
      "197/281, train_loss: 0.0979, step time: 0.2514\n",
      "198/281, train_loss: 0.2405, step time: 0.2514\n",
      "199/281, train_loss: 0.0857, step time: 0.2513\n",
      "200/281, train_loss: 0.0847, step time: 0.2497\n",
      "201/281, train_loss: 0.0530, step time: 0.2523\n",
      "202/281, train_loss: 0.0572, step time: 0.2555\n",
      "203/281, train_loss: 0.0765, step time: 0.2551\n",
      "204/281, train_loss: 0.0697, step time: 0.2546\n",
      "205/281, train_loss: 0.0818, step time: 0.2555\n",
      "206/281, train_loss: 0.0352, step time: 0.2546\n",
      "207/281, train_loss: 0.0575, step time: 0.2581\n",
      "208/281, train_loss: 0.0715, step time: 0.2552\n",
      "209/281, train_loss: 0.0957, step time: 0.2561\n",
      "210/281, train_loss: 0.0435, step time: 0.2509\n",
      "211/281, train_loss: 0.0621, step time: 0.2537\n",
      "212/281, train_loss: 0.0829, step time: 0.2488\n",
      "213/281, train_loss: 0.0615, step time: 0.2510\n",
      "214/281, train_loss: 0.0596, step time: 0.2559\n",
      "215/281, train_loss: 0.2256, step time: 0.2573\n",
      "216/281, train_loss: 0.0635, step time: 0.2483\n",
      "217/281, train_loss: 0.2198, step time: 0.2543\n",
      "218/281, train_loss: 0.3007, step time: 0.2547\n",
      "219/281, train_loss: 0.0501, step time: 0.2598\n",
      "220/281, train_loss: 0.0879, step time: 0.2531\n",
      "221/281, train_loss: 0.0378, step time: 0.2498\n",
      "222/281, train_loss: 0.0856, step time: 0.2535\n",
      "223/281, train_loss: 0.2462, step time: 0.2582\n",
      "224/281, train_loss: 0.0647, step time: 0.2569\n",
      "225/281, train_loss: 0.0274, step time: 0.2509\n",
      "226/281, train_loss: 0.2125, step time: 0.2516\n",
      "227/281, train_loss: 0.0678, step time: 0.2526\n",
      "228/281, train_loss: 0.2448, step time: 0.2548\n",
      "229/281, train_loss: 0.0366, step time: 0.2565\n",
      "230/281, train_loss: 0.0363, step time: 0.2533\n",
      "231/281, train_loss: 0.0560, step time: 0.2554\n",
      "232/281, train_loss: 0.0562, step time: 0.2547\n",
      "233/281, train_loss: 0.0771, step time: 0.2495\n",
      "234/281, train_loss: 0.0711, step time: 0.2531\n",
      "235/281, train_loss: 0.0893, step time: 0.2542\n",
      "236/281, train_loss: 0.2187, step time: 0.2553\n",
      "237/281, train_loss: 0.0629, step time: 0.2562\n",
      "238/281, train_loss: 0.0569, step time: 0.2543\n",
      "239/281, train_loss: 0.0563, step time: 0.2557\n",
      "240/281, train_loss: 0.0828, step time: 0.2585\n",
      "241/281, train_loss: 0.0587, step time: 0.2508\n",
      "242/281, train_loss: 0.0611, step time: 0.2534\n",
      "243/281, train_loss: 0.0395, step time: 0.2509\n",
      "244/281, train_loss: 0.0646, step time: 0.2494\n",
      "245/281, train_loss: 0.0654, step time: 0.2500\n",
      "246/281, train_loss: 0.2093, step time: 0.2554\n",
      "247/281, train_loss: 0.0853, step time: 0.2536\n",
      "248/281, train_loss: 0.0745, step time: 0.2509\n",
      "249/281, train_loss: 0.0656, step time: 0.2522\n",
      "250/281, train_loss: 0.0469, step time: 0.2541\n",
      "251/281, train_loss: 0.0790, step time: 0.2555\n",
      "252/281, train_loss: 0.0548, step time: 0.2487\n",
      "253/281, train_loss: 0.0915, step time: 0.2539\n",
      "254/281, train_loss: 0.3004, step time: 0.2554\n",
      "255/281, train_loss: 0.0822, step time: 0.2610\n",
      "256/281, train_loss: 0.0564, step time: 0.2590\n",
      "257/281, train_loss: 0.0796, step time: 0.2543\n",
      "258/281, train_loss: 0.2426, step time: 0.2542\n",
      "259/281, train_loss: 0.2331, step time: 0.2544\n",
      "260/281, train_loss: 0.0487, step time: 0.2539\n",
      "261/281, train_loss: 0.2503, step time: 0.2521\n",
      "262/281, train_loss: 0.0535, step time: 0.2514\n",
      "263/281, train_loss: 0.1043, step time: 0.2535\n",
      "264/281, train_loss: 0.3941, step time: 0.2511\n",
      "265/281, train_loss: 0.0751, step time: 0.2490\n",
      "266/281, train_loss: 0.0942, step time: 0.2491\n",
      "267/281, train_loss: 0.0918, step time: 0.2514\n",
      "268/281, train_loss: 0.0640, step time: 0.2589\n",
      "269/281, train_loss: 0.0923, step time: 0.2582\n",
      "270/281, train_loss: 0.0573, step time: 0.2540\n",
      "271/281, train_loss: 0.0636, step time: 0.2550\n",
      "272/281, train_loss: 0.0648, step time: 0.2580\n",
      "273/281, train_loss: 0.2336, step time: 0.2584\n",
      "274/281, train_loss: 0.0771, step time: 0.2541\n",
      "275/281, train_loss: 0.0492, step time: 0.2552\n",
      "276/281, train_loss: 0.0997, step time: 0.2532\n",
      "277/281, train_loss: 0.0656, step time: 0.2490\n",
      "278/281, train_loss: 0.0780, step time: 0.2505\n",
      "279/281, train_loss: 0.0429, step time: 0.2557\n",
      "280/281, train_loss: 0.0963, step time: 0.2581\n",
      "281/281, train_loss: 0.0752, step time: 0.2545\n",
      "282/281, train_loss: 0.0513, step time: 0.1495\n",
      "epoch 90 average loss: 0.0953\n",
      "current epoch: 90 current mean dice: 0.8951 tc: 0.8895 wt: 0.9230 et: 0.8818\n",
      "best mean dice: 0.8956 at epoch: 89\n",
      "time consuming of epoch 90 is: 420.5030\n",
      "----------\n",
      "epoch 91/200\n",
      "1/281, train_loss: 0.2390, step time: 0.2532\n",
      "2/281, train_loss: 0.2203, step time: 0.2454\n",
      "3/281, train_loss: 0.0921, step time: 0.2490\n",
      "4/281, train_loss: 0.0734, step time: 0.2522\n",
      "5/281, train_loss: 0.0647, step time: 0.2577\n",
      "6/281, train_loss: 0.0584, step time: 0.2540\n",
      "7/281, train_loss: 0.0704, step time: 0.2506\n",
      "8/281, train_loss: 0.0609, step time: 0.2510\n",
      "9/281, train_loss: 0.0646, step time: 0.2521\n",
      "10/281, train_loss: 0.0482, step time: 0.2459\n",
      "11/281, train_loss: 0.0811, step time: 0.2469\n",
      "12/281, train_loss: 0.0754, step time: 0.2476\n",
      "13/281, train_loss: 0.4035, step time: 0.2467\n",
      "14/281, train_loss: 0.0717, step time: 0.2567\n",
      "15/281, train_loss: 0.2322, step time: 0.2532\n",
      "16/281, train_loss: 0.0776, step time: 0.2430\n",
      "17/281, train_loss: 0.0663, step time: 0.2497\n",
      "18/281, train_loss: 0.0464, step time: 0.2483\n",
      "19/281, train_loss: 0.2297, step time: 0.2493\n",
      "20/281, train_loss: 0.0885, step time: 0.2446\n",
      "21/281, train_loss: 0.0409, step time: 0.2624\n",
      "22/281, train_loss: 0.0798, step time: 0.2557\n",
      "23/281, train_loss: 0.0755, step time: 0.2493\n",
      "24/281, train_loss: 0.0708, step time: 0.2558\n",
      "25/281, train_loss: 0.0930, step time: 0.2522\n",
      "26/281, train_loss: 0.1190, step time: 0.2539\n",
      "27/281, train_loss: 0.0626, step time: 0.2525\n",
      "28/281, train_loss: 0.0720, step time: 0.2470\n",
      "29/281, train_loss: 0.0689, step time: 0.2481\n",
      "30/281, train_loss: 0.0535, step time: 0.2579\n",
      "31/281, train_loss: 0.0852, step time: 0.2534\n",
      "32/281, train_loss: 0.2357, step time: 0.2504\n",
      "33/281, train_loss: 0.2203, step time: 0.2495\n",
      "34/281, train_loss: 0.2295, step time: 0.2546\n",
      "35/281, train_loss: 0.0439, step time: 0.2473\n",
      "36/281, train_loss: 0.0833, step time: 0.2537\n",
      "37/281, train_loss: 0.0618, step time: 0.2476\n",
      "38/281, train_loss: 0.2117, step time: 0.2440\n",
      "39/281, train_loss: 0.0697, step time: 0.2439\n",
      "40/281, train_loss: 0.0657, step time: 0.2442\n",
      "41/281, train_loss: 0.0792, step time: 0.2485\n",
      "42/281, train_loss: 0.1004, step time: 0.2436\n",
      "43/281, train_loss: 0.0514, step time: 0.2464\n",
      "44/281, train_loss: 0.0855, step time: 0.2476\n",
      "45/281, train_loss: 0.0934, step time: 0.2517\n",
      "46/281, train_loss: 0.0955, step time: 0.2541\n",
      "47/281, train_loss: 0.0539, step time: 0.2529\n",
      "48/281, train_loss: 0.0689, step time: 0.2537\n",
      "49/281, train_loss: 0.0815, step time: 0.2479\n",
      "50/281, train_loss: 0.0798, step time: 0.2518\n",
      "51/281, train_loss: 0.0657, step time: 0.2544\n",
      "52/281, train_loss: 0.0780, step time: 0.2476\n",
      "53/281, train_loss: 0.0709, step time: 0.2463\n",
      "54/281, train_loss: 0.2321, step time: 0.2539\n",
      "55/281, train_loss: 0.0887, step time: 0.2472\n",
      "56/281, train_loss: 0.0579, step time: 0.2495\n",
      "57/281, train_loss: 0.1109, step time: 0.2444\n",
      "58/281, train_loss: 0.0578, step time: 0.2500\n",
      "59/281, train_loss: 0.0489, step time: 0.2554\n",
      "60/281, train_loss: 0.2559, step time: 0.2490\n",
      "61/281, train_loss: 0.1067, step time: 0.2479\n",
      "62/281, train_loss: 0.2643, step time: 0.2457\n",
      "63/281, train_loss: 0.0506, step time: 0.2435\n",
      "64/281, train_loss: 0.0493, step time: 0.2445\n",
      "65/281, train_loss: 0.0535, step time: 0.2491\n",
      "66/281, train_loss: 0.0838, step time: 0.2508\n",
      "67/281, train_loss: 0.0681, step time: 0.2485\n",
      "68/281, train_loss: 0.0462, step time: 0.2505\n",
      "69/281, train_loss: 0.0660, step time: 0.2515\n",
      "70/281, train_loss: 0.0733, step time: 0.2502\n",
      "71/281, train_loss: 0.1091, step time: 0.2495\n",
      "72/281, train_loss: 0.2145, step time: 0.2566\n",
      "73/281, train_loss: 0.0547, step time: 0.2540\n",
      "74/281, train_loss: 0.0983, step time: 0.2478\n",
      "75/281, train_loss: 0.0700, step time: 0.2435\n",
      "76/281, train_loss: 0.0604, step time: 0.2514\n",
      "77/281, train_loss: 0.0745, step time: 0.2480\n",
      "78/281, train_loss: 0.0876, step time: 0.2428\n",
      "79/281, train_loss: 0.0642, step time: 0.2418\n",
      "80/281, train_loss: 0.0836, step time: 0.2433\n",
      "81/281, train_loss: 0.0821, step time: 0.2505\n",
      "82/281, train_loss: 0.0601, step time: 0.2524\n",
      "83/281, train_loss: 0.2093, step time: 0.2534\n",
      "84/281, train_loss: 0.0650, step time: 0.2456\n",
      "85/281, train_loss: 0.2191, step time: 0.2422\n",
      "86/281, train_loss: 0.0839, step time: 0.2537\n",
      "87/281, train_loss: 0.0631, step time: 0.2565\n",
      "88/281, train_loss: 0.2269, step time: 0.2524\n",
      "89/281, train_loss: 0.0654, step time: 0.2456\n",
      "90/281, train_loss: 0.0762, step time: 0.2560\n",
      "91/281, train_loss: 0.2244, step time: 0.2517\n",
      "92/281, train_loss: 0.0868, step time: 0.2533\n",
      "93/281, train_loss: 0.0585, step time: 0.2524\n",
      "94/281, train_loss: 0.0603, step time: 0.2544\n",
      "95/281, train_loss: 0.2331, step time: 0.2514\n",
      "96/281, train_loss: 0.0747, step time: 0.2496\n",
      "97/281, train_loss: 0.0854, step time: 0.2530\n",
      "98/281, train_loss: 0.1334, step time: 0.2503\n",
      "99/281, train_loss: 0.2234, step time: 0.2496\n",
      "100/281, train_loss: 0.0766, step time: 0.2458\n",
      "101/281, train_loss: 0.1017, step time: 0.2472\n",
      "102/281, train_loss: 0.2145, step time: 0.2499\n",
      "103/281, train_loss: 0.0574, step time: 0.2538\n",
      "104/281, train_loss: 0.1278, step time: 0.2551\n",
      "105/281, train_loss: 0.0971, step time: 0.2539\n",
      "106/281, train_loss: 0.0573, step time: 0.2498\n",
      "107/281, train_loss: 0.0582, step time: 0.2467\n",
      "108/281, train_loss: 0.0879, step time: 0.2418\n",
      "109/281, train_loss: 0.0791, step time: 0.2468\n",
      "110/281, train_loss: 0.0736, step time: 0.2524\n",
      "111/281, train_loss: 0.0336, step time: 0.2521\n",
      "112/281, train_loss: 0.0506, step time: 0.2462\n",
      "113/281, train_loss: 0.0539, step time: 0.2475\n",
      "114/281, train_loss: 0.2336, step time: 0.2498\n",
      "115/281, train_loss: 0.0490, step time: 0.2481\n",
      "116/281, train_loss: 0.0645, step time: 0.2495\n",
      "117/281, train_loss: 0.0797, step time: 0.2504\n",
      "118/281, train_loss: 0.0542, step time: 0.2526\n",
      "119/281, train_loss: 0.2133, step time: 0.2524\n",
      "120/281, train_loss: 0.0339, step time: 0.2568\n",
      "121/281, train_loss: 0.0863, step time: 0.2539\n",
      "122/281, train_loss: 0.0592, step time: 0.2497\n",
      "123/281, train_loss: 0.0744, step time: 0.2512\n",
      "124/281, train_loss: 0.0461, step time: 0.2481\n",
      "125/281, train_loss: 0.0909, step time: 0.2493\n",
      "126/281, train_loss: 0.0654, step time: 0.2525\n",
      "127/281, train_loss: 0.1956, step time: 0.2484\n",
      "128/281, train_loss: 0.0615, step time: 0.2495\n",
      "129/281, train_loss: 0.0916, step time: 0.2513\n",
      "130/281, train_loss: 0.2343, step time: 0.2547\n",
      "131/281, train_loss: 0.0648, step time: 0.2701\n",
      "132/281, train_loss: 0.0812, step time: 0.2623\n",
      "133/281, train_loss: 0.0821, step time: 0.2720\n",
      "134/281, train_loss: 0.0555, step time: 0.2557\n",
      "135/281, train_loss: 0.0719, step time: 0.2518\n",
      "136/281, train_loss: 0.0935, step time: 0.2520\n",
      "137/281, train_loss: 0.0738, step time: 0.2451\n",
      "138/281, train_loss: 0.0525, step time: 0.2499\n",
      "139/281, train_loss: 0.0647, step time: 0.2545\n",
      "140/281, train_loss: 0.0492, step time: 0.2511\n",
      "141/281, train_loss: 0.0483, step time: 0.2484\n",
      "142/281, train_loss: 0.0599, step time: 0.2475\n",
      "143/281, train_loss: 0.0696, step time: 0.2480\n",
      "144/281, train_loss: 0.0666, step time: 0.2494\n",
      "145/281, train_loss: 0.2033, step time: 0.2526\n",
      "146/281, train_loss: 0.0673, step time: 0.2569\n",
      "147/281, train_loss: 0.0916, step time: 0.2583\n",
      "148/281, train_loss: 0.0536, step time: 0.2512\n",
      "149/281, train_loss: 0.0748, step time: 0.2451\n",
      "150/281, train_loss: 0.0694, step time: 0.2579\n",
      "151/281, train_loss: 0.0752, step time: 0.2563\n",
      "152/281, train_loss: 0.0437, step time: 0.2506\n",
      "153/281, train_loss: 0.0750, step time: 0.2476\n",
      "154/281, train_loss: 0.0810, step time: 0.2524\n",
      "155/281, train_loss: 0.1118, step time: 0.2497\n",
      "156/281, train_loss: 0.0657, step time: 0.2499\n",
      "157/281, train_loss: 0.0591, step time: 0.2504\n",
      "158/281, train_loss: 0.0437, step time: 0.2519\n",
      "159/281, train_loss: 0.2115, step time: 0.2502\n",
      "160/281, train_loss: 0.2093, step time: 0.2526\n",
      "161/281, train_loss: 0.0775, step time: 0.2458\n",
      "162/281, train_loss: 0.0901, step time: 0.2504\n",
      "163/281, train_loss: 0.0867, step time: 0.2504\n",
      "164/281, train_loss: 0.3742, step time: 0.2479\n",
      "165/281, train_loss: 0.1025, step time: 0.2448\n",
      "166/281, train_loss: 0.0833, step time: 0.2555\n",
      "167/281, train_loss: 0.0761, step time: 0.2506\n",
      "168/281, train_loss: 0.1098, step time: 0.2555\n",
      "169/281, train_loss: 0.0539, step time: 0.2498\n",
      "170/281, train_loss: 0.0825, step time: 0.2508\n",
      "171/281, train_loss: 0.0651, step time: 0.2518\n",
      "172/281, train_loss: 0.0555, step time: 0.2504\n",
      "173/281, train_loss: 0.0484, step time: 0.2473\n",
      "174/281, train_loss: 0.0453, step time: 0.2485\n",
      "175/281, train_loss: 0.0576, step time: 0.2531\n",
      "176/281, train_loss: 0.2345, step time: 0.2508\n",
      "177/281, train_loss: 0.0574, step time: 0.2501\n",
      "178/281, train_loss: 0.0887, step time: 0.2506\n",
      "179/281, train_loss: 0.0493, step time: 0.2436\n",
      "180/281, train_loss: 0.0589, step time: 0.2456\n",
      "181/281, train_loss: 0.0817, step time: 0.2512\n",
      "182/281, train_loss: 0.0833, step time: 0.2614\n",
      "183/281, train_loss: 0.2230, step time: 0.2509\n",
      "184/281, train_loss: 0.0455, step time: 0.2532\n",
      "185/281, train_loss: 0.1127, step time: 0.2521\n",
      "186/281, train_loss: 0.0597, step time: 0.2571\n",
      "187/281, train_loss: 0.1242, step time: 0.2484\n",
      "188/281, train_loss: 0.0692, step time: 0.2548\n",
      "189/281, train_loss: 0.0694, step time: 0.2577\n",
      "190/281, train_loss: 0.2290, step time: 0.2515\n",
      "191/281, train_loss: 0.0381, step time: 0.2555\n",
      "192/281, train_loss: 0.0623, step time: 0.2514\n",
      "193/281, train_loss: 0.0723, step time: 0.2509\n",
      "194/281, train_loss: 0.0775, step time: 0.2553\n",
      "195/281, train_loss: 0.0969, step time: 0.2532\n",
      "196/281, train_loss: 0.0942, step time: 0.2513\n",
      "197/281, train_loss: 0.0839, step time: 0.2503\n",
      "198/281, train_loss: 0.2257, step time: 0.2569\n",
      "199/281, train_loss: 0.0785, step time: 0.2580\n",
      "200/281, train_loss: 0.2369, step time: 0.2583\n",
      "201/281, train_loss: 0.1120, step time: 0.2510\n",
      "202/281, train_loss: 0.0753, step time: 0.2553\n",
      "203/281, train_loss: 0.0629, step time: 0.2497\n",
      "204/281, train_loss: 0.0670, step time: 0.2529\n",
      "205/281, train_loss: 0.1024, step time: 0.2496\n",
      "206/281, train_loss: 0.0764, step time: 0.2544\n",
      "207/281, train_loss: 0.0634, step time: 0.2514\n",
      "208/281, train_loss: 0.0970, step time: 0.2535\n",
      "209/281, train_loss: 0.0761, step time: 0.2535\n",
      "210/281, train_loss: 0.2172, step time: 0.2519\n",
      "211/281, train_loss: 0.0941, step time: 0.2544\n",
      "212/281, train_loss: 0.0810, step time: 0.2536\n",
      "213/281, train_loss: 0.0563, step time: 0.2520\n",
      "214/281, train_loss: 0.0790, step time: 0.2534\n",
      "215/281, train_loss: 0.0535, step time: 0.2531\n",
      "216/281, train_loss: 0.0673, step time: 0.2507\n",
      "217/281, train_loss: 0.2123, step time: 0.2521\n",
      "218/281, train_loss: 0.2471, step time: 0.2530\n",
      "219/281, train_loss: 0.0657, step time: 0.2569\n",
      "220/281, train_loss: 0.0754, step time: 0.2565\n",
      "221/281, train_loss: 0.0395, step time: 0.2516\n",
      "222/281, train_loss: 0.0839, step time: 0.2566\n",
      "223/281, train_loss: 0.0453, step time: 0.2510\n",
      "224/281, train_loss: 0.0547, step time: 0.2527\n",
      "225/281, train_loss: 0.0599, step time: 0.2493\n",
      "226/281, train_loss: 0.2133, step time: 0.2514\n",
      "227/281, train_loss: 0.0729, step time: 0.2582\n",
      "228/281, train_loss: 0.2563, step time: 0.2560\n",
      "229/281, train_loss: 0.0868, step time: 0.2589\n",
      "230/281, train_loss: 0.0576, step time: 0.2522\n",
      "231/281, train_loss: 0.0327, step time: 0.2514\n",
      "232/281, train_loss: 0.0826, step time: 0.2516\n",
      "233/281, train_loss: 0.0599, step time: 0.2565\n",
      "234/281, train_loss: 0.1026, step time: 0.2565\n",
      "235/281, train_loss: 0.0646, step time: 0.2532\n",
      "236/281, train_loss: 0.0667, step time: 0.2541\n",
      "237/281, train_loss: 0.0764, step time: 0.2548\n",
      "238/281, train_loss: 0.2559, step time: 0.2506\n",
      "239/281, train_loss: 0.0867, step time: 0.2585\n",
      "240/281, train_loss: 0.0439, step time: 0.2564\n",
      "241/281, train_loss: 0.0795, step time: 0.2589\n",
      "242/281, train_loss: 0.0787, step time: 0.2576\n",
      "243/281, train_loss: 0.0590, step time: 0.2534\n",
      "244/281, train_loss: 0.0843, step time: 0.2567\n",
      "245/281, train_loss: 0.0644, step time: 0.2541\n",
      "246/281, train_loss: 0.0480, step time: 0.2500\n",
      "247/281, train_loss: 0.0735, step time: 0.2516\n",
      "248/281, train_loss: 0.2251, step time: 0.2538\n",
      "249/281, train_loss: 0.2206, step time: 0.2539\n",
      "250/281, train_loss: 0.0560, step time: 0.2550\n",
      "251/281, train_loss: 0.1393, step time: 0.2500\n",
      "252/281, train_loss: 0.0643, step time: 0.2521\n",
      "253/281, train_loss: 0.0620, step time: 0.2510\n",
      "254/281, train_loss: 0.0752, step time: 0.2488\n",
      "255/281, train_loss: 0.0905, step time: 0.2525\n",
      "256/281, train_loss: 0.0764, step time: 0.2535\n",
      "257/281, train_loss: 0.0734, step time: 0.2569\n",
      "258/281, train_loss: 0.0942, step time: 0.2582\n",
      "259/281, train_loss: 0.0940, step time: 0.2614\n",
      "260/281, train_loss: 0.0842, step time: 0.2594\n",
      "261/281, train_loss: 0.0491, step time: 0.2586\n",
      "262/281, train_loss: 0.0603, step time: 0.2581\n",
      "263/281, train_loss: 0.0691, step time: 0.2601\n",
      "264/281, train_loss: 0.0646, step time: 0.2554\n",
      "265/281, train_loss: 0.0755, step time: 0.2560\n",
      "266/281, train_loss: 0.0551, step time: 0.2561\n",
      "267/281, train_loss: 0.0634, step time: 0.2564\n",
      "268/281, train_loss: 0.0540, step time: 0.2577\n",
      "269/281, train_loss: 0.0627, step time: 0.2573\n",
      "270/281, train_loss: 0.0508, step time: 0.2571\n",
      "271/281, train_loss: 0.0770, step time: 0.2544\n",
      "272/281, train_loss: 0.0773, step time: 0.2555\n",
      "273/281, train_loss: 0.0393, step time: 0.2517\n",
      "274/281, train_loss: 0.0783, step time: 0.2655\n",
      "275/281, train_loss: 0.0839, step time: 0.2551\n",
      "276/281, train_loss: 0.0506, step time: 0.2512\n",
      "277/281, train_loss: 0.1580, step time: 0.2549\n",
      "278/281, train_loss: 0.0655, step time: 0.2586\n",
      "279/281, train_loss: 0.0632, step time: 0.2544\n",
      "280/281, train_loss: 0.0800, step time: 0.2574\n",
      "281/281, train_loss: 0.0672, step time: 0.2584\n",
      "282/281, train_loss: 0.1100, step time: 0.1556\n",
      "epoch 91 average loss: 0.0958\n",
      "current epoch: 91 current mean dice: 0.8940 tc: 0.8887 wt: 0.9211 et: 0.8816\n",
      "best mean dice: 0.8956 at epoch: 89\n",
      "time consuming of epoch 91 is: 415.0405\n",
      "----------\n",
      "epoch 92/200\n",
      "1/281, train_loss: 0.2399, step time: 0.2631\n",
      "2/281, train_loss: 0.2042, step time: 0.2585\n",
      "3/281, train_loss: 0.0525, step time: 0.2643\n",
      "4/281, train_loss: 0.2246, step time: 0.2717\n",
      "5/281, train_loss: 0.0752, step time: 0.2570\n",
      "6/281, train_loss: 0.0706, step time: 0.2576\n",
      "7/281, train_loss: 0.0533, step time: 0.2606\n",
      "8/281, train_loss: 0.0740, step time: 0.2590\n",
      "9/281, train_loss: 0.0503, step time: 0.2609\n",
      "10/281, train_loss: 0.0495, step time: 0.2565\n",
      "11/281, train_loss: 0.0866, step time: 0.2585\n",
      "12/281, train_loss: 0.0532, step time: 0.2548\n",
      "13/281, train_loss: 0.0735, step time: 0.2601\n",
      "14/281, train_loss: 0.0714, step time: 0.2589\n",
      "15/281, train_loss: 0.0683, step time: 0.2579\n",
      "16/281, train_loss: 0.0699, step time: 0.2561\n",
      "17/281, train_loss: 0.2223, step time: 0.2607\n",
      "18/281, train_loss: 0.0804, step time: 0.2585\n",
      "19/281, train_loss: 0.0944, step time: 0.2565\n",
      "20/281, train_loss: 0.0457, step time: 0.2574\n",
      "21/281, train_loss: 0.0863, step time: 0.2532\n",
      "22/281, train_loss: 0.2807, step time: 0.2533\n",
      "23/281, train_loss: 0.0477, step time: 0.2547\n",
      "24/281, train_loss: 0.0552, step time: 0.2535\n",
      "25/281, train_loss: 0.0756, step time: 0.2558\n",
      "26/281, train_loss: 0.1275, step time: 0.2659\n",
      "27/281, train_loss: 0.0878, step time: 0.2579\n",
      "28/281, train_loss: 0.0847, step time: 0.2556\n",
      "29/281, train_loss: 0.0685, step time: 0.2578\n",
      "30/281, train_loss: 0.0806, step time: 0.2590\n",
      "31/281, train_loss: 0.0633, step time: 0.2561\n",
      "32/281, train_loss: 0.0704, step time: 0.2522\n",
      "33/281, train_loss: 0.0565, step time: 0.2575\n",
      "34/281, train_loss: 0.0804, step time: 0.2570\n",
      "35/281, train_loss: 0.0665, step time: 0.2562\n",
      "36/281, train_loss: 0.0770, step time: 0.2569\n",
      "37/281, train_loss: 0.0564, step time: 0.2537\n",
      "38/281, train_loss: 0.0967, step time: 0.2551\n",
      "39/281, train_loss: 0.0935, step time: 0.2581\n",
      "40/281, train_loss: 0.0426, step time: 0.2575\n",
      "41/281, train_loss: 0.0583, step time: 0.2621\n",
      "42/281, train_loss: 0.0664, step time: 0.2594\n",
      "43/281, train_loss: 0.0709, step time: 0.2563\n",
      "44/281, train_loss: 0.0795, step time: 0.2574\n",
      "45/281, train_loss: 0.0796, step time: 0.2523\n",
      "46/281, train_loss: 0.0744, step time: 0.2530\n",
      "47/281, train_loss: 0.0808, step time: 0.2539\n",
      "48/281, train_loss: 0.0530, step time: 0.2520\n",
      "49/281, train_loss: 0.0630, step time: 0.2557\n",
      "50/281, train_loss: 0.0789, step time: 0.2512\n",
      "51/281, train_loss: 0.0745, step time: 0.2518\n",
      "52/281, train_loss: 0.0492, step time: 0.2562\n",
      "53/281, train_loss: 0.0840, step time: 0.2524\n",
      "54/281, train_loss: 0.0589, step time: 0.2534\n",
      "55/281, train_loss: 0.0596, step time: 0.2455\n",
      "56/281, train_loss: 0.0632, step time: 0.2563\n",
      "57/281, train_loss: 0.0589, step time: 0.2492\n",
      "58/281, train_loss: 0.2268, step time: 0.2514\n",
      "59/281, train_loss: 0.3781, step time: 0.2451\n",
      "60/281, train_loss: 0.0788, step time: 0.2496\n",
      "61/281, train_loss: 0.0701, step time: 0.2599\n",
      "62/281, train_loss: 0.0764, step time: 0.2598\n",
      "63/281, train_loss: 0.0560, step time: 0.2557\n",
      "64/281, train_loss: 0.0641, step time: 0.2633\n",
      "65/281, train_loss: 0.0650, step time: 0.2580\n",
      "66/281, train_loss: 0.1108, step time: 0.2624\n",
      "67/281, train_loss: 0.2058, step time: 0.2647\n",
      "68/281, train_loss: 0.0739, step time: 0.2621\n",
      "69/281, train_loss: 0.2358, step time: 0.2574\n",
      "70/281, train_loss: 0.0631, step time: 0.2568\n",
      "71/281, train_loss: 0.0522, step time: 0.2553\n",
      "72/281, train_loss: 0.0814, step time: 0.2473\n",
      "73/281, train_loss: 0.0886, step time: 0.2526\n",
      "74/281, train_loss: 0.0655, step time: 0.2555\n",
      "75/281, train_loss: 0.0529, step time: 0.2601\n",
      "76/281, train_loss: 0.1028, step time: 0.2589\n",
      "77/281, train_loss: 0.2329, step time: 0.2521\n",
      "78/281, train_loss: 0.1067, step time: 0.2521\n",
      "79/281, train_loss: 0.1029, step time: 0.2564\n",
      "80/281, train_loss: 0.0515, step time: 0.2548\n",
      "81/281, train_loss: 0.0695, step time: 0.2502\n",
      "82/281, train_loss: 0.0705, step time: 0.2540\n",
      "83/281, train_loss: 0.0823, step time: 0.2501\n",
      "84/281, train_loss: 0.2266, step time: 0.2540\n",
      "85/281, train_loss: 0.0484, step time: 0.2533\n",
      "86/281, train_loss: 0.0403, step time: 0.2572\n",
      "87/281, train_loss: 0.0702, step time: 0.2489\n",
      "88/281, train_loss: 0.0765, step time: 0.2523\n",
      "89/281, train_loss: 0.1404, step time: 0.2567\n",
      "90/281, train_loss: 0.1118, step time: 0.2547\n",
      "91/281, train_loss: 0.0567, step time: 0.2600\n",
      "92/281, train_loss: 0.0815, step time: 0.2562\n",
      "93/281, train_loss: 0.0853, step time: 0.2611\n",
      "94/281, train_loss: 0.0576, step time: 0.2589\n",
      "95/281, train_loss: 0.1023, step time: 0.2578\n",
      "96/281, train_loss: 0.0687, step time: 0.2581\n",
      "97/281, train_loss: 0.0639, step time: 0.2573\n",
      "98/281, train_loss: 0.0733, step time: 0.2589\n",
      "99/281, train_loss: 0.2324, step time: 0.2501\n",
      "100/281, train_loss: 0.0573, step time: 0.2570\n",
      "101/281, train_loss: 0.0624, step time: 0.2548\n",
      "102/281, train_loss: 0.0645, step time: 0.2586\n",
      "103/281, train_loss: 0.0545, step time: 0.2618\n",
      "104/281, train_loss: 0.0662, step time: 0.2649\n",
      "105/281, train_loss: 0.0596, step time: 0.2686\n",
      "106/281, train_loss: 0.1017, step time: 0.2532\n",
      "107/281, train_loss: 0.0998, step time: 0.2758\n",
      "108/281, train_loss: 0.0579, step time: 0.2577\n",
      "109/281, train_loss: 0.0714, step time: 0.2571\n",
      "110/281, train_loss: 0.0628, step time: 0.2562\n",
      "111/281, train_loss: 0.0634, step time: 0.2557\n",
      "112/281, train_loss: 0.2270, step time: 0.2527\n",
      "113/281, train_loss: 0.1250, step time: 0.2582\n",
      "114/281, train_loss: 0.0765, step time: 0.2559\n",
      "115/281, train_loss: 0.2400, step time: 0.2581\n",
      "116/281, train_loss: 0.0928, step time: 0.2589\n",
      "117/281, train_loss: 0.0509, step time: 0.2533\n",
      "118/281, train_loss: 0.0531, step time: 0.2522\n",
      "119/281, train_loss: 0.0467, step time: 0.2504\n",
      "120/281, train_loss: 0.1008, step time: 0.2560\n",
      "121/281, train_loss: 0.0905, step time: 0.2541\n",
      "122/281, train_loss: 0.0439, step time: 0.2594\n",
      "123/281, train_loss: 0.0644, step time: 0.2562\n",
      "124/281, train_loss: 0.0859, step time: 0.2513\n",
      "125/281, train_loss: 0.0544, step time: 0.2499\n",
      "126/281, train_loss: 0.2576, step time: 0.2505\n",
      "127/281, train_loss: 0.0802, step time: 0.2568\n",
      "128/281, train_loss: 0.0578, step time: 0.2530\n",
      "129/281, train_loss: 0.0766, step time: 0.2595\n",
      "130/281, train_loss: 0.0420, step time: 0.2569\n",
      "131/281, train_loss: 0.1002, step time: 0.2514\n",
      "132/281, train_loss: 0.0677, step time: 0.2518\n",
      "133/281, train_loss: 0.2145, step time: 0.2539\n",
      "134/281, train_loss: 0.0699, step time: 0.2502\n",
      "135/281, train_loss: 0.0947, step time: 0.2498\n",
      "136/281, train_loss: 0.0478, step time: 0.2579\n",
      "137/281, train_loss: 0.0593, step time: 0.2623\n",
      "138/281, train_loss: 0.0858, step time: 0.2556\n",
      "139/281, train_loss: 0.2298, step time: 0.2594\n",
      "140/281, train_loss: 0.0787, step time: 0.2575\n",
      "141/281, train_loss: 0.0674, step time: 0.2526\n",
      "142/281, train_loss: 0.0773, step time: 0.2538\n",
      "143/281, train_loss: 0.2794, step time: 0.2561\n",
      "144/281, train_loss: 0.0787, step time: 0.2568\n",
      "145/281, train_loss: 0.0546, step time: 0.2586\n",
      "146/281, train_loss: 0.2082, step time: 0.2567\n",
      "147/281, train_loss: 0.0643, step time: 0.2637\n",
      "148/281, train_loss: 0.2430, step time: 0.2642\n",
      "149/281, train_loss: 0.0589, step time: 0.2555\n",
      "150/281, train_loss: 0.2127, step time: 0.2574\n",
      "151/281, train_loss: 0.0559, step time: 0.2564\n",
      "152/281, train_loss: 0.2205, step time: 0.2551\n",
      "153/281, train_loss: 0.0737, step time: 0.2509\n",
      "154/281, train_loss: 0.0448, step time: 0.2541\n",
      "155/281, train_loss: 0.0801, step time: 0.2546\n",
      "156/281, train_loss: 0.0814, step time: 0.2576\n",
      "157/281, train_loss: 0.0583, step time: 0.2570\n",
      "158/281, train_loss: 0.0881, step time: 0.2521\n",
      "159/281, train_loss: 0.0707, step time: 0.2578\n",
      "160/281, train_loss: 0.0963, step time: 0.2561\n",
      "161/281, train_loss: 0.0430, step time: 0.2576\n",
      "162/281, train_loss: 0.0770, step time: 0.2595\n",
      "163/281, train_loss: 0.0648, step time: 0.2526\n",
      "164/281, train_loss: 0.0825, step time: 0.2541\n",
      "165/281, train_loss: 0.1278, step time: 0.2572\n",
      "166/281, train_loss: 0.0654, step time: 0.2589\n",
      "167/281, train_loss: 0.2500, step time: 0.2584\n",
      "168/281, train_loss: 0.2281, step time: 0.2558\n",
      "169/281, train_loss: 0.1421, step time: 0.2526\n",
      "170/281, train_loss: 0.1040, step time: 0.2605\n",
      "171/281, train_loss: 0.0815, step time: 0.2598\n",
      "172/281, train_loss: 0.0565, step time: 0.2586\n",
      "173/281, train_loss: 0.0748, step time: 0.2555\n",
      "174/281, train_loss: 0.0545, step time: 0.2566\n",
      "175/281, train_loss: 0.0439, step time: 0.2504\n",
      "176/281, train_loss: 0.2131, step time: 0.2558\n",
      "177/281, train_loss: 0.2328, step time: 0.2546\n",
      "178/281, train_loss: 0.0788, step time: 0.2520\n",
      "179/281, train_loss: 0.2338, step time: 0.2536\n",
      "180/281, train_loss: 0.2316, step time: 0.2552\n",
      "181/281, train_loss: 0.0608, step time: 0.2666\n",
      "182/281, train_loss: 0.0477, step time: 0.2540\n",
      "183/281, train_loss: 0.0608, step time: 0.2539\n",
      "184/281, train_loss: 0.2207, step time: 0.2529\n",
      "185/281, train_loss: 0.0791, step time: 0.2506\n",
      "186/281, train_loss: 0.0552, step time: 0.2555\n",
      "187/281, train_loss: 0.0344, step time: 0.2556\n",
      "188/281, train_loss: 0.0772, step time: 0.2601\n",
      "189/281, train_loss: 0.0647, step time: 0.2531\n",
      "190/281, train_loss: 0.0548, step time: 0.2549\n",
      "191/281, train_loss: 0.2305, step time: 0.2512\n",
      "192/281, train_loss: 0.0661, step time: 0.2618\n",
      "193/281, train_loss: 0.0586, step time: 0.2596\n",
      "194/281, train_loss: 0.0955, step time: 0.2596\n",
      "195/281, train_loss: 0.0571, step time: 0.2542\n",
      "196/281, train_loss: 0.1269, step time: 0.2571\n",
      "197/281, train_loss: 0.0522, step time: 0.2584\n",
      "198/281, train_loss: 0.0892, step time: 0.2576\n",
      "199/281, train_loss: 0.0517, step time: 0.2575\n",
      "200/281, train_loss: 0.0668, step time: 0.2595\n",
      "201/281, train_loss: 0.0575, step time: 0.2660\n",
      "202/281, train_loss: 0.0998, step time: 0.2525\n",
      "203/281, train_loss: 0.0655, step time: 0.2530\n",
      "204/281, train_loss: 0.0577, step time: 0.2514\n",
      "205/281, train_loss: 0.0506, step time: 0.2515\n",
      "206/281, train_loss: 0.0395, step time: 0.2551\n",
      "207/281, train_loss: 0.0759, step time: 0.2565\n",
      "208/281, train_loss: 0.0866, step time: 0.2511\n",
      "209/281, train_loss: 0.2211, step time: 0.2532\n",
      "210/281, train_loss: 0.0713, step time: 0.2546\n",
      "211/281, train_loss: 0.0665, step time: 0.2558\n",
      "212/281, train_loss: 0.0549, step time: 0.2581\n",
      "213/281, train_loss: 0.0910, step time: 0.2524\n",
      "214/281, train_loss: 0.0879, step time: 0.2525\n",
      "215/281, train_loss: 0.1250, step time: 0.2537\n",
      "216/281, train_loss: 0.0540, step time: 0.2529\n",
      "217/281, train_loss: 0.0678, step time: 0.2523\n",
      "218/281, train_loss: 0.0683, step time: 0.2510\n",
      "219/281, train_loss: 0.0804, step time: 0.2506\n",
      "220/281, train_loss: 0.0802, step time: 0.2564\n",
      "221/281, train_loss: 0.0554, step time: 0.2502\n",
      "222/281, train_loss: 0.0780, step time: 0.2536\n",
      "223/281, train_loss: 0.2530, step time: 0.2560\n",
      "224/281, train_loss: 0.0804, step time: 0.2630\n",
      "225/281, train_loss: 0.0706, step time: 0.2472\n",
      "226/281, train_loss: 0.0922, step time: 0.2469\n",
      "227/281, train_loss: 0.2253, step time: 0.2527\n",
      "228/281, train_loss: 0.0679, step time: 0.2536\n",
      "229/281, train_loss: 0.2477, step time: 0.2535\n",
      "230/281, train_loss: 0.0626, step time: 0.2532\n",
      "231/281, train_loss: 0.0741, step time: 0.2549\n",
      "232/281, train_loss: 0.0761, step time: 0.2528\n",
      "233/281, train_loss: 0.0601, step time: 0.2565\n",
      "234/281, train_loss: 0.0698, step time: 0.2494\n",
      "235/281, train_loss: 0.0756, step time: 0.2537\n",
      "236/281, train_loss: 0.2110, step time: 0.2498\n",
      "237/281, train_loss: 0.1016, step time: 0.2545\n",
      "238/281, train_loss: 0.0741, step time: 0.2578\n",
      "239/281, train_loss: 0.0792, step time: 0.2511\n",
      "240/281, train_loss: 0.0942, step time: 0.2579\n",
      "241/281, train_loss: 0.0376, step time: 0.2526\n",
      "242/281, train_loss: 0.0521, step time: 0.2496\n",
      "243/281, train_loss: 0.0858, step time: 0.2529\n",
      "244/281, train_loss: 0.0723, step time: 0.2536\n",
      "245/281, train_loss: 0.0730, step time: 0.2586\n",
      "246/281, train_loss: 0.0724, step time: 0.2562\n",
      "247/281, train_loss: 0.2117, step time: 0.2543\n",
      "248/281, train_loss: 0.0598, step time: 0.2526\n",
      "249/281, train_loss: 0.0598, step time: 0.2507\n",
      "250/281, train_loss: 0.2013, step time: 0.2491\n",
      "251/281, train_loss: 0.0744, step time: 0.2480\n",
      "252/281, train_loss: 0.0540, step time: 0.2522\n",
      "253/281, train_loss: 0.0624, step time: 0.2532\n",
      "254/281, train_loss: 0.0626, step time: 0.2503\n",
      "255/281, train_loss: 0.0338, step time: 0.2481\n",
      "256/281, train_loss: 0.0537, step time: 0.2526\n",
      "257/281, train_loss: 0.0526, step time: 0.2538\n",
      "258/281, train_loss: 0.0337, step time: 0.2515\n",
      "259/281, train_loss: 0.1031, step time: 0.2484\n",
      "260/281, train_loss: 0.2164, step time: 0.2543\n",
      "261/281, train_loss: 0.0569, step time: 0.2556\n",
      "262/281, train_loss: 0.1039, step time: 0.2511\n",
      "263/281, train_loss: 0.2174, step time: 0.2586\n",
      "264/281, train_loss: 0.0767, step time: 0.2550\n",
      "265/281, train_loss: 0.0619, step time: 0.2528\n",
      "266/281, train_loss: 0.0875, step time: 0.2564\n",
      "267/281, train_loss: 0.0590, step time: 0.2545\n",
      "268/281, train_loss: 0.0949, step time: 0.2526\n",
      "269/281, train_loss: 0.2383, step time: 0.2523\n",
      "270/281, train_loss: 0.1337, step time: 0.2507\n",
      "271/281, train_loss: 0.2200, step time: 0.2546\n",
      "272/281, train_loss: 0.0528, step time: 0.2555\n",
      "273/281, train_loss: 0.0900, step time: 0.2539\n",
      "274/281, train_loss: 0.0956, step time: 0.2511\n",
      "275/281, train_loss: 0.0421, step time: 0.2532\n",
      "276/281, train_loss: 0.0786, step time: 0.2564\n",
      "277/281, train_loss: 0.0706, step time: 0.2524\n",
      "278/281, train_loss: 0.0757, step time: 0.2537\n",
      "279/281, train_loss: 0.2275, step time: 0.2551\n",
      "280/281, train_loss: 0.0678, step time: 0.2547\n",
      "281/281, train_loss: 0.0768, step time: 0.2493\n",
      "282/281, train_loss: 0.0804, step time: 0.1485\n",
      "epoch 92 average loss: 0.0958\n",
      "current epoch: 92 current mean dice: 0.8947 tc: 0.8870 wt: 0.9242 et: 0.8818\n",
      "best mean dice: 0.8956 at epoch: 89\n",
      "time consuming of epoch 92 is: 395.5327\n",
      "----------\n",
      "epoch 93/200\n",
      "1/281, train_loss: 0.0630, step time: 0.2621\n",
      "2/281, train_loss: 0.0929, step time: 0.2567\n",
      "3/281, train_loss: 0.2409, step time: 0.2585\n",
      "4/281, train_loss: 0.0725, step time: 0.2514\n",
      "5/281, train_loss: 0.0632, step time: 0.2535\n",
      "6/281, train_loss: 0.0743, step time: 0.2502\n",
      "7/281, train_loss: 0.0644, step time: 0.2527\n",
      "8/281, train_loss: 0.0638, step time: 0.2545\n",
      "9/281, train_loss: 0.1014, step time: 0.2563\n",
      "10/281, train_loss: 0.2552, step time: 0.2582\n",
      "11/281, train_loss: 0.0493, step time: 0.2604\n",
      "12/281, train_loss: 0.0926, step time: 0.2590\n",
      "13/281, train_loss: 0.0547, step time: 0.2590\n",
      "14/281, train_loss: 0.0503, step time: 0.2478\n",
      "15/281, train_loss: 0.0567, step time: 0.2526\n",
      "16/281, train_loss: 0.1166, step time: 0.2487\n",
      "17/281, train_loss: 0.0576, step time: 0.2516\n",
      "18/281, train_loss: 0.0745, step time: 0.2481\n",
      "19/281, train_loss: 0.0599, step time: 0.2527\n",
      "20/281, train_loss: 0.0784, step time: 0.2545\n",
      "21/281, train_loss: 0.0596, step time: 0.2653\n",
      "22/281, train_loss: 0.0636, step time: 0.2576\n",
      "23/281, train_loss: 0.0588, step time: 0.2591\n",
      "24/281, train_loss: 0.0529, step time: 0.2549\n",
      "25/281, train_loss: 0.2354, step time: 0.2576\n",
      "26/281, train_loss: 0.2137, step time: 0.2665\n",
      "27/281, train_loss: 0.1053, step time: 0.2620\n",
      "28/281, train_loss: 0.0774, step time: 0.2617\n",
      "29/281, train_loss: 0.0733, step time: 0.2553\n",
      "30/281, train_loss: 0.1029, step time: 0.2557\n",
      "31/281, train_loss: 0.0790, step time: 0.2539\n",
      "32/281, train_loss: 0.1076, step time: 0.2449\n",
      "33/281, train_loss: 0.0663, step time: 0.2493\n",
      "34/281, train_loss: 0.2458, step time: 0.2587\n",
      "35/281, train_loss: 0.0592, step time: 0.2574\n",
      "36/281, train_loss: 0.0787, step time: 0.2541\n",
      "37/281, train_loss: 0.2093, step time: 0.2495\n",
      "38/281, train_loss: 0.0630, step time: 0.2537\n",
      "39/281, train_loss: 0.0889, step time: 0.2471\n",
      "40/281, train_loss: 0.0603, step time: 0.2560\n",
      "41/281, train_loss: 0.0393, step time: 0.2552\n",
      "42/281, train_loss: 0.2126, step time: 0.2528\n",
      "43/281, train_loss: 0.0903, step time: 0.2555\n",
      "44/281, train_loss: 0.0638, step time: 0.2468\n",
      "45/281, train_loss: 0.0670, step time: 0.2518\n",
      "46/281, train_loss: 0.0667, step time: 0.2571\n",
      "47/281, train_loss: 0.2411, step time: 0.2537\n",
      "48/281, train_loss: 0.2310, step time: 0.2440\n",
      "49/281, train_loss: 0.0860, step time: 0.2441\n",
      "50/281, train_loss: 0.2265, step time: 0.2600\n",
      "51/281, train_loss: 0.0426, step time: 0.2570\n",
      "52/281, train_loss: 0.0575, step time: 0.2548\n",
      "53/281, train_loss: 0.0841, step time: 0.2483\n",
      "54/281, train_loss: 0.2371, step time: 0.2582\n",
      "55/281, train_loss: 0.0407, step time: 0.2574\n",
      "56/281, train_loss: 0.0512, step time: 0.2577\n",
      "57/281, train_loss: 0.0826, step time: 0.2568\n",
      "58/281, train_loss: 0.0922, step time: 0.2546\n",
      "59/281, train_loss: 0.0863, step time: 0.2614\n",
      "60/281, train_loss: 0.0662, step time: 0.2594\n",
      "61/281, train_loss: 0.0787, step time: 0.2728\n",
      "62/281, train_loss: 0.0677, step time: 0.2505\n",
      "63/281, train_loss: 0.0551, step time: 0.2549\n",
      "64/281, train_loss: 0.0994, step time: 0.2494\n",
      "65/281, train_loss: 0.0599, step time: 0.2515\n",
      "66/281, train_loss: 0.0335, step time: 0.2570\n",
      "67/281, train_loss: 0.1003, step time: 0.2606\n",
      "68/281, train_loss: 0.2070, step time: 0.2633\n",
      "69/281, train_loss: 0.2471, step time: 0.2542\n",
      "70/281, train_loss: 0.0678, step time: 0.2630\n",
      "71/281, train_loss: 0.0355, step time: 0.2583\n",
      "72/281, train_loss: 0.0740, step time: 0.2523\n",
      "73/281, train_loss: 0.0684, step time: 0.2571\n",
      "74/281, train_loss: 0.0575, step time: 0.2623\n",
      "75/281, train_loss: 0.0694, step time: 0.2570\n",
      "76/281, train_loss: 0.2198, step time: 0.2569\n",
      "77/281, train_loss: 0.1318, step time: 0.2516\n",
      "78/281, train_loss: 0.2622, step time: 0.2541\n",
      "79/281, train_loss: 0.0524, step time: 0.2517\n",
      "80/281, train_loss: 0.0860, step time: 0.2461\n",
      "81/281, train_loss: 0.0503, step time: 0.2468\n",
      "82/281, train_loss: 0.0947, step time: 0.2591\n",
      "83/281, train_loss: 0.0717, step time: 0.2539\n",
      "84/281, train_loss: 0.2375, step time: 0.2547\n",
      "85/281, train_loss: 0.0477, step time: 0.2526\n",
      "86/281, train_loss: 0.0615, step time: 0.2499\n",
      "87/281, train_loss: 0.0525, step time: 0.2526\n",
      "88/281, train_loss: 0.0507, step time: 0.2599\n",
      "89/281, train_loss: 0.0946, step time: 0.2600\n",
      "90/281, train_loss: 0.0943, step time: 0.2550\n",
      "91/281, train_loss: 0.0869, step time: 0.2583\n",
      "92/281, train_loss: 0.2294, step time: 0.2552\n",
      "93/281, train_loss: 0.1148, step time: 0.2557\n",
      "94/281, train_loss: 0.0639, step time: 0.2481\n",
      "95/281, train_loss: 0.2222, step time: 0.2470\n",
      "96/281, train_loss: 0.0787, step time: 0.2459\n",
      "97/281, train_loss: 0.0656, step time: 0.2502\n",
      "98/281, train_loss: 0.0715, step time: 0.2534\n",
      "99/281, train_loss: 0.2834, step time: 0.2480\n",
      "100/281, train_loss: 0.0662, step time: 0.2517\n",
      "101/281, train_loss: 0.0670, step time: 0.2475\n",
      "102/281, train_loss: 0.0836, step time: 0.2474\n",
      "103/281, train_loss: 0.1016, step time: 0.2442\n",
      "104/281, train_loss: 0.0782, step time: 0.2444\n",
      "105/281, train_loss: 0.0770, step time: 0.2468\n",
      "106/281, train_loss: 0.0926, step time: 0.2539\n",
      "107/281, train_loss: 0.0667, step time: 0.2533\n",
      "108/281, train_loss: 0.0676, step time: 0.2466\n",
      "109/281, train_loss: 0.0615, step time: 0.2482\n",
      "110/281, train_loss: 0.0554, step time: 0.2524\n",
      "111/281, train_loss: 0.2290, step time: 0.2530\n",
      "112/281, train_loss: 0.0554, step time: 0.2486\n",
      "113/281, train_loss: 0.0562, step time: 0.2521\n",
      "114/281, train_loss: 0.0751, step time: 0.2500\n",
      "115/281, train_loss: 0.1145, step time: 0.2464\n",
      "116/281, train_loss: 0.0741, step time: 0.2512\n",
      "117/281, train_loss: 0.1015, step time: 0.2446\n",
      "118/281, train_loss: 0.0730, step time: 0.2622\n",
      "119/281, train_loss: 0.0420, step time: 0.2570\n",
      "120/281, train_loss: 0.2129, step time: 0.2486\n",
      "121/281, train_loss: 0.0796, step time: 0.2506\n",
      "122/281, train_loss: 0.0546, step time: 0.2578\n",
      "123/281, train_loss: 0.2340, step time: 0.2534\n",
      "124/281, train_loss: 0.0860, step time: 0.2511\n",
      "125/281, train_loss: 0.0462, step time: 0.2544\n",
      "126/281, train_loss: 0.0498, step time: 0.2550\n",
      "127/281, train_loss: 0.0685, step time: 0.2591\n",
      "128/281, train_loss: 0.0595, step time: 0.2483\n",
      "129/281, train_loss: 0.0863, step time: 0.2478\n",
      "130/281, train_loss: 0.0523, step time: 0.2472\n",
      "131/281, train_loss: 0.2275, step time: 0.2554\n",
      "132/281, train_loss: 0.0669, step time: 0.2547\n",
      "133/281, train_loss: 0.0815, step time: 0.2550\n",
      "134/281, train_loss: 0.0407, step time: 0.2542\n",
      "135/281, train_loss: 0.0731, step time: 0.2520\n",
      "136/281, train_loss: 0.0707, step time: 0.2482\n",
      "137/281, train_loss: 0.0492, step time: 0.2495\n",
      "138/281, train_loss: 0.0457, step time: 0.2562\n",
      "139/281, train_loss: 0.0550, step time: 0.2577\n",
      "140/281, train_loss: 0.0757, step time: 0.2523\n",
      "141/281, train_loss: 0.0652, step time: 0.2493\n",
      "142/281, train_loss: 0.0324, step time: 0.2472\n",
      "143/281, train_loss: 0.0709, step time: 0.2543\n",
      "144/281, train_loss: 0.1033, step time: 0.2522\n",
      "145/281, train_loss: 0.2297, step time: 0.2555\n",
      "146/281, train_loss: 0.2207, step time: 0.2519\n",
      "147/281, train_loss: 0.1102, step time: 0.2475\n",
      "148/281, train_loss: 0.2363, step time: 0.2494\n",
      "149/281, train_loss: 0.0988, step time: 0.2514\n",
      "150/281, train_loss: 0.0886, step time: 0.2509\n",
      "151/281, train_loss: 0.0434, step time: 0.2559\n",
      "152/281, train_loss: 0.0666, step time: 0.2488\n",
      "153/281, train_loss: 0.0527, step time: 0.2543\n",
      "154/281, train_loss: 0.0706, step time: 0.2688\n",
      "155/281, train_loss: 0.0872, step time: 0.2506\n",
      "156/281, train_loss: 0.0754, step time: 0.2523\n",
      "157/281, train_loss: 0.0707, step time: 0.2568\n",
      "158/281, train_loss: 0.0690, step time: 0.2588\n",
      "159/281, train_loss: 0.1092, step time: 0.2587\n",
      "160/281, train_loss: 0.0665, step time: 0.2578\n",
      "161/281, train_loss: 0.0638, step time: 0.2553\n",
      "162/281, train_loss: 0.0578, step time: 0.2566\n",
      "163/281, train_loss: 0.1002, step time: 0.2576\n",
      "164/281, train_loss: 0.0756, step time: 0.2528\n",
      "165/281, train_loss: 0.0845, step time: 0.2563\n",
      "166/281, train_loss: 0.0873, step time: 0.2594\n",
      "167/281, train_loss: 0.0705, step time: 0.2511\n",
      "168/281, train_loss: 0.2181, step time: 0.2497\n",
      "169/281, train_loss: 0.0415, step time: 0.2472\n",
      "170/281, train_loss: 0.0416, step time: 0.2482\n",
      "171/281, train_loss: 0.0553, step time: 0.2503\n",
      "172/281, train_loss: 0.0574, step time: 0.2522\n",
      "173/281, train_loss: 0.0516, step time: 0.2518\n",
      "174/281, train_loss: 0.2206, step time: 0.2511\n",
      "175/281, train_loss: 0.0648, step time: 0.2534\n",
      "176/281, train_loss: 0.0696, step time: 0.2578\n",
      "177/281, train_loss: 0.0707, step time: 0.2541\n",
      "178/281, train_loss: 0.0732, step time: 0.2547\n",
      "179/281, train_loss: 0.0661, step time: 0.2543\n",
      "180/281, train_loss: 0.0370, step time: 0.2592\n",
      "181/281, train_loss: 0.0492, step time: 0.2658\n",
      "182/281, train_loss: 0.0931, step time: 0.2531\n",
      "183/281, train_loss: 0.0715, step time: 0.2529\n",
      "184/281, train_loss: 0.1152, step time: 0.2449\n",
      "185/281, train_loss: 0.0638, step time: 0.2419\n",
      "186/281, train_loss: 0.0806, step time: 0.2475\n",
      "187/281, train_loss: 0.0988, step time: 0.2517\n",
      "188/281, train_loss: 0.0776, step time: 0.2575\n",
      "189/281, train_loss: 0.0451, step time: 0.2512\n",
      "190/281, train_loss: 0.2346, step time: 0.2460\n",
      "191/281, train_loss: 0.0925, step time: 0.2441\n",
      "192/281, train_loss: 0.0668, step time: 0.2497\n",
      "193/281, train_loss: 0.0480, step time: 0.2462\n",
      "194/281, train_loss: 0.0926, step time: 0.2547\n",
      "195/281, train_loss: 0.0869, step time: 0.2554\n",
      "196/281, train_loss: 0.0533, step time: 0.2569\n",
      "197/281, train_loss: 0.0626, step time: 0.2540\n",
      "198/281, train_loss: 0.0643, step time: 0.2508\n",
      "199/281, train_loss: 0.0565, step time: 0.2535\n",
      "200/281, train_loss: 0.0728, step time: 0.2484\n",
      "201/281, train_loss: 0.0714, step time: 0.2445\n",
      "202/281, train_loss: 0.2435, step time: 0.2538\n",
      "203/281, train_loss: 0.0792, step time: 0.2514\n",
      "204/281, train_loss: 0.0522, step time: 0.2545\n",
      "205/281, train_loss: 0.0361, step time: 0.2489\n",
      "206/281, train_loss: 0.0524, step time: 0.2533\n",
      "207/281, train_loss: 0.0958, step time: 0.2514\n",
      "208/281, train_loss: 0.0766, step time: 0.2435\n",
      "209/281, train_loss: 0.0708, step time: 0.2465\n",
      "210/281, train_loss: 0.0443, step time: 0.2495\n",
      "211/281, train_loss: 0.2300, step time: 0.2522\n",
      "212/281, train_loss: 0.0970, step time: 0.2526\n",
      "213/281, train_loss: 0.0499, step time: 0.2586\n",
      "214/281, train_loss: 0.0659, step time: 0.2602\n",
      "215/281, train_loss: 0.0639, step time: 0.2467\n",
      "216/281, train_loss: 0.0760, step time: 0.2465\n",
      "217/281, train_loss: 0.1074, step time: 0.2463\n",
      "218/281, train_loss: 0.0869, step time: 0.2561\n",
      "219/281, train_loss: 0.0458, step time: 0.2496\n",
      "220/281, train_loss: 0.1237, step time: 0.2496\n",
      "221/281, train_loss: 0.0584, step time: 0.2498\n",
      "222/281, train_loss: 0.0597, step time: 0.2507\n",
      "223/281, train_loss: 0.0831, step time: 0.2518\n",
      "224/281, train_loss: 0.0693, step time: 0.2555\n",
      "225/281, train_loss: 0.0985, step time: 0.2462\n",
      "226/281, train_loss: 0.0568, step time: 0.2534\n",
      "227/281, train_loss: 0.2219, step time: 0.2480\n",
      "228/281, train_loss: 0.2167, step time: 0.2440\n",
      "229/281, train_loss: 0.0725, step time: 0.2445\n",
      "230/281, train_loss: 0.0713, step time: 0.2476\n",
      "231/281, train_loss: 0.0629, step time: 0.2496\n",
      "232/281, train_loss: 0.2149, step time: 0.2508\n",
      "233/281, train_loss: 0.0927, step time: 0.2517\n",
      "234/281, train_loss: 0.2204, step time: 0.2466\n",
      "235/281, train_loss: 0.0589, step time: 0.2502\n",
      "236/281, train_loss: 0.1022, step time: 0.2508\n",
      "237/281, train_loss: 0.0366, step time: 0.2473\n",
      "238/281, train_loss: 0.0767, step time: 0.2542\n",
      "239/281, train_loss: 0.0934, step time: 0.2554\n",
      "240/281, train_loss: 0.2174, step time: 0.2537\n",
      "241/281, train_loss: 0.0500, step time: 0.2812\n",
      "242/281, train_loss: 0.0728, step time: 0.2529\n",
      "243/281, train_loss: 0.0377, step time: 0.2539\n",
      "244/281, train_loss: 0.0701, step time: 0.2571\n",
      "245/281, train_loss: 0.0803, step time: 0.2556\n",
      "246/281, train_loss: 0.0873, step time: 0.2502\n",
      "247/281, train_loss: 0.2353, step time: 0.2582\n",
      "248/281, train_loss: 0.0192, step time: 0.2535\n",
      "249/281, train_loss: 0.0473, step time: 0.2509\n",
      "250/281, train_loss: 0.0815, step time: 0.2580\n",
      "251/281, train_loss: 0.1987, step time: 0.2552\n",
      "252/281, train_loss: 0.0409, step time: 0.2602\n",
      "253/281, train_loss: 0.0509, step time: 0.2488\n",
      "254/281, train_loss: 0.0693, step time: 0.2551\n",
      "255/281, train_loss: 0.2231, step time: 0.2520\n",
      "256/281, train_loss: 0.2427, step time: 0.2475\n",
      "257/281, train_loss: 0.0708, step time: 0.2515\n",
      "258/281, train_loss: 0.0620, step time: 0.2539\n",
      "259/281, train_loss: 0.0608, step time: 0.2542\n",
      "260/281, train_loss: 0.0498, step time: 0.2508\n",
      "261/281, train_loss: 0.0660, step time: 0.2537\n",
      "262/281, train_loss: 0.0489, step time: 0.2514\n",
      "263/281, train_loss: 0.0629, step time: 0.2618\n",
      "264/281, train_loss: 0.0823, step time: 0.2582\n",
      "265/281, train_loss: 0.2288, step time: 0.2502\n",
      "266/281, train_loss: 0.2258, step time: 0.2581\n",
      "267/281, train_loss: 0.0970, step time: 0.2469\n",
      "268/281, train_loss: 0.1155, step time: 0.2561\n",
      "269/281, train_loss: 0.2454, step time: 0.2534\n",
      "270/281, train_loss: 0.0514, step time: 0.2520\n",
      "271/281, train_loss: 0.0672, step time: 0.2588\n",
      "272/281, train_loss: 0.0550, step time: 0.2557\n",
      "273/281, train_loss: 0.0535, step time: 0.2510\n",
      "274/281, train_loss: 0.0530, step time: 0.2541\n",
      "275/281, train_loss: 0.0910, step time: 0.2522\n",
      "276/281, train_loss: 0.0688, step time: 0.2512\n",
      "277/281, train_loss: 0.0663, step time: 0.2513\n",
      "278/281, train_loss: 0.0596, step time: 0.2514\n",
      "279/281, train_loss: 0.0789, step time: 0.2518\n",
      "280/281, train_loss: 0.0494, step time: 0.2456\n",
      "281/281, train_loss: 0.0858, step time: 0.2500\n",
      "282/281, train_loss: 0.0518, step time: 0.1472\n",
      "epoch 93 average loss: 0.0945\n",
      "current epoch: 93 current mean dice: 0.8951 tc: 0.8891 wt: 0.9228 et: 0.8822\n",
      "best mean dice: 0.8956 at epoch: 89\n",
      "time consuming of epoch 93 is: 345.5027\n",
      "----------\n",
      "epoch 94/200\n",
      "1/281, train_loss: 0.0580, step time: 0.2622\n",
      "2/281, train_loss: 0.0729, step time: 0.2549\n",
      "3/281, train_loss: 0.0461, step time: 0.2495\n",
      "4/281, train_loss: 0.0701, step time: 0.2596\n",
      "5/281, train_loss: 0.0901, step time: 0.2551\n",
      "6/281, train_loss: 0.0764, step time: 0.2487\n",
      "7/281, train_loss: 0.0839, step time: 0.2582\n",
      "8/281, train_loss: 0.0656, step time: 0.2590\n",
      "9/281, train_loss: 0.0437, step time: 0.2608\n",
      "10/281, train_loss: 0.0697, step time: 0.2557\n",
      "11/281, train_loss: 0.2146, step time: 0.2524\n",
      "12/281, train_loss: 0.1098, step time: 0.2548\n",
      "13/281, train_loss: 0.0579, step time: 0.2520\n",
      "14/281, train_loss: 0.2379, step time: 0.2528\n",
      "15/281, train_loss: 0.0411, step time: 0.2568\n",
      "16/281, train_loss: 0.0800, step time: 0.2536\n",
      "17/281, train_loss: 0.0632, step time: 0.2560\n",
      "18/281, train_loss: 0.0576, step time: 0.2629\n",
      "19/281, train_loss: 0.0854, step time: 0.2573\n",
      "20/281, train_loss: 0.0240, step time: 0.2662\n",
      "21/281, train_loss: 0.0809, step time: 0.2566\n",
      "22/281, train_loss: 0.0548, step time: 0.2588\n",
      "23/281, train_loss: 0.0478, step time: 0.2599\n",
      "24/281, train_loss: 0.0470, step time: 0.2604\n",
      "25/281, train_loss: 0.0752, step time: 0.2582\n",
      "26/281, train_loss: 0.0650, step time: 0.2505\n",
      "27/281, train_loss: 0.0577, step time: 0.2484\n",
      "28/281, train_loss: 0.0748, step time: 0.2663\n",
      "29/281, train_loss: 0.2539, step time: 0.2612\n",
      "30/281, train_loss: 0.0802, step time: 0.2559\n",
      "31/281, train_loss: 0.2392, step time: 0.2629\n",
      "32/281, train_loss: 0.0393, step time: 0.2819\n",
      "33/281, train_loss: 0.0399, step time: 0.2547\n",
      "34/281, train_loss: 0.0660, step time: 0.2604\n",
      "35/281, train_loss: 0.0561, step time: 0.2603\n",
      "36/281, train_loss: 0.1568, step time: 0.2606\n",
      "37/281, train_loss: 0.0734, step time: 0.2622\n",
      "38/281, train_loss: 0.0485, step time: 0.2520\n",
      "39/281, train_loss: 0.0614, step time: 0.2550\n",
      "40/281, train_loss: 0.0703, step time: 0.2579\n",
      "41/281, train_loss: 0.0664, step time: 0.2572\n",
      "42/281, train_loss: 0.2441, step time: 0.2594\n",
      "43/281, train_loss: 0.0652, step time: 0.2575\n",
      "44/281, train_loss: 0.0575, step time: 0.2639\n",
      "45/281, train_loss: 0.0564, step time: 0.2646\n",
      "46/281, train_loss: 0.2520, step time: 0.2629\n",
      "47/281, train_loss: 0.0782, step time: 0.2549\n",
      "48/281, train_loss: 0.0486, step time: 0.2544\n",
      "49/281, train_loss: 0.0869, step time: 0.2573\n",
      "50/281, train_loss: 0.0626, step time: 0.2589\n",
      "51/281, train_loss: 0.0839, step time: 0.2565\n",
      "52/281, train_loss: 0.0585, step time: 0.2588\n",
      "53/281, train_loss: 0.0733, step time: 0.2536\n",
      "54/281, train_loss: 0.0551, step time: 0.2588\n",
      "55/281, train_loss: 0.0574, step time: 0.2568\n",
      "56/281, train_loss: 0.0875, step time: 0.2549\n",
      "57/281, train_loss: 0.0757, step time: 0.2534\n",
      "58/281, train_loss: 0.0739, step time: 0.2578\n",
      "59/281, train_loss: 0.0748, step time: 0.2556\n",
      "60/281, train_loss: 0.1111, step time: 0.2526\n",
      "61/281, train_loss: 0.0976, step time: 0.2544\n",
      "62/281, train_loss: 0.2353, step time: 0.2583\n",
      "63/281, train_loss: 0.2187, step time: 0.2571\n",
      "64/281, train_loss: 0.0543, step time: 0.2586\n",
      "65/281, train_loss: 0.0798, step time: 0.2547\n",
      "66/281, train_loss: 0.0946, step time: 0.2558\n",
      "67/281, train_loss: 0.0395, step time: 0.2508\n",
      "68/281, train_loss: 0.0543, step time: 0.2558\n",
      "69/281, train_loss: 0.0424, step time: 0.2557\n",
      "70/281, train_loss: 0.0909, step time: 0.2583\n",
      "71/281, train_loss: 0.1039, step time: 0.2569\n",
      "72/281, train_loss: 0.0384, step time: 0.2522\n",
      "73/281, train_loss: 0.0771, step time: 0.2577\n",
      "74/281, train_loss: 0.0556, step time: 0.2574\n",
      "75/281, train_loss: 0.0692, step time: 0.2509\n",
      "76/281, train_loss: 0.0486, step time: 0.2500\n",
      "77/281, train_loss: 0.0947, step time: 0.2552\n",
      "78/281, train_loss: 0.2202, step time: 0.2515\n",
      "79/281, train_loss: 0.2015, step time: 0.2527\n",
      "80/281, train_loss: 0.2441, step time: 0.2523\n",
      "81/281, train_loss: 0.0544, step time: 0.2652\n",
      "82/281, train_loss: 0.0705, step time: 0.2671\n",
      "83/281, train_loss: 0.0853, step time: 0.2657\n",
      "84/281, train_loss: 0.0788, step time: 0.2564\n",
      "85/281, train_loss: 0.0494, step time: 0.2560\n",
      "86/281, train_loss: 0.1104, step time: 0.2593\n",
      "87/281, train_loss: 0.0568, step time: 0.2549\n",
      "88/281, train_loss: 0.2536, step time: 0.2597\n",
      "89/281, train_loss: 0.2291, step time: 0.2574\n",
      "90/281, train_loss: 0.0844, step time: 0.2551\n",
      "91/281, train_loss: 0.0787, step time: 0.2593\n",
      "92/281, train_loss: 0.0552, step time: 0.2607\n",
      "93/281, train_loss: 0.0514, step time: 0.2586\n",
      "94/281, train_loss: 0.2619, step time: 0.2631\n",
      "95/281, train_loss: 0.0604, step time: 0.2587\n",
      "96/281, train_loss: 0.0649, step time: 0.2611\n",
      "97/281, train_loss: 0.2362, step time: 0.2586\n",
      "98/281, train_loss: 0.0929, step time: 0.2548\n",
      "99/281, train_loss: 0.0759, step time: 0.2515\n",
      "100/281, train_loss: 0.0940, step time: 0.2577\n",
      "101/281, train_loss: 0.0469, step time: 0.2550\n",
      "102/281, train_loss: 0.1050, step time: 0.2598\n",
      "103/281, train_loss: 0.0693, step time: 0.2638\n",
      "104/281, train_loss: 0.0690, step time: 0.2592\n",
      "105/281, train_loss: 0.0790, step time: 0.2571\n",
      "106/281, train_loss: 0.1110, step time: 0.2585\n",
      "107/281, train_loss: 0.0847, step time: 0.2589\n",
      "108/281, train_loss: 0.0723, step time: 0.2559\n",
      "109/281, train_loss: 0.2332, step time: 0.2564\n",
      "110/281, train_loss: 0.0664, step time: 0.2558\n",
      "111/281, train_loss: 0.0841, step time: 0.2630\n",
      "112/281, train_loss: 0.0560, step time: 0.2636\n",
      "113/281, train_loss: 0.0584, step time: 0.2584\n",
      "114/281, train_loss: 0.0787, step time: 0.2587\n",
      "115/281, train_loss: 0.0709, step time: 0.2584\n",
      "116/281, train_loss: 0.0809, step time: 0.2620\n",
      "117/281, train_loss: 0.0706, step time: 0.2626\n",
      "118/281, train_loss: 0.0459, step time: 0.2686\n",
      "119/281, train_loss: 0.0762, step time: 0.2555\n",
      "120/281, train_loss: 0.0873, step time: 0.2577\n",
      "121/281, train_loss: 0.0746, step time: 0.2576\n",
      "122/281, train_loss: 0.0785, step time: 0.2572\n",
      "123/281, train_loss: 0.2242, step time: 0.2566\n",
      "124/281, train_loss: 0.0543, step time: 0.2592\n",
      "125/281, train_loss: 0.0519, step time: 0.2613\n",
      "126/281, train_loss: 0.0472, step time: 0.2583\n",
      "127/281, train_loss: 0.0612, step time: 0.2633\n",
      "128/281, train_loss: 0.2306, step time: 0.2576\n",
      "129/281, train_loss: 0.1143, step time: 0.2614\n",
      "130/281, train_loss: 0.0989, step time: 0.2600\n",
      "131/281, train_loss: 0.0518, step time: 0.2548\n",
      "132/281, train_loss: 0.0792, step time: 0.2585\n",
      "133/281, train_loss: 0.0590, step time: 0.2589\n",
      "134/281, train_loss: 0.2168, step time: 0.2589\n",
      "135/281, train_loss: 0.0682, step time: 0.2588\n",
      "136/281, train_loss: 0.0949, step time: 0.2599\n",
      "137/281, train_loss: 0.2205, step time: 0.2596\n",
      "138/281, train_loss: 0.0812, step time: 0.2601\n",
      "139/281, train_loss: 0.2154, step time: 0.2583\n",
      "140/281, train_loss: 0.1164, step time: 0.2588\n",
      "141/281, train_loss: 0.0644, step time: 0.2589\n",
      "142/281, train_loss: 0.0549, step time: 0.2593\n",
      "143/281, train_loss: 0.0279, step time: 0.2520\n",
      "144/281, train_loss: 0.0428, step time: 0.2526\n",
      "145/281, train_loss: 0.0832, step time: 0.2594\n",
      "146/281, train_loss: 0.0756, step time: 0.2566\n",
      "147/281, train_loss: 0.2178, step time: 0.2574\n",
      "148/281, train_loss: 0.2387, step time: 0.2548\n",
      "149/281, train_loss: 0.1084, step time: 0.2576\n",
      "150/281, train_loss: 0.0489, step time: 0.2528\n",
      "151/281, train_loss: 0.0595, step time: 0.2631\n",
      "152/281, train_loss: 0.0889, step time: 0.2625\n",
      "153/281, train_loss: 0.0552, step time: 0.2559\n",
      "154/281, train_loss: 0.0792, step time: 0.2588\n",
      "155/281, train_loss: 0.0482, step time: 0.2588\n",
      "156/281, train_loss: 0.0312, step time: 0.2610\n",
      "157/281, train_loss: 0.0782, step time: 0.2601\n",
      "158/281, train_loss: 0.2129, step time: 0.2581\n",
      "159/281, train_loss: 0.0773, step time: 0.2555\n",
      "160/281, train_loss: 0.0744, step time: 0.2579\n",
      "161/281, train_loss: 0.2444, step time: 0.2519\n",
      "162/281, train_loss: 0.0892, step time: 0.2568\n",
      "163/281, train_loss: 0.0704, step time: 0.2507\n",
      "164/281, train_loss: 0.2234, step time: 0.2506\n",
      "165/281, train_loss: 0.0574, step time: 0.2556\n",
      "166/281, train_loss: 0.0876, step time: 0.2545\n",
      "167/281, train_loss: 0.0563, step time: 0.2541\n",
      "168/281, train_loss: 0.0827, step time: 0.2525\n",
      "169/281, train_loss: 0.0728, step time: 0.2564\n",
      "170/281, train_loss: 0.0694, step time: 0.2547\n",
      "171/281, train_loss: 0.0520, step time: 0.2531\n",
      "172/281, train_loss: 0.0665, step time: 0.2560\n",
      "173/281, train_loss: 0.0635, step time: 0.2546\n",
      "174/281, train_loss: 0.0578, step time: 0.2600\n",
      "175/281, train_loss: 0.0500, step time: 0.2573\n",
      "176/281, train_loss: 0.0662, step time: 0.2732\n",
      "177/281, train_loss: 0.0503, step time: 0.2571\n",
      "178/281, train_loss: 0.0637, step time: 0.2566\n",
      "179/281, train_loss: 0.0722, step time: 0.2554\n",
      "180/281, train_loss: 0.0525, step time: 0.2534\n",
      "181/281, train_loss: 0.2321, step time: 0.2574\n",
      "182/281, train_loss: 0.0498, step time: 0.2580\n",
      "183/281, train_loss: 0.0759, step time: 0.2574\n",
      "184/281, train_loss: 0.0785, step time: 0.2551\n",
      "185/281, train_loss: 0.0362, step time: 0.2553\n",
      "186/281, train_loss: 0.0467, step time: 0.2547\n",
      "187/281, train_loss: 0.0455, step time: 0.2520\n",
      "188/281, train_loss: 0.0946, step time: 0.2528\n",
      "189/281, train_loss: 0.0582, step time: 0.2575\n",
      "190/281, train_loss: 0.0763, step time: 0.2579\n",
      "191/281, train_loss: 0.2459, step time: 0.2506\n",
      "192/281, train_loss: 0.0884, step time: 0.2561\n",
      "193/281, train_loss: 0.0850, step time: 0.2533\n",
      "194/281, train_loss: 0.2131, step time: 0.2570\n",
      "195/281, train_loss: 0.0670, step time: 0.2590\n",
      "196/281, train_loss: 0.0787, step time: 0.2603\n",
      "197/281, train_loss: 0.0798, step time: 0.2618\n",
      "198/281, train_loss: 0.0723, step time: 0.2612\n",
      "199/281, train_loss: 0.1996, step time: 0.2546\n",
      "200/281, train_loss: 0.0754, step time: 0.2566\n",
      "201/281, train_loss: 0.0773, step time: 0.2600\n",
      "202/281, train_loss: 0.0800, step time: 0.2590\n",
      "203/281, train_loss: 0.2471, step time: 0.2574\n",
      "204/281, train_loss: 0.1231, step time: 0.2559\n",
      "205/281, train_loss: 0.2205, step time: 0.2590\n",
      "206/281, train_loss: 0.0597, step time: 0.2593\n",
      "207/281, train_loss: 0.0849, step time: 0.2573\n",
      "208/281, train_loss: 0.2305, step time: 0.2586\n",
      "209/281, train_loss: 0.0678, step time: 0.2569\n",
      "210/281, train_loss: 0.0739, step time: 0.2567\n",
      "211/281, train_loss: 0.1109, step time: 0.2526\n",
      "212/281, train_loss: 0.0893, step time: 0.2596\n",
      "213/281, train_loss: 0.0387, step time: 0.2605\n",
      "214/281, train_loss: 0.0520, step time: 0.2554\n",
      "215/281, train_loss: 0.0686, step time: 0.2519\n",
      "216/281, train_loss: 0.0715, step time: 0.2568\n",
      "217/281, train_loss: 0.0723, step time: 0.2597\n",
      "218/281, train_loss: 0.0445, step time: 0.2562\n",
      "219/281, train_loss: 0.0550, step time: 0.2534\n",
      "220/281, train_loss: 0.1062, step time: 0.2558\n",
      "221/281, train_loss: 0.0937, step time: 0.2572\n",
      "222/281, train_loss: 0.2091, step time: 0.2521\n",
      "223/281, train_loss: 0.0584, step time: 0.2580\n",
      "224/281, train_loss: 0.0764, step time: 0.2526\n",
      "225/281, train_loss: 0.0561, step time: 0.2537\n",
      "226/281, train_loss: 0.0760, step time: 0.2544\n",
      "227/281, train_loss: 0.0500, step time: 0.2588\n",
      "228/281, train_loss: 0.0577, step time: 0.2567\n",
      "229/281, train_loss: 0.0544, step time: 0.2548\n",
      "230/281, train_loss: 0.0455, step time: 0.2547\n",
      "231/281, train_loss: 0.0807, step time: 0.2589\n",
      "232/281, train_loss: 0.0564, step time: 0.2592\n",
      "233/281, train_loss: 0.1987, step time: 0.2669\n",
      "234/281, train_loss: 0.0569, step time: 0.2499\n",
      "235/281, train_loss: 0.0673, step time: 0.2507\n",
      "236/281, train_loss: 0.0784, step time: 0.2588\n",
      "237/281, train_loss: 0.0787, step time: 0.2533\n",
      "238/281, train_loss: 0.0620, step time: 0.2583\n",
      "239/281, train_loss: 0.0707, step time: 0.2587\n",
      "240/281, train_loss: 0.0461, step time: 0.2542\n",
      "241/281, train_loss: 0.0609, step time: 0.2601\n",
      "242/281, train_loss: 0.0307, step time: 0.2604\n",
      "243/281, train_loss: 0.0950, step time: 0.2645\n",
      "244/281, train_loss: 0.0590, step time: 0.2527\n",
      "245/281, train_loss: 0.0740, step time: 0.2583\n",
      "246/281, train_loss: 0.2328, step time: 0.2598\n",
      "247/281, train_loss: 0.0764, step time: 0.2625\n",
      "248/281, train_loss: 0.2084, step time: 0.2583\n",
      "249/281, train_loss: 0.0480, step time: 0.2541\n",
      "250/281, train_loss: 0.0658, step time: 0.2604\n",
      "251/281, train_loss: 0.1150, step time: 0.2556\n",
      "252/281, train_loss: 0.0486, step time: 0.2541\n",
      "253/281, train_loss: 0.0329, step time: 0.2545\n",
      "254/281, train_loss: 0.0475, step time: 0.2752\n",
      "255/281, train_loss: 0.2151, step time: 0.2638\n",
      "256/281, train_loss: 0.0634, step time: 0.2525\n",
      "257/281, train_loss: 0.0520, step time: 0.2815\n",
      "258/281, train_loss: 0.0706, step time: 0.2852\n",
      "259/281, train_loss: 0.2357, step time: 0.2517\n",
      "260/281, train_loss: 0.0816, step time: 0.2508\n",
      "261/281, train_loss: 0.0933, step time: 0.2511\n",
      "262/281, train_loss: 0.0661, step time: 0.2521\n",
      "263/281, train_loss: 0.0699, step time: 0.2558\n",
      "264/281, train_loss: 0.3780, step time: 0.2571\n",
      "265/281, train_loss: 0.0695, step time: 0.2612\n",
      "266/281, train_loss: 0.1030, step time: 0.2560\n",
      "267/281, train_loss: 0.0689, step time: 0.2573\n",
      "268/281, train_loss: 0.0678, step time: 0.2562\n",
      "269/281, train_loss: 0.0706, step time: 0.2568\n",
      "270/281, train_loss: 0.0883, step time: 0.2546\n",
      "271/281, train_loss: 0.0427, step time: 0.2537\n",
      "272/281, train_loss: 0.0839, step time: 0.2536\n",
      "273/281, train_loss: 0.0592, step time: 0.2573\n",
      "274/281, train_loss: 0.0708, step time: 0.2587\n",
      "275/281, train_loss: 0.2193, step time: 0.2576\n",
      "276/281, train_loss: 0.0415, step time: 0.2510\n",
      "277/281, train_loss: 0.0411, step time: 0.2547\n",
      "278/281, train_loss: 0.1071, step time: 0.2567\n",
      "279/281, train_loss: 0.0812, step time: 0.2548\n",
      "280/281, train_loss: 0.2249, step time: 0.2557\n",
      "281/281, train_loss: 0.0584, step time: 0.2527\n",
      "282/281, train_loss: 0.0717, step time: 0.1508\n",
      "epoch 94 average loss: 0.0934\n",
      "current epoch: 94 current mean dice: 0.8945 tc: 0.8825 wt: 0.9251 et: 0.8854\n",
      "best mean dice: 0.8956 at epoch: 89\n",
      "time consuming of epoch 94 is: 340.3435\n",
      "----------\n",
      "epoch 95/200\n",
      "1/281, train_loss: 0.1029, step time: 0.2681\n",
      "2/281, train_loss: 0.0793, step time: 0.2598\n",
      "3/281, train_loss: 0.0636, step time: 0.2571\n",
      "4/281, train_loss: 0.0734, step time: 0.2549\n",
      "5/281, train_loss: 0.0524, step time: 0.2537\n",
      "6/281, train_loss: 0.1093, step time: 0.2573\n",
      "7/281, train_loss: 0.0804, step time: 0.2588\n",
      "8/281, train_loss: 0.0481, step time: 0.2540\n",
      "9/281, train_loss: 0.0836, step time: 0.2538\n",
      "10/281, train_loss: 0.2262, step time: 0.2532\n",
      "11/281, train_loss: 0.0880, step time: 0.2515\n",
      "12/281, train_loss: 0.2327, step time: 0.2575\n",
      "13/281, train_loss: 0.0655, step time: 0.2508\n",
      "14/281, train_loss: 0.0718, step time: 0.2549\n",
      "15/281, train_loss: 0.0593, step time: 0.2517\n",
      "16/281, train_loss: 0.0697, step time: 0.2550\n",
      "17/281, train_loss: 0.2572, step time: 0.2504\n",
      "18/281, train_loss: 0.2272, step time: 0.2582\n",
      "19/281, train_loss: 0.2245, step time: 0.2531\n",
      "20/281, train_loss: 0.0852, step time: 0.2488\n",
      "21/281, train_loss: 0.0604, step time: 0.2533\n",
      "22/281, train_loss: 0.0688, step time: 0.2549\n",
      "23/281, train_loss: 0.1060, step time: 0.2521\n",
      "24/281, train_loss: 0.0644, step time: 0.2485\n",
      "25/281, train_loss: 0.0635, step time: 0.2496\n",
      "26/281, train_loss: 0.0577, step time: 0.2530\n",
      "27/281, train_loss: 0.0456, step time: 0.2508\n",
      "28/281, train_loss: 0.0494, step time: 0.2531\n",
      "29/281, train_loss: 0.0449, step time: 0.2533\n",
      "30/281, train_loss: 0.0549, step time: 0.2513\n",
      "31/281, train_loss: 0.0663, step time: 0.2540\n",
      "32/281, train_loss: 0.0797, step time: 0.2551\n",
      "33/281, train_loss: 0.0690, step time: 0.2567\n",
      "34/281, train_loss: 0.0858, step time: 0.2524\n",
      "35/281, train_loss: 0.2384, step time: 0.2457\n",
      "36/281, train_loss: 0.0608, step time: 0.2502\n",
      "37/281, train_loss: 0.0833, step time: 0.2631\n",
      "38/281, train_loss: 0.2256, step time: 0.2535\n",
      "39/281, train_loss: 0.0856, step time: 0.2776\n",
      "40/281, train_loss: 0.0891, step time: 0.2522\n",
      "41/281, train_loss: 0.0469, step time: 0.2555\n",
      "42/281, train_loss: 0.0301, step time: 0.2570\n",
      "43/281, train_loss: 0.0627, step time: 0.2554\n",
      "44/281, train_loss: 0.0905, step time: 0.2512\n",
      "45/281, train_loss: 0.0574, step time: 0.2477\n",
      "46/281, train_loss: 0.0824, step time: 0.2472\n",
      "47/281, train_loss: 0.0824, step time: 0.2551\n",
      "48/281, train_loss: 0.2601, step time: 0.2546\n",
      "49/281, train_loss: 0.0764, step time: 0.2562\n",
      "50/281, train_loss: 0.2210, step time: 0.2561\n",
      "51/281, train_loss: 0.0619, step time: 0.2606\n",
      "52/281, train_loss: 0.0609, step time: 0.2547\n",
      "53/281, train_loss: 0.0844, step time: 0.2688\n",
      "54/281, train_loss: 0.0572, step time: 0.2531\n",
      "55/281, train_loss: 0.0524, step time: 0.2521\n",
      "56/281, train_loss: 0.0782, step time: 0.2519\n",
      "57/281, train_loss: 0.1029, step time: 0.2573\n",
      "58/281, train_loss: 0.0743, step time: 0.2469\n",
      "59/281, train_loss: 0.0679, step time: 0.2499\n",
      "60/281, train_loss: 0.0726, step time: 0.2490\n",
      "61/281, train_loss: 0.3941, step time: 0.2704\n",
      "62/281, train_loss: 0.0835, step time: 0.2484\n",
      "63/281, train_loss: 0.0810, step time: 0.2542\n",
      "64/281, train_loss: 0.0616, step time: 0.2484\n",
      "65/281, train_loss: 0.0756, step time: 0.2520\n",
      "66/281, train_loss: 0.0830, step time: 0.2523\n",
      "67/281, train_loss: 0.0645, step time: 0.2494\n",
      "68/281, train_loss: 0.0545, step time: 0.2557\n",
      "69/281, train_loss: 0.0891, step time: 0.2575\n",
      "70/281, train_loss: 0.0759, step time: 0.2557\n",
      "71/281, train_loss: 0.2197, step time: 0.2680\n",
      "72/281, train_loss: 0.0774, step time: 0.2512\n",
      "73/281, train_loss: 0.0620, step time: 0.2523\n",
      "74/281, train_loss: 0.2296, step time: 0.2584\n",
      "75/281, train_loss: 0.0794, step time: 0.2573\n",
      "76/281, train_loss: 0.0445, step time: 0.2547\n",
      "77/281, train_loss: 0.0518, step time: 0.2536\n",
      "78/281, train_loss: 0.0730, step time: 0.2497\n",
      "79/281, train_loss: 0.0682, step time: 0.2562\n",
      "80/281, train_loss: 0.0457, step time: 0.2532\n",
      "81/281, train_loss: 0.0560, step time: 0.2556\n",
      "82/281, train_loss: 0.2162, step time: 0.2550\n",
      "83/281, train_loss: 0.0483, step time: 0.2518\n",
      "84/281, train_loss: 0.0657, step time: 0.2513\n",
      "85/281, train_loss: 0.0838, step time: 0.2535\n",
      "86/281, train_loss: 0.0336, step time: 0.2591\n",
      "87/281, train_loss: 0.2420, step time: 0.2588\n",
      "88/281, train_loss: 0.0497, step time: 0.2560\n",
      "89/281, train_loss: 0.1009, step time: 0.2546\n",
      "90/281, train_loss: 0.0490, step time: 0.2539\n",
      "91/281, train_loss: 0.0970, step time: 0.2519\n",
      "92/281, train_loss: 0.0455, step time: 0.2541\n",
      "93/281, train_loss: 0.0397, step time: 0.2526\n",
      "94/281, train_loss: 0.0554, step time: 0.2539\n",
      "95/281, train_loss: 0.0891, step time: 0.2555\n",
      "96/281, train_loss: 0.0777, step time: 0.2551\n",
      "97/281, train_loss: 0.0731, step time: 0.2564\n",
      "98/281, train_loss: 0.0594, step time: 0.2516\n",
      "99/281, train_loss: 0.0456, step time: 0.2474\n",
      "100/281, train_loss: 0.0356, step time: 0.2567\n",
      "101/281, train_loss: 0.0872, step time: 0.2524\n",
      "102/281, train_loss: 0.0407, step time: 0.2498\n",
      "103/281, train_loss: 0.0490, step time: 0.2550\n",
      "104/281, train_loss: 0.0424, step time: 0.2577\n",
      "105/281, train_loss: 0.0607, step time: 0.2552\n",
      "106/281, train_loss: 0.1202, step time: 0.2533\n",
      "107/281, train_loss: 0.0812, step time: 0.2504\n",
      "108/281, train_loss: 0.1053, step time: 0.2554\n",
      "109/281, train_loss: 0.2066, step time: 0.2560\n",
      "110/281, train_loss: 0.2027, step time: 0.2562\n",
      "111/281, train_loss: 0.0716, step time: 0.2490\n",
      "112/281, train_loss: 0.0379, step time: 0.2567\n",
      "113/281, train_loss: 0.0334, step time: 0.2516\n",
      "114/281, train_loss: 0.0524, step time: 0.2549\n",
      "115/281, train_loss: 0.0571, step time: 0.2553\n",
      "116/281, train_loss: 0.0916, step time: 0.2580\n",
      "117/281, train_loss: 0.2186, step time: 0.2573\n",
      "118/281, train_loss: 0.0409, step time: 0.2581\n",
      "119/281, train_loss: 0.1058, step time: 0.2568\n",
      "120/281, train_loss: 0.0537, step time: 0.2502\n",
      "121/281, train_loss: 0.0427, step time: 0.2516\n",
      "122/281, train_loss: 0.2205, step time: 0.2498\n",
      "123/281, train_loss: 0.0553, step time: 0.2492\n",
      "124/281, train_loss: 0.0694, step time: 0.2531\n",
      "125/281, train_loss: 0.0622, step time: 0.2543\n",
      "126/281, train_loss: 0.0571, step time: 0.2654\n",
      "127/281, train_loss: 0.0583, step time: 0.2668\n",
      "128/281, train_loss: 0.0984, step time: 0.2523\n",
      "129/281, train_loss: 0.2206, step time: 0.2514\n",
      "130/281, train_loss: 0.2665, step time: 0.2541\n",
      "131/281, train_loss: 0.2170, step time: 0.2502\n",
      "132/281, train_loss: 0.0906, step time: 0.2570\n",
      "133/281, train_loss: 0.0571, step time: 0.2593\n",
      "134/281, train_loss: 0.0958, step time: 0.2576\n",
      "135/281, train_loss: 0.0670, step time: 0.2516\n",
      "136/281, train_loss: 0.0819, step time: 0.2594\n",
      "137/281, train_loss: 0.0777, step time: 0.2528\n",
      "138/281, train_loss: 0.0619, step time: 0.2643\n",
      "139/281, train_loss: 0.0832, step time: 0.2554\n",
      "140/281, train_loss: 0.0520, step time: 0.2546\n",
      "141/281, train_loss: 0.1116, step time: 0.2615\n",
      "142/281, train_loss: 0.0927, step time: 0.2592\n",
      "143/281, train_loss: 0.0782, step time: 0.2569\n",
      "144/281, train_loss: 0.0583, step time: 0.2565\n",
      "145/281, train_loss: 0.0907, step time: 0.2536\n",
      "146/281, train_loss: 0.0701, step time: 0.2629\n",
      "147/281, train_loss: 0.0757, step time: 0.2571\n",
      "148/281, train_loss: 0.2160, step time: 0.2488\n",
      "149/281, train_loss: 0.0613, step time: 0.2586\n",
      "150/281, train_loss: 0.1000, step time: 0.2537\n",
      "151/281, train_loss: 0.0602, step time: 0.2559\n",
      "152/281, train_loss: 0.2166, step time: 0.2531\n",
      "153/281, train_loss: 0.2095, step time: 0.2536\n",
      "154/281, train_loss: 0.0886, step time: 0.2553\n",
      "155/281, train_loss: 0.0939, step time: 0.2555\n",
      "156/281, train_loss: 0.0730, step time: 0.2615\n",
      "157/281, train_loss: 0.2183, step time: 0.2558\n",
      "158/281, train_loss: 0.0621, step time: 0.2545\n",
      "159/281, train_loss: 0.2128, step time: 0.2546\n",
      "160/281, train_loss: 0.0800, step time: 0.2465\n",
      "161/281, train_loss: 0.0643, step time: 0.2513\n",
      "162/281, train_loss: 0.0435, step time: 0.2481\n",
      "163/281, train_loss: 0.2520, step time: 0.2499\n",
      "164/281, train_loss: 0.0525, step time: 0.2536\n",
      "165/281, train_loss: 0.0731, step time: 0.2508\n",
      "166/281, train_loss: 0.0714, step time: 0.2551\n",
      "167/281, train_loss: 0.0468, step time: 0.2522\n",
      "168/281, train_loss: 0.2294, step time: 0.2546\n",
      "169/281, train_loss: 0.0912, step time: 0.2569\n",
      "170/281, train_loss: 0.0477, step time: 0.2573\n",
      "171/281, train_loss: 0.2314, step time: 0.2584\n",
      "172/281, train_loss: 0.2280, step time: 0.2531\n",
      "173/281, train_loss: 0.0599, step time: 0.2548\n",
      "174/281, train_loss: 0.0899, step time: 0.2480\n",
      "175/281, train_loss: 0.0685, step time: 0.2508\n",
      "176/281, train_loss: 0.0483, step time: 0.2494\n",
      "177/281, train_loss: 0.0519, step time: 0.2470\n",
      "178/281, train_loss: 0.0714, step time: 0.2511\n",
      "179/281, train_loss: 0.0779, step time: 0.2495\n",
      "180/281, train_loss: 0.0512, step time: 0.2559\n",
      "181/281, train_loss: 0.0738, step time: 0.2516\n",
      "182/281, train_loss: 0.2335, step time: 0.2502\n",
      "183/281, train_loss: 0.0631, step time: 0.2465\n",
      "184/281, train_loss: 0.0517, step time: 0.2502\n",
      "185/281, train_loss: 0.0811, step time: 0.2488\n",
      "186/281, train_loss: 0.0712, step time: 0.2548\n",
      "187/281, train_loss: 0.0573, step time: 0.2565\n",
      "188/281, train_loss: 0.0645, step time: 0.2489\n",
      "189/281, train_loss: 0.2312, step time: 0.2436\n",
      "190/281, train_loss: 0.0510, step time: 0.2454\n",
      "191/281, train_loss: 0.0839, step time: 0.2461\n",
      "192/281, train_loss: 0.0704, step time: 0.2508\n",
      "193/281, train_loss: 0.0512, step time: 0.2512\n",
      "194/281, train_loss: 0.1044, step time: 0.2492\n",
      "195/281, train_loss: 0.0550, step time: 0.2507\n",
      "196/281, train_loss: 0.0597, step time: 0.2486\n",
      "197/281, train_loss: 0.0871, step time: 0.2495\n",
      "198/281, train_loss: 0.0596, step time: 0.2545\n",
      "199/281, train_loss: 0.0624, step time: 0.2506\n",
      "200/281, train_loss: 0.2161, step time: 0.2490\n",
      "201/281, train_loss: 0.0770, step time: 0.2465\n",
      "202/281, train_loss: 0.0612, step time: 0.2443\n",
      "203/281, train_loss: 0.0866, step time: 0.2462\n",
      "204/281, train_loss: 0.0535, step time: 0.2538\n",
      "205/281, train_loss: 0.0381, step time: 0.2539\n",
      "206/281, train_loss: 0.0565, step time: 0.2492\n",
      "207/281, train_loss: 0.0380, step time: 0.2457\n",
      "208/281, train_loss: 0.0680, step time: 0.2544\n",
      "209/281, train_loss: 0.0587, step time: 0.2547\n",
      "210/281, train_loss: 0.0694, step time: 0.2472\n",
      "211/281, train_loss: 0.0846, step time: 0.2449\n",
      "212/281, train_loss: 0.0910, step time: 0.2546\n",
      "213/281, train_loss: 0.0760, step time: 0.2524\n",
      "214/281, train_loss: 0.0865, step time: 0.2500\n",
      "215/281, train_loss: 0.0519, step time: 0.2473\n",
      "216/281, train_loss: 0.0667, step time: 0.2523\n",
      "217/281, train_loss: 0.2171, step time: 0.2473\n",
      "218/281, train_loss: 0.1075, step time: 0.2514\n",
      "219/281, train_loss: 0.0477, step time: 0.2531\n",
      "220/281, train_loss: 0.0436, step time: 0.2505\n",
      "221/281, train_loss: 0.0484, step time: 0.2530\n",
      "222/281, train_loss: 0.0759, step time: 0.2539\n",
      "223/281, train_loss: 0.0900, step time: 0.2566\n",
      "224/281, train_loss: 0.1115, step time: 0.2511\n",
      "225/281, train_loss: 0.1044, step time: 0.2507\n",
      "226/281, train_loss: 0.3810, step time: 0.2548\n",
      "227/281, train_loss: 0.0559, step time: 0.2470\n",
      "228/281, train_loss: 0.0721, step time: 0.2525\n",
      "229/281, train_loss: 0.0551, step time: 0.2463\n",
      "230/281, train_loss: 0.0962, step time: 0.2452\n",
      "231/281, train_loss: 0.0579, step time: 0.2506\n",
      "232/281, train_loss: 0.0934, step time: 0.2512\n",
      "233/281, train_loss: 0.0458, step time: 0.2460\n",
      "234/281, train_loss: 0.0956, step time: 0.2477\n",
      "235/281, train_loss: 0.0701, step time: 0.2505\n",
      "236/281, train_loss: 0.0527, step time: 0.2489\n",
      "237/281, train_loss: 0.0506, step time: 0.2566\n",
      "238/281, train_loss: 0.0765, step time: 0.2453\n",
      "239/281, train_loss: 0.1120, step time: 0.2455\n",
      "240/281, train_loss: 0.2164, step time: 0.2521\n",
      "241/281, train_loss: 0.0989, step time: 0.2506\n",
      "242/281, train_loss: 0.0542, step time: 0.2495\n",
      "243/281, train_loss: 0.0780, step time: 0.2497\n",
      "244/281, train_loss: 0.0741, step time: 0.2515\n",
      "245/281, train_loss: 0.0981, step time: 0.2546\n",
      "246/281, train_loss: 0.0730, step time: 0.2518\n",
      "247/281, train_loss: 0.0765, step time: 0.2515\n",
      "248/281, train_loss: 0.0557, step time: 0.2515\n",
      "249/281, train_loss: 0.0938, step time: 0.2520\n",
      "250/281, train_loss: 0.0486, step time: 0.2509\n",
      "251/281, train_loss: 0.0557, step time: 0.2502\n",
      "252/281, train_loss: 0.2142, step time: 0.2553\n",
      "253/281, train_loss: 0.0896, step time: 0.2476\n",
      "254/281, train_loss: 0.0474, step time: 0.2641\n",
      "255/281, train_loss: 0.0534, step time: 0.2539\n",
      "256/281, train_loss: 0.0981, step time: 0.2538\n",
      "257/281, train_loss: 0.0428, step time: 0.2493\n",
      "258/281, train_loss: 0.2015, step time: 0.2520\n",
      "259/281, train_loss: 0.0452, step time: 0.2523\n",
      "260/281, train_loss: 0.0402, step time: 0.2548\n",
      "261/281, train_loss: 0.0592, step time: 0.2546\n",
      "262/281, train_loss: 0.0720, step time: 0.2496\n",
      "263/281, train_loss: 0.0673, step time: 0.2545\n",
      "264/281, train_loss: 0.0704, step time: 0.2512\n",
      "265/281, train_loss: 0.0542, step time: 0.2489\n",
      "266/281, train_loss: 0.0581, step time: 0.2458\n",
      "267/281, train_loss: 0.0728, step time: 0.2556\n",
      "268/281, train_loss: 0.2466, step time: 0.2578\n",
      "269/281, train_loss: 0.0427, step time: 0.2564\n",
      "270/281, train_loss: 0.0751, step time: 0.2554\n",
      "271/281, train_loss: 0.2171, step time: 0.2502\n",
      "272/281, train_loss: 0.0808, step time: 0.2493\n",
      "273/281, train_loss: 0.0603, step time: 0.2507\n",
      "274/281, train_loss: 0.0554, step time: 0.2474\n",
      "275/281, train_loss: 0.0827, step time: 0.2506\n",
      "276/281, train_loss: 0.2329, step time: 0.2489\n",
      "277/281, train_loss: 0.0760, step time: 0.2500\n",
      "278/281, train_loss: 0.0581, step time: 0.2470\n",
      "279/281, train_loss: 0.0958, step time: 0.2492\n",
      "280/281, train_loss: 0.0680, step time: 0.2443\n",
      "281/281, train_loss: 0.1022, step time: 0.2447\n",
      "282/281, train_loss: 0.0732, step time: 0.1463\n",
      "epoch 95 average loss: 0.0931\n",
      "saved new best metric model\n",
      "current epoch: 95 current mean dice: 0.8960 tc: 0.8864 wt: 0.9255 et: 0.8856\n",
      "best mean dice: 0.8960 at epoch: 95\n",
      "time consuming of epoch 95 is: 370.0820\n",
      "----------\n",
      "epoch 96/200\n",
      "1/281, train_loss: 0.0470, step time: 0.2530\n",
      "2/281, train_loss: 0.0627, step time: 0.2580\n",
      "3/281, train_loss: 0.0694, step time: 0.2522\n",
      "4/281, train_loss: 0.2050, step time: 0.2457\n",
      "5/281, train_loss: 0.0713, step time: 0.2524\n",
      "6/281, train_loss: 0.0846, step time: 0.2505\n",
      "7/281, train_loss: 0.0670, step time: 0.2513\n",
      "8/281, train_loss: 0.0830, step time: 0.2482\n",
      "9/281, train_loss: 0.0465, step time: 0.2438\n",
      "10/281, train_loss: 0.0733, step time: 0.2612\n",
      "11/281, train_loss: 0.0413, step time: 0.2501\n",
      "12/281, train_loss: 0.0422, step time: 0.2543\n",
      "13/281, train_loss: 0.1058, step time: 0.2466\n",
      "14/281, train_loss: 0.0415, step time: 0.2590\n",
      "15/281, train_loss: 0.0393, step time: 0.2510\n",
      "16/281, train_loss: 0.0609, step time: 0.2460\n",
      "17/281, train_loss: 0.0658, step time: 0.2498\n",
      "18/281, train_loss: 0.1100, step time: 0.2555\n",
      "19/281, train_loss: 0.0606, step time: 0.2539\n",
      "20/281, train_loss: 0.0471, step time: 0.2522\n",
      "21/281, train_loss: 0.0787, step time: 0.2504\n",
      "22/281, train_loss: 0.0677, step time: 0.2609\n",
      "23/281, train_loss: 0.0337, step time: 0.2591\n",
      "24/281, train_loss: 0.1105, step time: 0.2550\n",
      "25/281, train_loss: 0.0690, step time: 0.2572\n",
      "26/281, train_loss: 0.0652, step time: 0.2547\n",
      "27/281, train_loss: 0.0647, step time: 0.2490\n",
      "28/281, train_loss: 0.0963, step time: 0.2538\n",
      "29/281, train_loss: 0.0865, step time: 0.2511\n",
      "30/281, train_loss: 0.0656, step time: 0.2481\n",
      "31/281, train_loss: 0.1004, step time: 0.2569\n",
      "32/281, train_loss: 0.0731, step time: 0.2523\n",
      "33/281, train_loss: 0.2293, step time: 0.2514\n",
      "34/281, train_loss: 0.0540, step time: 0.2535\n",
      "35/281, train_loss: 0.0769, step time: 0.2500\n",
      "36/281, train_loss: 0.0675, step time: 0.2514\n",
      "37/281, train_loss: 0.0705, step time: 0.2452\n",
      "38/281, train_loss: 0.0420, step time: 0.2498\n",
      "39/281, train_loss: 0.0839, step time: 0.2544\n",
      "40/281, train_loss: 0.2180, step time: 0.2572\n",
      "41/281, train_loss: 0.0606, step time: 0.2563\n",
      "42/281, train_loss: 0.0669, step time: 0.2500\n",
      "43/281, train_loss: 0.0686, step time: 0.2507\n",
      "44/281, train_loss: 0.0532, step time: 0.2524\n",
      "45/281, train_loss: 0.0777, step time: 0.2535\n",
      "46/281, train_loss: 0.0457, step time: 0.2558\n",
      "47/281, train_loss: 0.0554, step time: 0.2500\n",
      "48/281, train_loss: 0.0586, step time: 0.2579\n",
      "49/281, train_loss: 0.0692, step time: 0.2532\n",
      "50/281, train_loss: 0.0691, step time: 0.2431\n",
      "51/281, train_loss: 0.2233, step time: 0.2474\n",
      "52/281, train_loss: 0.0557, step time: 0.2457\n",
      "53/281, train_loss: 0.0788, step time: 0.2489\n",
      "54/281, train_loss: 0.0641, step time: 0.2515\n",
      "55/281, train_loss: 0.0714, step time: 0.2535\n",
      "56/281, train_loss: 0.0870, step time: 0.2560\n",
      "57/281, train_loss: 0.0512, step time: 0.2589\n",
      "58/281, train_loss: 0.0440, step time: 0.2560\n",
      "59/281, train_loss: 0.0498, step time: 0.2588\n",
      "60/281, train_loss: 0.0574, step time: 0.2542\n",
      "61/281, train_loss: 0.2147, step time: 0.2501\n",
      "62/281, train_loss: 0.0802, step time: 0.2532\n",
      "63/281, train_loss: 0.0523, step time: 0.2493\n",
      "64/281, train_loss: 0.2019, step time: 0.2495\n",
      "65/281, train_loss: 0.3904, step time: 0.2494\n",
      "66/281, train_loss: 0.2153, step time: 0.2496\n",
      "67/281, train_loss: 0.0628, step time: 0.2539\n",
      "68/281, train_loss: 0.0771, step time: 0.2565\n",
      "69/281, train_loss: 0.2255, step time: 0.2539\n",
      "70/281, train_loss: 0.0789, step time: 0.2514\n",
      "71/281, train_loss: 0.0683, step time: 0.2498\n",
      "72/281, train_loss: 0.0472, step time: 0.2538\n",
      "73/281, train_loss: 0.0445, step time: 0.2487\n",
      "74/281, train_loss: 0.0535, step time: 0.2485\n",
      "75/281, train_loss: 0.0614, step time: 0.2550\n",
      "76/281, train_loss: 0.0864, step time: 0.2536\n",
      "77/281, train_loss: 0.0461, step time: 0.2560\n",
      "78/281, train_loss: 0.2130, step time: 0.2604\n",
      "79/281, train_loss: 0.0613, step time: 0.2552\n",
      "80/281, train_loss: 0.0476, step time: 0.2569\n",
      "81/281, train_loss: 0.0545, step time: 0.2523\n",
      "82/281, train_loss: 0.0743, step time: 0.2547\n",
      "83/281, train_loss: 0.0584, step time: 0.2571\n",
      "84/281, train_loss: 0.2296, step time: 0.2537\n",
      "85/281, train_loss: 0.0517, step time: 0.2579\n",
      "86/281, train_loss: 0.0869, step time: 0.2548\n",
      "87/281, train_loss: 0.0612, step time: 0.2618\n",
      "88/281, train_loss: 0.0392, step time: 0.2554\n",
      "89/281, train_loss: 0.0861, step time: 0.2526\n",
      "90/281, train_loss: 0.0659, step time: 0.2534\n",
      "91/281, train_loss: 0.2275, step time: 0.2573\n",
      "92/281, train_loss: 0.0558, step time: 0.2517\n",
      "93/281, train_loss: 0.0636, step time: 0.2574\n",
      "94/281, train_loss: 0.0794, step time: 0.2557\n",
      "95/281, train_loss: 0.0619, step time: 0.2561\n",
      "96/281, train_loss: 0.0567, step time: 0.2532\n",
      "97/281, train_loss: 0.0470, step time: 0.2533\n",
      "98/281, train_loss: 0.2477, step time: 0.2522\n",
      "99/281, train_loss: 0.0933, step time: 0.2535\n",
      "100/281, train_loss: 0.0539, step time: 0.2561\n",
      "101/281, train_loss: 0.0723, step time: 0.2547\n",
      "102/281, train_loss: 0.0782, step time: 0.2653\n",
      "103/281, train_loss: 0.0756, step time: 0.2545\n",
      "104/281, train_loss: 0.0526, step time: 0.2761\n",
      "105/281, train_loss: 0.0584, step time: 0.2587\n",
      "106/281, train_loss: 0.0641, step time: 0.2567\n",
      "107/281, train_loss: 0.2115, step time: 0.2561\n",
      "108/281, train_loss: 0.2435, step time: 0.2558\n",
      "109/281, train_loss: 0.0656, step time: 0.2523\n",
      "110/281, train_loss: 0.0495, step time: 0.2539\n",
      "111/281, train_loss: 0.0453, step time: 0.2551\n",
      "112/281, train_loss: 0.0747, step time: 0.2577\n",
      "113/281, train_loss: 0.0808, step time: 0.2539\n",
      "114/281, train_loss: 0.0767, step time: 0.2543\n",
      "115/281, train_loss: 0.0556, step time: 0.2574\n",
      "116/281, train_loss: 0.0513, step time: 0.2572\n",
      "117/281, train_loss: 0.0708, step time: 0.2559\n",
      "118/281, train_loss: 0.0826, step time: 0.2523\n",
      "119/281, train_loss: 0.0980, step time: 0.2591\n",
      "120/281, train_loss: 0.0576, step time: 0.2574\n",
      "121/281, train_loss: 0.0719, step time: 0.2550\n",
      "122/281, train_loss: 0.0812, step time: 0.2577\n",
      "123/281, train_loss: 0.0597, step time: 0.2574\n",
      "124/281, train_loss: 0.0527, step time: 0.2557\n",
      "125/281, train_loss: 0.0785, step time: 0.2582\n",
      "126/281, train_loss: 0.0753, step time: 0.2584\n",
      "127/281, train_loss: 0.0551, step time: 0.2545\n",
      "128/281, train_loss: 0.0472, step time: 0.2532\n",
      "129/281, train_loss: 0.0645, step time: 0.2559\n",
      "130/281, train_loss: 0.0642, step time: 0.2594\n",
      "131/281, train_loss: 0.1053, step time: 0.2608\n",
      "132/281, train_loss: 0.0774, step time: 0.2569\n",
      "133/281, train_loss: 0.0436, step time: 0.2588\n",
      "134/281, train_loss: 0.2592, step time: 0.2572\n",
      "135/281, train_loss: 0.2306, step time: 0.2579\n",
      "136/281, train_loss: 0.0989, step time: 0.2595\n",
      "137/281, train_loss: 0.0803, step time: 0.2585\n",
      "138/281, train_loss: 0.0744, step time: 0.2585\n",
      "139/281, train_loss: 0.0566, step time: 0.2548\n",
      "140/281, train_loss: 0.3886, step time: 0.2584\n",
      "141/281, train_loss: 0.2150, step time: 0.2585\n",
      "142/281, train_loss: 0.0616, step time: 0.2557\n",
      "143/281, train_loss: 0.0750, step time: 0.2587\n",
      "144/281, train_loss: 0.0673, step time: 0.2592\n",
      "145/281, train_loss: 0.0676, step time: 0.2556\n",
      "146/281, train_loss: 0.0623, step time: 0.2551\n",
      "147/281, train_loss: 0.1042, step time: 0.2535\n",
      "148/281, train_loss: 0.0578, step time: 0.2598\n",
      "149/281, train_loss: 0.0452, step time: 0.2589\n",
      "150/281, train_loss: 0.0608, step time: 0.2564\n",
      "151/281, train_loss: 0.0988, step time: 0.2554\n",
      "152/281, train_loss: 0.0775, step time: 0.2621\n",
      "153/281, train_loss: 0.0530, step time: 0.2545\n",
      "154/281, train_loss: 0.0804, step time: 0.2585\n",
      "155/281, train_loss: 0.0458, step time: 0.2544\n",
      "156/281, train_loss: 0.0970, step time: 0.2635\n",
      "157/281, train_loss: 0.0607, step time: 0.2573\n",
      "158/281, train_loss: 0.1093, step time: 0.2547\n",
      "159/281, train_loss: 0.0523, step time: 0.2512\n",
      "160/281, train_loss: 0.0691, step time: 0.2540\n",
      "161/281, train_loss: 0.0816, step time: 0.2592\n",
      "162/281, train_loss: 0.0620, step time: 0.2600\n",
      "163/281, train_loss: 0.0770, step time: 0.2509\n",
      "164/281, train_loss: 0.0388, step time: 0.2550\n",
      "165/281, train_loss: 0.0881, step time: 0.2589\n",
      "166/281, train_loss: 0.0540, step time: 0.2594\n",
      "167/281, train_loss: 0.2462, step time: 0.2555\n",
      "168/281, train_loss: 0.0645, step time: 0.2598\n",
      "169/281, train_loss: 0.0639, step time: 0.2534\n",
      "170/281, train_loss: 0.0742, step time: 0.2517\n",
      "171/281, train_loss: 0.2196, step time: 0.2517\n",
      "172/281, train_loss: 0.0783, step time: 0.2584\n",
      "173/281, train_loss: 0.0751, step time: 0.2587\n",
      "174/281, train_loss: 0.1138, step time: 0.2596\n",
      "175/281, train_loss: 0.0691, step time: 0.2575\n",
      "176/281, train_loss: 0.0728, step time: 0.2569\n",
      "177/281, train_loss: 0.0819, step time: 0.2561\n",
      "178/281, train_loss: 0.2480, step time: 0.2585\n",
      "179/281, train_loss: 0.0634, step time: 0.2580\n",
      "180/281, train_loss: 0.0921, step time: 0.2609\n",
      "181/281, train_loss: 0.0540, step time: 0.2557\n",
      "182/281, train_loss: 0.0535, step time: 0.2581\n",
      "183/281, train_loss: 0.2232, step time: 0.2589\n",
      "184/281, train_loss: 0.0571, step time: 0.2533\n",
      "185/281, train_loss: 0.0595, step time: 0.2543\n",
      "186/281, train_loss: 0.1015, step time: 0.2549\n",
      "187/281, train_loss: 0.0870, step time: 0.2574\n",
      "188/281, train_loss: 0.0747, step time: 0.2601\n",
      "189/281, train_loss: 0.0691, step time: 0.2592\n",
      "190/281, train_loss: 0.0774, step time: 0.2565\n",
      "191/281, train_loss: 0.0414, step time: 0.2506\n",
      "192/281, train_loss: 0.0657, step time: 0.2626\n",
      "193/281, train_loss: 0.0476, step time: 0.2539\n",
      "194/281, train_loss: 0.0422, step time: 0.2555\n",
      "195/281, train_loss: 0.0692, step time: 0.2587\n",
      "196/281, train_loss: 0.2335, step time: 0.2605\n",
      "197/281, train_loss: 0.1099, step time: 0.2550\n",
      "198/281, train_loss: 0.2127, step time: 0.2505\n",
      "199/281, train_loss: 0.0669, step time: 0.2584\n",
      "200/281, train_loss: 0.0592, step time: 0.2588\n",
      "201/281, train_loss: 0.0943, step time: 0.2533\n",
      "202/281, train_loss: 0.0645, step time: 0.2545\n",
      "203/281, train_loss: 0.2209, step time: 0.2605\n",
      "204/281, train_loss: 0.0745, step time: 0.2579\n",
      "205/281, train_loss: 0.3803, step time: 0.2497\n",
      "206/281, train_loss: 0.0556, step time: 0.2505\n",
      "207/281, train_loss: 0.0661, step time: 0.2600\n",
      "208/281, train_loss: 0.2402, step time: 0.2622\n",
      "209/281, train_loss: 0.0883, step time: 0.2576\n",
      "210/281, train_loss: 0.2337, step time: 0.2578\n",
      "211/281, train_loss: 0.0619, step time: 0.2554\n",
      "212/281, train_loss: 0.0732, step time: 0.2537\n",
      "213/281, train_loss: 0.0722, step time: 0.2591\n",
      "214/281, train_loss: 0.0647, step time: 0.2579\n",
      "215/281, train_loss: 0.0914, step time: 0.2558\n",
      "216/281, train_loss: 0.0573, step time: 0.2542\n",
      "217/281, train_loss: 0.2440, step time: 0.2558\n",
      "218/281, train_loss: 0.0879, step time: 0.2516\n",
      "219/281, train_loss: 0.2410, step time: 0.2510\n",
      "220/281, train_loss: 0.0472, step time: 0.2548\n",
      "221/281, train_loss: 0.0627, step time: 0.2550\n",
      "222/281, train_loss: 0.1224, step time: 0.2568\n",
      "223/281, train_loss: 0.0778, step time: 0.2567\n",
      "224/281, train_loss: 0.0558, step time: 0.2583\n",
      "225/281, train_loss: 0.0361, step time: 0.2581\n",
      "226/281, train_loss: 0.0572, step time: 0.2582\n",
      "227/281, train_loss: 0.2030, step time: 0.2542\n",
      "228/281, train_loss: 0.2322, step time: 0.2504\n",
      "229/281, train_loss: 0.0375, step time: 0.2542\n",
      "230/281, train_loss: 0.0821, step time: 0.2530\n",
      "231/281, train_loss: 0.2299, step time: 0.2552\n",
      "232/281, train_loss: 0.0869, step time: 0.2556\n",
      "233/281, train_loss: 0.0558, step time: 0.2487\n",
      "234/281, train_loss: 0.0674, step time: 0.2544\n",
      "235/281, train_loss: 0.0537, step time: 0.2542\n",
      "236/281, train_loss: 0.0685, step time: 0.2520\n",
      "237/281, train_loss: 0.0447, step time: 0.2538\n",
      "238/281, train_loss: 0.0659, step time: 0.2540\n",
      "239/281, train_loss: 0.0667, step time: 0.2534\n",
      "240/281, train_loss: 0.0441, step time: 0.2557\n",
      "241/281, train_loss: 0.1054, step time: 0.2566\n",
      "242/281, train_loss: 0.0637, step time: 0.2564\n",
      "243/281, train_loss: 0.0480, step time: 0.2572\n",
      "244/281, train_loss: 0.2479, step time: 0.2522\n",
      "245/281, train_loss: 0.0360, step time: 0.2601\n",
      "246/281, train_loss: 0.2135, step time: 0.2543\n",
      "247/281, train_loss: 0.0612, step time: 0.2590\n",
      "248/281, train_loss: 0.0846, step time: 0.2569\n",
      "249/281, train_loss: 0.0577, step time: 0.2549\n",
      "250/281, train_loss: 0.0708, step time: 0.2529\n",
      "251/281, train_loss: 0.0480, step time: 0.2515\n",
      "252/281, train_loss: 0.2374, step time: 0.2515\n",
      "253/281, train_loss: 0.0771, step time: 0.2567\n",
      "254/281, train_loss: 0.0776, step time: 0.2500\n",
      "255/281, train_loss: 0.2495, step time: 0.2535\n",
      "256/281, train_loss: 0.2679, step time: 0.2540\n",
      "257/281, train_loss: 0.0839, step time: 0.2467\n",
      "258/281, train_loss: 0.0792, step time: 0.2481\n",
      "259/281, train_loss: 0.0897, step time: 0.2464\n",
      "260/281, train_loss: 0.0685, step time: 0.2508\n",
      "261/281, train_loss: 0.0482, step time: 0.2520\n",
      "262/281, train_loss: 0.0607, step time: 0.2551\n",
      "263/281, train_loss: 0.0848, step time: 0.2526\n",
      "264/281, train_loss: 0.0637, step time: 0.2508\n",
      "265/281, train_loss: 0.0518, step time: 0.2528\n",
      "266/281, train_loss: 0.2182, step time: 0.2512\n",
      "267/281, train_loss: 0.0706, step time: 0.2566\n",
      "268/281, train_loss: 0.0667, step time: 0.2589\n",
      "269/281, train_loss: 0.0547, step time: 0.2569\n",
      "270/281, train_loss: 0.0802, step time: 0.2550\n",
      "271/281, train_loss: 0.0874, step time: 0.2580\n",
      "272/281, train_loss: 0.0948, step time: 0.2561\n",
      "273/281, train_loss: 0.0523, step time: 0.2569\n",
      "274/281, train_loss: 0.0820, step time: 0.2571\n",
      "275/281, train_loss: 0.0672, step time: 0.2582\n",
      "276/281, train_loss: 0.0481, step time: 0.2571\n",
      "277/281, train_loss: 0.0381, step time: 0.2598\n",
      "278/281, train_loss: 0.0657, step time: 0.2582\n",
      "279/281, train_loss: 0.0864, step time: 0.2589\n",
      "280/281, train_loss: 0.0538, step time: 0.2543\n",
      "281/281, train_loss: 0.0584, step time: 0.2573\n",
      "282/281, train_loss: 0.0829, step time: 0.1555\n",
      "epoch 96 average loss: 0.0921\n",
      "saved new best metric model\n",
      "current epoch: 96 current mean dice: 0.8962 tc: 0.8896 wt: 0.9237 et: 0.8837\n",
      "best mean dice: 0.8962 at epoch: 96\n",
      "time consuming of epoch 96 is: 402.9384\n",
      "----------\n",
      "epoch 97/200\n",
      "1/281, train_loss: 0.0863, step time: 0.2669\n",
      "2/281, train_loss: 0.0484, step time: 0.2539\n",
      "3/281, train_loss: 0.2308, step time: 0.2514\n",
      "4/281, train_loss: 0.0627, step time: 0.2467\n",
      "5/281, train_loss: 0.0791, step time: 0.2485\n",
      "6/281, train_loss: 0.0444, step time: 0.2467\n",
      "7/281, train_loss: 0.0656, step time: 0.2519\n",
      "8/281, train_loss: 0.0480, step time: 0.2544\n",
      "9/281, train_loss: 0.2185, step time: 0.2809\n",
      "10/281, train_loss: 0.0555, step time: 0.2569\n",
      "11/281, train_loss: 0.0643, step time: 0.2555\n",
      "12/281, train_loss: 0.0765, step time: 0.2504\n",
      "13/281, train_loss: 0.2353, step time: 0.2575\n",
      "14/281, train_loss: 0.0792, step time: 0.2483\n",
      "15/281, train_loss: 0.0693, step time: 0.2535\n",
      "16/281, train_loss: 0.0456, step time: 0.2750\n",
      "17/281, train_loss: 0.0427, step time: 0.2895\n",
      "18/281, train_loss: 0.0523, step time: 0.2733\n",
      "19/281, train_loss: 0.2456, step time: 0.2483\n",
      "20/281, train_loss: 0.0599, step time: 0.2485\n",
      "21/281, train_loss: 0.0544, step time: 0.2536\n",
      "22/281, train_loss: 0.0499, step time: 0.2562\n",
      "23/281, train_loss: 0.0792, step time: 0.2603\n",
      "24/281, train_loss: 0.1024, step time: 0.2619\n",
      "25/281, train_loss: 0.0575, step time: 0.2556\n",
      "26/281, train_loss: 0.0780, step time: 0.2529\n",
      "27/281, train_loss: 0.0607, step time: 0.2558\n",
      "28/281, train_loss: 0.2708, step time: 0.2608\n",
      "29/281, train_loss: 0.0714, step time: 0.2534\n",
      "30/281, train_loss: 0.0633, step time: 0.2681\n",
      "31/281, train_loss: 0.2303, step time: 0.2607\n",
      "32/281, train_loss: 0.0652, step time: 0.2567\n",
      "33/281, train_loss: 0.0730, step time: 0.2601\n",
      "34/281, train_loss: 0.0672, step time: 0.2507\n",
      "35/281, train_loss: 0.0623, step time: 0.2567\n",
      "36/281, train_loss: 0.0515, step time: 0.2549\n",
      "37/281, train_loss: 0.0734, step time: 0.2604\n",
      "38/281, train_loss: 0.0539, step time: 0.2595\n",
      "39/281, train_loss: 0.0465, step time: 0.2549\n",
      "40/281, train_loss: 0.0849, step time: 0.2590\n",
      "41/281, train_loss: 0.0623, step time: 0.2596\n",
      "42/281, train_loss: 0.0883, step time: 0.2687\n",
      "43/281, train_loss: 0.0554, step time: 0.2556\n",
      "44/281, train_loss: 0.0778, step time: 0.2559\n",
      "45/281, train_loss: 0.2243, step time: 0.2549\n",
      "46/281, train_loss: 0.0875, step time: 0.2529\n",
      "47/281, train_loss: 0.2404, step time: 0.2498\n",
      "48/281, train_loss: 0.0734, step time: 0.2496\n",
      "49/281, train_loss: 0.0641, step time: 0.2521\n",
      "50/281, train_loss: 0.0618, step time: 0.2540\n",
      "51/281, train_loss: 0.0869, step time: 0.2491\n",
      "52/281, train_loss: 0.0482, step time: 0.2480\n",
      "53/281, train_loss: 0.0573, step time: 0.2474\n",
      "54/281, train_loss: 0.0555, step time: 0.2451\n",
      "55/281, train_loss: 0.2237, step time: 0.2481\n",
      "56/281, train_loss: 0.0849, step time: 0.2502\n",
      "57/281, train_loss: 0.0589, step time: 0.2524\n",
      "58/281, train_loss: 0.0697, step time: 0.2540\n",
      "59/281, train_loss: 0.2391, step time: 0.2553\n",
      "60/281, train_loss: 0.0629, step time: 0.2530\n",
      "61/281, train_loss: 0.0618, step time: 0.2518\n",
      "62/281, train_loss: 0.0493, step time: 0.2507\n",
      "63/281, train_loss: 0.0836, step time: 0.2527\n",
      "64/281, train_loss: 0.2207, step time: 0.2511\n",
      "65/281, train_loss: 0.0531, step time: 0.2519\n",
      "66/281, train_loss: 0.0891, step time: 0.2556\n",
      "67/281, train_loss: 0.0687, step time: 0.2517\n",
      "68/281, train_loss: 0.0789, step time: 0.2543\n",
      "69/281, train_loss: 0.0530, step time: 0.2546\n",
      "70/281, train_loss: 0.0785, step time: 0.2565\n",
      "71/281, train_loss: 0.0892, step time: 0.2535\n",
      "72/281, train_loss: 0.0703, step time: 0.2505\n",
      "73/281, train_loss: 0.2369, step time: 0.2516\n",
      "74/281, train_loss: 0.0756, step time: 0.2529\n",
      "75/281, train_loss: 0.0805, step time: 0.2525\n",
      "76/281, train_loss: 0.0563, step time: 0.2560\n",
      "77/281, train_loss: 0.0350, step time: 0.2527\n",
      "78/281, train_loss: 0.0597, step time: 0.2594\n",
      "79/281, train_loss: 0.0684, step time: 0.2597\n",
      "80/281, train_loss: 0.2438, step time: 0.2548\n",
      "81/281, train_loss: 0.0685, step time: 0.2508\n",
      "82/281, train_loss: 0.0688, step time: 0.2671\n",
      "83/281, train_loss: 0.0785, step time: 0.2524\n",
      "84/281, train_loss: 0.0993, step time: 0.2526\n",
      "85/281, train_loss: 0.0493, step time: 0.2573\n",
      "86/281, train_loss: 0.0522, step time: 0.2500\n",
      "87/281, train_loss: 0.2216, step time: 0.2522\n",
      "88/281, train_loss: 0.0990, step time: 0.2485\n",
      "89/281, train_loss: 0.0566, step time: 0.2556\n",
      "90/281, train_loss: 0.0606, step time: 0.2596\n",
      "91/281, train_loss: 0.0620, step time: 0.2575\n",
      "92/281, train_loss: 0.0811, step time: 0.2595\n",
      "93/281, train_loss: 0.0632, step time: 0.2536\n",
      "94/281, train_loss: 0.0807, step time: 0.2533\n",
      "95/281, train_loss: 0.0600, step time: 0.2513\n",
      "96/281, train_loss: 0.2398, step time: 0.2516\n",
      "97/281, train_loss: 0.0900, step time: 0.2616\n",
      "98/281, train_loss: 0.0672, step time: 0.2593\n",
      "99/281, train_loss: 0.0855, step time: 0.2533\n",
      "100/281, train_loss: 0.0949, step time: 0.2497\n",
      "101/281, train_loss: 0.0612, step time: 0.2586\n",
      "102/281, train_loss: 0.0634, step time: 0.2537\n",
      "103/281, train_loss: 0.0571, step time: 0.2580\n",
      "104/281, train_loss: 0.0481, step time: 0.2657\n",
      "105/281, train_loss: 0.1020, step time: 0.2553\n",
      "106/281, train_loss: 0.0412, step time: 0.2496\n",
      "107/281, train_loss: 0.0612, step time: 0.2530\n",
      "108/281, train_loss: 0.0890, step time: 0.2535\n",
      "109/281, train_loss: 0.0736, step time: 0.2512\n",
      "110/281, train_loss: 0.0523, step time: 0.2509\n",
      "111/281, train_loss: 0.0876, step time: 0.2618\n",
      "112/281, train_loss: 0.0728, step time: 0.2479\n",
      "113/281, train_loss: 0.0327, step time: 0.2496\n",
      "114/281, train_loss: 0.0668, step time: 0.2504\n",
      "115/281, train_loss: 0.0677, step time: 0.2499\n",
      "116/281, train_loss: 0.0451, step time: 0.2532\n",
      "117/281, train_loss: 0.0722, step time: 0.2538\n",
      "118/281, train_loss: 0.0605, step time: 0.2537\n",
      "119/281, train_loss: 0.0664, step time: 0.2527\n",
      "120/281, train_loss: 0.0415, step time: 0.2553\n",
      "121/281, train_loss: 0.0446, step time: 0.2556\n",
      "122/281, train_loss: 0.0790, step time: 0.2544\n",
      "123/281, train_loss: 0.0686, step time: 0.2483\n",
      "124/281, train_loss: 0.0641, step time: 0.2590\n",
      "125/281, train_loss: 0.0758, step time: 0.2522\n",
      "126/281, train_loss: 0.2082, step time: 0.2531\n",
      "127/281, train_loss: 0.0550, step time: 0.2470\n",
      "128/281, train_loss: 0.0654, step time: 0.2503\n",
      "129/281, train_loss: 0.0639, step time: 0.2579\n",
      "130/281, train_loss: 0.1034, step time: 0.2565\n",
      "131/281, train_loss: 0.0837, step time: 0.2531\n",
      "132/281, train_loss: 0.0589, step time: 0.2592\n",
      "133/281, train_loss: 0.2026, step time: 0.2529\n",
      "134/281, train_loss: 0.0577, step time: 0.2538\n",
      "135/281, train_loss: 0.2306, step time: 0.2533\n",
      "136/281, train_loss: 0.0827, step time: 0.2497\n",
      "137/281, train_loss: 0.0391, step time: 0.2543\n",
      "138/281, train_loss: 0.3795, step time: 0.2517\n",
      "139/281, train_loss: 0.0643, step time: 0.2456\n",
      "140/281, train_loss: 0.0548, step time: 0.2551\n",
      "141/281, train_loss: 0.0568, step time: 0.2503\n",
      "142/281, train_loss: 0.2353, step time: 0.2491\n",
      "143/281, train_loss: 0.0428, step time: 0.2501\n",
      "144/281, train_loss: 0.0744, step time: 0.2540\n",
      "145/281, train_loss: 0.0716, step time: 0.2558\n",
      "146/281, train_loss: 0.0562, step time: 0.2605\n",
      "147/281, train_loss: 0.0709, step time: 0.2606\n",
      "148/281, train_loss: 0.0654, step time: 0.2558\n",
      "149/281, train_loss: 0.0840, step time: 0.2530\n",
      "150/281, train_loss: 0.0604, step time: 0.2515\n",
      "151/281, train_loss: 0.0568, step time: 0.2509\n",
      "152/281, train_loss: 0.0494, step time: 0.2559\n",
      "153/281, train_loss: 0.3835, step time: 0.2526\n",
      "154/281, train_loss: 0.2073, step time: 0.2513\n",
      "155/281, train_loss: 0.0378, step time: 0.2527\n",
      "156/281, train_loss: 0.0727, step time: 0.2549\n",
      "157/281, train_loss: 0.0371, step time: 0.2548\n",
      "158/281, train_loss: 0.0579, step time: 0.2529\n",
      "159/281, train_loss: 0.0862, step time: 0.2474\n",
      "160/281, train_loss: 0.0934, step time: 0.2547\n",
      "161/281, train_loss: 0.2545, step time: 0.2535\n",
      "162/281, train_loss: 0.0666, step time: 0.2492\n",
      "163/281, train_loss: 0.3899, step time: 0.2558\n",
      "164/281, train_loss: 0.0783, step time: 0.2497\n",
      "165/281, train_loss: 0.2164, step time: 0.2578\n",
      "166/281, train_loss: 0.0633, step time: 0.2563\n",
      "167/281, train_loss: 0.0622, step time: 0.2570\n",
      "168/281, train_loss: 0.0789, step time: 0.2557\n",
      "169/281, train_loss: 0.0481, step time: 0.2530\n",
      "170/281, train_loss: 0.1099, step time: 0.2489\n",
      "171/281, train_loss: 0.0525, step time: 0.2491\n",
      "172/281, train_loss: 0.0771, step time: 0.2557\n",
      "173/281, train_loss: 0.0515, step time: 0.2489\n",
      "174/281, train_loss: 0.2205, step time: 0.2530\n",
      "175/281, train_loss: 0.0576, step time: 0.2567\n",
      "176/281, train_loss: 0.1033, step time: 0.2531\n",
      "177/281, train_loss: 0.0906, step time: 0.2545\n",
      "178/281, train_loss: 0.0459, step time: 0.2554\n",
      "179/281, train_loss: 0.2520, step time: 0.2575\n",
      "180/281, train_loss: 0.0559, step time: 0.2541\n",
      "181/281, train_loss: 0.2182, step time: 0.2568\n",
      "182/281, train_loss: 0.0590, step time: 0.2565\n",
      "183/281, train_loss: 0.0815, step time: 0.2580\n",
      "184/281, train_loss: 0.0561, step time: 0.2487\n",
      "185/281, train_loss: 0.0423, step time: 0.2543\n",
      "186/281, train_loss: 0.0625, step time: 0.2533\n",
      "187/281, train_loss: 0.2177, step time: 0.2520\n",
      "188/281, train_loss: 0.0768, step time: 0.2509\n",
      "189/281, train_loss: 0.0576, step time: 0.2515\n",
      "190/281, train_loss: 0.0598, step time: 0.2524\n",
      "191/281, train_loss: 0.0741, step time: 0.2492\n",
      "192/281, train_loss: 0.0428, step time: 0.2557\n",
      "193/281, train_loss: 0.0678, step time: 0.2459\n",
      "194/281, train_loss: 0.0576, step time: 0.2490\n",
      "195/281, train_loss: 0.0810, step time: 0.2489\n",
      "196/281, train_loss: 0.0840, step time: 0.2488\n",
      "197/281, train_loss: 0.0731, step time: 0.2491\n",
      "198/281, train_loss: 0.2202, step time: 0.2524\n",
      "199/281, train_loss: 0.0683, step time: 0.2519\n",
      "200/281, train_loss: 0.0675, step time: 0.2509\n",
      "201/281, train_loss: 0.0803, step time: 0.2499\n",
      "202/281, train_loss: 0.0570, step time: 0.2544\n",
      "203/281, train_loss: 0.1160, step time: 0.2550\n",
      "204/281, train_loss: 0.0722, step time: 0.2560\n",
      "205/281, train_loss: 0.0563, step time: 0.2575\n",
      "206/281, train_loss: 0.0929, step time: 0.2588\n",
      "207/281, train_loss: 0.0674, step time: 0.2561\n",
      "208/281, train_loss: 0.0447, step time: 0.2565\n",
      "209/281, train_loss: 0.2378, step time: 0.2540\n",
      "210/281, train_loss: 0.0518, step time: 0.2498\n",
      "211/281, train_loss: 0.0554, step time: 0.2487\n",
      "212/281, train_loss: 0.0752, step time: 0.2590\n",
      "213/281, train_loss: 0.0370, step time: 0.2540\n",
      "214/281, train_loss: 0.1015, step time: 0.2551\n",
      "215/281, train_loss: 0.0516, step time: 0.2484\n",
      "216/281, train_loss: 0.0781, step time: 0.2498\n",
      "217/281, train_loss: 0.0775, step time: 0.2562\n",
      "218/281, train_loss: 0.1000, step time: 0.2487\n",
      "219/281, train_loss: 0.0612, step time: 0.2485\n",
      "220/281, train_loss: 0.0531, step time: 0.2600\n",
      "221/281, train_loss: 0.0485, step time: 0.2517\n",
      "222/281, train_loss: 0.0831, step time: 0.2541\n",
      "223/281, train_loss: 0.2374, step time: 0.2502\n",
      "224/281, train_loss: 0.0448, step time: 0.2519\n",
      "225/281, train_loss: 0.0791, step time: 0.2563\n",
      "226/281, train_loss: 0.2131, step time: 0.2545\n",
      "227/281, train_loss: 0.0730, step time: 0.2511\n",
      "228/281, train_loss: 0.2253, step time: 0.2532\n",
      "229/281, train_loss: 0.2460, step time: 0.2498\n",
      "230/281, train_loss: 0.0410, step time: 0.2533\n",
      "231/281, train_loss: 0.2233, step time: 0.2581\n",
      "232/281, train_loss: 0.0690, step time: 0.2542\n",
      "233/281, train_loss: 0.0541, step time: 0.2534\n",
      "234/281, train_loss: 0.0396, step time: 0.2641\n",
      "235/281, train_loss: 0.0439, step time: 0.2584\n",
      "236/281, train_loss: 0.0621, step time: 0.2527\n",
      "237/281, train_loss: 0.0766, step time: 0.2493\n",
      "238/281, train_loss: 0.0409, step time: 0.2542\n",
      "239/281, train_loss: 0.1216, step time: 0.2556\n",
      "240/281, train_loss: 0.0799, step time: 0.2520\n",
      "241/281, train_loss: 0.0788, step time: 0.2551\n",
      "242/281, train_loss: 0.0970, step time: 0.2507\n",
      "243/281, train_loss: 0.0406, step time: 0.2488\n",
      "244/281, train_loss: 0.2293, step time: 0.2481\n",
      "245/281, train_loss: 0.0649, step time: 0.2501\n",
      "246/281, train_loss: 0.0755, step time: 0.2506\n",
      "247/281, train_loss: 0.0701, step time: 0.2462\n",
      "248/281, train_loss: 0.0894, step time: 0.2496\n",
      "249/281, train_loss: 0.0814, step time: 0.2473\n",
      "250/281, train_loss: 0.0985, step time: 0.2512\n",
      "251/281, train_loss: 0.2245, step time: 0.2520\n",
      "252/281, train_loss: 0.0838, step time: 0.2482\n",
      "253/281, train_loss: 0.0744, step time: 0.2487\n",
      "254/281, train_loss: 0.0570, step time: 0.2511\n",
      "255/281, train_loss: 0.0533, step time: 0.2447\n",
      "256/281, train_loss: 0.2363, step time: 0.2502\n",
      "257/281, train_loss: 0.0631, step time: 0.2512\n",
      "258/281, train_loss: 0.0829, step time: 0.2481\n",
      "259/281, train_loss: 0.0554, step time: 0.2496\n",
      "260/281, train_loss: 0.0821, step time: 0.2446\n",
      "261/281, train_loss: 0.0665, step time: 0.2460\n",
      "262/281, train_loss: 0.0509, step time: 0.2446\n",
      "263/281, train_loss: 0.0793, step time: 0.2500\n",
      "264/281, train_loss: 0.0584, step time: 0.2557\n",
      "265/281, train_loss: 0.0561, step time: 0.2484\n",
      "266/281, train_loss: 0.0758, step time: 0.2501\n",
      "267/281, train_loss: 0.0452, step time: 0.2562\n",
      "268/281, train_loss: 0.0485, step time: 0.2512\n",
      "269/281, train_loss: 0.0561, step time: 0.2581\n",
      "270/281, train_loss: 0.0632, step time: 0.2499\n",
      "271/281, train_loss: 0.0418, step time: 0.2436\n",
      "272/281, train_loss: 0.0541, step time: 0.2519\n",
      "273/281, train_loss: 0.2182, step time: 0.2502\n",
      "274/281, train_loss: 0.0698, step time: 0.2460\n",
      "275/281, train_loss: 0.0586, step time: 0.2478\n",
      "276/281, train_loss: 0.0591, step time: 0.2508\n",
      "277/281, train_loss: 0.0767, step time: 0.2474\n",
      "278/281, train_loss: 0.0806, step time: 0.2504\n",
      "279/281, train_loss: 0.1251, step time: 0.2545\n",
      "280/281, train_loss: 0.0685, step time: 0.2565\n",
      "281/281, train_loss: 0.0489, step time: 0.2497\n",
      "282/281, train_loss: 0.0630, step time: 0.1500\n",
      "epoch 97 average loss: 0.0916\n",
      "saved new best metric model\n",
      "current epoch: 97 current mean dice: 0.8972 tc: 0.8895 wt: 0.9248 et: 0.8857\n",
      "best mean dice: 0.8972 at epoch: 97\n",
      "time consuming of epoch 97 is: 394.8548\n",
      "----------\n",
      "epoch 98/200\n",
      "1/281, train_loss: 0.0653, step time: 0.2568\n",
      "2/281, train_loss: 0.0418, step time: 0.2497\n",
      "3/281, train_loss: 0.0626, step time: 0.2471\n",
      "4/281, train_loss: 0.0728, step time: 0.2476\n",
      "5/281, train_loss: 0.0385, step time: 0.2457\n",
      "6/281, train_loss: 0.0560, step time: 0.2484\n",
      "7/281, train_loss: 0.0646, step time: 0.2584\n",
      "8/281, train_loss: 0.0466, step time: 0.2548\n",
      "9/281, train_loss: 0.0628, step time: 0.2580\n",
      "10/281, train_loss: 0.0631, step time: 0.2550\n",
      "11/281, train_loss: 0.0501, step time: 0.2567\n",
      "12/281, train_loss: 0.0694, step time: 0.2617\n",
      "13/281, train_loss: 0.0739, step time: 0.2592\n",
      "14/281, train_loss: 0.0970, step time: 0.2776\n",
      "15/281, train_loss: 0.0750, step time: 0.3032\n",
      "16/281, train_loss: 0.0742, step time: 0.2552\n",
      "17/281, train_loss: 0.1050, step time: 0.2560\n",
      "18/281, train_loss: 0.2112, step time: 0.2583\n",
      "19/281, train_loss: 0.2111, step time: 0.2473\n",
      "20/281, train_loss: 0.2357, step time: 0.2550\n",
      "21/281, train_loss: 0.0454, step time: 0.2542\n",
      "22/281, train_loss: 0.0651, step time: 0.2540\n",
      "23/281, train_loss: 0.2196, step time: 0.2502\n",
      "24/281, train_loss: 0.0426, step time: 0.2527\n",
      "25/281, train_loss: 0.0523, step time: 0.2549\n",
      "26/281, train_loss: 0.0741, step time: 0.2451\n",
      "27/281, train_loss: 0.0708, step time: 0.2579\n",
      "28/281, train_loss: 0.0427, step time: 0.2586\n",
      "29/281, train_loss: 0.0553, step time: 0.2503\n",
      "30/281, train_loss: 0.0729, step time: 0.2610\n",
      "31/281, train_loss: 0.2052, step time: 0.2735\n",
      "32/281, train_loss: 0.0669, step time: 0.2743\n",
      "33/281, train_loss: 0.0745, step time: 0.2552\n",
      "34/281, train_loss: 0.0712, step time: 0.2476\n",
      "35/281, train_loss: 0.0897, step time: 0.2468\n",
      "36/281, train_loss: 0.0679, step time: 0.2444\n",
      "37/281, train_loss: 0.0708, step time: 0.2434\n",
      "38/281, train_loss: 0.0758, step time: 0.2479\n",
      "39/281, train_loss: 0.0543, step time: 0.2408\n",
      "40/281, train_loss: 0.0673, step time: 0.2455\n",
      "41/281, train_loss: 0.0568, step time: 0.2438\n",
      "42/281, train_loss: 0.0991, step time: 0.2478\n",
      "43/281, train_loss: 0.2244, step time: 0.2494\n",
      "44/281, train_loss: 0.0568, step time: 0.2527\n",
      "45/281, train_loss: 0.0584, step time: 0.2460\n",
      "46/281, train_loss: 0.0814, step time: 0.2468\n",
      "47/281, train_loss: 0.0664, step time: 0.2522\n",
      "48/281, train_loss: 0.0350, step time: 0.2537\n",
      "49/281, train_loss: 0.0767, step time: 0.2481\n",
      "50/281, train_loss: 0.0370, step time: 0.2478\n",
      "51/281, train_loss: 0.0583, step time: 0.2523\n",
      "52/281, train_loss: 0.0402, step time: 0.2487\n",
      "53/281, train_loss: 0.0578, step time: 0.2550\n",
      "54/281, train_loss: 0.0542, step time: 0.2571\n",
      "55/281, train_loss: 0.0939, step time: 0.2516\n",
      "56/281, train_loss: 0.0453, step time: 0.2513\n",
      "57/281, train_loss: 0.0637, step time: 0.2478\n",
      "58/281, train_loss: 0.2072, step time: 0.2542\n",
      "59/281, train_loss: 0.0796, step time: 0.2466\n",
      "60/281, train_loss: 0.0667, step time: 0.2491\n",
      "61/281, train_loss: 0.0619, step time: 0.2450\n",
      "62/281, train_loss: 0.0790, step time: 0.2466\n",
      "63/281, train_loss: 0.0424, step time: 0.2522\n",
      "64/281, train_loss: 0.0939, step time: 0.2595\n",
      "65/281, train_loss: 0.0722, step time: 0.2579\n",
      "66/281, train_loss: 0.0643, step time: 0.2640\n",
      "67/281, train_loss: 0.0486, step time: 0.2524\n",
      "68/281, train_loss: 0.2651, step time: 0.2475\n",
      "69/281, train_loss: 0.0466, step time: 0.2509\n",
      "70/281, train_loss: 0.0506, step time: 0.2477\n",
      "71/281, train_loss: 0.0630, step time: 0.2707\n",
      "72/281, train_loss: 0.0642, step time: 0.2517\n",
      "73/281, train_loss: 0.2385, step time: 0.2532\n",
      "74/281, train_loss: 0.0700, step time: 0.2528\n",
      "75/281, train_loss: 0.0590, step time: 0.2488\n",
      "76/281, train_loss: 0.0725, step time: 0.2465\n",
      "77/281, train_loss: 0.0560, step time: 0.2463\n",
      "78/281, train_loss: 0.0767, step time: 0.2512\n",
      "79/281, train_loss: 0.1208, step time: 0.2499\n",
      "80/281, train_loss: 0.0683, step time: 0.2508\n",
      "81/281, train_loss: 0.0415, step time: 0.2548\n",
      "82/281, train_loss: 0.0769, step time: 0.2544\n",
      "83/281, train_loss: 0.0869, step time: 0.2508\n",
      "84/281, train_loss: 0.0777, step time: 0.2537\n",
      "85/281, train_loss: 0.0641, step time: 0.2503\n",
      "86/281, train_loss: 0.0811, step time: 0.2565\n",
      "87/281, train_loss: 0.0789, step time: 0.2575\n",
      "88/281, train_loss: 0.0786, step time: 0.2497\n",
      "89/281, train_loss: 0.2138, step time: 0.2476\n",
      "90/281, train_loss: 0.1329, step time: 0.2476\n",
      "91/281, train_loss: 0.0714, step time: 0.2530\n",
      "92/281, train_loss: 0.0716, step time: 0.2513\n",
      "93/281, train_loss: 0.2503, step time: 0.2523\n",
      "94/281, train_loss: 0.0751, step time: 0.2483\n",
      "95/281, train_loss: 0.2424, step time: 0.2518\n",
      "96/281, train_loss: 0.0828, step time: 0.2535\n",
      "97/281, train_loss: 0.0692, step time: 0.2489\n",
      "98/281, train_loss: 0.0818, step time: 0.2521\n",
      "99/281, train_loss: 0.2231, step time: 0.2508\n",
      "100/281, train_loss: 0.0447, step time: 0.2518\n",
      "101/281, train_loss: 0.2273, step time: 0.2519\n",
      "102/281, train_loss: 0.0417, step time: 0.2567\n",
      "103/281, train_loss: 0.0645, step time: 0.2501\n",
      "104/281, train_loss: 0.2393, step time: 0.2580\n",
      "105/281, train_loss: 0.0680, step time: 0.2508\n",
      "106/281, train_loss: 0.0908, step time: 0.2511\n",
      "107/281, train_loss: 0.2181, step time: 0.2590\n",
      "108/281, train_loss: 0.2072, step time: 0.2538\n",
      "109/281, train_loss: 0.0631, step time: 0.2545\n",
      "110/281, train_loss: 0.0759, step time: 0.2534\n",
      "111/281, train_loss: 0.2069, step time: 0.2521\n",
      "112/281, train_loss: 0.2314, step time: 0.2488\n",
      "113/281, train_loss: 0.0620, step time: 0.2501\n",
      "114/281, train_loss: 0.2475, step time: 0.2488\n",
      "115/281, train_loss: 0.0551, step time: 0.2488\n",
      "116/281, train_loss: 0.0513, step time: 0.2511\n",
      "117/281, train_loss: 0.0508, step time: 0.2496\n",
      "118/281, train_loss: 0.2242, step time: 0.2520\n",
      "119/281, train_loss: 0.0687, step time: 0.2480\n",
      "120/281, train_loss: 0.0532, step time: 0.2604\n",
      "121/281, train_loss: 0.0292, step time: 0.2559\n",
      "122/281, train_loss: 0.2492, step time: 0.2561\n",
      "123/281, train_loss: 0.0668, step time: 0.2549\n",
      "124/281, train_loss: 0.0770, step time: 0.2501\n",
      "125/281, train_loss: 0.0584, step time: 0.2509\n",
      "126/281, train_loss: 0.2102, step time: 0.2504\n",
      "127/281, train_loss: 0.0655, step time: 0.2638\n",
      "128/281, train_loss: 0.0877, step time: 0.2518\n",
      "129/281, train_loss: 0.0526, step time: 0.2469\n",
      "130/281, train_loss: 0.0734, step time: 0.2524\n",
      "131/281, train_loss: 0.0679, step time: 0.2480\n",
      "132/281, train_loss: 0.0519, step time: 0.2495\n",
      "133/281, train_loss: 0.0760, step time: 0.2510\n",
      "134/281, train_loss: 0.0598, step time: 0.2547\n",
      "135/281, train_loss: 0.0471, step time: 0.2527\n",
      "136/281, train_loss: 0.2538, step time: 0.2553\n",
      "137/281, train_loss: 0.2169, step time: 0.2538\n",
      "138/281, train_loss: 0.0650, step time: 0.2558\n",
      "139/281, train_loss: 0.0535, step time: 0.2562\n",
      "140/281, train_loss: 0.0661, step time: 0.2565\n",
      "141/281, train_loss: 0.1110, step time: 0.2538\n",
      "142/281, train_loss: 0.0762, step time: 0.2508\n",
      "143/281, train_loss: 0.0832, step time: 0.2494\n",
      "144/281, train_loss: 0.2107, step time: 0.2548\n",
      "145/281, train_loss: 0.0588, step time: 0.2474\n",
      "146/281, train_loss: 0.0593, step time: 0.2514\n",
      "147/281, train_loss: 0.0669, step time: 0.2523\n",
      "148/281, train_loss: 0.0615, step time: 0.2541\n",
      "149/281, train_loss: 0.0620, step time: 0.2573\n",
      "150/281, train_loss: 0.0921, step time: 0.2559\n",
      "151/281, train_loss: 0.0657, step time: 0.2584\n",
      "152/281, train_loss: 0.0505, step time: 0.2547\n",
      "153/281, train_loss: 0.0584, step time: 0.2548\n",
      "154/281, train_loss: 0.0842, step time: 0.2541\n",
      "155/281, train_loss: 0.0758, step time: 0.2522\n",
      "156/281, train_loss: 0.0912, step time: 0.2582\n",
      "157/281, train_loss: 0.0510, step time: 0.2569\n",
      "158/281, train_loss: 0.2254, step time: 0.2582\n",
      "159/281, train_loss: 0.2211, step time: 0.2617\n",
      "160/281, train_loss: 0.0672, step time: 0.2522\n",
      "161/281, train_loss: 0.0585, step time: 0.2506\n",
      "162/281, train_loss: 0.0963, step time: 0.2543\n",
      "163/281, train_loss: 0.0620, step time: 0.2551\n",
      "164/281, train_loss: 0.0818, step time: 0.2530\n",
      "165/281, train_loss: 0.2026, step time: 0.2826\n",
      "166/281, train_loss: 0.0814, step time: 0.2726\n",
      "167/281, train_loss: 0.0725, step time: 0.2569\n",
      "168/281, train_loss: 0.0489, step time: 0.2544\n",
      "169/281, train_loss: 0.0714, step time: 0.2525\n",
      "170/281, train_loss: 0.0979, step time: 0.2532\n",
      "171/281, train_loss: 0.0875, step time: 0.2619\n",
      "172/281, train_loss: 0.0748, step time: 0.2533\n",
      "173/281, train_loss: 0.0802, step time: 0.2554\n",
      "174/281, train_loss: 0.0487, step time: 0.2557\n",
      "175/281, train_loss: 0.0624, step time: 0.2571\n",
      "176/281, train_loss: 0.0469, step time: 0.2547\n",
      "177/281, train_loss: 0.0866, step time: 0.2535\n",
      "178/281, train_loss: 0.0575, step time: 0.2515\n",
      "179/281, train_loss: 0.0836, step time: 0.2583\n",
      "180/281, train_loss: 0.0536, step time: 0.2541\n",
      "181/281, train_loss: 0.2043, step time: 0.2483\n",
      "182/281, train_loss: 0.0958, step time: 0.2529\n",
      "183/281, train_loss: 0.0871, step time: 0.2574\n",
      "184/281, train_loss: 0.1060, step time: 0.2512\n",
      "185/281, train_loss: 0.0656, step time: 0.2552\n",
      "186/281, train_loss: 0.0548, step time: 0.2523\n",
      "187/281, train_loss: 0.0496, step time: 0.2513\n",
      "188/281, train_loss: 0.0789, step time: 0.2563\n",
      "189/281, train_loss: 0.0776, step time: 0.2560\n",
      "190/281, train_loss: 0.0727, step time: 0.2583\n",
      "191/281, train_loss: 0.0893, step time: 0.2577\n",
      "192/281, train_loss: 0.0751, step time: 0.2524\n",
      "193/281, train_loss: 0.0480, step time: 0.2551\n",
      "194/281, train_loss: 0.1262, step time: 0.2516\n",
      "195/281, train_loss: 0.0667, step time: 0.2560\n",
      "196/281, train_loss: 0.0895, step time: 0.2568\n",
      "197/281, train_loss: 0.0557, step time: 0.2580\n",
      "198/281, train_loss: 0.0668, step time: 0.2582\n",
      "199/281, train_loss: 0.0529, step time: 0.2558\n",
      "200/281, train_loss: 0.0521, step time: 0.2595\n",
      "201/281, train_loss: 0.0515, step time: 0.2579\n",
      "202/281, train_loss: 0.0422, step time: 0.2543\n",
      "203/281, train_loss: 0.2408, step time: 0.2569\n",
      "204/281, train_loss: 0.0827, step time: 0.2605\n",
      "205/281, train_loss: 0.2324, step time: 0.2599\n",
      "206/281, train_loss: 0.0537, step time: 0.2584\n",
      "207/281, train_loss: 0.0517, step time: 0.2567\n",
      "208/281, train_loss: 0.0346, step time: 0.2593\n",
      "209/281, train_loss: 0.0631, step time: 0.2563\n",
      "210/281, train_loss: 0.0738, step time: 0.2562\n",
      "211/281, train_loss: 0.0725, step time: 0.2545\n",
      "212/281, train_loss: 0.0608, step time: 0.2538\n",
      "213/281, train_loss: 0.0377, step time: 0.2578\n",
      "214/281, train_loss: 0.0951, step time: 0.2556\n",
      "215/281, train_loss: 0.0555, step time: 0.2531\n",
      "216/281, train_loss: 0.0602, step time: 0.2521\n",
      "217/281, train_loss: 0.0413, step time: 0.2526\n",
      "218/281, train_loss: 0.0583, step time: 0.2576\n",
      "219/281, train_loss: 0.0496, step time: 0.2529\n",
      "220/281, train_loss: 0.0903, step time: 0.2704\n",
      "221/281, train_loss: 0.0723, step time: 0.2515\n",
      "222/281, train_loss: 0.0699, step time: 0.2565\n",
      "223/281, train_loss: 0.2158, step time: 0.2554\n",
      "224/281, train_loss: 0.0621, step time: 0.2572\n",
      "225/281, train_loss: 0.1077, step time: 0.2535\n",
      "226/281, train_loss: 0.0994, step time: 0.2568\n",
      "227/281, train_loss: 0.2102, step time: 0.2561\n",
      "228/281, train_loss: 0.0397, step time: 0.2553\n",
      "229/281, train_loss: 0.0666, step time: 0.2536\n",
      "230/281, train_loss: 0.0328, step time: 0.2554\n",
      "231/281, train_loss: 0.2135, step time: 0.2653\n",
      "232/281, train_loss: 0.2191, step time: 0.2548\n",
      "233/281, train_loss: 0.0430, step time: 0.2535\n",
      "234/281, train_loss: 0.0649, step time: 0.2600\n",
      "235/281, train_loss: 0.0536, step time: 0.2617\n",
      "236/281, train_loss: 0.0789, step time: 0.2569\n",
      "237/281, train_loss: 0.0503, step time: 0.2704\n",
      "238/281, train_loss: 0.0367, step time: 0.2558\n",
      "239/281, train_loss: 0.0931, step time: 0.2563\n",
      "240/281, train_loss: 0.0586, step time: 0.2595\n",
      "241/281, train_loss: 0.0621, step time: 0.2599\n",
      "242/281, train_loss: 0.0339, step time: 0.2577\n",
      "243/281, train_loss: 0.2043, step time: 0.2580\n",
      "244/281, train_loss: 0.0608, step time: 0.2565\n",
      "245/281, train_loss: 0.0453, step time: 0.2585\n",
      "246/281, train_loss: 0.0895, step time: 0.2566\n",
      "247/281, train_loss: 0.0616, step time: 0.2550\n",
      "248/281, train_loss: 0.0938, step time: 0.2534\n",
      "249/281, train_loss: 0.0435, step time: 0.2519\n",
      "250/281, train_loss: 0.0614, step time: 0.2639\n",
      "251/281, train_loss: 0.0653, step time: 0.2545\n",
      "252/281, train_loss: 0.2337, step time: 0.2529\n",
      "253/281, train_loss: 0.0607, step time: 0.2530\n",
      "254/281, train_loss: 0.0550, step time: 0.2503\n",
      "255/281, train_loss: 0.0385, step time: 0.2527\n",
      "256/281, train_loss: 0.0676, step time: 0.2608\n",
      "257/281, train_loss: 0.0553, step time: 0.2563\n",
      "258/281, train_loss: 0.0611, step time: 0.2597\n",
      "259/281, train_loss: 0.2442, step time: 0.2568\n",
      "260/281, train_loss: 0.0568, step time: 0.2593\n",
      "261/281, train_loss: 0.0627, step time: 0.2520\n",
      "262/281, train_loss: 0.0737, step time: 0.2505\n",
      "263/281, train_loss: 0.0808, step time: 0.2600\n",
      "264/281, train_loss: 0.0660, step time: 0.2542\n",
      "265/281, train_loss: 0.2040, step time: 0.2500\n",
      "266/281, train_loss: 0.0606, step time: 0.2566\n",
      "267/281, train_loss: 0.0692, step time: 0.2584\n",
      "268/281, train_loss: 0.2356, step time: 0.2533\n",
      "269/281, train_loss: 0.0567, step time: 0.2543\n",
      "270/281, train_loss: 0.0384, step time: 0.2506\n",
      "271/281, train_loss: 0.0797, step time: 0.2518\n",
      "272/281, train_loss: 0.2312, step time: 0.2476\n",
      "273/281, train_loss: 0.0536, step time: 0.2470\n",
      "274/281, train_loss: 0.2390, step time: 0.2514\n",
      "275/281, train_loss: 0.1078, step time: 0.2519\n",
      "276/281, train_loss: 0.0255, step time: 0.2487\n",
      "277/281, train_loss: 0.0545, step time: 0.2491\n",
      "278/281, train_loss: 0.0972, step time: 0.2518\n",
      "279/281, train_loss: 0.0797, step time: 0.2515\n",
      "280/281, train_loss: 0.0678, step time: 0.2514\n",
      "281/281, train_loss: 0.0477, step time: 0.2527\n",
      "282/281, train_loss: 0.1303, step time: 0.1523\n",
      "epoch 98 average loss: 0.0907\n",
      "current epoch: 98 current mean dice: 0.8972 tc: 0.8893 wt: 0.9272 et: 0.8831\n",
      "best mean dice: 0.8972 at epoch: 97\n",
      "time consuming of epoch 98 is: 407.8265\n",
      "----------\n",
      "epoch 99/200\n",
      "1/281, train_loss: 0.0701, step time: 0.2611\n",
      "2/281, train_loss: 0.0398, step time: 0.2551\n",
      "3/281, train_loss: 0.0531, step time: 0.2525\n",
      "4/281, train_loss: 0.0638, step time: 0.2525\n",
      "5/281, train_loss: 0.0626, step time: 0.2620\n",
      "6/281, train_loss: 0.0477, step time: 0.2639\n",
      "7/281, train_loss: 0.0422, step time: 0.2633\n",
      "8/281, train_loss: 0.0870, step time: 0.2568\n",
      "9/281, train_loss: 0.0377, step time: 0.2635\n",
      "10/281, train_loss: 0.0633, step time: 0.2541\n",
      "11/281, train_loss: 0.0802, step time: 0.2545\n",
      "12/281, train_loss: 0.2056, step time: 0.2521\n",
      "13/281, train_loss: 0.2091, step time: 0.2584\n",
      "14/281, train_loss: 0.0618, step time: 0.2570\n",
      "15/281, train_loss: 0.0424, step time: 0.2597\n",
      "16/281, train_loss: 0.0518, step time: 0.2560\n",
      "17/281, train_loss: 0.2293, step time: 0.2534\n",
      "18/281, train_loss: 0.0680, step time: 0.2509\n",
      "19/281, train_loss: 0.0550, step time: 0.2513\n",
      "20/281, train_loss: 0.0690, step time: 0.2579\n",
      "21/281, train_loss: 0.0536, step time: 0.2641\n",
      "22/281, train_loss: 0.0615, step time: 0.2721\n",
      "23/281, train_loss: 0.0642, step time: 0.2663\n",
      "24/281, train_loss: 0.0537, step time: 0.2577\n",
      "25/281, train_loss: 0.0326, step time: 0.2582\n",
      "26/281, train_loss: 0.0434, step time: 0.2583\n",
      "27/281, train_loss: 0.0643, step time: 0.2573\n",
      "28/281, train_loss: 0.0592, step time: 0.2564\n",
      "29/281, train_loss: 0.0555, step time: 0.2558\n",
      "30/281, train_loss: 0.0447, step time: 0.2513\n",
      "31/281, train_loss: 0.0505, step time: 0.2546\n",
      "32/281, train_loss: 0.0859, step time: 0.2537\n",
      "33/281, train_loss: 0.0595, step time: 0.2546\n",
      "34/281, train_loss: 0.0618, step time: 0.2502\n",
      "35/281, train_loss: 0.0983, step time: 0.2514\n",
      "36/281, train_loss: 0.1032, step time: 0.2537\n",
      "37/281, train_loss: 0.2520, step time: 0.2501\n",
      "38/281, train_loss: 0.2035, step time: 0.2518\n",
      "39/281, train_loss: 0.2503, step time: 0.2572\n",
      "40/281, train_loss: 0.0758, step time: 0.2445\n",
      "41/281, train_loss: 0.0927, step time: 0.2508\n",
      "42/281, train_loss: 0.0704, step time: 0.2509\n",
      "43/281, train_loss: 0.2249, step time: 0.2525\n",
      "44/281, train_loss: 0.0705, step time: 0.2520\n",
      "45/281, train_loss: 0.0705, step time: 0.2564\n",
      "46/281, train_loss: 0.0967, step time: 0.2611\n",
      "47/281, train_loss: 0.0635, step time: 0.2576\n",
      "48/281, train_loss: 0.0549, step time: 0.2571\n",
      "49/281, train_loss: 0.0813, step time: 0.2599\n",
      "50/281, train_loss: 0.0495, step time: 0.2575\n",
      "51/281, train_loss: 0.0623, step time: 0.2552\n",
      "52/281, train_loss: 0.0810, step time: 0.2535\n",
      "53/281, train_loss: 0.0677, step time: 0.2489\n",
      "54/281, train_loss: 0.2118, step time: 0.2508\n",
      "55/281, train_loss: 0.0774, step time: 0.2503\n",
      "56/281, train_loss: 0.0999, step time: 0.2513\n",
      "57/281, train_loss: 0.0366, step time: 0.2524\n",
      "58/281, train_loss: 0.1210, step time: 0.2561\n",
      "59/281, train_loss: 0.2213, step time: 0.2598\n",
      "60/281, train_loss: 0.2103, step time: 0.2533\n",
      "61/281, train_loss: 0.0430, step time: 0.2623\n",
      "62/281, train_loss: 0.0489, step time: 0.2595\n",
      "63/281, train_loss: 0.0561, step time: 0.2527\n",
      "64/281, train_loss: 0.0704, step time: 0.2577\n",
      "65/281, train_loss: 0.2187, step time: 0.2558\n",
      "66/281, train_loss: 0.0801, step time: 0.2519\n",
      "67/281, train_loss: 0.1050, step time: 0.2530\n",
      "68/281, train_loss: 0.0589, step time: 0.2557\n",
      "69/281, train_loss: 0.0573, step time: 0.2584\n",
      "70/281, train_loss: 0.0745, step time: 0.2538\n",
      "71/281, train_loss: 0.0541, step time: 0.2490\n",
      "72/281, train_loss: 0.1099, step time: 0.2483\n",
      "73/281, train_loss: 0.0600, step time: 0.2472\n",
      "74/281, train_loss: 0.0755, step time: 0.2512\n",
      "75/281, train_loss: 0.0503, step time: 0.2497\n",
      "76/281, train_loss: 0.2010, step time: 0.2457\n",
      "77/281, train_loss: 0.0956, step time: 0.2529\n",
      "78/281, train_loss: 0.0858, step time: 0.2499\n",
      "79/281, train_loss: 0.3874, step time: 0.2529\n",
      "80/281, train_loss: 0.0545, step time: 0.2555\n",
      "81/281, train_loss: 0.0927, step time: 0.2854\n",
      "82/281, train_loss: 0.0672, step time: 0.2556\n",
      "83/281, train_loss: 0.2467, step time: 0.2541\n",
      "84/281, train_loss: 0.0934, step time: 0.2572\n",
      "85/281, train_loss: 0.0490, step time: 0.2558\n",
      "86/281, train_loss: 0.0825, step time: 0.2568\n",
      "87/281, train_loss: 0.0658, step time: 0.2594\n",
      "88/281, train_loss: 0.0789, step time: 0.2569\n",
      "89/281, train_loss: 0.0595, step time: 0.2557\n",
      "90/281, train_loss: 0.0579, step time: 0.2525\n",
      "91/281, train_loss: 0.0696, step time: 0.2554\n",
      "92/281, train_loss: 0.0445, step time: 0.2603\n",
      "93/281, train_loss: 0.0664, step time: 0.2614\n",
      "94/281, train_loss: 0.0990, step time: 0.2583\n",
      "95/281, train_loss: 0.2215, step time: 0.2542\n",
      "96/281, train_loss: 0.0623, step time: 0.2561\n",
      "97/281, train_loss: 0.0601, step time: 0.2556\n",
      "98/281, train_loss: 0.0436, step time: 0.2533\n",
      "99/281, train_loss: 0.0609, step time: 0.2561\n",
      "100/281, train_loss: 0.0985, step time: 0.2515\n",
      "101/281, train_loss: 0.0514, step time: 0.2546\n",
      "102/281, train_loss: 0.0664, step time: 0.2510\n",
      "103/281, train_loss: 0.0696, step time: 0.2563\n",
      "104/281, train_loss: 0.0841, step time: 0.2477\n",
      "105/281, train_loss: 0.0524, step time: 0.2545\n",
      "106/281, train_loss: 0.0688, step time: 0.2642\n",
      "107/281, train_loss: 0.0595, step time: 0.2686\n",
      "108/281, train_loss: 0.2097, step time: 0.2572\n",
      "109/281, train_loss: 0.3740, step time: 0.2553\n",
      "110/281, train_loss: 0.0810, step time: 0.2516\n",
      "111/281, train_loss: 0.0647, step time: 0.2580\n",
      "112/281, train_loss: 0.0664, step time: 0.2545\n",
      "113/281, train_loss: 0.0727, step time: 0.2541\n",
      "114/281, train_loss: 0.0622, step time: 0.2535\n",
      "115/281, train_loss: 0.0799, step time: 0.2577\n",
      "116/281, train_loss: 0.2152, step time: 0.2545\n",
      "117/281, train_loss: 0.0546, step time: 0.2557\n",
      "118/281, train_loss: 0.0812, step time: 0.2590\n",
      "119/281, train_loss: 0.1072, step time: 0.2580\n",
      "120/281, train_loss: 0.1041, step time: 0.2558\n",
      "121/281, train_loss: 0.0738, step time: 0.2570\n",
      "122/281, train_loss: 0.0661, step time: 0.2596\n",
      "123/281, train_loss: 0.0628, step time: 0.2554\n",
      "124/281, train_loss: 0.0743, step time: 0.2561\n",
      "125/281, train_loss: 0.0328, step time: 0.2562\n",
      "126/281, train_loss: 0.2367, step time: 0.2591\n",
      "127/281, train_loss: 0.0354, step time: 0.2533\n",
      "128/281, train_loss: 0.0599, step time: 0.2542\n",
      "129/281, train_loss: 0.0577, step time: 0.2480\n",
      "130/281, train_loss: 0.0538, step time: 0.2570\n",
      "131/281, train_loss: 0.0566, step time: 0.2548\n",
      "132/281, train_loss: 0.0676, step time: 0.2527\n",
      "133/281, train_loss: 0.0437, step time: 0.2524\n",
      "134/281, train_loss: 0.2064, step time: 0.2541\n",
      "135/281, train_loss: 0.0534, step time: 0.2505\n",
      "136/281, train_loss: 0.0659, step time: 0.2533\n",
      "137/281, train_loss: 0.2248, step time: 0.2492\n",
      "138/281, train_loss: 0.2441, step time: 0.2542\n",
      "139/281, train_loss: 0.0652, step time: 0.2509\n",
      "140/281, train_loss: 0.2831, step time: 0.2535\n",
      "141/281, train_loss: 0.0822, step time: 0.2491\n",
      "142/281, train_loss: 0.0593, step time: 0.2611\n",
      "143/281, train_loss: 0.0553, step time: 0.2571\n",
      "144/281, train_loss: 0.0493, step time: 0.2552\n",
      "145/281, train_loss: 0.0589, step time: 0.2510\n",
      "146/281, train_loss: 0.2218, step time: 0.2595\n",
      "147/281, train_loss: 0.0850, step time: 0.2587\n",
      "148/281, train_loss: 0.0568, step time: 0.2544\n",
      "149/281, train_loss: 0.0508, step time: 0.2568\n",
      "150/281, train_loss: 0.0526, step time: 0.2539\n",
      "151/281, train_loss: 0.2231, step time: 0.2545\n",
      "152/281, train_loss: 0.0649, step time: 0.2533\n",
      "153/281, train_loss: 0.0627, step time: 0.2535\n",
      "154/281, train_loss: 0.0518, step time: 0.2564\n",
      "155/281, train_loss: 0.0619, step time: 0.2551\n",
      "156/281, train_loss: 0.0652, step time: 0.2520\n",
      "157/281, train_loss: 0.0772, step time: 0.2502\n",
      "158/281, train_loss: 0.0814, step time: 0.2523\n",
      "159/281, train_loss: 0.0627, step time: 0.2495\n",
      "160/281, train_loss: 0.0469, step time: 0.2541\n",
      "161/281, train_loss: 0.0603, step time: 0.2505\n",
      "162/281, train_loss: 0.0285, step time: 0.2559\n",
      "163/281, train_loss: 0.0444, step time: 0.2538\n",
      "164/281, train_loss: 0.0762, step time: 0.2536\n",
      "165/281, train_loss: 0.0530, step time: 0.2552\n",
      "166/281, train_loss: 0.0591, step time: 0.2559\n",
      "167/281, train_loss: 0.0632, step time: 0.2582\n",
      "168/281, train_loss: 0.0581, step time: 0.2660\n",
      "169/281, train_loss: 0.0712, step time: 0.2554\n",
      "170/281, train_loss: 0.0943, step time: 0.2590\n",
      "171/281, train_loss: 0.0575, step time: 0.2610\n",
      "172/281, train_loss: 0.0525, step time: 0.2573\n",
      "173/281, train_loss: 0.0781, step time: 0.2567\n",
      "174/281, train_loss: 0.0611, step time: 0.2488\n",
      "175/281, train_loss: 0.0448, step time: 0.2500\n",
      "176/281, train_loss: 0.1011, step time: 0.2538\n",
      "177/281, train_loss: 0.0938, step time: 0.2482\n",
      "178/281, train_loss: 0.0884, step time: 0.2580\n",
      "179/281, train_loss: 0.0680, step time: 0.2674\n",
      "180/281, train_loss: 0.0711, step time: 0.2602\n",
      "181/281, train_loss: 0.0818, step time: 0.2572\n",
      "182/281, train_loss: 0.0781, step time: 0.2543\n",
      "183/281, train_loss: 0.0609, step time: 0.2570\n",
      "184/281, train_loss: 0.1109, step time: 0.2563\n",
      "185/281, train_loss: 0.0630, step time: 0.2519\n",
      "186/281, train_loss: 0.0572, step time: 0.2527\n",
      "187/281, train_loss: 0.0742, step time: 0.2517\n",
      "188/281, train_loss: 0.0389, step time: 0.2560\n",
      "189/281, train_loss: 0.0653, step time: 0.2538\n",
      "190/281, train_loss: 0.0361, step time: 0.2568\n",
      "191/281, train_loss: 0.0722, step time: 0.2541\n",
      "192/281, train_loss: 0.0475, step time: 0.2528\n",
      "193/281, train_loss: 0.0677, step time: 0.2611\n",
      "194/281, train_loss: 0.0425, step time: 0.2533\n",
      "195/281, train_loss: 0.0951, step time: 0.2524\n",
      "196/281, train_loss: 0.0548, step time: 0.2534\n",
      "197/281, train_loss: 0.0399, step time: 0.2485\n",
      "198/281, train_loss: 0.0592, step time: 0.2529\n",
      "199/281, train_loss: 0.0575, step time: 0.2511\n",
      "200/281, train_loss: 0.2238, step time: 0.2520\n",
      "201/281, train_loss: 0.0521, step time: 0.2510\n",
      "202/281, train_loss: 0.2382, step time: 0.2513\n",
      "203/281, train_loss: 0.0897, step time: 0.2517\n",
      "204/281, train_loss: 0.2214, step time: 0.2533\n",
      "205/281, train_loss: 0.0970, step time: 0.2545\n",
      "206/281, train_loss: 0.0497, step time: 0.2517\n",
      "207/281, train_loss: 0.0864, step time: 0.2533\n",
      "208/281, train_loss: 0.0527, step time: 0.2589\n",
      "209/281, train_loss: 0.0644, step time: 0.2551\n",
      "210/281, train_loss: 0.0552, step time: 0.2553\n",
      "211/281, train_loss: 0.0645, step time: 0.2564\n",
      "212/281, train_loss: 0.0478, step time: 0.2497\n",
      "213/281, train_loss: 0.0888, step time: 0.2593\n",
      "214/281, train_loss: 0.0839, step time: 0.2542\n",
      "215/281, train_loss: 0.0549, step time: 0.2512\n",
      "216/281, train_loss: 0.0604, step time: 0.2463\n",
      "217/281, train_loss: 0.0962, step time: 0.2522\n",
      "218/281, train_loss: 0.0544, step time: 0.2581\n",
      "219/281, train_loss: 0.2301, step time: 0.2570\n",
      "220/281, train_loss: 0.0553, step time: 0.2585\n",
      "221/281, train_loss: 0.0607, step time: 0.2493\n",
      "222/281, train_loss: 0.2178, step time: 0.2526\n",
      "223/281, train_loss: 0.2308, step time: 0.2562\n",
      "224/281, train_loss: 0.0921, step time: 0.2557\n",
      "225/281, train_loss: 0.2437, step time: 0.2510\n",
      "226/281, train_loss: 0.0529, step time: 0.2544\n",
      "227/281, train_loss: 0.0866, step time: 0.2507\n",
      "228/281, train_loss: 0.0327, step time: 0.2526\n",
      "229/281, train_loss: 0.0979, step time: 0.2461\n",
      "230/281, train_loss: 0.0438, step time: 0.2582\n",
      "231/281, train_loss: 0.0669, step time: 0.2466\n",
      "232/281, train_loss: 0.0586, step time: 0.2489\n",
      "233/281, train_loss: 0.0642, step time: 0.2502\n",
      "234/281, train_loss: 0.2247, step time: 0.2504\n",
      "235/281, train_loss: 0.0712, step time: 0.2480\n",
      "236/281, train_loss: 0.0697, step time: 0.2530\n",
      "237/281, train_loss: 0.0312, step time: 0.2499\n",
      "238/281, train_loss: 0.0733, step time: 0.2536\n",
      "239/281, train_loss: 0.0890, step time: 0.2537\n",
      "240/281, train_loss: 0.1004, step time: 0.2502\n",
      "241/281, train_loss: 0.2434, step time: 0.2431\n",
      "242/281, train_loss: 0.0326, step time: 0.2488\n",
      "243/281, train_loss: 0.0715, step time: 0.2508\n",
      "244/281, train_loss: 0.0614, step time: 0.2497\n",
      "245/281, train_loss: 0.0591, step time: 0.2477\n",
      "246/281, train_loss: 0.0559, step time: 0.2505\n",
      "247/281, train_loss: 0.0808, step time: 0.2545\n",
      "248/281, train_loss: 0.0672, step time: 0.2470\n",
      "249/281, train_loss: 0.0530, step time: 0.2431\n",
      "250/281, train_loss: 0.0714, step time: 0.2437\n",
      "251/281, train_loss: 0.0544, step time: 0.2559\n",
      "252/281, train_loss: 0.0611, step time: 0.2531\n",
      "253/281, train_loss: 0.0882, step time: 0.2464\n",
      "254/281, train_loss: 0.2583, step time: 0.2491\n",
      "255/281, train_loss: 0.2476, step time: 0.2552\n",
      "256/281, train_loss: 0.0601, step time: 0.2486\n",
      "257/281, train_loss: 0.0478, step time: 0.2474\n",
      "258/281, train_loss: 0.0953, step time: 0.2474\n",
      "259/281, train_loss: 0.0666, step time: 0.2519\n",
      "260/281, train_loss: 0.2273, step time: 0.2500\n",
      "261/281, train_loss: 0.0718, step time: 0.2462\n",
      "262/281, train_loss: 0.0599, step time: 0.2514\n",
      "263/281, train_loss: 0.0840, step time: 0.2525\n",
      "264/281, train_loss: 0.3855, step time: 0.2520\n",
      "265/281, train_loss: 0.0726, step time: 0.2534\n",
      "266/281, train_loss: 0.0748, step time: 0.2538\n",
      "267/281, train_loss: 0.0736, step time: 0.2522\n",
      "268/281, train_loss: 0.0602, step time: 0.2527\n",
      "269/281, train_loss: 0.0885, step time: 0.2509\n",
      "270/281, train_loss: 0.1071, step time: 0.2484\n",
      "271/281, train_loss: 0.0678, step time: 0.2502\n",
      "272/281, train_loss: 0.0348, step time: 0.2516\n",
      "273/281, train_loss: 0.0663, step time: 0.2419\n",
      "274/281, train_loss: 0.0489, step time: 0.2431\n",
      "275/281, train_loss: 0.0569, step time: 0.2535\n",
      "276/281, train_loss: 0.0532, step time: 0.2535\n",
      "277/281, train_loss: 0.2517, step time: 0.2452\n",
      "278/281, train_loss: 0.0635, step time: 0.2467\n",
      "279/281, train_loss: 0.2308, step time: 0.2475\n",
      "280/281, train_loss: 0.0693, step time: 0.2477\n",
      "281/281, train_loss: 0.0606, step time: 0.2455\n",
      "282/281, train_loss: 0.0567, step time: 0.1452\n",
      "epoch 99 average loss: 0.0907\n",
      "saved new best metric model\n",
      "current epoch: 99 current mean dice: 0.8975 tc: 0.8904 wt: 0.9254 et: 0.8864\n",
      "best mean dice: 0.8975 at epoch: 99\n",
      "time consuming of epoch 99 is: 385.3787\n",
      "----------\n",
      "epoch 100/200\n",
      "1/281, train_loss: 0.0481, step time: 0.2568\n",
      "2/281, train_loss: 0.0466, step time: 0.2472\n",
      "3/281, train_loss: 0.0586, step time: 0.2477\n",
      "4/281, train_loss: 0.0923, step time: 0.2511\n",
      "5/281, train_loss: 0.2165, step time: 0.2555\n",
      "6/281, train_loss: 0.0556, step time: 0.2502\n",
      "7/281, train_loss: 0.1291, step time: 0.2512\n",
      "8/281, train_loss: 0.1036, step time: 0.2516\n",
      "9/281, train_loss: 0.0580, step time: 0.2553\n",
      "10/281, train_loss: 0.2407, step time: 0.2565\n",
      "11/281, train_loss: 0.0809, step time: 0.2511\n",
      "12/281, train_loss: 0.3646, step time: 0.2526\n",
      "13/281, train_loss: 0.0690, step time: 0.2493\n",
      "14/281, train_loss: 0.0452, step time: 0.2488\n",
      "15/281, train_loss: 0.1989, step time: 0.2484\n",
      "16/281, train_loss: 0.1049, step time: 0.2539\n",
      "17/281, train_loss: 0.0561, step time: 0.2496\n",
      "18/281, train_loss: 0.2432, step time: 0.2506\n",
      "19/281, train_loss: 0.0558, step time: 0.2501\n",
      "20/281, train_loss: 0.0918, step time: 0.2541\n",
      "21/281, train_loss: 0.0658, step time: 0.2542\n",
      "22/281, train_loss: 0.0590, step time: 0.2489\n",
      "23/281, train_loss: 0.0555, step time: 0.2458\n",
      "24/281, train_loss: 0.1093, step time: 0.2459\n",
      "25/281, train_loss: 0.0963, step time: 0.2588\n",
      "26/281, train_loss: 0.0584, step time: 0.2514\n",
      "27/281, train_loss: 0.0860, step time: 0.2504\n",
      "28/281, train_loss: 0.0619, step time: 0.2499\n",
      "29/281, train_loss: 0.0628, step time: 0.2496\n",
      "30/281, train_loss: 0.0639, step time: 0.2518\n",
      "31/281, train_loss: 0.0475, step time: 0.2499\n",
      "32/281, train_loss: 0.0404, step time: 0.2489\n",
      "33/281, train_loss: 0.0544, step time: 0.2492\n",
      "34/281, train_loss: 0.0849, step time: 0.2512\n",
      "35/281, train_loss: 0.0602, step time: 0.2519\n",
      "36/281, train_loss: 0.2304, step time: 0.2566\n",
      "37/281, train_loss: 0.0569, step time: 0.2473\n",
      "38/281, train_loss: 0.0623, step time: 0.2465\n",
      "39/281, train_loss: 0.2338, step time: 0.2472\n",
      "40/281, train_loss: 0.0690, step time: 0.2446\n",
      "41/281, train_loss: 0.0433, step time: 0.2526\n",
      "42/281, train_loss: 0.0475, step time: 0.2552\n",
      "43/281, train_loss: 0.0389, step time: 0.2467\n",
      "44/281, train_loss: 0.2008, step time: 0.2483\n",
      "45/281, train_loss: 0.0722, step time: 0.2552\n",
      "46/281, train_loss: 0.2311, step time: 0.2534\n",
      "47/281, train_loss: 0.0840, step time: 0.2504\n",
      "48/281, train_loss: 0.0953, step time: 0.2479\n",
      "49/281, train_loss: 0.0838, step time: 0.2547\n",
      "50/281, train_loss: 0.0773, step time: 0.2448\n",
      "51/281, train_loss: 0.4104, step time: 0.2449\n",
      "52/281, train_loss: 0.0511, step time: 0.2481\n",
      "53/281, train_loss: 0.2236, step time: 0.2581\n",
      "54/281, train_loss: 0.0509, step time: 0.2549\n",
      "55/281, train_loss: 0.0909, step time: 0.2557\n",
      "56/281, train_loss: 0.1049, step time: 0.2517\n",
      "57/281, train_loss: 0.0605, step time: 0.2563\n",
      "58/281, train_loss: 0.0456, step time: 0.2484\n",
      "59/281, train_loss: 0.0520, step time: 0.2473\n",
      "60/281, train_loss: 0.0620, step time: 0.2475\n",
      "61/281, train_loss: 0.0655, step time: 0.2525\n",
      "62/281, train_loss: 0.0474, step time: 0.2525\n",
      "63/281, train_loss: 0.0434, step time: 0.2517\n",
      "64/281, train_loss: 0.0672, step time: 0.2479\n",
      "65/281, train_loss: 0.0564, step time: 0.2521\n",
      "66/281, train_loss: 0.0879, step time: 0.2520\n",
      "67/281, train_loss: 0.0571, step time: 0.2484\n",
      "68/281, train_loss: 0.0762, step time: 0.2472\n",
      "69/281, train_loss: 0.0562, step time: 0.2562\n",
      "70/281, train_loss: 0.0623, step time: 0.2521\n",
      "71/281, train_loss: 0.0505, step time: 0.2515\n",
      "72/281, train_loss: 0.0686, step time: 0.2487\n",
      "73/281, train_loss: 0.0851, step time: 0.2544\n",
      "74/281, train_loss: 0.0825, step time: 0.2499\n",
      "75/281, train_loss: 0.0408, step time: 0.2532\n",
      "76/281, train_loss: 0.0509, step time: 0.2499\n",
      "77/281, train_loss: 0.0622, step time: 0.2490\n",
      "78/281, train_loss: 0.0483, step time: 0.2534\n",
      "79/281, train_loss: 0.2076, step time: 0.2572\n",
      "80/281, train_loss: 0.0569, step time: 0.2525\n",
      "81/281, train_loss: 0.0796, step time: 0.2562\n",
      "82/281, train_loss: 0.0684, step time: 0.2553\n",
      "83/281, train_loss: 0.0387, step time: 0.2530\n",
      "84/281, train_loss: 0.1004, step time: 0.2487\n",
      "85/281, train_loss: 0.0456, step time: 0.2539\n",
      "86/281, train_loss: 0.0650, step time: 0.2473\n",
      "87/281, train_loss: 0.0917, step time: 0.2450\n",
      "88/281, train_loss: 0.0476, step time: 0.2471\n",
      "89/281, train_loss: 0.0817, step time: 0.2626\n",
      "90/281, train_loss: 0.0762, step time: 0.2436\n",
      "91/281, train_loss: 0.0530, step time: 0.2464\n",
      "92/281, train_loss: 0.0619, step time: 0.2512\n",
      "93/281, train_loss: 0.0857, step time: 0.2490\n",
      "94/281, train_loss: 0.0531, step time: 0.2525\n",
      "95/281, train_loss: 0.2155, step time: 0.2485\n",
      "96/281, train_loss: 0.0345, step time: 0.2460\n",
      "97/281, train_loss: 0.2083, step time: 0.2589\n",
      "98/281, train_loss: 0.0720, step time: 0.2557\n",
      "99/281, train_loss: 0.0752, step time: 0.2535\n",
      "100/281, train_loss: 0.0598, step time: 0.2486\n",
      "101/281, train_loss: 0.0475, step time: 0.2500\n",
      "102/281, train_loss: 0.2289, step time: 0.2515\n",
      "103/281, train_loss: 0.0740, step time: 0.2517\n",
      "104/281, train_loss: 0.0854, step time: 0.2496\n",
      "105/281, train_loss: 0.0496, step time: 0.2499\n",
      "106/281, train_loss: 0.0849, step time: 0.2479\n",
      "107/281, train_loss: 0.0712, step time: 0.2537\n",
      "108/281, train_loss: 0.0625, step time: 0.2557\n",
      "109/281, train_loss: 0.0569, step time: 0.2568\n",
      "110/281, train_loss: 0.0733, step time: 0.2548\n",
      "111/281, train_loss: 0.0741, step time: 0.2538\n",
      "112/281, train_loss: 0.0686, step time: 0.2519\n",
      "113/281, train_loss: 0.0701, step time: 0.2490\n",
      "114/281, train_loss: 0.0780, step time: 0.2464\n",
      "115/281, train_loss: 0.0373, step time: 0.2462\n",
      "116/281, train_loss: 0.0785, step time: 0.2566\n",
      "117/281, train_loss: 0.0619, step time: 0.2527\n",
      "118/281, train_loss: 0.0519, step time: 0.2543\n",
      "119/281, train_loss: 0.0652, step time: 0.2501\n",
      "120/281, train_loss: 0.0789, step time: 0.2506\n",
      "121/281, train_loss: 0.0581, step time: 0.2532\n",
      "122/281, train_loss: 0.2363, step time: 0.2474\n",
      "123/281, train_loss: 0.0850, step time: 0.2515\n",
      "124/281, train_loss: 0.0867, step time: 0.2472\n",
      "125/281, train_loss: 0.0556, step time: 0.2510\n",
      "126/281, train_loss: 0.0560, step time: 0.2478\n",
      "127/281, train_loss: 0.0608, step time: 0.2518\n",
      "128/281, train_loss: 0.1023, step time: 0.2561\n",
      "129/281, train_loss: 0.0832, step time: 0.2529\n",
      "130/281, train_loss: 0.0485, step time: 0.2476\n",
      "131/281, train_loss: 0.0616, step time: 0.2503\n",
      "132/281, train_loss: 0.0447, step time: 0.2472\n",
      "133/281, train_loss: 0.0506, step time: 0.2517\n",
      "134/281, train_loss: 0.2468, step time: 0.2523\n",
      "135/281, train_loss: 0.0663, step time: 0.2477\n",
      "136/281, train_loss: 0.0705, step time: 0.2507\n",
      "137/281, train_loss: 0.0467, step time: 0.2537\n",
      "138/281, train_loss: 0.0729, step time: 0.2527\n",
      "139/281, train_loss: 0.0957, step time: 0.2573\n",
      "140/281, train_loss: 0.2218, step time: 0.2534\n",
      "141/281, train_loss: 0.0816, step time: 0.2527\n",
      "142/281, train_loss: 0.0542, step time: 0.2558\n",
      "143/281, train_loss: 0.0618, step time: 0.2520\n",
      "144/281, train_loss: 0.0396, step time: 0.2524\n",
      "145/281, train_loss: 0.0793, step time: 0.2529\n",
      "146/281, train_loss: 0.0848, step time: 0.2560\n",
      "147/281, train_loss: 0.0820, step time: 0.2557\n",
      "148/281, train_loss: 0.0663, step time: 0.2594\n",
      "149/281, train_loss: 0.0559, step time: 0.2542\n",
      "150/281, train_loss: 0.0448, step time: 0.2555\n",
      "151/281, train_loss: 0.2335, step time: 0.2537\n",
      "152/281, train_loss: 0.0627, step time: 0.2522\n",
      "153/281, train_loss: 0.0620, step time: 0.2587\n",
      "154/281, train_loss: 0.2266, step time: 0.2550\n",
      "155/281, train_loss: 0.0623, step time: 0.2533\n",
      "156/281, train_loss: 0.0628, step time: 0.2526\n",
      "157/281, train_loss: 0.0718, step time: 0.2585\n",
      "158/281, train_loss: 0.0810, step time: 0.2534\n",
      "159/281, train_loss: 0.0816, step time: 0.2493\n",
      "160/281, train_loss: 0.0506, step time: 0.2503\n",
      "161/281, train_loss: 0.0553, step time: 0.2559\n",
      "162/281, train_loss: 0.0535, step time: 0.2578\n",
      "163/281, train_loss: 0.0989, step time: 0.2504\n",
      "164/281, train_loss: 0.0748, step time: 0.2545\n",
      "165/281, train_loss: 0.0832, step time: 0.2549\n",
      "166/281, train_loss: 0.0823, step time: 0.2531\n",
      "167/281, train_loss: 0.1126, step time: 0.2562\n",
      "168/281, train_loss: 0.2300, step time: 0.2571\n",
      "169/281, train_loss: 0.0643, step time: 0.2509\n",
      "170/281, train_loss: 0.0448, step time: 0.2593\n",
      "171/281, train_loss: 0.0873, step time: 0.2582\n",
      "172/281, train_loss: 0.0686, step time: 0.2524\n",
      "173/281, train_loss: 0.2229, step time: 0.2535\n",
      "174/281, train_loss: 0.0578, step time: 0.2588\n",
      "175/281, train_loss: 0.0827, step time: 0.2626\n",
      "176/281, train_loss: 0.0603, step time: 0.2574\n",
      "177/281, train_loss: 0.0525, step time: 0.2563\n",
      "178/281, train_loss: 0.0718, step time: 0.2576\n",
      "179/281, train_loss: 0.0690, step time: 0.2577\n",
      "180/281, train_loss: 0.0478, step time: 0.2543\n",
      "181/281, train_loss: 0.2122, step time: 0.2559\n",
      "182/281, train_loss: 0.0725, step time: 0.2579\n",
      "183/281, train_loss: 0.2221, step time: 0.2534\n",
      "184/281, train_loss: 0.0553, step time: 0.2529\n",
      "185/281, train_loss: 0.0657, step time: 0.2545\n",
      "186/281, train_loss: 0.0665, step time: 0.2638\n",
      "187/281, train_loss: 0.0354, step time: 0.2592\n",
      "188/281, train_loss: 0.0834, step time: 0.2575\n",
      "189/281, train_loss: 0.0556, step time: 0.2599\n",
      "190/281, train_loss: 0.0683, step time: 0.2564\n",
      "191/281, train_loss: 0.0698, step time: 0.2549\n",
      "192/281, train_loss: 0.2135, step time: 0.2575\n",
      "193/281, train_loss: 0.0841, step time: 0.2558\n",
      "194/281, train_loss: 0.0711, step time: 0.2572\n",
      "195/281, train_loss: 0.0809, step time: 0.2538\n",
      "196/281, train_loss: 0.0705, step time: 0.2555\n",
      "197/281, train_loss: 0.0718, step time: 0.2582\n",
      "198/281, train_loss: 0.0405, step time: 0.2590\n",
      "199/281, train_loss: 0.2619, step time: 0.2593\n",
      "200/281, train_loss: 0.2232, step time: 0.2534\n",
      "201/281, train_loss: 0.0841, step time: 0.2538\n",
      "202/281, train_loss: 0.0893, step time: 0.2601\n",
      "203/281, train_loss: 0.0527, step time: 0.2645\n",
      "204/281, train_loss: 0.0632, step time: 0.2595\n",
      "205/281, train_loss: 0.0648, step time: 0.2747\n",
      "206/281, train_loss: 0.0714, step time: 0.2593\n",
      "207/281, train_loss: 0.0950, step time: 0.2586\n",
      "208/281, train_loss: 0.0744, step time: 0.2547\n",
      "209/281, train_loss: 0.2399, step time: 0.2511\n",
      "210/281, train_loss: 0.0908, step time: 0.2577\n",
      "211/281, train_loss: 0.2133, step time: 0.2538\n",
      "212/281, train_loss: 0.2016, step time: 0.2583\n",
      "213/281, train_loss: 0.2266, step time: 0.2565\n",
      "214/281, train_loss: 0.0536, step time: 0.2585\n",
      "215/281, train_loss: 0.0590, step time: 0.2561\n",
      "216/281, train_loss: 0.0912, step time: 0.2601\n",
      "217/281, train_loss: 0.0335, step time: 0.2576\n",
      "218/281, train_loss: 0.0847, step time: 0.2523\n",
      "219/281, train_loss: 0.2040, step time: 0.2569\n",
      "220/281, train_loss: 0.0387, step time: 0.2588\n",
      "221/281, train_loss: 0.0924, step time: 0.2542\n",
      "222/281, train_loss: 0.0405, step time: 0.2525\n",
      "223/281, train_loss: 0.0385, step time: 0.2557\n",
      "224/281, train_loss: 0.0459, step time: 0.2554\n",
      "225/281, train_loss: 0.0778, step time: 0.2486\n",
      "226/281, train_loss: 0.0666, step time: 0.2596\n",
      "227/281, train_loss: 0.0740, step time: 0.2556\n",
      "228/281, train_loss: 0.0737, step time: 0.2597\n",
      "229/281, train_loss: 0.0847, step time: 0.2527\n",
      "230/281, train_loss: 0.0924, step time: 0.2547\n",
      "231/281, train_loss: 0.0929, step time: 0.2568\n",
      "232/281, train_loss: 0.2105, step time: 0.2557\n",
      "233/281, train_loss: 0.0403, step time: 0.2494\n",
      "234/281, train_loss: 0.0727, step time: 0.2606\n",
      "235/281, train_loss: 0.0801, step time: 0.2548\n",
      "236/281, train_loss: 0.0486, step time: 0.2536\n",
      "237/281, train_loss: 0.0668, step time: 0.2578\n",
      "238/281, train_loss: 0.1116, step time: 0.2557\n",
      "239/281, train_loss: 0.0622, step time: 0.2546\n",
      "240/281, train_loss: 0.0523, step time: 0.2562\n",
      "241/281, train_loss: 0.2143, step time: 0.2588\n",
      "242/281, train_loss: 0.0261, step time: 0.2548\n",
      "243/281, train_loss: 0.0660, step time: 0.2591\n",
      "244/281, train_loss: 0.0874, step time: 0.2552\n",
      "245/281, train_loss: 0.0610, step time: 0.2581\n",
      "246/281, train_loss: 0.2158, step time: 0.2565\n",
      "247/281, train_loss: 0.2308, step time: 0.2581\n",
      "248/281, train_loss: 0.0860, step time: 0.2513\n",
      "249/281, train_loss: 0.0539, step time: 0.2510\n",
      "250/281, train_loss: 0.0482, step time: 0.2510\n",
      "251/281, train_loss: 0.0471, step time: 0.2541\n",
      "252/281, train_loss: 0.0835, step time: 0.2566\n",
      "253/281, train_loss: 0.0546, step time: 0.2576\n",
      "254/281, train_loss: 0.0833, step time: 0.2526\n",
      "255/281, train_loss: 0.2049, step time: 0.2519\n",
      "256/281, train_loss: 0.0436, step time: 0.2508\n",
      "257/281, train_loss: 0.0554, step time: 0.2506\n",
      "258/281, train_loss: 0.0723, step time: 0.2562\n",
      "259/281, train_loss: 0.0786, step time: 0.2556\n",
      "260/281, train_loss: 0.3867, step time: 0.2501\n",
      "261/281, train_loss: 0.0865, step time: 0.2525\n",
      "262/281, train_loss: 0.0753, step time: 0.2521\n",
      "263/281, train_loss: 0.0698, step time: 0.2543\n",
      "264/281, train_loss: 0.2126, step time: 0.2562\n",
      "265/281, train_loss: 0.0590, step time: 0.2560\n",
      "266/281, train_loss: 0.0868, step time: 0.2541\n",
      "267/281, train_loss: 0.0598, step time: 0.2549\n",
      "268/281, train_loss: 0.0948, step time: 0.2524\n",
      "269/281, train_loss: 0.0615, step time: 0.2569\n",
      "270/281, train_loss: 0.0636, step time: 0.2555\n",
      "271/281, train_loss: 0.0729, step time: 0.2565\n",
      "272/281, train_loss: 0.1090, step time: 0.2521\n",
      "273/281, train_loss: 0.0555, step time: 0.2511\n",
      "274/281, train_loss: 0.0751, step time: 0.2589\n",
      "275/281, train_loss: 0.0821, step time: 0.2581\n",
      "276/281, train_loss: 0.1179, step time: 0.2573\n",
      "277/281, train_loss: 0.0766, step time: 0.2580\n",
      "278/281, train_loss: 0.2208, step time: 0.2505\n",
      "279/281, train_loss: 0.0580, step time: 0.2515\n",
      "280/281, train_loss: 0.0721, step time: 0.2516\n",
      "281/281, train_loss: 0.0645, step time: 0.2489\n",
      "282/281, train_loss: 0.0315, step time: 0.1489\n",
      "epoch 100 average loss: 0.0912\n",
      "current epoch: 100 current mean dice: 0.8972 tc: 0.8878 wt: 0.9261 et: 0.8861\n",
      "best mean dice: 0.8975 at epoch: 99\n",
      "time consuming of epoch 100 is: 438.2410\n",
      "----------\n",
      "epoch 101/200\n",
      "1/281, train_loss: 0.0700, step time: 0.2675\n",
      "2/281, train_loss: 0.1271, step time: 0.2795\n",
      "3/281, train_loss: 0.0898, step time: 0.2784\n",
      "4/281, train_loss: 0.0679, step time: 0.2716\n",
      "5/281, train_loss: 0.0559, step time: 0.2609\n",
      "6/281, train_loss: 0.0939, step time: 0.2690\n",
      "7/281, train_loss: 0.0883, step time: 0.2605\n",
      "8/281, train_loss: 0.0800, step time: 0.2670\n",
      "9/281, train_loss: 0.0782, step time: 0.2549\n",
      "10/281, train_loss: 0.0671, step time: 0.2752\n",
      "11/281, train_loss: 0.2038, step time: 0.2604\n",
      "12/281, train_loss: 0.0738, step time: 0.2532\n",
      "13/281, train_loss: 0.0768, step time: 0.2471\n",
      "14/281, train_loss: 0.0831, step time: 0.2471\n",
      "15/281, train_loss: 0.0732, step time: 0.2568\n",
      "16/281, train_loss: 0.0353, step time: 0.2586\n",
      "17/281, train_loss: 0.2152, step time: 0.2574\n",
      "18/281, train_loss: 0.0600, step time: 0.2791\n",
      "19/281, train_loss: 0.0471, step time: 0.2570\n",
      "20/281, train_loss: 0.0598, step time: 0.2560\n",
      "21/281, train_loss: 0.0426, step time: 0.2553\n",
      "22/281, train_loss: 0.0353, step time: 0.2524\n",
      "23/281, train_loss: 0.0705, step time: 0.2576\n",
      "24/281, train_loss: 0.0733, step time: 0.2593\n",
      "25/281, train_loss: 0.0877, step time: 0.2578\n",
      "26/281, train_loss: 0.0925, step time: 0.2568\n",
      "27/281, train_loss: 0.0924, step time: 0.2529\n",
      "28/281, train_loss: 0.0634, step time: 0.2574\n",
      "29/281, train_loss: 0.0775, step time: 0.2501\n",
      "30/281, train_loss: 0.0922, step time: 0.2565\n",
      "31/281, train_loss: 0.0926, step time: 0.2626\n",
      "32/281, train_loss: 0.0717, step time: 0.2578\n",
      "33/281, train_loss: 0.0547, step time: 0.2875\n",
      "34/281, train_loss: 0.0725, step time: 0.2602\n",
      "35/281, train_loss: 0.3697, step time: 0.2540\n",
      "36/281, train_loss: 0.0759, step time: 0.2519\n",
      "37/281, train_loss: 0.0714, step time: 0.2527\n",
      "38/281, train_loss: 0.0592, step time: 0.2578\n",
      "39/281, train_loss: 0.0500, step time: 0.2600\n",
      "40/281, train_loss: 0.2288, step time: 0.2593\n",
      "41/281, train_loss: 0.0843, step time: 0.2578\n",
      "42/281, train_loss: 0.2071, step time: 0.2581\n",
      "43/281, train_loss: 0.0604, step time: 0.2628\n",
      "44/281, train_loss: 0.0382, step time: 0.2616\n",
      "45/281, train_loss: 0.0787, step time: 0.2554\n",
      "46/281, train_loss: 0.0739, step time: 0.2535\n",
      "47/281, train_loss: 0.0487, step time: 0.2584\n",
      "48/281, train_loss: 0.0985, step time: 0.2600\n",
      "49/281, train_loss: 0.2344, step time: 0.2549\n",
      "50/281, train_loss: 0.2279, step time: 0.2575\n",
      "51/281, train_loss: 0.2319, step time: 0.2552\n",
      "52/281, train_loss: 0.0477, step time: 0.2825\n",
      "53/281, train_loss: 0.0446, step time: 0.2615\n",
      "54/281, train_loss: 0.0671, step time: 0.2595\n",
      "55/281, train_loss: 0.0765, step time: 0.2599\n",
      "56/281, train_loss: 0.0494, step time: 0.2574\n",
      "57/281, train_loss: 0.0769, step time: 0.2518\n",
      "58/281, train_loss: 0.0675, step time: 0.2501\n",
      "59/281, train_loss: 0.0471, step time: 0.2517\n",
      "60/281, train_loss: 0.0569, step time: 0.2551\n",
      "61/281, train_loss: 0.2194, step time: 0.2523\n",
      "62/281, train_loss: 0.0509, step time: 0.2493\n",
      "63/281, train_loss: 0.0725, step time: 0.2539\n",
      "64/281, train_loss: 0.0832, step time: 0.2572\n",
      "65/281, train_loss: 0.0541, step time: 0.2589\n",
      "66/281, train_loss: 0.0494, step time: 0.2536\n",
      "67/281, train_loss: 0.0857, step time: 0.2552\n",
      "68/281, train_loss: 0.0419, step time: 0.2462\n",
      "69/281, train_loss: 0.0659, step time: 0.2566\n",
      "70/281, train_loss: 0.0663, step time: 0.2617\n",
      "71/281, train_loss: 0.0953, step time: 0.2636\n",
      "72/281, train_loss: 0.0727, step time: 0.2520\n",
      "73/281, train_loss: 0.0947, step time: 0.2622\n",
      "74/281, train_loss: 0.0516, step time: 0.2584\n",
      "75/281, train_loss: 0.2876, step time: 0.2594\n",
      "76/281, train_loss: 0.0421, step time: 0.2575\n",
      "77/281, train_loss: 0.0536, step time: 0.2550\n",
      "78/281, train_loss: 0.2214, step time: 0.2531\n",
      "79/281, train_loss: 0.2364, step time: 0.2596\n",
      "80/281, train_loss: 0.0426, step time: 0.2636\n",
      "81/281, train_loss: 0.0488, step time: 0.2572\n",
      "82/281, train_loss: 0.0604, step time: 0.2585\n",
      "83/281, train_loss: 0.0719, step time: 0.2583\n",
      "84/281, train_loss: 0.0710, step time: 0.2630\n",
      "85/281, train_loss: 0.0936, step time: 0.2668\n",
      "86/281, train_loss: 0.0515, step time: 0.2664\n",
      "87/281, train_loss: 0.2103, step time: 0.2678\n",
      "88/281, train_loss: 0.0594, step time: 0.2691\n",
      "89/281, train_loss: 0.0672, step time: 0.2601\n",
      "90/281, train_loss: 0.0697, step time: 0.2595\n",
      "91/281, train_loss: 0.0471, step time: 0.2556\n",
      "92/281, train_loss: 0.0624, step time: 0.2531\n",
      "93/281, train_loss: 0.0373, step time: 0.2634\n",
      "94/281, train_loss: 0.0603, step time: 0.2602\n",
      "95/281, train_loss: 0.0509, step time: 0.2610\n",
      "96/281, train_loss: 0.0968, step time: 0.2622\n",
      "97/281, train_loss: 0.0406, step time: 0.2592\n",
      "98/281, train_loss: 0.0373, step time: 0.2580\n",
      "99/281, train_loss: 0.0483, step time: 0.2584\n",
      "100/281, train_loss: 0.2722, step time: 0.2539\n",
      "101/281, train_loss: 0.0722, step time: 0.2580\n",
      "102/281, train_loss: 0.0631, step time: 0.2715\n",
      "103/281, train_loss: 0.0470, step time: 0.2588\n",
      "104/281, train_loss: 0.0673, step time: 0.2572\n",
      "105/281, train_loss: 0.0472, step time: 0.2586\n",
      "106/281, train_loss: 0.0774, step time: 0.2681\n",
      "107/281, train_loss: 0.0456, step time: 0.2552\n",
      "108/281, train_loss: 0.0862, step time: 0.2562\n",
      "109/281, train_loss: 0.0635, step time: 0.2616\n",
      "110/281, train_loss: 0.0542, step time: 0.2578\n",
      "111/281, train_loss: 0.0876, step time: 0.2560\n",
      "112/281, train_loss: 0.0721, step time: 0.2577\n",
      "113/281, train_loss: 0.0672, step time: 0.2558\n",
      "114/281, train_loss: 0.0626, step time: 0.2667\n",
      "115/281, train_loss: 0.0463, step time: 0.2676\n",
      "116/281, train_loss: 0.0581, step time: 0.2512\n",
      "117/281, train_loss: 0.0685, step time: 0.2590\n",
      "118/281, train_loss: 0.0803, step time: 0.2562\n",
      "119/281, train_loss: 0.0436, step time: 0.2702\n",
      "120/281, train_loss: 0.0640, step time: 0.2785\n",
      "121/281, train_loss: 0.0844, step time: 0.2534\n",
      "122/281, train_loss: 0.0347, step time: 0.2580\n",
      "123/281, train_loss: 0.0566, step time: 0.2573\n",
      "124/281, train_loss: 0.0869, step time: 0.2498\n",
      "125/281, train_loss: 0.2215, step time: 0.2542\n",
      "126/281, train_loss: 0.2564, step time: 0.2564\n",
      "127/281, train_loss: 0.0563, step time: 0.2563\n",
      "128/281, train_loss: 0.0533, step time: 0.2523\n",
      "129/281, train_loss: 0.0824, step time: 0.2569\n",
      "130/281, train_loss: 0.0826, step time: 0.2561\n",
      "131/281, train_loss: 0.0504, step time: 0.2575\n",
      "132/281, train_loss: 0.0425, step time: 0.2581\n",
      "133/281, train_loss: 0.0626, step time: 0.2504\n",
      "134/281, train_loss: 0.0611, step time: 0.2521\n",
      "135/281, train_loss: 0.2106, step time: 0.2535\n",
      "136/281, train_loss: 0.0635, step time: 0.2591\n",
      "137/281, train_loss: 0.0481, step time: 0.2515\n",
      "138/281, train_loss: 0.0609, step time: 0.2585\n",
      "139/281, train_loss: 0.0612, step time: 0.2600\n",
      "140/281, train_loss: 0.0581, step time: 0.2552\n",
      "141/281, train_loss: 0.0751, step time: 0.2606\n",
      "142/281, train_loss: 0.0495, step time: 0.2684\n",
      "143/281, train_loss: 0.0593, step time: 0.2635\n",
      "144/281, train_loss: 0.0699, step time: 0.2549\n",
      "145/281, train_loss: 0.0369, step time: 0.2636\n",
      "146/281, train_loss: 0.0491, step time: 0.2609\n",
      "147/281, train_loss: 0.0749, step time: 0.2619\n",
      "148/281, train_loss: 0.0675, step time: 0.2596\n",
      "149/281, train_loss: 0.0506, step time: 0.2557\n",
      "150/281, train_loss: 0.3714, step time: 0.2558\n",
      "151/281, train_loss: 0.2479, step time: 0.2595\n",
      "152/281, train_loss: 0.0635, step time: 0.2541\n",
      "153/281, train_loss: 0.0736, step time: 0.2527\n",
      "154/281, train_loss: 0.0702, step time: 0.2570\n",
      "155/281, train_loss: 0.0478, step time: 0.2548\n",
      "156/281, train_loss: 0.2700, step time: 0.2535\n",
      "157/281, train_loss: 0.0547, step time: 0.2600\n",
      "158/281, train_loss: 0.0531, step time: 0.2512\n",
      "159/281, train_loss: 0.0296, step time: 0.2598\n",
      "160/281, train_loss: 0.2197, step time: 0.2533\n",
      "161/281, train_loss: 0.0445, step time: 0.2628\n",
      "162/281, train_loss: 0.2279, step time: 0.2555\n",
      "163/281, train_loss: 0.0738, step time: 0.2587\n",
      "164/281, train_loss: 0.0547, step time: 0.2573\n",
      "165/281, train_loss: 0.0574, step time: 0.2521\n",
      "166/281, train_loss: 0.0421, step time: 0.2528\n",
      "167/281, train_loss: 0.0647, step time: 0.2572\n",
      "168/281, train_loss: 0.0449, step time: 0.2601\n",
      "169/281, train_loss: 0.0611, step time: 0.2522\n",
      "170/281, train_loss: 0.0589, step time: 0.2551\n",
      "171/281, train_loss: 0.0725, step time: 0.2554\n",
      "172/281, train_loss: 0.0390, step time: 0.2543\n",
      "173/281, train_loss: 0.0321, step time: 0.2583\n",
      "174/281, train_loss: 0.0419, step time: 0.2565\n",
      "175/281, train_loss: 0.0686, step time: 0.2554\n",
      "176/281, train_loss: 0.1040, step time: 0.2545\n",
      "177/281, train_loss: 0.0540, step time: 0.2573\n",
      "178/281, train_loss: 0.0646, step time: 0.2611\n",
      "179/281, train_loss: 0.0450, step time: 0.2798\n",
      "180/281, train_loss: 0.2436, step time: 0.2577\n",
      "181/281, train_loss: 0.0570, step time: 0.2614\n",
      "182/281, train_loss: 0.2186, step time: 0.2553\n",
      "183/281, train_loss: 0.0818, step time: 0.2510\n",
      "184/281, train_loss: 0.0729, step time: 0.2541\n",
      "185/281, train_loss: 0.0859, step time: 0.2610\n",
      "186/281, train_loss: 0.0775, step time: 0.2538\n",
      "187/281, train_loss: 0.0720, step time: 0.2523\n",
      "188/281, train_loss: 0.0560, step time: 0.2554\n",
      "189/281, train_loss: 0.0691, step time: 0.2577\n",
      "190/281, train_loss: 0.0729, step time: 0.2592\n",
      "191/281, train_loss: 0.1027, step time: 0.2612\n",
      "192/281, train_loss: 0.0605, step time: 0.2592\n",
      "193/281, train_loss: 0.0590, step time: 0.2558\n",
      "194/281, train_loss: 0.1255, step time: 0.2587\n",
      "195/281, train_loss: 0.0902, step time: 0.2565\n",
      "196/281, train_loss: 0.0524, step time: 0.2574\n",
      "197/281, train_loss: 0.0629, step time: 0.2514\n",
      "198/281, train_loss: 0.2290, step time: 0.2535\n",
      "199/281, train_loss: 0.0560, step time: 0.2520\n",
      "200/281, train_loss: 0.0770, step time: 0.2537\n",
      "201/281, train_loss: 0.0527, step time: 0.2527\n",
      "202/281, train_loss: 0.2018, step time: 0.2529\n",
      "203/281, train_loss: 0.2204, step time: 0.2604\n",
      "204/281, train_loss: 0.0378, step time: 0.2722\n",
      "205/281, train_loss: 0.0674, step time: 0.2554\n",
      "206/281, train_loss: 0.0783, step time: 0.2548\n",
      "207/281, train_loss: 0.2178, step time: 0.2539\n",
      "208/281, train_loss: 0.2138, step time: 0.2555\n",
      "209/281, train_loss: 0.0330, step time: 0.2507\n",
      "210/281, train_loss: 0.0700, step time: 0.2536\n",
      "211/281, train_loss: 0.0694, step time: 0.2543\n",
      "212/281, train_loss: 0.1053, step time: 0.2493\n",
      "213/281, train_loss: 0.0793, step time: 0.2590\n",
      "214/281, train_loss: 0.0578, step time: 0.2581\n",
      "215/281, train_loss: 0.1965, step time: 0.2654\n",
      "216/281, train_loss: 0.0568, step time: 0.2514\n",
      "217/281, train_loss: 0.0831, step time: 0.2517\n",
      "218/281, train_loss: 0.0615, step time: 0.2571\n",
      "219/281, train_loss: 0.0699, step time: 0.2574\n",
      "220/281, train_loss: 0.0755, step time: 0.2560\n",
      "221/281, train_loss: 0.1067, step time: 0.2586\n",
      "222/281, train_loss: 0.2064, step time: 0.2575\n",
      "223/281, train_loss: 0.0467, step time: 0.2597\n",
      "224/281, train_loss: 0.0415, step time: 0.2543\n",
      "225/281, train_loss: 0.0557, step time: 0.2558\n",
      "226/281, train_loss: 0.0811, step time: 0.2521\n",
      "227/281, train_loss: 0.2397, step time: 0.2508\n",
      "228/281, train_loss: 0.0536, step time: 0.2523\n",
      "229/281, train_loss: 0.0342, step time: 0.2529\n",
      "230/281, train_loss: 0.2272, step time: 0.2571\n",
      "231/281, train_loss: 0.0385, step time: 0.2538\n",
      "232/281, train_loss: 0.0611, step time: 0.2526\n",
      "233/281, train_loss: 0.0544, step time: 0.2495\n",
      "234/281, train_loss: 0.0739, step time: 0.2499\n",
      "235/281, train_loss: 0.2078, step time: 0.2514\n",
      "236/281, train_loss: 0.0840, step time: 0.2501\n",
      "237/281, train_loss: 0.2406, step time: 0.2513\n",
      "238/281, train_loss: 0.0738, step time: 0.2524\n",
      "239/281, train_loss: 0.0924, step time: 0.2563\n",
      "240/281, train_loss: 0.0646, step time: 0.2542\n",
      "241/281, train_loss: 0.0507, step time: 0.2596\n",
      "242/281, train_loss: 0.2224, step time: 0.2552\n",
      "243/281, train_loss: 0.0751, step time: 0.2524\n",
      "244/281, train_loss: 0.0816, step time: 0.2560\n",
      "245/281, train_loss: 0.0758, step time: 0.2585\n",
      "246/281, train_loss: 0.2125, step time: 0.2557\n",
      "247/281, train_loss: 0.0674, step time: 0.2533\n",
      "248/281, train_loss: 0.0500, step time: 0.2543\n",
      "249/281, train_loss: 0.0798, step time: 0.2539\n",
      "250/281, train_loss: 0.0758, step time: 0.2579\n",
      "251/281, train_loss: 0.0681, step time: 0.2514\n",
      "252/281, train_loss: 0.0617, step time: 0.2493\n",
      "253/281, train_loss: 0.0666, step time: 0.2542\n",
      "254/281, train_loss: 0.0613, step time: 0.2501\n",
      "255/281, train_loss: 0.0723, step time: 0.2517\n",
      "256/281, train_loss: 0.0488, step time: 0.2508\n",
      "257/281, train_loss: 0.0599, step time: 0.2502\n",
      "258/281, train_loss: 0.0657, step time: 0.2542\n",
      "259/281, train_loss: 0.2096, step time: 0.2539\n",
      "260/281, train_loss: 0.1031, step time: 0.2466\n",
      "261/281, train_loss: 0.0676, step time: 0.2527\n",
      "262/281, train_loss: 0.2124, step time: 0.2480\n",
      "263/281, train_loss: 0.0444, step time: 0.2513\n",
      "264/281, train_loss: 0.0692, step time: 0.2502\n",
      "265/281, train_loss: 0.0566, step time: 0.2549\n",
      "266/281, train_loss: 0.2304, step time: 0.2526\n",
      "267/281, train_loss: 0.0805, step time: 0.2502\n",
      "268/281, train_loss: 0.0504, step time: 0.2520\n",
      "269/281, train_loss: 0.1353, step time: 0.2523\n",
      "270/281, train_loss: 0.0570, step time: 0.2510\n",
      "271/281, train_loss: 0.0636, step time: 0.2481\n",
      "272/281, train_loss: 0.0798, step time: 0.2514\n",
      "273/281, train_loss: 0.0758, step time: 0.2477\n",
      "274/281, train_loss: 0.2197, step time: 0.2550\n",
      "275/281, train_loss: 0.1111, step time: 0.2557\n",
      "276/281, train_loss: 0.0536, step time: 0.2537\n",
      "277/281, train_loss: 0.0866, step time: 0.2516\n",
      "278/281, train_loss: 0.0745, step time: 0.2507\n",
      "279/281, train_loss: 0.1002, step time: 0.2536\n",
      "280/281, train_loss: 0.0772, step time: 0.2508\n",
      "281/281, train_loss: 0.0717, step time: 0.2497\n",
      "282/281, train_loss: 0.0367, step time: 0.1489\n",
      "epoch 101 average loss: 0.0901\n",
      "current epoch: 101 current mean dice: 0.8972 tc: 0.8901 wt: 0.9246 et: 0.8861\n",
      "best mean dice: 0.8975 at epoch: 99\n",
      "time consuming of epoch 101 is: 344.8709\n",
      "----------\n",
      "epoch 102/200\n",
      "1/281, train_loss: 0.0716, step time: 0.2494\n",
      "2/281, train_loss: 0.0612, step time: 0.2498\n",
      "3/281, train_loss: 0.0672, step time: 0.2505\n",
      "4/281, train_loss: 0.0865, step time: 0.2479\n",
      "5/281, train_loss: 0.0594, step time: 0.2444\n",
      "6/281, train_loss: 0.0574, step time: 0.2492\n",
      "7/281, train_loss: 0.1174, step time: 0.2459\n",
      "8/281, train_loss: 0.1133, step time: 0.2441\n",
      "9/281, train_loss: 0.0548, step time: 0.2674\n",
      "10/281, train_loss: 0.0408, step time: 0.2480\n",
      "11/281, train_loss: 0.0846, step time: 0.2551\n",
      "12/281, train_loss: 0.0762, step time: 0.2515\n",
      "13/281, train_loss: 0.0857, step time: 0.2534\n",
      "14/281, train_loss: 0.0470, step time: 0.2563\n",
      "15/281, train_loss: 0.0519, step time: 0.2529\n",
      "16/281, train_loss: 0.0745, step time: 0.2515\n",
      "17/281, train_loss: 0.1001, step time: 0.2600\n",
      "18/281, train_loss: 0.0554, step time: 0.2587\n",
      "19/281, train_loss: 0.0465, step time: 0.2588\n",
      "20/281, train_loss: 0.0655, step time: 0.2543\n",
      "21/281, train_loss: 0.0628, step time: 0.2516\n",
      "22/281, train_loss: 0.2306, step time: 0.2511\n",
      "23/281, train_loss: 0.0715, step time: 0.2545\n",
      "24/281, train_loss: 0.0759, step time: 0.2500\n",
      "25/281, train_loss: 0.0674, step time: 0.2469\n",
      "26/281, train_loss: 0.0732, step time: 0.2445\n",
      "27/281, train_loss: 0.0526, step time: 0.2503\n",
      "28/281, train_loss: 0.2084, step time: 0.2553\n",
      "29/281, train_loss: 0.0661, step time: 0.2536\n",
      "30/281, train_loss: 0.0617, step time: 0.2601\n",
      "31/281, train_loss: 0.0666, step time: 0.2561\n",
      "32/281, train_loss: 0.2160, step time: 0.2486\n",
      "33/281, train_loss: 0.0503, step time: 0.2541\n",
      "34/281, train_loss: 0.0292, step time: 0.2491\n",
      "35/281, train_loss: 0.1135, step time: 0.2505\n",
      "36/281, train_loss: 0.0600, step time: 0.2535\n",
      "37/281, train_loss: 0.0610, step time: 0.2516\n",
      "38/281, train_loss: 0.0644, step time: 0.2580\n",
      "39/281, train_loss: 0.0659, step time: 0.2549\n",
      "40/281, train_loss: 0.2041, step time: 0.2518\n",
      "41/281, train_loss: 0.0554, step time: 0.2521\n",
      "42/281, train_loss: 0.0852, step time: 0.2476\n",
      "43/281, train_loss: 0.0668, step time: 0.2513\n",
      "44/281, train_loss: 0.0503, step time: 0.2504\n",
      "45/281, train_loss: 0.2145, step time: 0.2493\n",
      "46/281, train_loss: 0.0855, step time: 0.2532\n",
      "47/281, train_loss: 0.2368, step time: 0.2551\n",
      "48/281, train_loss: 0.0470, step time: 0.2488\n",
      "49/281, train_loss: 0.0446, step time: 0.2522\n",
      "50/281, train_loss: 0.0493, step time: 0.2553\n",
      "51/281, train_loss: 0.0591, step time: 0.2790\n",
      "52/281, train_loss: 0.2125, step time: 0.2460\n",
      "53/281, train_loss: 0.0658, step time: 0.2491\n",
      "54/281, train_loss: 0.0690, step time: 0.2592\n",
      "55/281, train_loss: 0.2473, step time: 0.2553\n",
      "56/281, train_loss: 0.0653, step time: 0.2518\n",
      "57/281, train_loss: 0.0567, step time: 0.2542\n",
      "58/281, train_loss: 0.0580, step time: 0.2597\n",
      "59/281, train_loss: 0.0384, step time: 0.2642\n",
      "60/281, train_loss: 0.2130, step time: 0.2560\n",
      "61/281, train_loss: 0.3700, step time: 0.2559\n",
      "62/281, train_loss: 0.0783, step time: 0.2530\n",
      "63/281, train_loss: 0.0886, step time: 0.2503\n",
      "64/281, train_loss: 0.0428, step time: 0.2563\n",
      "65/281, train_loss: 0.0494, step time: 0.2505\n",
      "66/281, train_loss: 0.2184, step time: 0.2527\n",
      "67/281, train_loss: 0.0793, step time: 0.2525\n",
      "68/281, train_loss: 0.0719, step time: 0.2515\n",
      "69/281, train_loss: 0.0629, step time: 0.2494\n",
      "70/281, train_loss: 0.0445, step time: 0.2544\n",
      "71/281, train_loss: 0.0652, step time: 0.2510\n",
      "72/281, train_loss: 0.0820, step time: 0.2536\n",
      "73/281, train_loss: 0.0695, step time: 0.2567\n",
      "74/281, train_loss: 0.0657, step time: 0.2537\n",
      "75/281, train_loss: 0.2197, step time: 0.2617\n",
      "76/281, train_loss: 0.0470, step time: 0.2537\n",
      "77/281, train_loss: 0.0568, step time: 0.2532\n",
      "78/281, train_loss: 0.0798, step time: 0.2514\n",
      "79/281, train_loss: 0.0845, step time: 0.2615\n",
      "80/281, train_loss: 0.0858, step time: 0.2529\n",
      "81/281, train_loss: 0.0691, step time: 0.2519\n",
      "82/281, train_loss: 0.0484, step time: 0.2476\n",
      "83/281, train_loss: 0.0700, step time: 0.2531\n",
      "84/281, train_loss: 0.1349, step time: 0.2520\n",
      "85/281, train_loss: 0.0587, step time: 0.2531\n",
      "86/281, train_loss: 0.2178, step time: 0.2600\n",
      "87/281, train_loss: 0.0752, step time: 0.2632\n",
      "88/281, train_loss: 0.3809, step time: 0.2580\n",
      "89/281, train_loss: 0.0555, step time: 0.2562\n",
      "90/281, train_loss: 0.0964, step time: 0.2561\n",
      "91/281, train_loss: 0.0435, step time: 0.2523\n",
      "92/281, train_loss: 0.0516, step time: 0.2509\n",
      "93/281, train_loss: 0.0533, step time: 0.2465\n",
      "94/281, train_loss: 0.0685, step time: 0.2577\n",
      "95/281, train_loss: 0.0461, step time: 0.2578\n",
      "96/281, train_loss: 0.0569, step time: 0.2436\n",
      "97/281, train_loss: 0.2085, step time: 0.2516\n",
      "98/281, train_loss: 0.0508, step time: 0.2464\n",
      "99/281, train_loss: 0.0613, step time: 0.2537\n",
      "100/281, train_loss: 0.0630, step time: 0.2506\n",
      "101/281, train_loss: 0.0751, step time: 0.2523\n",
      "102/281, train_loss: 0.1987, step time: 0.2530\n",
      "103/281, train_loss: 0.0830, step time: 0.2520\n",
      "104/281, train_loss: 0.0566, step time: 0.2559\n",
      "105/281, train_loss: 0.0533, step time: 0.2517\n",
      "106/281, train_loss: 0.0332, step time: 0.2538\n",
      "107/281, train_loss: 0.0753, step time: 0.2490\n",
      "108/281, train_loss: 0.0863, step time: 0.2523\n",
      "109/281, train_loss: 0.0642, step time: 0.2549\n",
      "110/281, train_loss: 0.0746, step time: 0.2517\n",
      "111/281, train_loss: 0.2141, step time: 0.2555\n",
      "112/281, train_loss: 0.0660, step time: 0.2520\n",
      "113/281, train_loss: 0.0688, step time: 0.2549\n",
      "114/281, train_loss: 0.0477, step time: 0.2548\n",
      "115/281, train_loss: 0.0411, step time: 0.2546\n",
      "116/281, train_loss: 0.2134, step time: 0.2519\n",
      "117/281, train_loss: 0.0365, step time: 0.2531\n",
      "118/281, train_loss: 0.0399, step time: 0.2522\n",
      "119/281, train_loss: 0.1073, step time: 0.2564\n",
      "120/281, train_loss: 0.2517, step time: 0.2531\n",
      "121/281, train_loss: 0.0466, step time: 0.2555\n",
      "122/281, train_loss: 0.0715, step time: 0.2509\n",
      "123/281, train_loss: 0.0610, step time: 0.2530\n",
      "124/281, train_loss: 0.2300, step time: 0.2500\n",
      "125/281, train_loss: 0.0363, step time: 0.2591\n",
      "126/281, train_loss: 0.2125, step time: 0.2516\n",
      "127/281, train_loss: 0.0379, step time: 0.2550\n",
      "128/281, train_loss: 0.0624, step time: 0.2520\n",
      "129/281, train_loss: 0.0516, step time: 0.2547\n",
      "130/281, train_loss: 0.0322, step time: 0.2460\n",
      "131/281, train_loss: 0.0571, step time: 0.2449\n",
      "132/281, train_loss: 0.2261, step time: 0.2465\n",
      "133/281, train_loss: 0.0637, step time: 0.2455\n",
      "134/281, train_loss: 0.0584, step time: 0.2553\n",
      "135/281, train_loss: 0.0367, step time: 0.2521\n",
      "136/281, train_loss: 0.2133, step time: 0.2474\n",
      "137/281, train_loss: 0.0883, step time: 0.2520\n",
      "138/281, train_loss: 0.2172, step time: 0.2541\n",
      "139/281, train_loss: 0.0812, step time: 0.2559\n",
      "140/281, train_loss: 0.0514, step time: 0.2532\n",
      "141/281, train_loss: 0.0461, step time: 0.2533\n",
      "142/281, train_loss: 0.0862, step time: 0.2518\n",
      "143/281, train_loss: 0.0640, step time: 0.2498\n",
      "144/281, train_loss: 0.0763, step time: 0.2477\n",
      "145/281, train_loss: 0.0718, step time: 0.2485\n",
      "146/281, train_loss: 0.0418, step time: 0.2485\n",
      "147/281, train_loss: 0.0886, step time: 0.2531\n",
      "148/281, train_loss: 0.0514, step time: 0.2494\n",
      "149/281, train_loss: 0.0756, step time: 0.2535\n",
      "150/281, train_loss: 0.0794, step time: 0.2483\n",
      "151/281, train_loss: 0.0617, step time: 0.2579\n",
      "152/281, train_loss: 0.0588, step time: 0.2539\n",
      "153/281, train_loss: 0.0805, step time: 0.2513\n",
      "154/281, train_loss: 0.0606, step time: 0.2572\n",
      "155/281, train_loss: 0.2452, step time: 0.2476\n",
      "156/281, train_loss: 0.0582, step time: 0.2545\n",
      "157/281, train_loss: 0.0568, step time: 0.2509\n",
      "158/281, train_loss: 0.0629, step time: 0.2556\n",
      "159/281, train_loss: 0.0656, step time: 0.2576\n",
      "160/281, train_loss: 0.0396, step time: 0.2629\n",
      "161/281, train_loss: 0.0843, step time: 0.2476\n",
      "162/281, train_loss: 0.0538, step time: 0.2508\n",
      "163/281, train_loss: 0.0655, step time: 0.2496\n",
      "164/281, train_loss: 0.0532, step time: 0.2677\n",
      "165/281, train_loss: 0.0596, step time: 0.2519\n",
      "166/281, train_loss: 0.0658, step time: 0.2559\n",
      "167/281, train_loss: 0.0759, step time: 0.2521\n",
      "168/281, train_loss: 0.2199, step time: 0.2505\n",
      "169/281, train_loss: 0.0546, step time: 0.2504\n",
      "170/281, train_loss: 0.0752, step time: 0.2513\n",
      "171/281, train_loss: 0.0368, step time: 0.2470\n",
      "172/281, train_loss: 0.0965, step time: 0.2502\n",
      "173/281, train_loss: 0.0738, step time: 0.2502\n",
      "174/281, train_loss: 0.0451, step time: 0.2510\n",
      "175/281, train_loss: 0.0484, step time: 0.2530\n",
      "176/281, train_loss: 0.0929, step time: 0.2481\n",
      "177/281, train_loss: 0.0725, step time: 0.2552\n",
      "178/281, train_loss: 0.0773, step time: 0.2533\n",
      "179/281, train_loss: 0.2341, step time: 0.2499\n",
      "180/281, train_loss: 0.0798, step time: 0.2470\n",
      "181/281, train_loss: 0.2132, step time: 0.2478\n",
      "182/281, train_loss: 0.0453, step time: 0.2545\n",
      "183/281, train_loss: 0.1133, step time: 0.2507\n",
      "184/281, train_loss: 0.0636, step time: 0.2525\n",
      "185/281, train_loss: 0.0424, step time: 0.2528\n",
      "186/281, train_loss: 0.0611, step time: 0.2540\n",
      "187/281, train_loss: 0.0506, step time: 0.2475\n",
      "188/281, train_loss: 0.0583, step time: 0.2513\n",
      "189/281, train_loss: 0.0571, step time: 0.2537\n",
      "190/281, train_loss: 0.2787, step time: 0.2519\n",
      "191/281, train_loss: 0.0663, step time: 0.2476\n",
      "192/281, train_loss: 0.0336, step time: 0.2491\n",
      "193/281, train_loss: 0.0562, step time: 0.2590\n",
      "194/281, train_loss: 0.0779, step time: 0.2544\n",
      "195/281, train_loss: 0.0615, step time: 0.2555\n",
      "196/281, train_loss: 0.0985, step time: 0.2543\n",
      "197/281, train_loss: 0.0732, step time: 0.2549\n",
      "198/281, train_loss: 0.0483, step time: 0.2510\n",
      "199/281, train_loss: 0.1952, step time: 0.2470\n",
      "200/281, train_loss: 0.0744, step time: 0.2533\n",
      "201/281, train_loss: 0.0605, step time: 0.2522\n",
      "202/281, train_loss: 0.0436, step time: 0.2571\n",
      "203/281, train_loss: 0.0484, step time: 0.2562\n",
      "204/281, train_loss: 0.0560, step time: 0.2510\n",
      "205/281, train_loss: 0.0777, step time: 0.2577\n",
      "206/281, train_loss: 0.0393, step time: 0.2583\n",
      "207/281, train_loss: 0.2302, step time: 0.2530\n",
      "208/281, train_loss: 0.2222, step time: 0.2506\n",
      "209/281, train_loss: 0.0373, step time: 0.2549\n",
      "210/281, train_loss: 0.0499, step time: 0.2534\n",
      "211/281, train_loss: 0.0750, step time: 0.2500\n",
      "212/281, train_loss: 0.2129, step time: 0.2506\n",
      "213/281, train_loss: 0.2201, step time: 0.2583\n",
      "214/281, train_loss: 0.0610, step time: 0.2635\n",
      "215/281, train_loss: 0.0841, step time: 0.2540\n",
      "216/281, train_loss: 0.0894, step time: 0.2550\n",
      "217/281, train_loss: 0.0540, step time: 0.2592\n",
      "218/281, train_loss: 0.0959, step time: 0.2531\n",
      "219/281, train_loss: 0.2632, step time: 0.2583\n",
      "220/281, train_loss: 0.0717, step time: 0.2481\n",
      "221/281, train_loss: 0.0478, step time: 0.2714\n",
      "222/281, train_loss: 0.0477, step time: 0.2534\n",
      "223/281, train_loss: 0.2127, step time: 0.2517\n",
      "224/281, train_loss: 0.2213, step time: 0.2492\n",
      "225/281, train_loss: 0.0693, step time: 0.2490\n",
      "226/281, train_loss: 0.0821, step time: 0.2512\n",
      "227/281, train_loss: 0.0865, step time: 0.2505\n",
      "228/281, train_loss: 0.0387, step time: 0.2555\n",
      "229/281, train_loss: 0.0581, step time: 0.2489\n",
      "230/281, train_loss: 0.1036, step time: 0.2545\n",
      "231/281, train_loss: 0.0654, step time: 0.2565\n",
      "232/281, train_loss: 0.0955, step time: 0.2586\n",
      "233/281, train_loss: 0.0581, step time: 0.2531\n",
      "234/281, train_loss: 0.0711, step time: 0.2519\n",
      "235/281, train_loss: 0.1028, step time: 0.2549\n",
      "236/281, train_loss: 0.0604, step time: 0.2548\n",
      "237/281, train_loss: 0.0466, step time: 0.2552\n",
      "238/281, train_loss: 0.0695, step time: 0.2554\n",
      "239/281, train_loss: 0.0788, step time: 0.2561\n",
      "240/281, train_loss: 0.0652, step time: 0.2560\n",
      "241/281, train_loss: 0.0752, step time: 0.2717\n",
      "242/281, train_loss: 0.2223, step time: 0.2545\n",
      "243/281, train_loss: 0.0535, step time: 0.2814\n",
      "244/281, train_loss: 0.0833, step time: 0.2570\n",
      "245/281, train_loss: 0.0425, step time: 0.2597\n",
      "246/281, train_loss: 0.0605, step time: 0.2555\n",
      "247/281, train_loss: 0.0670, step time: 0.2587\n",
      "248/281, train_loss: 0.0503, step time: 0.2586\n",
      "249/281, train_loss: 0.0690, step time: 0.2541\n",
      "250/281, train_loss: 0.0641, step time: 0.2582\n",
      "251/281, train_loss: 0.0396, step time: 0.2555\n",
      "252/281, train_loss: 0.0758, step time: 0.2602\n",
      "253/281, train_loss: 0.0701, step time: 0.2548\n",
      "254/281, train_loss: 0.0695, step time: 0.2596\n",
      "255/281, train_loss: 0.0439, step time: 0.2538\n",
      "256/281, train_loss: 0.1023, step time: 0.2533\n",
      "257/281, train_loss: 0.0677, step time: 0.2568\n",
      "258/281, train_loss: 0.0812, step time: 0.2553\n",
      "259/281, train_loss: 0.0523, step time: 0.2505\n",
      "260/281, train_loss: 0.0580, step time: 0.2517\n",
      "261/281, train_loss: 0.0738, step time: 0.2548\n",
      "262/281, train_loss: 0.0479, step time: 0.2550\n",
      "263/281, train_loss: 0.2374, step time: 0.2576\n",
      "264/281, train_loss: 0.0568, step time: 0.2576\n",
      "265/281, train_loss: 0.0573, step time: 0.2567\n",
      "266/281, train_loss: 0.0558, step time: 0.2557\n",
      "267/281, train_loss: 0.0618, step time: 0.2527\n",
      "268/281, train_loss: 0.0901, step time: 0.2607\n",
      "269/281, train_loss: 0.0552, step time: 0.2508\n",
      "270/281, train_loss: 0.0483, step time: 0.2517\n",
      "271/281, train_loss: 0.0551, step time: 0.2581\n",
      "272/281, train_loss: 0.0743, step time: 0.2557\n",
      "273/281, train_loss: 0.0738, step time: 0.2573\n",
      "274/281, train_loss: 0.0804, step time: 0.2605\n",
      "275/281, train_loss: 0.0654, step time: 0.2602\n",
      "276/281, train_loss: 0.0618, step time: 0.2529\n",
      "277/281, train_loss: 0.0661, step time: 0.2582\n",
      "278/281, train_loss: 0.0777, step time: 0.2574\n",
      "279/281, train_loss: 0.2332, step time: 0.2582\n",
      "280/281, train_loss: 0.0415, step time: 0.2565\n",
      "281/281, train_loss: 0.0972, step time: 0.2552\n",
      "282/281, train_loss: 0.3760, step time: 0.1524\n",
      "epoch 102 average loss: 0.0895\n",
      "saved new best metric model\n",
      "current epoch: 102 current mean dice: 0.8995 tc: 0.8927 wt: 0.9274 et: 0.8868\n",
      "best mean dice: 0.8995 at epoch: 102\n",
      "time consuming of epoch 102 is: 393.1306\n",
      "----------\n",
      "epoch 103/200\n",
      "1/281, train_loss: 0.0407, step time: 0.2610\n",
      "2/281, train_loss: 0.0826, step time: 0.2506\n",
      "3/281, train_loss: 0.0867, step time: 0.2486\n",
      "4/281, train_loss: 0.0800, step time: 0.2875\n",
      "5/281, train_loss: 0.0608, step time: 0.2520\n",
      "6/281, train_loss: 0.0496, step time: 0.2504\n",
      "7/281, train_loss: 0.1984, step time: 0.2519\n",
      "8/281, train_loss: 0.0474, step time: 0.2588\n",
      "9/281, train_loss: 0.0760, step time: 0.2747\n",
      "10/281, train_loss: 0.0455, step time: 0.2581\n",
      "11/281, train_loss: 0.0501, step time: 0.2582\n",
      "12/281, train_loss: 0.0466, step time: 0.2531\n",
      "13/281, train_loss: 0.0558, step time: 0.2634\n",
      "14/281, train_loss: 0.0431, step time: 0.2498\n",
      "15/281, train_loss: 0.2445, step time: 0.2595\n",
      "16/281, train_loss: 0.0643, step time: 0.2618\n",
      "17/281, train_loss: 0.0625, step time: 0.2506\n",
      "18/281, train_loss: 0.0665, step time: 0.2538\n",
      "19/281, train_loss: 0.0465, step time: 0.2497\n",
      "20/281, train_loss: 0.0645, step time: 0.2516\n",
      "21/281, train_loss: 0.2263, step time: 0.2484\n",
      "22/281, train_loss: 0.0479, step time: 0.2482\n",
      "23/281, train_loss: 0.0454, step time: 0.2541\n",
      "24/281, train_loss: 0.0822, step time: 0.2483\n",
      "25/281, train_loss: 0.0442, step time: 0.2538\n",
      "26/281, train_loss: 0.0469, step time: 0.2508\n",
      "27/281, train_loss: 0.2424, step time: 0.2503\n",
      "28/281, train_loss: 0.0597, step time: 0.2502\n",
      "29/281, train_loss: 0.1026, step time: 0.2516\n",
      "30/281, train_loss: 0.0545, step time: 0.2561\n",
      "31/281, train_loss: 0.2079, step time: 0.2528\n",
      "32/281, train_loss: 0.0412, step time: 0.2490\n",
      "33/281, train_loss: 0.0701, step time: 0.2547\n",
      "34/281, train_loss: 0.0622, step time: 0.2518\n",
      "35/281, train_loss: 0.3788, step time: 0.2538\n",
      "36/281, train_loss: 0.0697, step time: 0.2530\n",
      "37/281, train_loss: 0.0691, step time: 0.2568\n",
      "38/281, train_loss: 0.2230, step time: 0.2573\n",
      "39/281, train_loss: 0.0757, step time: 0.2502\n",
      "40/281, train_loss: 0.0472, step time: 0.2517\n",
      "41/281, train_loss: 0.0544, step time: 0.2597\n",
      "42/281, train_loss: 0.0421, step time: 0.2532\n",
      "43/281, train_loss: 0.2354, step time: 0.2511\n",
      "44/281, train_loss: 0.0417, step time: 0.2531\n",
      "45/281, train_loss: 0.0598, step time: 0.2557\n",
      "46/281, train_loss: 0.0617, step time: 0.2496\n",
      "47/281, train_loss: 0.0493, step time: 0.2516\n",
      "48/281, train_loss: 0.0815, step time: 0.2515\n",
      "49/281, train_loss: 0.0770, step time: 0.2548\n",
      "50/281, train_loss: 0.2101, step time: 0.2549\n",
      "51/281, train_loss: 0.0613, step time: 0.2512\n",
      "52/281, train_loss: 0.0533, step time: 0.2517\n",
      "53/281, train_loss: 0.2436, step time: 0.2578\n",
      "54/281, train_loss: 0.0658, step time: 0.2565\n",
      "55/281, train_loss: 0.0529, step time: 0.2512\n",
      "56/281, train_loss: 0.0969, step time: 0.2509\n",
      "57/281, train_loss: 0.0649, step time: 0.2553\n",
      "58/281, train_loss: 0.0744, step time: 0.2581\n",
      "59/281, train_loss: 0.1195, step time: 0.2587\n",
      "60/281, train_loss: 0.2203, step time: 0.2529\n",
      "61/281, train_loss: 0.0732, step time: 0.2560\n",
      "62/281, train_loss: 0.0826, step time: 0.2524\n",
      "63/281, train_loss: 0.0881, step time: 0.2521\n",
      "64/281, train_loss: 0.0643, step time: 0.2536\n",
      "65/281, train_loss: 0.0432, step time: 0.2566\n",
      "66/281, train_loss: 0.0864, step time: 0.2563\n",
      "67/281, train_loss: 0.1030, step time: 0.2529\n",
      "68/281, train_loss: 0.0445, step time: 0.2501\n",
      "69/281, train_loss: 0.0636, step time: 0.2602\n",
      "70/281, train_loss: 0.0379, step time: 0.2563\n",
      "71/281, train_loss: 0.0700, step time: 0.2568\n",
      "72/281, train_loss: 0.0742, step time: 0.2576\n",
      "73/281, train_loss: 0.0342, step time: 0.2648\n",
      "74/281, train_loss: 0.0755, step time: 0.3052\n",
      "75/281, train_loss: 0.0884, step time: 0.2679\n",
      "76/281, train_loss: 0.0525, step time: 0.2564\n",
      "77/281, train_loss: 0.0566, step time: 0.2568\n",
      "78/281, train_loss: 0.2403, step time: 0.2564\n",
      "79/281, train_loss: 0.0791, step time: 0.2566\n",
      "80/281, train_loss: 0.0626, step time: 0.2583\n",
      "81/281, train_loss: 0.2115, step time: 0.2607\n",
      "82/281, train_loss: 0.0548, step time: 0.2636\n",
      "83/281, train_loss: 0.2317, step time: 0.2667\n",
      "84/281, train_loss: 0.0731, step time: 0.2626\n",
      "85/281, train_loss: 0.0527, step time: 0.2503\n",
      "86/281, train_loss: 0.0747, step time: 0.2509\n",
      "87/281, train_loss: 0.1086, step time: 0.2579\n",
      "88/281, train_loss: 0.0388, step time: 0.2621\n",
      "89/281, train_loss: 0.0736, step time: 0.2573\n",
      "90/281, train_loss: 0.0935, step time: 0.2519\n",
      "91/281, train_loss: 0.0496, step time: 0.2557\n",
      "92/281, train_loss: 0.0770, step time: 0.2566\n",
      "93/281, train_loss: 0.0784, step time: 0.2586\n",
      "94/281, train_loss: 0.2241, step time: 0.2586\n",
      "95/281, train_loss: 0.0511, step time: 0.2579\n",
      "96/281, train_loss: 0.0514, step time: 0.2571\n",
      "97/281, train_loss: 0.0862, step time: 0.2530\n",
      "98/281, train_loss: 0.0688, step time: 0.2567\n",
      "99/281, train_loss: 0.0709, step time: 0.2586\n",
      "100/281, train_loss: 0.2318, step time: 0.2676\n",
      "101/281, train_loss: 0.2248, step time: 0.2553\n",
      "102/281, train_loss: 0.0534, step time: 0.2589\n",
      "103/281, train_loss: 0.0722, step time: 0.2703\n",
      "104/281, train_loss: 0.0668, step time: 0.2740\n",
      "105/281, train_loss: 0.0602, step time: 0.2493\n",
      "106/281, train_loss: 0.0625, step time: 0.2512\n",
      "107/281, train_loss: 0.2109, step time: 0.2548\n",
      "108/281, train_loss: 0.0538, step time: 0.2524\n",
      "109/281, train_loss: 0.0591, step time: 0.2503\n",
      "110/281, train_loss: 0.0527, step time: 0.2510\n",
      "111/281, train_loss: 0.0709, step time: 0.2554\n",
      "112/281, train_loss: 0.0618, step time: 0.2566\n",
      "113/281, train_loss: 0.0454, step time: 0.2585\n",
      "114/281, train_loss: 0.0487, step time: 0.2600\n",
      "115/281, train_loss: 0.0664, step time: 0.2580\n",
      "116/281, train_loss: 0.0607, step time: 0.2586\n",
      "117/281, train_loss: 0.0991, step time: 0.2550\n",
      "118/281, train_loss: 0.0564, step time: 0.2569\n",
      "119/281, train_loss: 0.0407, step time: 0.2600\n",
      "120/281, train_loss: 0.0707, step time: 0.2666\n",
      "121/281, train_loss: 0.0401, step time: 0.2583\n",
      "122/281, train_loss: 0.2001, step time: 0.2550\n",
      "123/281, train_loss: 0.0586, step time: 0.2586\n",
      "124/281, train_loss: 0.0573, step time: 0.2589\n",
      "125/281, train_loss: 0.2269, step time: 0.2550\n",
      "126/281, train_loss: 0.0425, step time: 0.2591\n",
      "127/281, train_loss: 0.0509, step time: 0.2630\n",
      "128/281, train_loss: 0.2387, step time: 0.2588\n",
      "129/281, train_loss: 0.0454, step time: 0.2582\n",
      "130/281, train_loss: 0.0435, step time: 0.2521\n",
      "131/281, train_loss: 0.0535, step time: 0.2548\n",
      "132/281, train_loss: 0.0539, step time: 0.2591\n",
      "133/281, train_loss: 0.2392, step time: 0.2536\n",
      "134/281, train_loss: 0.0745, step time: 0.2554\n",
      "135/281, train_loss: 0.0429, step time: 0.2521\n",
      "136/281, train_loss: 0.0926, step time: 0.2596\n",
      "137/281, train_loss: 0.0613, step time: 0.2623\n",
      "138/281, train_loss: 0.1081, step time: 0.2620\n",
      "139/281, train_loss: 0.0559, step time: 0.2602\n",
      "140/281, train_loss: 0.0684, step time: 0.2582\n",
      "141/281, train_loss: 0.0570, step time: 0.2622\n",
      "142/281, train_loss: 0.0731, step time: 0.2549\n",
      "143/281, train_loss: 0.0881, step time: 0.2546\n",
      "144/281, train_loss: 0.0712, step time: 0.2507\n",
      "145/281, train_loss: 0.0665, step time: 0.2545\n",
      "146/281, train_loss: 0.0638, step time: 0.2598\n",
      "147/281, train_loss: 0.0692, step time: 0.2487\n",
      "148/281, train_loss: 0.0689, step time: 0.2507\n",
      "149/281, train_loss: 0.1913, step time: 0.2579\n",
      "150/281, train_loss: 0.0551, step time: 0.2605\n",
      "151/281, train_loss: 0.0609, step time: 0.2555\n",
      "152/281, train_loss: 0.0975, step time: 0.2527\n",
      "153/281, train_loss: 0.0687, step time: 0.2510\n",
      "154/281, train_loss: 0.0556, step time: 0.2521\n",
      "155/281, train_loss: 0.0602, step time: 0.2559\n",
      "156/281, train_loss: 0.0955, step time: 0.2588\n",
      "157/281, train_loss: 0.0411, step time: 0.2528\n",
      "158/281, train_loss: 0.0813, step time: 0.2609\n",
      "159/281, train_loss: 0.0492, step time: 0.2594\n",
      "160/281, train_loss: 0.0555, step time: 0.2581\n",
      "161/281, train_loss: 0.0588, step time: 0.2555\n",
      "162/281, train_loss: 0.0631, step time: 0.2562\n",
      "163/281, train_loss: 0.0331, step time: 0.2542\n",
      "164/281, train_loss: 0.0785, step time: 0.2536\n",
      "165/281, train_loss: 0.0496, step time: 0.2576\n",
      "166/281, train_loss: 0.0632, step time: 0.2564\n",
      "167/281, train_loss: 0.0600, step time: 0.2477\n",
      "168/281, train_loss: 0.0698, step time: 0.2507\n",
      "169/281, train_loss: 0.0310, step time: 0.2494\n",
      "170/281, train_loss: 0.0589, step time: 0.2644\n",
      "171/281, train_loss: 0.0380, step time: 0.2967\n",
      "172/281, train_loss: 0.2458, step time: 0.2556\n",
      "173/281, train_loss: 0.0504, step time: 0.2528\n",
      "174/281, train_loss: 0.0699, step time: 0.2525\n",
      "175/281, train_loss: 0.0536, step time: 0.2535\n",
      "176/281, train_loss: 0.0605, step time: 0.2499\n",
      "177/281, train_loss: 0.0737, step time: 0.2502\n",
      "178/281, train_loss: 0.2351, step time: 0.2694\n",
      "179/281, train_loss: 0.2064, step time: 0.2602\n",
      "180/281, train_loss: 0.0612, step time: 0.2624\n",
      "181/281, train_loss: 0.0703, step time: 0.2576\n",
      "182/281, train_loss: 0.0614, step time: 0.2596\n",
      "183/281, train_loss: 0.0647, step time: 0.2512\n",
      "184/281, train_loss: 0.0486, step time: 0.2627\n",
      "185/281, train_loss: 0.0644, step time: 0.2576\n",
      "186/281, train_loss: 0.2185, step time: 0.2606\n",
      "187/281, train_loss: 0.0506, step time: 0.2564\n",
      "188/281, train_loss: 0.0690, step time: 0.2631\n",
      "189/281, train_loss: 0.0439, step time: 0.2576\n",
      "190/281, train_loss: 0.2237, step time: 0.2596\n",
      "191/281, train_loss: 0.0711, step time: 0.2604\n",
      "192/281, train_loss: 0.3837, step time: 0.2558\n",
      "193/281, train_loss: 0.0716, step time: 0.2539\n",
      "194/281, train_loss: 0.0667, step time: 0.2596\n",
      "195/281, train_loss: 0.0338, step time: 0.2571\n",
      "196/281, train_loss: 0.0436, step time: 0.2567\n",
      "197/281, train_loss: 0.2254, step time: 0.2560\n",
      "198/281, train_loss: 0.0682, step time: 0.2576\n",
      "199/281, train_loss: 0.0640, step time: 0.2571\n",
      "200/281, train_loss: 0.0925, step time: 0.2574\n",
      "201/281, train_loss: 0.0451, step time: 0.2551\n",
      "202/281, train_loss: 0.0291, step time: 0.2565\n",
      "203/281, train_loss: 0.0628, step time: 0.2559\n",
      "204/281, train_loss: 0.0831, step time: 0.2499\n",
      "205/281, train_loss: 0.1011, step time: 0.2542\n",
      "206/281, train_loss: 0.0590, step time: 0.2545\n",
      "207/281, train_loss: 0.0431, step time: 0.2540\n",
      "208/281, train_loss: 0.0732, step time: 0.2534\n",
      "209/281, train_loss: 0.0635, step time: 0.2480\n",
      "210/281, train_loss: 0.0750, step time: 0.2604\n",
      "211/281, train_loss: 0.0826, step time: 0.2543\n",
      "212/281, train_loss: 0.0526, step time: 0.2492\n",
      "213/281, train_loss: 0.0570, step time: 0.2495\n",
      "214/281, train_loss: 0.2388, step time: 0.2535\n",
      "215/281, train_loss: 0.0269, step time: 0.2544\n",
      "216/281, train_loss: 0.0554, step time: 0.2530\n",
      "217/281, train_loss: 0.0725, step time: 0.2498\n",
      "218/281, train_loss: 0.0765, step time: 0.2617\n",
      "219/281, train_loss: 0.0682, step time: 0.2605\n",
      "220/281, train_loss: 0.0891, step time: 0.2553\n",
      "221/281, train_loss: 0.0564, step time: 0.2522\n",
      "222/281, train_loss: 0.0469, step time: 0.2524\n",
      "223/281, train_loss: 0.0961, step time: 0.2591\n",
      "224/281, train_loss: 0.2161, step time: 0.2532\n",
      "225/281, train_loss: 0.0525, step time: 0.2500\n",
      "226/281, train_loss: 0.0590, step time: 0.2547\n",
      "227/281, train_loss: 0.0419, step time: 0.2535\n",
      "228/281, train_loss: 0.0730, step time: 0.2574\n",
      "229/281, train_loss: 0.0792, step time: 0.2556\n",
      "230/281, train_loss: 0.0454, step time: 0.2603\n",
      "231/281, train_loss: 0.0507, step time: 0.2537\n",
      "232/281, train_loss: 0.0635, step time: 0.2552\n",
      "233/281, train_loss: 0.0516, step time: 0.2559\n",
      "234/281, train_loss: 0.0591, step time: 0.2544\n",
      "235/281, train_loss: 0.0693, step time: 0.2587\n",
      "236/281, train_loss: 0.0591, step time: 0.2552\n",
      "237/281, train_loss: 0.2740, step time: 0.2520\n",
      "238/281, train_loss: 0.0522, step time: 0.2530\n",
      "239/281, train_loss: 0.0748, step time: 0.2507\n",
      "240/281, train_loss: 0.1110, step time: 0.2523\n",
      "241/281, train_loss: 0.0490, step time: 0.2507\n",
      "242/281, train_loss: 0.0606, step time: 0.2578\n",
      "243/281, train_loss: 0.3808, step time: 0.2503\n",
      "244/281, train_loss: 0.0804, step time: 0.2544\n",
      "245/281, train_loss: 0.0886, step time: 0.2609\n",
      "246/281, train_loss: 0.0952, step time: 0.2527\n",
      "247/281, train_loss: 0.1257, step time: 0.2540\n",
      "248/281, train_loss: 0.0955, step time: 0.2550\n",
      "249/281, train_loss: 0.0536, step time: 0.2537\n",
      "250/281, train_loss: 0.0806, step time: 0.2543\n",
      "251/281, train_loss: 0.0766, step time: 0.2554\n",
      "252/281, train_loss: 0.0749, step time: 0.2531\n",
      "253/281, train_loss: 0.2347, step time: 0.2534\n",
      "254/281, train_loss: 0.0756, step time: 0.2527\n",
      "255/281, train_loss: 0.0531, step time: 0.2539\n",
      "256/281, train_loss: 0.0729, step time: 0.2471\n",
      "257/281, train_loss: 0.0712, step time: 0.2484\n",
      "258/281, train_loss: 0.0962, step time: 0.2558\n",
      "259/281, train_loss: 0.0929, step time: 0.2577\n",
      "260/281, train_loss: 0.0397, step time: 0.2575\n",
      "261/281, train_loss: 0.2144, step time: 0.2524\n",
      "262/281, train_loss: 0.0381, step time: 0.2557\n",
      "263/281, train_loss: 0.0505, step time: 0.2546\n",
      "264/281, train_loss: 0.2358, step time: 0.2578\n",
      "265/281, train_loss: 0.0991, step time: 0.2580\n",
      "266/281, train_loss: 0.0477, step time: 0.2555\n",
      "267/281, train_loss: 0.2647, step time: 0.2531\n",
      "268/281, train_loss: 0.0500, step time: 0.2557\n",
      "269/281, train_loss: 0.0570, step time: 0.2580\n",
      "270/281, train_loss: 0.0562, step time: 0.2568\n",
      "271/281, train_loss: 0.0704, step time: 0.2554\n",
      "272/281, train_loss: 0.0615, step time: 0.2541\n",
      "273/281, train_loss: 0.0648, step time: 0.2557\n",
      "274/281, train_loss: 0.0634, step time: 0.2574\n",
      "275/281, train_loss: 0.2145, step time: 0.2557\n",
      "276/281, train_loss: 0.0491, step time: 0.2551\n",
      "277/281, train_loss: 0.2188, step time: 0.2580\n",
      "278/281, train_loss: 0.0781, step time: 0.2593\n",
      "279/281, train_loss: 0.0700, step time: 0.2556\n",
      "280/281, train_loss: 0.0934, step time: 0.2581\n",
      "281/281, train_loss: 0.0537, step time: 0.2551\n",
      "282/281, train_loss: 0.0184, step time: 0.1527\n",
      "epoch 103 average loss: 0.0887\n",
      "current epoch: 103 current mean dice: 0.8978 tc: 0.8913 wt: 0.9247 et: 0.8863\n",
      "best mean dice: 0.8995 at epoch: 102\n",
      "time consuming of epoch 103 is: 392.1444\n",
      "----------\n",
      "epoch 104/200\n",
      "1/281, train_loss: 0.0836, step time: 0.2607\n",
      "2/281, train_loss: 0.0736, step time: 0.2512\n",
      "3/281, train_loss: 0.0590, step time: 0.2512\n",
      "4/281, train_loss: 0.0679, step time: 0.2465\n",
      "5/281, train_loss: 0.0516, step time: 0.2523\n",
      "6/281, train_loss: 0.0350, step time: 0.2574\n",
      "7/281, train_loss: 0.0626, step time: 0.2484\n",
      "8/281, train_loss: 0.2477, step time: 0.2493\n",
      "9/281, train_loss: 0.2154, step time: 0.2505\n",
      "10/281, train_loss: 0.1922, step time: 0.2552\n",
      "11/281, train_loss: 0.2226, step time: 0.2533\n",
      "12/281, train_loss: 0.0767, step time: 0.2574\n",
      "13/281, train_loss: 0.0339, step time: 0.2505\n",
      "14/281, train_loss: 0.0498, step time: 0.2547\n",
      "15/281, train_loss: 0.1079, step time: 0.2591\n",
      "16/281, train_loss: 0.0799, step time: 0.2605\n",
      "17/281, train_loss: 0.0474, step time: 0.2505\n",
      "18/281, train_loss: 0.0566, step time: 0.2539\n",
      "19/281, train_loss: 0.2428, step time: 0.2503\n",
      "20/281, train_loss: 0.0442, step time: 0.2513\n",
      "21/281, train_loss: 0.0651, step time: 0.2562\n",
      "22/281, train_loss: 0.0606, step time: 0.2613\n",
      "23/281, train_loss: 0.0741, step time: 0.2502\n",
      "24/281, train_loss: 0.0510, step time: 0.2464\n",
      "25/281, train_loss: 0.0779, step time: 0.2526\n",
      "26/281, train_loss: 0.0500, step time: 0.2536\n",
      "27/281, train_loss: 0.0693, step time: 0.2502\n",
      "28/281, train_loss: 0.0489, step time: 0.2602\n",
      "29/281, train_loss: 0.0694, step time: 0.2454\n",
      "30/281, train_loss: 0.0737, step time: 0.2516\n",
      "31/281, train_loss: 0.0669, step time: 0.2476\n",
      "32/281, train_loss: 0.0475, step time: 0.2511\n",
      "33/281, train_loss: 0.0574, step time: 0.2499\n",
      "34/281, train_loss: 0.2406, step time: 0.2524\n",
      "35/281, train_loss: 0.0516, step time: 0.2556\n",
      "36/281, train_loss: 0.0653, step time: 0.2457\n",
      "37/281, train_loss: 0.0588, step time: 0.2541\n",
      "38/281, train_loss: 0.0775, step time: 0.2562\n",
      "39/281, train_loss: 0.0772, step time: 0.2496\n",
      "40/281, train_loss: 0.0601, step time: 0.2490\n",
      "41/281, train_loss: 0.0607, step time: 0.2465\n",
      "42/281, train_loss: 0.0570, step time: 0.2487\n",
      "43/281, train_loss: 0.2211, step time: 0.2485\n",
      "44/281, train_loss: 0.0833, step time: 0.2486\n",
      "45/281, train_loss: 0.0424, step time: 0.2560\n",
      "46/281, train_loss: 0.0606, step time: 0.2585\n",
      "47/281, train_loss: 0.0693, step time: 0.2525\n",
      "48/281, train_loss: 0.0522, step time: 0.2493\n",
      "49/281, train_loss: 0.0803, step time: 0.2516\n",
      "50/281, train_loss: 0.2207, step time: 0.2550\n",
      "51/281, train_loss: 0.0714, step time: 0.2525\n",
      "52/281, train_loss: 0.2253, step time: 0.2510\n",
      "53/281, train_loss: 0.0545, step time: 0.2531\n",
      "54/281, train_loss: 0.0648, step time: 0.2598\n",
      "55/281, train_loss: 0.0771, step time: 0.2522\n",
      "56/281, train_loss: 0.0494, step time: 0.2526\n",
      "57/281, train_loss: 0.0603, step time: 0.2504\n",
      "58/281, train_loss: 0.0824, step time: 0.2522\n",
      "59/281, train_loss: 0.0886, step time: 0.2455\n",
      "60/281, train_loss: 0.0739, step time: 0.2470\n",
      "61/281, train_loss: 0.3704, step time: 0.2521\n",
      "62/281, train_loss: 0.0666, step time: 0.2488\n",
      "63/281, train_loss: 0.0304, step time: 0.2492\n",
      "64/281, train_loss: 0.0587, step time: 0.2507\n",
      "65/281, train_loss: 0.2339, step time: 0.2529\n",
      "66/281, train_loss: 0.0369, step time: 0.2584\n",
      "67/281, train_loss: 0.0896, step time: 0.2555\n",
      "68/281, train_loss: 0.0784, step time: 0.2490\n",
      "69/281, train_loss: 0.0794, step time: 0.2551\n",
      "70/281, train_loss: 0.0382, step time: 0.2480\n",
      "71/281, train_loss: 0.0858, step time: 0.2602\n",
      "72/281, train_loss: 0.0558, step time: 0.2557\n",
      "73/281, train_loss: 0.0461, step time: 0.2556\n",
      "74/281, train_loss: 0.0519, step time: 0.2532\n",
      "75/281, train_loss: 0.0437, step time: 0.2538\n",
      "76/281, train_loss: 0.0545, step time: 0.2530\n",
      "77/281, train_loss: 0.0733, step time: 0.2515\n",
      "78/281, train_loss: 0.0856, step time: 0.2500\n",
      "79/281, train_loss: 0.0476, step time: 0.2565\n",
      "80/281, train_loss: 0.0608, step time: 0.2500\n",
      "81/281, train_loss: 0.0502, step time: 0.2473\n",
      "82/281, train_loss: 0.2029, step time: 0.2478\n",
      "83/281, train_loss: 0.0566, step time: 0.2478\n",
      "84/281, train_loss: 0.2170, step time: 0.2519\n",
      "85/281, train_loss: 0.0642, step time: 0.2486\n",
      "86/281, train_loss: 0.0654, step time: 0.2514\n",
      "87/281, train_loss: 0.0514, step time: 0.2524\n",
      "88/281, train_loss: 0.1047, step time: 0.2515\n",
      "89/281, train_loss: 0.2095, step time: 0.2506\n",
      "90/281, train_loss: 0.1064, step time: 0.2471\n",
      "91/281, train_loss: 0.0635, step time: 0.2548\n",
      "92/281, train_loss: 0.2309, step time: 0.2572\n",
      "93/281, train_loss: 0.0469, step time: 0.2540\n",
      "94/281, train_loss: 0.0522, step time: 0.2551\n",
      "95/281, train_loss: 0.2218, step time: 0.2580\n",
      "96/281, train_loss: 0.0448, step time: 0.2595\n",
      "97/281, train_loss: 0.0667, step time: 0.2562\n",
      "98/281, train_loss: 0.0658, step time: 0.2556\n",
      "99/281, train_loss: 0.0455, step time: 0.2583\n",
      "100/281, train_loss: 0.0698, step time: 0.2566\n",
      "101/281, train_loss: 0.0648, step time: 0.2509\n",
      "102/281, train_loss: 0.0769, step time: 0.2569\n",
      "103/281, train_loss: 0.0615, step time: 0.2509\n",
      "104/281, train_loss: 0.0740, step time: 0.2491\n",
      "105/281, train_loss: 0.0447, step time: 0.2471\n",
      "106/281, train_loss: 0.0536, step time: 0.2479\n",
      "107/281, train_loss: 0.0736, step time: 0.2523\n",
      "108/281, train_loss: 0.0532, step time: 0.2624\n",
      "109/281, train_loss: 0.0703, step time: 0.2671\n",
      "110/281, train_loss: 0.0714, step time: 0.2561\n",
      "111/281, train_loss: 0.2283, step time: 0.2694\n",
      "112/281, train_loss: 0.0657, step time: 0.2519\n",
      "113/281, train_loss: 0.0560, step time: 0.2515\n",
      "114/281, train_loss: 0.0748, step time: 0.2492\n",
      "115/281, train_loss: 0.0423, step time: 0.2492\n",
      "116/281, train_loss: 0.1160, step time: 0.2434\n",
      "117/281, train_loss: 0.0499, step time: 0.2433\n",
      "118/281, train_loss: 0.0726, step time: 0.2415\n",
      "119/281, train_loss: 0.0755, step time: 0.2575\n",
      "120/281, train_loss: 0.3958, step time: 0.2447\n",
      "121/281, train_loss: 0.0585, step time: 0.2453\n",
      "122/281, train_loss: 0.0844, step time: 0.2434\n",
      "123/281, train_loss: 0.0765, step time: 0.2508\n",
      "124/281, train_loss: 0.0523, step time: 0.2473\n",
      "125/281, train_loss: 0.0757, step time: 0.2558\n",
      "126/281, train_loss: 0.0529, step time: 0.2526\n",
      "127/281, train_loss: 0.0691, step time: 0.2706\n",
      "128/281, train_loss: 0.1010, step time: 0.2621\n",
      "129/281, train_loss: 0.0676, step time: 0.2600\n",
      "130/281, train_loss: 0.2076, step time: 0.2559\n",
      "131/281, train_loss: 0.0654, step time: 0.2552\n",
      "132/281, train_loss: 0.0710, step time: 0.2247\n",
      "133/281, train_loss: 0.0273, step time: 0.2509\n",
      "134/281, train_loss: 0.0819, step time: 0.2501\n",
      "135/281, train_loss: 0.0682, step time: 0.2521\n",
      "136/281, train_loss: 0.0875, step time: 0.2505\n",
      "137/281, train_loss: 0.0512, step time: 0.2456\n",
      "138/281, train_loss: 0.0445, step time: 0.2472\n",
      "139/281, train_loss: 0.0786, step time: 0.2533\n",
      "140/281, train_loss: 0.0733, step time: 0.2563\n",
      "141/281, train_loss: 0.0531, step time: 0.2485\n",
      "142/281, train_loss: 0.0584, step time: 0.2492\n",
      "143/281, train_loss: 0.0782, step time: 0.2468\n",
      "144/281, train_loss: 0.0720, step time: 0.2450\n",
      "145/281, train_loss: 0.0860, step time: 0.2457\n",
      "146/281, train_loss: 0.0623, step time: 0.2572\n",
      "147/281, train_loss: 0.0626, step time: 0.2591\n",
      "148/281, train_loss: 0.0902, step time: 0.2554\n",
      "149/281, train_loss: 0.0430, step time: 0.2470\n",
      "150/281, train_loss: 0.0751, step time: 0.2508\n",
      "151/281, train_loss: 0.0687, step time: 0.2525\n",
      "152/281, train_loss: 0.0540, step time: 0.2526\n",
      "153/281, train_loss: 0.2212, step time: 0.2528\n",
      "154/281, train_loss: 0.0300, step time: 0.2519\n",
      "155/281, train_loss: 0.0722, step time: 0.2502\n",
      "156/281, train_loss: 0.0735, step time: 0.2518\n",
      "157/281, train_loss: 0.0455, step time: 0.2556\n",
      "158/281, train_loss: 0.2253, step time: 0.2562\n",
      "159/281, train_loss: 0.0584, step time: 0.2534\n",
      "160/281, train_loss: 0.0787, step time: 0.2481\n",
      "161/281, train_loss: 0.2063, step time: 0.2458\n",
      "162/281, train_loss: 0.0758, step time: 0.2525\n",
      "163/281, train_loss: 0.0351, step time: 0.2513\n",
      "164/281, train_loss: 0.0673, step time: 0.2522\n",
      "165/281, train_loss: 0.0504, step time: 0.2553\n",
      "166/281, train_loss: 0.0719, step time: 0.2499\n",
      "167/281, train_loss: 0.0501, step time: 0.2510\n",
      "168/281, train_loss: 0.0622, step time: 0.2513\n",
      "169/281, train_loss: 0.0851, step time: 0.2547\n",
      "170/281, train_loss: 0.0692, step time: 0.2502\n",
      "171/281, train_loss: 0.0863, step time: 0.2569\n",
      "172/281, train_loss: 0.2223, step time: 0.2561\n",
      "173/281, train_loss: 0.0607, step time: 0.2480\n",
      "174/281, train_loss: 0.2162, step time: 0.2497\n",
      "175/281, train_loss: 0.0761, step time: 0.2455\n",
      "176/281, train_loss: 0.0559, step time: 0.2493\n",
      "177/281, train_loss: 0.1142, step time: 0.2523\n",
      "178/281, train_loss: 0.0605, step time: 0.2508\n",
      "179/281, train_loss: 0.0687, step time: 0.2484\n",
      "180/281, train_loss: 0.1007, step time: 0.2537\n",
      "181/281, train_loss: 0.0790, step time: 0.2509\n",
      "182/281, train_loss: 0.2291, step time: 0.2556\n",
      "183/281, train_loss: 0.0570, step time: 0.2565\n",
      "184/281, train_loss: 0.0472, step time: 0.2507\n",
      "185/281, train_loss: 0.0511, step time: 0.2507\n",
      "186/281, train_loss: 0.0655, step time: 0.2551\n",
      "187/281, train_loss: 0.0700, step time: 0.2535\n",
      "188/281, train_loss: 0.0734, step time: 0.2499\n",
      "189/281, train_loss: 0.0382, step time: 0.2502\n",
      "190/281, train_loss: 0.0666, step time: 0.2522\n",
      "191/281, train_loss: 0.0453, step time: 0.2538\n",
      "192/281, train_loss: 0.0609, step time: 0.2453\n",
      "193/281, train_loss: 0.0633, step time: 0.2460\n",
      "194/281, train_loss: 0.2327, step time: 0.2526\n",
      "195/281, train_loss: 0.0631, step time: 0.2514\n",
      "196/281, train_loss: 0.0663, step time: 0.2510\n",
      "197/281, train_loss: 0.0750, step time: 0.2547\n",
      "198/281, train_loss: 0.0476, step time: 0.2506\n",
      "199/281, train_loss: 0.0695, step time: 0.2529\n",
      "200/281, train_loss: 0.0924, step time: 0.2529\n",
      "201/281, train_loss: 0.2058, step time: 0.2466\n",
      "202/281, train_loss: 0.0729, step time: 0.2527\n",
      "203/281, train_loss: 0.0720, step time: 0.2483\n",
      "204/281, train_loss: 0.0734, step time: 0.2501\n",
      "205/281, train_loss: 0.0605, step time: 0.2505\n",
      "206/281, train_loss: 0.1166, step time: 0.2532\n",
      "207/281, train_loss: 0.0465, step time: 0.2543\n",
      "208/281, train_loss: 0.0798, step time: 0.2496\n",
      "209/281, train_loss: 0.0626, step time: 0.2527\n",
      "210/281, train_loss: 0.0742, step time: 0.2505\n",
      "211/281, train_loss: 0.0655, step time: 0.2551\n",
      "212/281, train_loss: 0.0618, step time: 0.2556\n",
      "213/281, train_loss: 0.0773, step time: 0.2527\n",
      "214/281, train_loss: 0.2108, step time: 0.2546\n",
      "215/281, train_loss: 0.0920, step time: 0.2527\n",
      "216/281, train_loss: 0.0451, step time: 0.2527\n",
      "217/281, train_loss: 0.2248, step time: 0.2543\n",
      "218/281, train_loss: 0.2151, step time: 0.2508\n",
      "219/281, train_loss: 0.0731, step time: 0.2546\n",
      "220/281, train_loss: 0.0410, step time: 0.2779\n",
      "221/281, train_loss: 0.0413, step time: 0.2527\n",
      "222/281, train_loss: 0.0594, step time: 0.2574\n",
      "223/281, train_loss: 0.0812, step time: 0.2669\n",
      "224/281, train_loss: 0.0537, step time: 0.2619\n",
      "225/281, train_loss: 0.0786, step time: 0.2553\n",
      "226/281, train_loss: 0.0320, step time: 0.2578\n",
      "227/281, train_loss: 0.0625, step time: 0.2584\n",
      "228/281, train_loss: 0.2049, step time: 0.2600\n",
      "229/281, train_loss: 0.2095, step time: 0.2574\n",
      "230/281, train_loss: 0.0599, step time: 0.2568\n",
      "231/281, train_loss: 0.0665, step time: 0.2589\n",
      "232/281, train_loss: 0.0609, step time: 0.2586\n",
      "233/281, train_loss: 0.0531, step time: 0.2573\n",
      "234/281, train_loss: 0.0744, step time: 0.2499\n",
      "235/281, train_loss: 0.0370, step time: 0.2543\n",
      "236/281, train_loss: 0.0688, step time: 0.2503\n",
      "237/281, train_loss: 0.0626, step time: 0.2565\n",
      "238/281, train_loss: 0.0482, step time: 0.2524\n",
      "239/281, train_loss: 0.0333, step time: 0.2598\n",
      "240/281, train_loss: 0.0695, step time: 0.2528\n",
      "241/281, train_loss: 0.0654, step time: 0.2570\n",
      "242/281, train_loss: 0.0676, step time: 0.2577\n",
      "243/281, train_loss: 0.0775, step time: 0.2507\n",
      "244/281, train_loss: 0.0564, step time: 0.2755\n",
      "245/281, train_loss: 0.0762, step time: 0.2581\n",
      "246/281, train_loss: 0.0579, step time: 0.2542\n",
      "247/281, train_loss: 0.0681, step time: 0.2526\n",
      "248/281, train_loss: 0.2370, step time: 0.2517\n",
      "249/281, train_loss: 0.0388, step time: 0.2577\n",
      "250/281, train_loss: 0.0756, step time: 0.2589\n",
      "251/281, train_loss: 0.1153, step time: 0.2558\n",
      "252/281, train_loss: 0.2320, step time: 0.2565\n",
      "253/281, train_loss: 0.0610, step time: 0.2548\n",
      "254/281, train_loss: 0.0457, step time: 0.2530\n",
      "255/281, train_loss: 0.0625, step time: 0.2583\n",
      "256/281, train_loss: 0.0501, step time: 0.2600\n",
      "257/281, train_loss: 0.0717, step time: 0.2663\n",
      "258/281, train_loss: 0.0316, step time: 0.2563\n",
      "259/281, train_loss: 0.2189, step time: 0.2603\n",
      "260/281, train_loss: 0.0664, step time: 0.2592\n",
      "261/281, train_loss: 0.0542, step time: 0.2563\n",
      "262/281, train_loss: 0.0449, step time: 0.2560\n",
      "263/281, train_loss: 0.2499, step time: 0.2545\n",
      "264/281, train_loss: 0.0767, step time: 0.2567\n",
      "265/281, train_loss: 0.0426, step time: 0.2563\n",
      "266/281, train_loss: 0.0469, step time: 0.2598\n",
      "267/281, train_loss: 0.0678, step time: 0.2597\n",
      "268/281, train_loss: 0.0695, step time: 0.2631\n",
      "269/281, train_loss: 0.0615, step time: 0.2655\n",
      "270/281, train_loss: 0.2528, step time: 0.2598\n",
      "271/281, train_loss: 0.2281, step time: 0.2556\n",
      "272/281, train_loss: 0.0453, step time: 0.2586\n",
      "273/281, train_loss: 0.0666, step time: 0.2590\n",
      "274/281, train_loss: 0.0545, step time: 0.2601\n",
      "275/281, train_loss: 0.0715, step time: 0.2616\n",
      "276/281, train_loss: 0.2176, step time: 0.2545\n",
      "277/281, train_loss: 0.0444, step time: 0.2586\n",
      "278/281, train_loss: 0.0897, step time: 0.2613\n",
      "279/281, train_loss: 0.0738, step time: 0.2579\n",
      "280/281, train_loss: 0.2095, step time: 0.2542\n",
      "281/281, train_loss: 0.0861, step time: 0.2520\n",
      "282/281, train_loss: 0.3674, step time: 0.1528\n",
      "epoch 104 average loss: 0.0890\n",
      "current epoch: 104 current mean dice: 0.8959 tc: 0.8857 wt: 0.9258 et: 0.8867\n",
      "best mean dice: 0.8995 at epoch: 102\n",
      "time consuming of epoch 104 is: 381.8714\n",
      "----------\n",
      "epoch 105/200\n",
      "1/281, train_loss: 0.0592, step time: 0.2549\n",
      "2/281, train_loss: 0.0631, step time: 0.2547\n",
      "3/281, train_loss: 0.0503, step time: 0.2589\n",
      "4/281, train_loss: 0.0472, step time: 0.2532\n",
      "5/281, train_loss: 0.0722, step time: 0.2538\n",
      "6/281, train_loss: 0.2460, step time: 0.2589\n",
      "7/281, train_loss: 0.0730, step time: 0.2632\n",
      "8/281, train_loss: 0.0379, step time: 0.2689\n",
      "9/281, train_loss: 0.0296, step time: 0.2544\n",
      "10/281, train_loss: 0.0454, step time: 0.2549\n",
      "11/281, train_loss: 0.0517, step time: 0.2576\n",
      "12/281, train_loss: 0.0683, step time: 0.2559\n",
      "13/281, train_loss: 0.0501, step time: 0.2563\n",
      "14/281, train_loss: 0.2018, step time: 0.2539\n",
      "15/281, train_loss: 0.0502, step time: 0.2552\n",
      "16/281, train_loss: 0.0887, step time: 0.2578\n",
      "17/281, train_loss: 0.0365, step time: 0.2561\n",
      "18/281, train_loss: 0.0564, step time: 0.2605\n",
      "19/281, train_loss: 0.2318, step time: 0.2625\n",
      "20/281, train_loss: 0.2229, step time: 0.2609\n",
      "21/281, train_loss: 0.2553, step time: 0.2582\n",
      "22/281, train_loss: 0.0581, step time: 0.2607\n",
      "23/281, train_loss: 0.0788, step time: 0.2586\n",
      "24/281, train_loss: 0.0554, step time: 0.2574\n",
      "25/281, train_loss: 0.0710, step time: 0.2537\n",
      "26/281, train_loss: 0.0889, step time: 0.2558\n",
      "27/281, train_loss: 0.0406, step time: 0.2518\n",
      "28/281, train_loss: 0.0702, step time: 0.2538\n",
      "29/281, train_loss: 0.0486, step time: 0.2487\n",
      "30/281, train_loss: 0.2363, step time: 0.2506\n",
      "31/281, train_loss: 0.1003, step time: 0.2505\n",
      "32/281, train_loss: 0.0574, step time: 0.2599\n",
      "33/281, train_loss: 0.0452, step time: 0.2606\n",
      "34/281, train_loss: 0.1078, step time: 0.2551\n",
      "35/281, train_loss: 0.0523, step time: 0.2560\n",
      "36/281, train_loss: 0.1358, step time: 0.2556\n",
      "37/281, train_loss: 0.2339, step time: 0.2569\n",
      "38/281, train_loss: 0.0436, step time: 0.2518\n",
      "39/281, train_loss: 0.0308, step time: 0.2531\n",
      "40/281, train_loss: 0.0548, step time: 0.2464\n",
      "41/281, train_loss: 0.0655, step time: 0.2553\n",
      "42/281, train_loss: 0.0510, step time: 0.2519\n",
      "43/281, train_loss: 0.0618, step time: 0.2573\n",
      "44/281, train_loss: 0.0656, step time: 0.2589\n",
      "45/281, train_loss: 0.0402, step time: 0.2544\n",
      "46/281, train_loss: 0.0712, step time: 0.2517\n",
      "47/281, train_loss: 0.2296, step time: 0.2528\n",
      "48/281, train_loss: 0.0533, step time: 0.2584\n",
      "49/281, train_loss: 0.0650, step time: 0.2526\n",
      "50/281, train_loss: 0.0735, step time: 0.2585\n",
      "51/281, train_loss: 0.0942, step time: 0.2586\n",
      "52/281, train_loss: 0.0665, step time: 0.2549\n",
      "53/281, train_loss: 0.0530, step time: 0.2678\n",
      "54/281, train_loss: 0.0633, step time: 0.2472\n",
      "55/281, train_loss: 0.0652, step time: 0.2492\n",
      "56/281, train_loss: 0.2202, step time: 0.2505\n",
      "57/281, train_loss: 0.0844, step time: 0.2529\n",
      "58/281, train_loss: 0.0636, step time: 0.2483\n",
      "59/281, train_loss: 0.2028, step time: 0.2482\n",
      "60/281, train_loss: 0.0418, step time: 0.2499\n",
      "61/281, train_loss: 0.0507, step time: 0.2535\n",
      "62/281, train_loss: 0.0524, step time: 0.2497\n",
      "63/281, train_loss: 0.2205, step time: 0.2508\n",
      "64/281, train_loss: 0.0604, step time: 0.2486\n",
      "65/281, train_loss: 0.0327, step time: 0.2596\n",
      "66/281, train_loss: 0.0880, step time: 0.2550\n",
      "67/281, train_loss: 0.0915, step time: 0.2522\n",
      "68/281, train_loss: 0.0934, step time: 0.2556\n",
      "69/281, train_loss: 0.0818, step time: 0.2590\n",
      "70/281, train_loss: 0.0639, step time: 0.2650\n",
      "71/281, train_loss: 0.0532, step time: 0.2820\n",
      "72/281, train_loss: 0.0487, step time: 0.2540\n",
      "73/281, train_loss: 0.2203, step time: 0.2541\n",
      "74/281, train_loss: 0.0639, step time: 0.2546\n",
      "75/281, train_loss: 0.0839, step time: 0.2500\n",
      "76/281, train_loss: 0.2212, step time: 0.2538\n",
      "77/281, train_loss: 0.0729, step time: 0.2528\n",
      "78/281, train_loss: 0.0789, step time: 0.2496\n",
      "79/281, train_loss: 0.2126, step time: 0.2505\n",
      "80/281, train_loss: 0.0768, step time: 0.2510\n",
      "81/281, train_loss: 0.0663, step time: 0.2510\n",
      "82/281, train_loss: 0.0667, step time: 0.2499\n",
      "83/281, train_loss: 0.2361, step time: 0.2523\n",
      "84/281, train_loss: 0.0776, step time: 0.2541\n",
      "85/281, train_loss: 0.0961, step time: 0.2530\n",
      "86/281, train_loss: 0.0638, step time: 0.2528\n",
      "87/281, train_loss: 0.0558, step time: 0.2567\n",
      "88/281, train_loss: 0.2217, step time: 0.2557\n",
      "89/281, train_loss: 0.0578, step time: 0.2535\n",
      "90/281, train_loss: 0.0588, step time: 0.2573\n",
      "91/281, train_loss: 0.0543, step time: 0.2511\n",
      "92/281, train_loss: 0.0532, step time: 0.2520\n",
      "93/281, train_loss: 0.1096, step time: 0.2576\n",
      "94/281, train_loss: 0.0374, step time: 0.2561\n",
      "95/281, train_loss: 0.0590, step time: 0.2490\n",
      "96/281, train_loss: 0.0572, step time: 0.2511\n",
      "97/281, train_loss: 0.2226, step time: 0.2615\n",
      "98/281, train_loss: 0.0513, step time: 0.2609\n",
      "99/281, train_loss: 0.0571, step time: 0.2595\n",
      "100/281, train_loss: 0.2083, step time: 0.2521\n",
      "101/281, train_loss: 0.0656, step time: 0.2583\n",
      "102/281, train_loss: 0.0718, step time: 0.2568\n",
      "103/281, train_loss: 0.0635, step time: 0.2587\n",
      "104/281, train_loss: 0.0747, step time: 0.2568\n",
      "105/281, train_loss: 0.0534, step time: 0.2569\n",
      "106/281, train_loss: 0.0467, step time: 0.2590\n",
      "107/281, train_loss: 0.0475, step time: 0.2519\n",
      "108/281, train_loss: 0.2185, step time: 0.2572\n",
      "109/281, train_loss: 0.0881, step time: 0.2600\n",
      "110/281, train_loss: 0.0515, step time: 0.2601\n",
      "111/281, train_loss: 0.0477, step time: 0.2554\n",
      "112/281, train_loss: 0.0661, step time: 0.2499\n",
      "113/281, train_loss: 0.0391, step time: 0.2541\n",
      "114/281, train_loss: 0.0959, step time: 0.2569\n",
      "115/281, train_loss: 0.0768, step time: 0.2572\n",
      "116/281, train_loss: 0.2205, step time: 0.2542\n",
      "117/281, train_loss: 0.0923, step time: 0.2553\n",
      "118/281, train_loss: 0.0615, step time: 0.2536\n",
      "119/281, train_loss: 0.2123, step time: 0.2528\n",
      "120/281, train_loss: 0.0588, step time: 0.2565\n",
      "121/281, train_loss: 0.0614, step time: 0.2683\n",
      "122/281, train_loss: 0.1000, step time: 0.2737\n",
      "123/281, train_loss: 0.0491, step time: 0.2636\n",
      "124/281, train_loss: 0.0609, step time: 0.2595\n",
      "125/281, train_loss: 0.0629, step time: 0.2535\n",
      "126/281, train_loss: 0.2188, step time: 0.2563\n",
      "127/281, train_loss: 0.0514, step time: 0.2572\n",
      "128/281, train_loss: 0.2085, step time: 0.2590\n",
      "129/281, train_loss: 0.0721, step time: 0.2560\n",
      "130/281, train_loss: 0.0705, step time: 0.2602\n",
      "131/281, train_loss: 0.0590, step time: 0.2647\n",
      "132/281, train_loss: 0.2229, step time: 0.2514\n",
      "133/281, train_loss: 0.0547, step time: 0.2573\n",
      "134/281, train_loss: 0.0435, step time: 0.2581\n",
      "135/281, train_loss: 0.0709, step time: 0.2563\n",
      "136/281, train_loss: 0.0596, step time: 0.2564\n",
      "137/281, train_loss: 0.1444, step time: 0.2324\n",
      "138/281, train_loss: 0.0636, step time: 0.2577\n",
      "139/281, train_loss: 0.2086, step time: 0.2589\n",
      "140/281, train_loss: 0.0817, step time: 0.2609\n",
      "141/281, train_loss: 0.2210, step time: 0.2593\n",
      "142/281, train_loss: 0.0733, step time: 0.2578\n",
      "143/281, train_loss: 0.0691, step time: 0.2562\n",
      "144/281, train_loss: 0.2039, step time: 0.2540\n",
      "145/281, train_loss: 0.0315, step time: 0.2594\n",
      "146/281, train_loss: 0.0506, step time: 0.2544\n",
      "147/281, train_loss: 0.0405, step time: 0.2564\n",
      "148/281, train_loss: 0.0779, step time: 0.2531\n",
      "149/281, train_loss: 0.0575, step time: 0.2605\n",
      "150/281, train_loss: 0.0889, step time: 0.2574\n",
      "151/281, train_loss: 0.0775, step time: 0.2568\n",
      "152/281, train_loss: 0.2786, step time: 0.2582\n",
      "153/281, train_loss: 0.0627, step time: 0.2515\n",
      "154/281, train_loss: 0.0773, step time: 0.2554\n",
      "155/281, train_loss: 0.0389, step time: 0.2521\n",
      "156/281, train_loss: 0.0416, step time: 0.2583\n",
      "157/281, train_loss: 0.0755, step time: 0.2578\n",
      "158/281, train_loss: 0.0898, step time: 0.2550\n",
      "159/281, train_loss: 0.0671, step time: 0.2588\n",
      "160/281, train_loss: 0.0492, step time: 0.2593\n",
      "161/281, train_loss: 0.0493, step time: 0.2615\n",
      "162/281, train_loss: 0.0819, step time: 0.2588\n",
      "163/281, train_loss: 0.0373, step time: 0.2559\n",
      "164/281, train_loss: 0.0677, step time: 0.2595\n",
      "165/281, train_loss: 0.2151, step time: 0.2534\n",
      "166/281, train_loss: 0.0468, step time: 0.2548\n",
      "167/281, train_loss: 0.0486, step time: 0.2521\n",
      "168/281, train_loss: 0.0776, step time: 0.2562\n",
      "169/281, train_loss: 0.0288, step time: 0.2548\n",
      "170/281, train_loss: 0.0431, step time: 0.2555\n",
      "171/281, train_loss: 0.0760, step time: 0.2522\n",
      "172/281, train_loss: 0.0793, step time: 0.2524\n",
      "173/281, train_loss: 0.0584, step time: 0.2475\n",
      "174/281, train_loss: 0.0558, step time: 0.2511\n",
      "175/281, train_loss: 0.0462, step time: 0.2542\n",
      "176/281, train_loss: 0.2226, step time: 0.2579\n",
      "177/281, train_loss: 0.0611, step time: 0.2565\n",
      "178/281, train_loss: 0.0836, step time: 0.2478\n",
      "179/281, train_loss: 0.2220, step time: 0.2480\n",
      "180/281, train_loss: 0.0451, step time: 0.2453\n",
      "181/281, train_loss: 0.0399, step time: 0.2508\n",
      "182/281, train_loss: 0.0692, step time: 0.2524\n",
      "183/281, train_loss: 0.0602, step time: 0.2500\n",
      "184/281, train_loss: 0.0619, step time: 0.2469\n",
      "185/281, train_loss: 0.0432, step time: 0.2474\n",
      "186/281, train_loss: 0.2110, step time: 0.2472\n",
      "187/281, train_loss: 0.0317, step time: 0.2515\n",
      "188/281, train_loss: 0.0376, step time: 0.2502\n",
      "189/281, train_loss: 0.0569, step time: 0.2532\n",
      "190/281, train_loss: 0.0757, step time: 0.2520\n",
      "191/281, train_loss: 0.0637, step time: 0.2538\n",
      "192/281, train_loss: 0.0343, step time: 0.2553\n",
      "193/281, train_loss: 0.0730, step time: 0.2485\n",
      "194/281, train_loss: 0.2053, step time: 0.2467\n",
      "195/281, train_loss: 0.0391, step time: 0.2547\n",
      "196/281, train_loss: 0.0593, step time: 0.2540\n",
      "197/281, train_loss: 0.0929, step time: 0.2513\n",
      "198/281, train_loss: 0.0805, step time: 0.2523\n",
      "199/281, train_loss: 0.0540, step time: 0.2567\n",
      "200/281, train_loss: 0.0505, step time: 0.2567\n",
      "201/281, train_loss: 0.0396, step time: 0.2564\n",
      "202/281, train_loss: 0.2411, step time: 0.2541\n",
      "203/281, train_loss: 0.0663, step time: 0.2566\n",
      "204/281, train_loss: 0.0830, step time: 0.2537\n",
      "205/281, train_loss: 0.0447, step time: 0.2537\n",
      "206/281, train_loss: 0.0484, step time: 0.2573\n",
      "207/281, train_loss: 0.0476, step time: 0.2562\n",
      "208/281, train_loss: 0.2207, step time: 0.2484\n",
      "209/281, train_loss: 0.0814, step time: 0.2546\n",
      "210/281, train_loss: 0.0669, step time: 0.2613\n",
      "211/281, train_loss: 0.2256, step time: 0.2550\n",
      "212/281, train_loss: 0.0595, step time: 0.2507\n",
      "213/281, train_loss: 0.0472, step time: 0.2512\n",
      "214/281, train_loss: 0.0893, step time: 0.2525\n",
      "215/281, train_loss: 0.0903, step time: 0.2541\n",
      "216/281, train_loss: 0.0529, step time: 0.2563\n",
      "217/281, train_loss: 0.0593, step time: 0.2505\n",
      "218/281, train_loss: 0.0798, step time: 0.2582\n",
      "219/281, train_loss: 0.0508, step time: 0.2514\n",
      "220/281, train_loss: 0.2260, step time: 0.2543\n",
      "221/281, train_loss: 0.0700, step time: 0.2539\n",
      "222/281, train_loss: 0.0897, step time: 0.2553\n",
      "223/281, train_loss: 0.0793, step time: 0.2518\n",
      "224/281, train_loss: 0.0699, step time: 0.2558\n",
      "225/281, train_loss: 0.0682, step time: 0.2539\n",
      "226/281, train_loss: 0.0625, step time: 0.2585\n",
      "227/281, train_loss: 0.0622, step time: 0.2544\n",
      "228/281, train_loss: 0.0566, step time: 0.2542\n",
      "229/281, train_loss: 0.0772, step time: 0.2563\n",
      "230/281, train_loss: 0.0303, step time: 0.2520\n",
      "231/281, train_loss: 0.0948, step time: 0.2600\n",
      "232/281, train_loss: 0.0675, step time: 0.2567\n",
      "233/281, train_loss: 0.0423, step time: 0.2570\n",
      "234/281, train_loss: 0.0464, step time: 0.2592\n",
      "235/281, train_loss: 0.0761, step time: 0.2554\n",
      "236/281, train_loss: 0.0299, step time: 0.2548\n",
      "237/281, train_loss: 0.0915, step time: 0.2564\n",
      "238/281, train_loss: 0.0611, step time: 0.2541\n",
      "239/281, train_loss: 0.0555, step time: 0.2544\n",
      "240/281, train_loss: 0.0642, step time: 0.2528\n",
      "241/281, train_loss: 0.0718, step time: 0.2495\n",
      "242/281, train_loss: 0.0607, step time: 0.2534\n",
      "243/281, train_loss: 0.0653, step time: 0.2504\n",
      "244/281, train_loss: 0.0520, step time: 0.2455\n",
      "245/281, train_loss: 0.0925, step time: 0.2498\n",
      "246/281, train_loss: 0.0576, step time: 0.2523\n",
      "247/281, train_loss: 0.2541, step time: 0.2533\n",
      "248/281, train_loss: 0.0945, step time: 0.2573\n",
      "249/281, train_loss: 0.0601, step time: 0.2556\n",
      "250/281, train_loss: 0.0490, step time: 0.2619\n",
      "251/281, train_loss: 0.0806, step time: 0.2560\n",
      "252/281, train_loss: 0.2377, step time: 0.2560\n",
      "253/281, train_loss: 0.0861, step time: 0.2549\n",
      "254/281, train_loss: 0.0437, step time: 0.2595\n",
      "255/281, train_loss: 0.0793, step time: 0.2570\n",
      "256/281, train_loss: 0.0538, step time: 0.2535\n",
      "257/281, train_loss: 0.0817, step time: 0.2522\n",
      "258/281, train_loss: 0.0686, step time: 0.2587\n",
      "259/281, train_loss: 0.1987, step time: 0.2546\n",
      "260/281, train_loss: 0.0541, step time: 0.2487\n",
      "261/281, train_loss: 0.2256, step time: 0.2541\n",
      "262/281, train_loss: 0.0608, step time: 0.2582\n",
      "263/281, train_loss: 0.0515, step time: 0.2532\n",
      "264/281, train_loss: 0.1175, step time: 0.2501\n",
      "265/281, train_loss: 0.0739, step time: 0.2542\n",
      "266/281, train_loss: 0.0487, step time: 0.2528\n",
      "267/281, train_loss: 0.0735, step time: 0.2571\n",
      "268/281, train_loss: 0.0816, step time: 0.2560\n",
      "269/281, train_loss: 0.0885, step time: 0.2485\n",
      "270/281, train_loss: 0.0735, step time: 0.2524\n",
      "271/281, train_loss: 0.0372, step time: 0.2545\n",
      "272/281, train_loss: 0.2235, step time: 0.2559\n",
      "273/281, train_loss: 0.0938, step time: 0.2554\n",
      "274/281, train_loss: 0.0793, step time: 0.2561\n",
      "275/281, train_loss: 0.0734, step time: 0.2596\n",
      "276/281, train_loss: 0.0640, step time: 0.2547\n",
      "277/281, train_loss: 0.0858, step time: 0.2563\n",
      "278/281, train_loss: 0.0510, step time: 0.2549\n",
      "279/281, train_loss: 0.0808, step time: 0.2533\n",
      "280/281, train_loss: 0.2436, step time: 0.2524\n",
      "281/281, train_loss: 0.0591, step time: 0.2502\n",
      "282/281, train_loss: 0.0568, step time: 0.1507\n",
      "epoch 105 average loss: 0.0884\n",
      "current epoch: 105 current mean dice: 0.8967 tc: 0.8903 wt: 0.9250 et: 0.8843\n",
      "best mean dice: 0.8995 at epoch: 102\n",
      "time consuming of epoch 105 is: 425.7318\n",
      "----------\n",
      "epoch 106/200\n",
      "1/281, train_loss: 0.1003, step time: 0.2634\n",
      "2/281, train_loss: 0.0938, step time: 0.2507\n",
      "3/281, train_loss: 0.0554, step time: 0.2531\n",
      "4/281, train_loss: 0.0890, step time: 0.2490\n",
      "5/281, train_loss: 0.2319, step time: 0.2561\n",
      "6/281, train_loss: 0.0824, step time: 0.2510\n",
      "7/281, train_loss: 0.0427, step time: 0.2481\n",
      "8/281, train_loss: 0.0976, step time: 0.2513\n",
      "9/281, train_loss: 0.0616, step time: 0.2539\n",
      "10/281, train_loss: 0.3993, step time: 0.2486\n",
      "11/281, train_loss: 0.0979, step time: 0.2551\n",
      "12/281, train_loss: 0.0936, step time: 0.2532\n",
      "13/281, train_loss: 0.0546, step time: 0.2517\n",
      "14/281, train_loss: 0.0419, step time: 0.2535\n",
      "15/281, train_loss: 0.0642, step time: 0.2506\n",
      "16/281, train_loss: 0.0474, step time: 0.2545\n",
      "17/281, train_loss: 0.0717, step time: 0.2558\n",
      "18/281, train_loss: 0.3711, step time: 0.2573\n",
      "19/281, train_loss: 0.0544, step time: 0.2553\n",
      "20/281, train_loss: 0.1043, step time: 0.2562\n",
      "21/281, train_loss: 0.0595, step time: 0.2579\n",
      "22/281, train_loss: 0.0433, step time: 0.2586\n",
      "23/281, train_loss: 0.0667, step time: 0.2544\n",
      "24/281, train_loss: 0.0594, step time: 0.2525\n",
      "25/281, train_loss: 0.0545, step time: 0.2489\n",
      "26/281, train_loss: 0.0525, step time: 0.2525\n",
      "27/281, train_loss: 0.0607, step time: 0.2467\n",
      "28/281, train_loss: 0.0769, step time: 0.2482\n",
      "29/281, train_loss: 0.0613, step time: 0.2450\n",
      "30/281, train_loss: 0.2556, step time: 0.2475\n",
      "31/281, train_loss: 0.0930, step time: 0.2528\n",
      "32/281, train_loss: 0.0434, step time: 0.2578\n",
      "33/281, train_loss: 0.0479, step time: 0.2552\n",
      "34/281, train_loss: 0.0431, step time: 0.2514\n",
      "35/281, train_loss: 0.2200, step time: 0.2498\n",
      "36/281, train_loss: 0.0513, step time: 0.2581\n",
      "37/281, train_loss: 0.0653, step time: 0.2574\n",
      "38/281, train_loss: 0.1995, step time: 0.2507\n",
      "39/281, train_loss: 0.0928, step time: 0.2564\n",
      "40/281, train_loss: 0.0790, step time: 0.2460\n",
      "41/281, train_loss: 0.2139, step time: 0.2507\n",
      "42/281, train_loss: 0.0771, step time: 0.2542\n",
      "43/281, train_loss: 0.0715, step time: 0.2568\n",
      "44/281, train_loss: 0.0559, step time: 0.2532\n",
      "45/281, train_loss: 0.0464, step time: 0.2530\n",
      "46/281, train_loss: 0.0788, step time: 0.2472\n",
      "47/281, train_loss: 0.0562, step time: 0.2472\n",
      "48/281, train_loss: 0.0391, step time: 0.2451\n",
      "49/281, train_loss: 0.0606, step time: 0.2503\n",
      "50/281, train_loss: 0.0459, step time: 0.2446\n",
      "51/281, train_loss: 0.2169, step time: 0.2451\n",
      "52/281, train_loss: 0.2079, step time: 0.2471\n",
      "53/281, train_loss: 0.0487, step time: 0.2463\n",
      "54/281, train_loss: 0.0818, step time: 0.2488\n",
      "55/281, train_loss: 0.0626, step time: 0.2430\n",
      "56/281, train_loss: 0.0431, step time: 0.2513\n",
      "57/281, train_loss: 0.0681, step time: 0.2491\n",
      "58/281, train_loss: 0.0390, step time: 0.2510\n",
      "59/281, train_loss: 0.0594, step time: 0.2537\n",
      "60/281, train_loss: 0.0682, step time: 0.2498\n",
      "61/281, train_loss: 0.0564, step time: 0.2499\n",
      "62/281, train_loss: 0.0564, step time: 0.2473\n",
      "63/281, train_loss: 0.0545, step time: 0.2552\n",
      "64/281, train_loss: 0.2091, step time: 0.2520\n",
      "65/281, train_loss: 0.0754, step time: 0.2511\n",
      "66/281, train_loss: 0.0526, step time: 0.2481\n",
      "67/281, train_loss: 0.0491, step time: 0.2550\n",
      "68/281, train_loss: 0.2052, step time: 0.2486\n",
      "69/281, train_loss: 0.0379, step time: 0.2504\n",
      "70/281, train_loss: 0.0569, step time: 0.2471\n",
      "71/281, train_loss: 0.0657, step time: 0.2455\n",
      "72/281, train_loss: 0.0644, step time: 0.2648\n",
      "73/281, train_loss: 0.0579, step time: 0.2489\n",
      "74/281, train_loss: 0.0885, step time: 0.2451\n",
      "75/281, train_loss: 0.0552, step time: 0.2434\n",
      "76/281, train_loss: 0.0653, step time: 0.2546\n",
      "77/281, train_loss: 0.0808, step time: 0.2526\n",
      "78/281, train_loss: 0.0485, step time: 0.2515\n",
      "79/281, train_loss: 0.0515, step time: 0.2462\n",
      "80/281, train_loss: 0.0519, step time: 0.2487\n",
      "81/281, train_loss: 0.0423, step time: 0.2483\n",
      "82/281, train_loss: 0.0532, step time: 0.2515\n",
      "83/281, train_loss: 0.0568, step time: 0.2466\n",
      "84/281, train_loss: 0.0828, step time: 0.2504\n",
      "85/281, train_loss: 0.2166, step time: 0.2530\n",
      "86/281, train_loss: 0.0420, step time: 0.2561\n",
      "87/281, train_loss: 0.0857, step time: 0.2613\n",
      "88/281, train_loss: 0.0639, step time: 0.2526\n",
      "89/281, train_loss: 0.0529, step time: 0.2541\n",
      "90/281, train_loss: 0.0632, step time: 0.2496\n",
      "91/281, train_loss: 0.0448, step time: 0.2545\n",
      "92/281, train_loss: 0.0538, step time: 0.2547\n",
      "93/281, train_loss: 0.0528, step time: 0.2628\n",
      "94/281, train_loss: 0.2102, step time: 0.2494\n",
      "95/281, train_loss: 0.0431, step time: 0.2529\n",
      "96/281, train_loss: 0.0785, step time: 0.2510\n",
      "97/281, train_loss: 0.0516, step time: 0.2519\n",
      "98/281, train_loss: 0.0555, step time: 0.2450\n",
      "99/281, train_loss: 0.0908, step time: 0.2449\n",
      "100/281, train_loss: 0.0637, step time: 0.2513\n",
      "101/281, train_loss: 0.0625, step time: 0.2536\n",
      "102/281, train_loss: 0.0593, step time: 0.2476\n",
      "103/281, train_loss: 0.0753, step time: 0.2512\n",
      "104/281, train_loss: 0.0694, step time: 0.2535\n",
      "105/281, train_loss: 0.0495, step time: 0.2519\n",
      "106/281, train_loss: 0.0633, step time: 0.2452\n",
      "107/281, train_loss: 0.0900, step time: 0.2445\n",
      "108/281, train_loss: 0.0460, step time: 0.2530\n",
      "109/281, train_loss: 0.0568, step time: 0.2592\n",
      "110/281, train_loss: 0.0735, step time: 0.2625\n",
      "111/281, train_loss: 0.0551, step time: 0.2590\n",
      "112/281, train_loss: 0.0395, step time: 0.2502\n",
      "113/281, train_loss: 0.0714, step time: 0.2536\n",
      "114/281, train_loss: 0.0605, step time: 0.2531\n",
      "115/281, train_loss: 0.1011, step time: 0.2496\n",
      "116/281, train_loss: 0.0518, step time: 0.2574\n",
      "117/281, train_loss: 0.0442, step time: 0.2606\n",
      "118/281, train_loss: 0.1246, step time: 0.2630\n",
      "119/281, train_loss: 0.0651, step time: 0.2540\n",
      "120/281, train_loss: 0.0562, step time: 0.2648\n",
      "121/281, train_loss: 0.0776, step time: 0.2616\n",
      "122/281, train_loss: 0.2072, step time: 0.2505\n",
      "123/281, train_loss: 0.0547, step time: 0.2514\n",
      "124/281, train_loss: 0.2223, step time: 0.2513\n",
      "125/281, train_loss: 0.2268, step time: 0.2538\n",
      "126/281, train_loss: 0.0678, step time: 0.2591\n",
      "127/281, train_loss: 0.0992, step time: 0.2554\n",
      "128/281, train_loss: 0.0782, step time: 0.2605\n",
      "129/281, train_loss: 0.0659, step time: 0.2597\n",
      "130/281, train_loss: 0.0547, step time: 0.2566\n",
      "131/281, train_loss: 0.0469, step time: 0.2555\n",
      "132/281, train_loss: 0.0357, step time: 0.2541\n",
      "133/281, train_loss: 0.0768, step time: 0.2494\n",
      "134/281, train_loss: 0.2323, step time: 0.2542\n",
      "135/281, train_loss: 0.0508, step time: 0.2580\n",
      "136/281, train_loss: 0.0681, step time: 0.2598\n",
      "137/281, train_loss: 0.0494, step time: 0.2569\n",
      "138/281, train_loss: 0.2076, step time: 0.2521\n",
      "139/281, train_loss: 0.0930, step time: 0.2543\n",
      "140/281, train_loss: 0.0925, step time: 0.2584\n",
      "141/281, train_loss: 0.0516, step time: 0.2543\n",
      "142/281, train_loss: 0.0618, step time: 0.2516\n",
      "143/281, train_loss: 0.0616, step time: 0.2574\n",
      "144/281, train_loss: 0.0809, step time: 0.2531\n",
      "145/281, train_loss: 0.0307, step time: 0.2537\n",
      "146/281, train_loss: 0.0817, step time: 0.2571\n",
      "147/281, train_loss: 0.0787, step time: 0.2590\n",
      "148/281, train_loss: 0.2205, step time: 0.2557\n",
      "149/281, train_loss: 0.0579, step time: 0.2539\n",
      "150/281, train_loss: 0.0735, step time: 0.2488\n",
      "151/281, train_loss: 0.0650, step time: 0.2509\n",
      "152/281, train_loss: 0.0677, step time: 0.2599\n",
      "153/281, train_loss: 0.0602, step time: 0.2599\n",
      "154/281, train_loss: 0.0577, step time: 0.2542\n",
      "155/281, train_loss: 0.0671, step time: 0.2507\n",
      "156/281, train_loss: 0.2152, step time: 0.2562\n",
      "157/281, train_loss: 0.0514, step time: 0.2564\n",
      "158/281, train_loss: 0.0780, step time: 0.2539\n",
      "159/281, train_loss: 0.0798, step time: 0.2577\n",
      "160/281, train_loss: 0.0659, step time: 0.2581\n",
      "161/281, train_loss: 0.0386, step time: 0.2565\n",
      "162/281, train_loss: 0.0725, step time: 0.2616\n",
      "163/281, train_loss: 0.0633, step time: 0.2522\n",
      "164/281, train_loss: 0.0862, step time: 0.2559\n",
      "165/281, train_loss: 0.0530, step time: 0.2568\n",
      "166/281, train_loss: 0.0722, step time: 0.2545\n",
      "167/281, train_loss: 0.0222, step time: 0.2516\n",
      "168/281, train_loss: 0.0605, step time: 0.2611\n",
      "169/281, train_loss: 0.2161, step time: 0.2602\n",
      "170/281, train_loss: 0.0768, step time: 0.2533\n",
      "171/281, train_loss: 0.0456, step time: 0.2505\n",
      "172/281, train_loss: 0.0418, step time: 0.2605\n",
      "173/281, train_loss: 0.0533, step time: 0.2601\n",
      "174/281, train_loss: 0.0900, step time: 0.2542\n",
      "175/281, train_loss: 0.0237, step time: 0.2571\n",
      "176/281, train_loss: 0.0566, step time: 0.2591\n",
      "177/281, train_loss: 0.0673, step time: 0.2557\n",
      "178/281, train_loss: 0.0808, step time: 0.2510\n",
      "179/281, train_loss: 0.0647, step time: 0.2530\n",
      "180/281, train_loss: 0.0599, step time: 0.2506\n",
      "181/281, train_loss: 0.0740, step time: 0.2491\n",
      "182/281, train_loss: 0.0691, step time: 0.2504\n",
      "183/281, train_loss: 0.2281, step time: 0.2520\n",
      "184/281, train_loss: 0.0854, step time: 0.2583\n",
      "185/281, train_loss: 0.2403, step time: 0.2593\n",
      "186/281, train_loss: 0.0430, step time: 0.2501\n",
      "187/281, train_loss: 0.0812, step time: 0.2522\n",
      "188/281, train_loss: 0.0840, step time: 0.2542\n",
      "189/281, train_loss: 0.0756, step time: 0.2547\n",
      "190/281, train_loss: 0.0571, step time: 0.2527\n",
      "191/281, train_loss: 0.0757, step time: 0.2572\n",
      "192/281, train_loss: 0.0761, step time: 0.2519\n",
      "193/281, train_loss: 0.0732, step time: 0.2517\n",
      "194/281, train_loss: 0.2105, step time: 0.2650\n",
      "195/281, train_loss: 0.2257, step time: 0.2592\n",
      "196/281, train_loss: 0.0686, step time: 0.2599\n",
      "197/281, train_loss: 0.0482, step time: 0.2503\n",
      "198/281, train_loss: 0.2330, step time: 0.2545\n",
      "199/281, train_loss: 0.0785, step time: 0.2535\n",
      "200/281, train_loss: 0.0522, step time: 0.2580\n",
      "201/281, train_loss: 0.0625, step time: 0.2624\n",
      "202/281, train_loss: 0.0458, step time: 0.2550\n",
      "203/281, train_loss: 0.0527, step time: 0.2564\n",
      "204/281, train_loss: 0.2293, step time: 0.2588\n",
      "205/281, train_loss: 0.0612, step time: 0.2608\n",
      "206/281, train_loss: 0.0935, step time: 0.2578\n",
      "207/281, train_loss: 0.0618, step time: 0.2559\n",
      "208/281, train_loss: 0.0405, step time: 0.2553\n",
      "209/281, train_loss: 0.0495, step time: 0.2553\n",
      "210/281, train_loss: 0.0439, step time: 0.2562\n",
      "211/281, train_loss: 0.1177, step time: 0.2560\n",
      "212/281, train_loss: 0.0506, step time: 0.2575\n",
      "213/281, train_loss: 0.0974, step time: 0.2529\n",
      "214/281, train_loss: 0.0462, step time: 0.2576\n",
      "215/281, train_loss: 0.2288, step time: 0.2578\n",
      "216/281, train_loss: 0.0715, step time: 0.2506\n",
      "217/281, train_loss: 0.0574, step time: 0.2543\n",
      "218/281, train_loss: 0.2353, step time: 0.2606\n",
      "219/281, train_loss: 0.0336, step time: 0.2593\n",
      "220/281, train_loss: 0.0981, step time: 0.2579\n",
      "221/281, train_loss: 0.0470, step time: 0.2540\n",
      "222/281, train_loss: 0.2257, step time: 0.2588\n",
      "223/281, train_loss: 0.2495, step time: 0.2565\n",
      "224/281, train_loss: 0.2120, step time: 0.2554\n",
      "225/281, train_loss: 0.0577, step time: 0.2509\n",
      "226/281, train_loss: 0.0589, step time: 0.2568\n",
      "227/281, train_loss: 0.0791, step time: 0.2529\n",
      "228/281, train_loss: 0.0632, step time: 0.2586\n",
      "229/281, train_loss: 0.0818, step time: 0.2556\n",
      "230/281, train_loss: 0.0587, step time: 0.2566\n",
      "231/281, train_loss: 0.0708, step time: 0.2525\n",
      "232/281, train_loss: 0.0713, step time: 0.2562\n",
      "233/281, train_loss: 0.0588, step time: 0.2526\n",
      "234/281, train_loss: 0.0872, step time: 0.2566\n",
      "235/281, train_loss: 0.0519, step time: 0.2575\n",
      "236/281, train_loss: 0.0656, step time: 0.2571\n",
      "237/281, train_loss: 0.0562, step time: 0.2499\n",
      "238/281, train_loss: 0.0692, step time: 0.2537\n",
      "239/281, train_loss: 0.0509, step time: 0.2560\n",
      "240/281, train_loss: 0.0832, step time: 0.2514\n",
      "241/281, train_loss: 0.2029, step time: 0.2534\n",
      "242/281, train_loss: 0.0539, step time: 0.2585\n",
      "243/281, train_loss: 0.0745, step time: 0.2589\n",
      "244/281, train_loss: 0.0422, step time: 0.2512\n",
      "245/281, train_loss: 0.0600, step time: 0.2510\n",
      "246/281, train_loss: 0.0863, step time: 0.2547\n",
      "247/281, train_loss: 0.2319, step time: 0.2533\n",
      "248/281, train_loss: 0.0490, step time: 0.2586\n",
      "249/281, train_loss: 0.0413, step time: 0.2567\n",
      "250/281, train_loss: 0.0435, step time: 0.2523\n",
      "251/281, train_loss: 0.2039, step time: 0.2505\n",
      "252/281, train_loss: 0.2103, step time: 0.2563\n",
      "253/281, train_loss: 0.0606, step time: 0.2523\n",
      "254/281, train_loss: 0.0755, step time: 0.2492\n",
      "255/281, train_loss: 0.0611, step time: 0.2562\n",
      "256/281, train_loss: 0.0883, step time: 0.2616\n",
      "257/281, train_loss: 0.2223, step time: 0.2575\n",
      "258/281, train_loss: 0.0594, step time: 0.2575\n",
      "259/281, train_loss: 0.0693, step time: 0.2540\n",
      "260/281, train_loss: 0.2178, step time: 0.2513\n",
      "261/281, train_loss: 0.0534, step time: 0.2515\n",
      "262/281, train_loss: 0.2237, step time: 0.2560\n",
      "263/281, train_loss: 0.0899, step time: 0.2596\n",
      "264/281, train_loss: 0.0689, step time: 0.2643\n",
      "265/281, train_loss: 0.3914, step time: 0.2571\n",
      "266/281, train_loss: 0.0629, step time: 0.2551\n",
      "267/281, train_loss: 0.0951, step time: 0.2585\n",
      "268/281, train_loss: 0.0518, step time: 0.2545\n",
      "269/281, train_loss: 0.0643, step time: 0.2532\n",
      "270/281, train_loss: 0.0321, step time: 0.2530\n",
      "271/281, train_loss: 0.0567, step time: 0.2568\n",
      "272/281, train_loss: 0.0402, step time: 0.2532\n",
      "273/281, train_loss: 0.0505, step time: 0.2557\n",
      "274/281, train_loss: 0.0616, step time: 0.2509\n",
      "275/281, train_loss: 0.0504, step time: 0.2512\n",
      "276/281, train_loss: 0.0384, step time: 0.2577\n",
      "277/281, train_loss: 0.0821, step time: 0.2567\n",
      "278/281, train_loss: 0.0436, step time: 0.2583\n",
      "279/281, train_loss: 0.0721, step time: 0.2633\n",
      "280/281, train_loss: 0.0559, step time: 0.2554\n",
      "281/281, train_loss: 0.0551, step time: 0.2550\n",
      "282/281, train_loss: 0.0771, step time: 0.1526\n",
      "epoch 106 average loss: 0.0876\n",
      "current epoch: 106 current mean dice: 0.8990 tc: 0.8902 wt: 0.9267 et: 0.8893\n",
      "best mean dice: 0.8995 at epoch: 102\n",
      "time consuming of epoch 106 is: 395.6295\n",
      "----------\n",
      "epoch 107/200\n",
      "1/281, train_loss: 0.0555, step time: 0.2629\n",
      "2/281, train_loss: 0.0586, step time: 0.2595\n",
      "3/281, train_loss: 0.0631, step time: 0.2587\n",
      "4/281, train_loss: 0.0813, step time: 0.2535\n",
      "5/281, train_loss: 0.0459, step time: 0.2662\n",
      "6/281, train_loss: 0.0549, step time: 0.2574\n",
      "7/281, train_loss: 0.0383, step time: 0.2559\n",
      "8/281, train_loss: 0.0618, step time: 0.2582\n",
      "9/281, train_loss: 0.0237, step time: 0.2585\n",
      "10/281, train_loss: 0.2203, step time: 0.2537\n",
      "11/281, train_loss: 0.0421, step time: 0.2526\n",
      "12/281, train_loss: 0.0762, step time: 0.2547\n",
      "13/281, train_loss: 0.0775, step time: 0.2574\n",
      "14/281, train_loss: 0.0484, step time: 0.2602\n",
      "15/281, train_loss: 0.2169, step time: 0.2570\n",
      "16/281, train_loss: 0.0572, step time: 0.2587\n",
      "17/281, train_loss: 0.0637, step time: 0.2552\n",
      "18/281, train_loss: 0.0770, step time: 0.2591\n",
      "19/281, train_loss: 0.0638, step time: 0.2616\n",
      "20/281, train_loss: 0.2098, step time: 0.2588\n",
      "21/281, train_loss: 0.0558, step time: 0.2574\n",
      "22/281, train_loss: 0.2360, step time: 0.2605\n",
      "23/281, train_loss: 0.2042, step time: 0.2611\n",
      "24/281, train_loss: 0.0791, step time: 0.2592\n",
      "25/281, train_loss: 0.0682, step time: 0.2587\n",
      "26/281, train_loss: 0.0737, step time: 0.2513\n",
      "27/281, train_loss: 0.0543, step time: 0.2565\n",
      "28/281, train_loss: 0.2126, step time: 0.2577\n",
      "29/281, train_loss: 0.0426, step time: 0.2577\n",
      "30/281, train_loss: 0.0716, step time: 0.2567\n",
      "31/281, train_loss: 0.0815, step time: 0.2611\n",
      "32/281, train_loss: 0.2190, step time: 0.2553\n",
      "33/281, train_loss: 0.0386, step time: 0.2536\n",
      "34/281, train_loss: 0.0565, step time: 0.2632\n",
      "35/281, train_loss: 0.0482, step time: 0.2597\n",
      "36/281, train_loss: 0.0700, step time: 0.2561\n",
      "37/281, train_loss: 0.0974, step time: 0.2543\n",
      "38/281, train_loss: 0.0858, step time: 0.2530\n",
      "39/281, train_loss: 0.0326, step time: 0.2545\n",
      "40/281, train_loss: 0.2170, step time: 0.2573\n",
      "41/281, train_loss: 0.0819, step time: 0.2593\n",
      "42/281, train_loss: 0.2065, step time: 0.2581\n",
      "43/281, train_loss: 0.0918, step time: 0.2589\n",
      "44/281, train_loss: 0.0919, step time: 0.2559\n",
      "45/281, train_loss: 0.0607, step time: 0.2567\n",
      "46/281, train_loss: 0.0635, step time: 0.2593\n",
      "47/281, train_loss: 0.0700, step time: 0.2582\n",
      "48/281, train_loss: 0.0997, step time: 0.2598\n",
      "49/281, train_loss: 0.0687, step time: 0.2601\n",
      "50/281, train_loss: 0.0762, step time: 0.2578\n",
      "51/281, train_loss: 0.0582, step time: 0.2581\n",
      "52/281, train_loss: 0.2310, step time: 0.2583\n",
      "53/281, train_loss: 0.0611, step time: 0.2577\n",
      "54/281, train_loss: 0.0670, step time: 0.2585\n",
      "55/281, train_loss: 0.0845, step time: 0.2529\n",
      "56/281, train_loss: 0.0447, step time: 0.2585\n",
      "57/281, train_loss: 0.2073, step time: 0.2566\n",
      "58/281, train_loss: 0.0467, step time: 0.2572\n",
      "59/281, train_loss: 0.0697, step time: 0.2646\n",
      "60/281, train_loss: 0.0409, step time: 0.2627\n",
      "61/281, train_loss: 0.0415, step time: 0.2550\n",
      "62/281, train_loss: 0.0510, step time: 0.2560\n",
      "63/281, train_loss: 0.2373, step time: 0.2622\n",
      "64/281, train_loss: 0.0584, step time: 0.2627\n",
      "65/281, train_loss: 0.0442, step time: 0.2575\n",
      "66/281, train_loss: 0.0696, step time: 0.2564\n",
      "67/281, train_loss: 0.0464, step time: 0.2565\n",
      "68/281, train_loss: 0.0468, step time: 0.2525\n",
      "69/281, train_loss: 0.0526, step time: 0.2798\n",
      "70/281, train_loss: 0.0341, step time: 0.2661\n",
      "71/281, train_loss: 0.0877, step time: 0.2536\n",
      "72/281, train_loss: 0.0701, step time: 0.2541\n",
      "73/281, train_loss: 0.0684, step time: 0.2618\n",
      "74/281, train_loss: 0.0416, step time: 0.2679\n",
      "75/281, train_loss: 0.0454, step time: 0.2566\n",
      "76/281, train_loss: 0.0721, step time: 0.2551\n",
      "77/281, train_loss: 0.0600, step time: 0.2548\n",
      "78/281, train_loss: 0.0612, step time: 0.2560\n",
      "79/281, train_loss: 0.2273, step time: 0.2565\n",
      "80/281, train_loss: 0.0278, step time: 0.2600\n",
      "81/281, train_loss: 0.0351, step time: 0.2476\n",
      "82/281, train_loss: 0.0331, step time: 0.2495\n",
      "83/281, train_loss: 0.0627, step time: 0.2558\n",
      "84/281, train_loss: 0.0511, step time: 0.2504\n",
      "85/281, train_loss: 0.0928, step time: 0.2509\n",
      "86/281, train_loss: 0.0514, step time: 0.2477\n",
      "87/281, train_loss: 0.0664, step time: 0.2595\n",
      "88/281, train_loss: 0.0459, step time: 0.2613\n",
      "89/281, train_loss: 0.0659, step time: 0.2563\n",
      "90/281, train_loss: 0.0596, step time: 0.2574\n",
      "91/281, train_loss: 0.0536, step time: 0.2552\n",
      "92/281, train_loss: 0.1130, step time: 0.2604\n",
      "93/281, train_loss: 0.0560, step time: 0.2525\n",
      "94/281, train_loss: 0.0693, step time: 0.2543\n",
      "95/281, train_loss: 0.0412, step time: 0.2558\n",
      "96/281, train_loss: 0.0754, step time: 0.2528\n",
      "97/281, train_loss: 0.0663, step time: 0.2488\n",
      "98/281, train_loss: 0.0847, step time: 0.2476\n",
      "99/281, train_loss: 0.2366, step time: 0.2549\n",
      "100/281, train_loss: 0.0528, step time: 0.2546\n",
      "101/281, train_loss: 0.0739, step time: 0.2492\n",
      "102/281, train_loss: 0.1997, step time: 0.2524\n",
      "103/281, train_loss: 0.0700, step time: 0.2561\n",
      "104/281, train_loss: 0.1958, step time: 0.2499\n",
      "105/281, train_loss: 0.2128, step time: 0.2527\n",
      "106/281, train_loss: 0.0621, step time: 0.2473\n",
      "107/281, train_loss: 0.0821, step time: 0.2523\n",
      "108/281, train_loss: 0.0550, step time: 0.2533\n",
      "109/281, train_loss: 0.0658, step time: 0.2525\n",
      "110/281, train_loss: 0.0517, step time: 0.2543\n",
      "111/281, train_loss: 0.0665, step time: 0.2552\n",
      "112/281, train_loss: 0.0815, step time: 0.2542\n",
      "113/281, train_loss: 0.0708, step time: 0.2581\n",
      "114/281, train_loss: 0.0408, step time: 0.2511\n",
      "115/281, train_loss: 0.0636, step time: 0.2543\n",
      "116/281, train_loss: 0.2315, step time: 0.2575\n",
      "117/281, train_loss: 0.0524, step time: 0.2515\n",
      "118/281, train_loss: 0.0371, step time: 0.2500\n",
      "119/281, train_loss: 0.0663, step time: 0.2481\n",
      "120/281, train_loss: 0.0569, step time: 0.2476\n",
      "121/281, train_loss: 0.0514, step time: 0.2502\n",
      "122/281, train_loss: 0.2012, step time: 0.2482\n",
      "123/281, train_loss: 0.0580, step time: 0.2540\n",
      "124/281, train_loss: 0.2102, step time: 0.2574\n",
      "125/281, train_loss: 0.0471, step time: 0.2557\n",
      "126/281, train_loss: 0.0618, step time: 0.2582\n",
      "127/281, train_loss: 0.0833, step time: 0.2458\n",
      "128/281, train_loss: 0.0511, step time: 0.2500\n",
      "129/281, train_loss: 0.0898, step time: 0.2491\n",
      "130/281, train_loss: 0.2152, step time: 0.2466\n",
      "131/281, train_loss: 0.0692, step time: 0.2513\n",
      "132/281, train_loss: 0.0890, step time: 0.2562\n",
      "133/281, train_loss: 0.0648, step time: 0.2515\n",
      "134/281, train_loss: 0.0562, step time: 0.2517\n",
      "135/281, train_loss: 0.0733, step time: 0.2498\n",
      "136/281, train_loss: 0.2202, step time: 0.2513\n",
      "137/281, train_loss: 0.0562, step time: 0.2492\n",
      "138/281, train_loss: 0.1947, step time: 0.2466\n",
      "139/281, train_loss: 0.1987, step time: 0.2578\n",
      "140/281, train_loss: 0.2376, step time: 0.2589\n",
      "141/281, train_loss: 0.0468, step time: 0.2512\n",
      "142/281, train_loss: 0.0620, step time: 0.2518\n",
      "143/281, train_loss: 0.0564, step time: 0.2551\n",
      "144/281, train_loss: 0.0688, step time: 0.2564\n",
      "145/281, train_loss: 0.0416, step time: 0.2569\n",
      "146/281, train_loss: 0.0700, step time: 0.2573\n",
      "147/281, train_loss: 0.1087, step time: 0.2554\n",
      "148/281, train_loss: 0.0482, step time: 0.2578\n",
      "149/281, train_loss: 0.3796, step time: 0.2550\n",
      "150/281, train_loss: 0.0472, step time: 0.2523\n",
      "151/281, train_loss: 0.0709, step time: 0.2505\n",
      "152/281, train_loss: 0.0543, step time: 0.2558\n",
      "153/281, train_loss: 0.0881, step time: 0.2587\n",
      "154/281, train_loss: 0.0826, step time: 0.2714\n",
      "155/281, train_loss: 0.2325, step time: 0.2487\n",
      "156/281, train_loss: 0.2244, step time: 0.2528\n",
      "157/281, train_loss: 0.0811, step time: 0.2565\n",
      "158/281, train_loss: 0.0661, step time: 0.2537\n",
      "159/281, train_loss: 0.0461, step time: 0.2565\n",
      "160/281, train_loss: 0.0675, step time: 0.2540\n",
      "161/281, train_loss: 0.1164, step time: 0.2503\n",
      "162/281, train_loss: 0.0457, step time: 0.2536\n",
      "163/281, train_loss: 0.0607, step time: 0.2554\n",
      "164/281, train_loss: 0.0714, step time: 0.2555\n",
      "165/281, train_loss: 0.0837, step time: 0.2525\n",
      "166/281, train_loss: 0.0554, step time: 0.2538\n",
      "167/281, train_loss: 0.0556, step time: 0.2585\n",
      "168/281, train_loss: 0.2362, step time: 0.2563\n",
      "169/281, train_loss: 0.0763, step time: 0.2546\n",
      "170/281, train_loss: 0.0505, step time: 0.2519\n",
      "171/281, train_loss: 0.0950, step time: 0.2513\n",
      "172/281, train_loss: 0.0495, step time: 0.2524\n",
      "173/281, train_loss: 0.0559, step time: 0.2555\n",
      "174/281, train_loss: 0.0564, step time: 0.2571\n",
      "175/281, train_loss: 0.0656, step time: 0.2484\n",
      "176/281, train_loss: 0.0579, step time: 0.2523\n",
      "177/281, train_loss: 0.0970, step time: 0.2513\n",
      "178/281, train_loss: 0.0535, step time: 0.2548\n",
      "179/281, train_loss: 0.2229, step time: 0.2516\n",
      "180/281, train_loss: 0.2169, step time: 0.2522\n",
      "181/281, train_loss: 0.0461, step time: 0.2510\n",
      "182/281, train_loss: 0.0893, step time: 0.2554\n",
      "183/281, train_loss: 0.2442, step time: 0.2528\n",
      "184/281, train_loss: 0.0857, step time: 0.2519\n",
      "185/281, train_loss: 0.0632, step time: 0.2543\n",
      "186/281, train_loss: 0.2309, step time: 0.2535\n",
      "187/281, train_loss: 0.0916, step time: 0.2494\n",
      "188/281, train_loss: 0.0865, step time: 0.2482\n",
      "189/281, train_loss: 0.0591, step time: 0.2520\n",
      "190/281, train_loss: 0.1519, step time: 0.2547\n",
      "191/281, train_loss: 0.0509, step time: 0.2460\n",
      "192/281, train_loss: 0.0715, step time: 0.2525\n",
      "193/281, train_loss: 0.0685, step time: 0.2496\n",
      "194/281, train_loss: 0.0582, step time: 0.2515\n",
      "195/281, train_loss: 0.0722, step time: 0.2492\n",
      "196/281, train_loss: 0.0591, step time: 0.2506\n",
      "197/281, train_loss: 0.0630, step time: 0.2529\n",
      "198/281, train_loss: 0.0436, step time: 0.2472\n",
      "199/281, train_loss: 0.2373, step time: 0.2474\n",
      "200/281, train_loss: 0.0726, step time: 0.2549\n",
      "201/281, train_loss: 0.0630, step time: 0.2456\n",
      "202/281, train_loss: 0.0421, step time: 0.2464\n",
      "203/281, train_loss: 0.0561, step time: 0.2493\n",
      "204/281, train_loss: 0.2181, step time: 0.2496\n",
      "205/281, train_loss: 0.0744, step time: 0.2503\n",
      "206/281, train_loss: 0.0720, step time: 0.2458\n",
      "207/281, train_loss: 0.0789, step time: 0.2478\n",
      "208/281, train_loss: 0.0793, step time: 0.2523\n",
      "209/281, train_loss: 0.0309, step time: 0.2529\n",
      "210/281, train_loss: 0.0582, step time: 0.2516\n",
      "211/281, train_loss: 0.0516, step time: 0.2463\n",
      "212/281, train_loss: 0.2688, step time: 0.2483\n",
      "213/281, train_loss: 0.0552, step time: 0.2513\n",
      "214/281, train_loss: 0.0643, step time: 0.2469\n",
      "215/281, train_loss: 0.0570, step time: 0.2491\n",
      "216/281, train_loss: 0.2089, step time: 0.2573\n",
      "217/281, train_loss: 0.0837, step time: 0.2563\n",
      "218/281, train_loss: 0.0677, step time: 0.2551\n",
      "219/281, train_loss: 0.0447, step time: 0.2565\n",
      "220/281, train_loss: 0.0905, step time: 0.2582\n",
      "221/281, train_loss: 0.0624, step time: 0.2547\n",
      "222/281, train_loss: 0.0993, step time: 0.2516\n",
      "223/281, train_loss: 0.0357, step time: 0.2505\n",
      "224/281, train_loss: 0.0563, step time: 0.2576\n",
      "225/281, train_loss: 0.0433, step time: 0.2624\n",
      "226/281, train_loss: 0.0468, step time: 0.2574\n",
      "227/281, train_loss: 0.0670, step time: 0.2566\n",
      "228/281, train_loss: 0.0841, step time: 0.2520\n",
      "229/281, train_loss: 0.0598, step time: 0.2509\n",
      "230/281, train_loss: 0.0861, step time: 0.2500\n",
      "231/281, train_loss: 0.0473, step time: 0.2506\n",
      "232/281, train_loss: 0.0511, step time: 0.2537\n",
      "233/281, train_loss: 0.0546, step time: 0.2569\n",
      "234/281, train_loss: 0.0450, step time: 0.2533\n",
      "235/281, train_loss: 0.0900, step time: 0.2499\n",
      "236/281, train_loss: 0.0558, step time: 0.2540\n",
      "237/281, train_loss: 0.0596, step time: 0.2536\n",
      "238/281, train_loss: 0.0495, step time: 0.2560\n",
      "239/281, train_loss: 0.0438, step time: 0.2547\n",
      "240/281, train_loss: 0.0775, step time: 0.2581\n",
      "241/281, train_loss: 0.0719, step time: 0.2496\n",
      "242/281, train_loss: 0.0733, step time: 0.2521\n",
      "243/281, train_loss: 0.0504, step time: 0.2536\n",
      "244/281, train_loss: 0.0456, step time: 0.2545\n",
      "245/281, train_loss: 0.2342, step time: 0.2559\n",
      "246/281, train_loss: 0.0513, step time: 0.2552\n",
      "247/281, train_loss: 0.0367, step time: 0.2490\n",
      "248/281, train_loss: 0.0566, step time: 0.2501\n",
      "249/281, train_loss: 0.0444, step time: 0.2552\n",
      "250/281, train_loss: 0.0795, step time: 0.2544\n",
      "251/281, train_loss: 0.0502, step time: 0.2523\n",
      "252/281, train_loss: 0.2499, step time: 0.2530\n",
      "253/281, train_loss: 0.0408, step time: 0.2535\n",
      "254/281, train_loss: 0.0502, step time: 0.2537\n",
      "255/281, train_loss: 0.0853, step time: 0.2513\n",
      "256/281, train_loss: 0.0603, step time: 0.2489\n",
      "257/281, train_loss: 0.0525, step time: 0.2484\n",
      "258/281, train_loss: 0.0599, step time: 0.2502\n",
      "259/281, train_loss: 0.0717, step time: 0.2513\n",
      "260/281, train_loss: 0.0589, step time: 0.2523\n",
      "261/281, train_loss: 0.0631, step time: 0.2515\n",
      "262/281, train_loss: 0.0477, step time: 0.2533\n",
      "263/281, train_loss: 0.0603, step time: 0.2524\n",
      "264/281, train_loss: 0.0668, step time: 0.2515\n",
      "265/281, train_loss: 0.0561, step time: 0.2485\n",
      "266/281, train_loss: 0.0758, step time: 0.2498\n",
      "267/281, train_loss: 0.0638, step time: 0.2539\n",
      "268/281, train_loss: 0.1025, step time: 0.2515\n",
      "269/281, train_loss: 0.0575, step time: 0.2488\n",
      "270/281, train_loss: 0.2360, step time: 0.2503\n",
      "271/281, train_loss: 0.0901, step time: 0.2503\n",
      "272/281, train_loss: 0.0465, step time: 0.2544\n",
      "273/281, train_loss: 0.0878, step time: 0.2530\n",
      "274/281, train_loss: 0.0483, step time: 0.2545\n",
      "275/281, train_loss: 0.0424, step time: 0.2499\n",
      "276/281, train_loss: 0.0681, step time: 0.2497\n",
      "277/281, train_loss: 0.2245, step time: 0.2507\n",
      "278/281, train_loss: 0.0437, step time: 0.2481\n",
      "279/281, train_loss: 0.2266, step time: 0.2481\n",
      "280/281, train_loss: 0.0753, step time: 0.2514\n",
      "281/281, train_loss: 0.1083, step time: 0.2474\n",
      "282/281, train_loss: 0.0656, step time: 0.1482\n",
      "epoch 107 average loss: 0.0876\n",
      "current epoch: 107 current mean dice: 0.8979 tc: 0.8914 wt: 0.9239 et: 0.8875\n",
      "best mean dice: 0.8995 at epoch: 102\n",
      "time consuming of epoch 107 is: 399.1627\n",
      "----------\n",
      "epoch 108/200\n",
      "1/281, train_loss: 0.0521, step time: 0.2574\n",
      "2/281, train_loss: 0.0596, step time: 0.2488\n",
      "3/281, train_loss: 0.0552, step time: 0.2490\n",
      "4/281, train_loss: 0.0915, step time: 0.2621\n",
      "5/281, train_loss: 0.1001, step time: 0.2812\n",
      "6/281, train_loss: 0.0779, step time: 0.2493\n",
      "7/281, train_loss: 0.0590, step time: 0.2493\n",
      "8/281, train_loss: 0.0622, step time: 0.2488\n",
      "9/281, train_loss: 0.0427, step time: 0.2601\n",
      "10/281, train_loss: 0.2448, step time: 0.2584\n",
      "11/281, train_loss: 0.0663, step time: 0.2594\n",
      "12/281, train_loss: 0.0292, step time: 0.2549\n",
      "13/281, train_loss: 0.0659, step time: 0.2482\n",
      "14/281, train_loss: 0.2342, step time: 0.2467\n",
      "15/281, train_loss: 0.0647, step time: 0.2550\n",
      "16/281, train_loss: 0.0503, step time: 0.2635\n",
      "17/281, train_loss: 0.0486, step time: 0.2548\n",
      "18/281, train_loss: 0.0693, step time: 0.2450\n",
      "19/281, train_loss: 0.0491, step time: 0.2453\n",
      "20/281, train_loss: 0.2100, step time: 0.2491\n",
      "21/281, train_loss: 0.0521, step time: 0.2531\n",
      "22/281, train_loss: 0.0625, step time: 0.2570\n",
      "23/281, train_loss: 0.0749, step time: 0.2553\n",
      "24/281, train_loss: 0.2133, step time: 0.2487\n",
      "25/281, train_loss: 0.0708, step time: 0.2474\n",
      "26/281, train_loss: 0.0947, step time: 0.2509\n",
      "27/281, train_loss: 0.0504, step time: 0.2482\n",
      "28/281, train_loss: 0.0758, step time: 0.2527\n",
      "29/281, train_loss: 0.0412, step time: 0.2555\n",
      "30/281, train_loss: 0.0902, step time: 0.2570\n",
      "31/281, train_loss: 0.0441, step time: 0.2539\n",
      "32/281, train_loss: 0.0539, step time: 0.2567\n",
      "33/281, train_loss: 0.0762, step time: 0.2590\n",
      "34/281, train_loss: 0.2133, step time: 0.2545\n",
      "35/281, train_loss: 0.0525, step time: 0.2475\n",
      "36/281, train_loss: 0.0919, step time: 0.2475\n",
      "37/281, train_loss: 0.0400, step time: 0.2554\n",
      "38/281, train_loss: 0.0686, step time: 0.2565\n",
      "39/281, train_loss: 0.0474, step time: 0.2549\n",
      "40/281, train_loss: 0.0336, step time: 0.2528\n",
      "41/281, train_loss: 0.0522, step time: 0.2458\n",
      "42/281, train_loss: 0.0893, step time: 0.2448\n",
      "43/281, train_loss: 0.0826, step time: 0.2614\n",
      "44/281, train_loss: 0.0565, step time: 0.2510\n",
      "45/281, train_loss: 0.2106, step time: 0.2519\n",
      "46/281, train_loss: 0.0618, step time: 0.2602\n",
      "47/281, train_loss: 0.0519, step time: 0.2547\n",
      "48/281, train_loss: 0.2148, step time: 0.2465\n",
      "49/281, train_loss: 0.0429, step time: 0.2500\n",
      "50/281, train_loss: 0.0619, step time: 0.2526\n",
      "51/281, train_loss: 0.0578, step time: 0.2485\n",
      "52/281, train_loss: 0.0422, step time: 0.2523\n",
      "53/281, train_loss: 0.0774, step time: 0.2544\n",
      "54/281, train_loss: 0.0416, step time: 0.2543\n",
      "55/281, train_loss: 0.0345, step time: 0.2503\n",
      "56/281, train_loss: 0.0423, step time: 0.2473\n",
      "57/281, train_loss: 0.0771, step time: 0.2592\n",
      "58/281, train_loss: 0.0314, step time: 0.2556\n",
      "59/281, train_loss: 0.0647, step time: 0.2500\n",
      "60/281, train_loss: 0.0845, step time: 0.2586\n",
      "61/281, train_loss: 0.0529, step time: 0.2534\n",
      "62/281, train_loss: 0.1159, step time: 0.2492\n",
      "63/281, train_loss: 0.0686, step time: 0.2494\n",
      "64/281, train_loss: 0.2271, step time: 0.2482\n",
      "65/281, train_loss: 0.0563, step time: 0.2482\n",
      "66/281, train_loss: 0.0569, step time: 0.2479\n",
      "67/281, train_loss: 0.0800, step time: 0.2468\n",
      "68/281, train_loss: 0.0547, step time: 0.2574\n",
      "69/281, train_loss: 0.0414, step time: 0.2544\n",
      "70/281, train_loss: 0.0634, step time: 0.2505\n",
      "71/281, train_loss: 0.0810, step time: 0.2496\n",
      "72/281, train_loss: 0.0442, step time: 0.2521\n",
      "73/281, train_loss: 0.0590, step time: 0.2501\n",
      "74/281, train_loss: 0.0886, step time: 0.2601\n",
      "75/281, train_loss: 0.2237, step time: 0.2576\n",
      "76/281, train_loss: 0.0372, step time: 0.2568\n",
      "77/281, train_loss: 0.0814, step time: 0.2597\n",
      "78/281, train_loss: 0.0477, step time: 0.2563\n",
      "79/281, train_loss: 0.0509, step time: 0.2546\n",
      "80/281, train_loss: 0.0323, step time: 0.2600\n",
      "81/281, train_loss: 0.0663, step time: 0.2537\n",
      "82/281, train_loss: 0.0727, step time: 0.2526\n",
      "83/281, train_loss: 0.0545, step time: 0.2513\n",
      "84/281, train_loss: 0.1049, step time: 0.2534\n",
      "85/281, train_loss: 0.0884, step time: 0.2608\n",
      "86/281, train_loss: 0.3851, step time: 0.2515\n",
      "87/281, train_loss: 0.0537, step time: 0.2514\n",
      "88/281, train_loss: 0.0962, step time: 0.2497\n",
      "89/281, train_loss: 0.0493, step time: 0.2515\n",
      "90/281, train_loss: 0.2236, step time: 0.2483\n",
      "91/281, train_loss: 0.0490, step time: 0.2568\n",
      "92/281, train_loss: 0.2325, step time: 0.2592\n",
      "93/281, train_loss: 0.0535, step time: 0.2525\n",
      "94/281, train_loss: 0.0451, step time: 0.2418\n",
      "95/281, train_loss: 0.0806, step time: 0.2448\n",
      "96/281, train_loss: 0.0518, step time: 0.2507\n",
      "97/281, train_loss: 0.0645, step time: 0.2561\n",
      "98/281, train_loss: 0.0619, step time: 0.2526\n",
      "99/281, train_loss: 0.0682, step time: 0.2557\n",
      "100/281, train_loss: 0.2056, step time: 0.2550\n",
      "101/281, train_loss: 0.0813, step time: 0.2533\n",
      "102/281, train_loss: 0.0469, step time: 0.2502\n",
      "103/281, train_loss: 0.0710, step time: 0.2496\n",
      "104/281, train_loss: 0.0690, step time: 0.2523\n",
      "105/281, train_loss: 0.2392, step time: 0.2608\n",
      "106/281, train_loss: 0.0690, step time: 0.2492\n",
      "107/281, train_loss: 0.0724, step time: 0.2465\n",
      "108/281, train_loss: 0.0621, step time: 0.2528\n",
      "109/281, train_loss: 0.1085, step time: 0.2526\n",
      "110/281, train_loss: 0.0620, step time: 0.2603\n",
      "111/281, train_loss: 0.0847, step time: 0.2593\n",
      "112/281, train_loss: 0.0486, step time: 0.2598\n",
      "113/281, train_loss: 0.2179, step time: 0.2568\n",
      "114/281, train_loss: 0.0653, step time: 0.2501\n",
      "115/281, train_loss: 0.0662, step time: 0.2497\n",
      "116/281, train_loss: 0.0535, step time: 0.2477\n",
      "117/281, train_loss: 0.2214, step time: 0.2501\n",
      "118/281, train_loss: 0.0544, step time: 0.2484\n",
      "119/281, train_loss: 0.0637, step time: 0.2497\n",
      "120/281, train_loss: 0.0438, step time: 0.2525\n",
      "121/281, train_loss: 0.0881, step time: 0.2531\n",
      "122/281, train_loss: 0.0408, step time: 0.2479\n",
      "123/281, train_loss: 0.0924, step time: 0.2509\n",
      "124/281, train_loss: 0.0743, step time: 0.2519\n",
      "125/281, train_loss: 0.0478, step time: 0.2497\n",
      "126/281, train_loss: 0.0378, step time: 0.2544\n",
      "127/281, train_loss: 0.0466, step time: 0.2506\n",
      "128/281, train_loss: 0.2196, step time: 0.2503\n",
      "129/281, train_loss: 0.2190, step time: 0.2474\n",
      "130/281, train_loss: 0.0835, step time: 0.2521\n",
      "131/281, train_loss: 0.0859, step time: 0.2554\n",
      "132/281, train_loss: 0.0537, step time: 0.2560\n",
      "133/281, train_loss: 0.0433, step time: 0.2547\n",
      "134/281, train_loss: 0.0895, step time: 0.2563\n",
      "135/281, train_loss: 0.0462, step time: 0.2565\n",
      "136/281, train_loss: 0.0361, step time: 0.2559\n",
      "137/281, train_loss: 0.0371, step time: 0.2561\n",
      "138/281, train_loss: 0.0547, step time: 0.2564\n",
      "139/281, train_loss: 0.0643, step time: 0.2582\n",
      "140/281, train_loss: 0.0545, step time: 0.2524\n",
      "141/281, train_loss: 0.0673, step time: 0.2543\n",
      "142/281, train_loss: 0.0373, step time: 0.2556\n",
      "143/281, train_loss: 0.0775, step time: 0.2578\n",
      "144/281, train_loss: 0.0584, step time: 0.2564\n",
      "145/281, train_loss: 0.2040, step time: 0.2573\n",
      "146/281, train_loss: 0.2299, step time: 0.2570\n",
      "147/281, train_loss: 0.0492, step time: 0.2502\n",
      "148/281, train_loss: 0.0732, step time: 0.2569\n",
      "149/281, train_loss: 0.0720, step time: 0.2567\n",
      "150/281, train_loss: 0.2083, step time: 0.2499\n",
      "151/281, train_loss: 0.0574, step time: 0.2553\n",
      "152/281, train_loss: 0.2079, step time: 0.2595\n",
      "153/281, train_loss: 0.0627, step time: 0.2578\n",
      "154/281, train_loss: 0.0269, step time: 0.2514\n",
      "155/281, train_loss: 0.0583, step time: 0.2522\n",
      "156/281, train_loss: 0.1017, step time: 0.2606\n",
      "157/281, train_loss: 0.0436, step time: 0.2552\n",
      "158/281, train_loss: 0.0445, step time: 0.2540\n",
      "159/281, train_loss: 0.0661, step time: 0.2512\n",
      "160/281, train_loss: 0.0371, step time: 0.2559\n",
      "161/281, train_loss: 0.0731, step time: 0.2564\n",
      "162/281, train_loss: 0.0552, step time: 0.2558\n",
      "163/281, train_loss: 0.0591, step time: 0.2506\n",
      "164/281, train_loss: 0.2594, step time: 0.2593\n",
      "165/281, train_loss: 0.0491, step time: 0.2520\n",
      "166/281, train_loss: 0.0482, step time: 0.2547\n",
      "167/281, train_loss: 0.0702, step time: 0.2588\n",
      "168/281, train_loss: 0.0785, step time: 0.2526\n",
      "169/281, train_loss: 0.0780, step time: 0.2565\n",
      "170/281, train_loss: 0.3792, step time: 0.2541\n",
      "171/281, train_loss: 0.0465, step time: 0.2585\n",
      "172/281, train_loss: 0.0650, step time: 0.2565\n",
      "173/281, train_loss: 0.0478, step time: 0.2530\n",
      "174/281, train_loss: 0.0735, step time: 0.2487\n",
      "175/281, train_loss: 0.0649, step time: 0.2516\n",
      "176/281, train_loss: 0.0588, step time: 0.2542\n",
      "177/281, train_loss: 0.0654, step time: 0.2614\n",
      "178/281, train_loss: 0.2018, step time: 0.2620\n",
      "179/281, train_loss: 0.0672, step time: 0.2578\n",
      "180/281, train_loss: 0.0749, step time: 0.2551\n",
      "181/281, train_loss: 0.0654, step time: 0.2579\n",
      "182/281, train_loss: 0.0500, step time: 0.2568\n",
      "183/281, train_loss: 0.2234, step time: 0.2558\n",
      "184/281, train_loss: 0.0467, step time: 0.2556\n",
      "185/281, train_loss: 0.0809, step time: 0.2562\n",
      "186/281, train_loss: 0.0632, step time: 0.2508\n",
      "187/281, train_loss: 0.0878, step time: 0.2554\n",
      "188/281, train_loss: 0.0431, step time: 0.2622\n",
      "189/281, train_loss: 0.0608, step time: 0.2601\n",
      "190/281, train_loss: 0.1000, step time: 0.2587\n",
      "191/281, train_loss: 0.0665, step time: 0.2492\n",
      "192/281, train_loss: 0.0504, step time: 0.2550\n",
      "193/281, train_loss: 0.0468, step time: 0.2551\n",
      "194/281, train_loss: 0.3923, step time: 0.2500\n",
      "195/281, train_loss: 0.2293, step time: 0.2534\n",
      "196/281, train_loss: 0.0473, step time: 0.2552\n",
      "197/281, train_loss: 0.0871, step time: 0.2566\n",
      "198/281, train_loss: 0.0616, step time: 0.2540\n",
      "199/281, train_loss: 0.0622, step time: 0.2512\n",
      "200/281, train_loss: 0.0796, step time: 0.2604\n",
      "201/281, train_loss: 0.2365, step time: 0.2573\n",
      "202/281, train_loss: 0.0493, step time: 0.2523\n",
      "203/281, train_loss: 0.0705, step time: 0.2501\n",
      "204/281, train_loss: 0.0454, step time: 0.2608\n",
      "205/281, train_loss: 0.0443, step time: 0.2582\n",
      "206/281, train_loss: 0.0650, step time: 0.2592\n",
      "207/281, train_loss: 0.0711, step time: 0.2568\n",
      "208/281, train_loss: 0.0515, step time: 0.2561\n",
      "209/281, train_loss: 0.0375, step time: 0.2594\n",
      "210/281, train_loss: 0.0628, step time: 0.2642\n",
      "211/281, train_loss: 0.0702, step time: 0.2556\n",
      "212/281, train_loss: 0.0654, step time: 0.2583\n",
      "213/281, train_loss: 0.0437, step time: 0.2516\n",
      "214/281, train_loss: 0.0525, step time: 0.2504\n",
      "215/281, train_loss: 0.1993, step time: 0.2546\n",
      "216/281, train_loss: 0.0553, step time: 0.2545\n",
      "217/281, train_loss: 0.0716, step time: 0.2513\n",
      "218/281, train_loss: 0.0812, step time: 0.2518\n",
      "219/281, train_loss: 0.0582, step time: 0.2528\n",
      "220/281, train_loss: 0.0756, step time: 0.2554\n",
      "221/281, train_loss: 0.0539, step time: 0.2574\n",
      "222/281, train_loss: 0.0822, step time: 0.2513\n",
      "223/281, train_loss: 0.0764, step time: 0.2486\n",
      "224/281, train_loss: 0.0539, step time: 0.2551\n",
      "225/281, train_loss: 0.2191, step time: 0.2575\n",
      "226/281, train_loss: 0.0380, step time: 0.2482\n",
      "227/281, train_loss: 0.0555, step time: 0.2507\n",
      "228/281, train_loss: 0.0474, step time: 0.2549\n",
      "229/281, train_loss: 0.1052, step time: 0.2525\n",
      "230/281, train_loss: 0.2234, step time: 0.2530\n",
      "231/281, train_loss: 0.0798, step time: 0.2556\n",
      "232/281, train_loss: 0.0698, step time: 0.2605\n",
      "233/281, train_loss: 0.0512, step time: 0.2533\n",
      "234/281, train_loss: 0.0433, step time: 0.2525\n",
      "235/281, train_loss: 0.0491, step time: 0.2513\n",
      "236/281, train_loss: 0.0495, step time: 0.2533\n",
      "237/281, train_loss: 0.2302, step time: 0.2552\n",
      "238/281, train_loss: 0.2044, step time: 0.2487\n",
      "239/281, train_loss: 0.0890, step time: 0.2524\n",
      "240/281, train_loss: 0.0696, step time: 0.2561\n",
      "241/281, train_loss: 0.0516, step time: 0.2573\n",
      "242/281, train_loss: 0.0465, step time: 0.2576\n",
      "243/281, train_loss: 0.2030, step time: 0.2559\n",
      "244/281, train_loss: 0.0464, step time: 0.2583\n",
      "245/281, train_loss: 0.0675, step time: 0.2585\n",
      "246/281, train_loss: 0.0833, step time: 0.2606\n",
      "247/281, train_loss: 0.0462, step time: 0.2575\n",
      "248/281, train_loss: 0.0620, step time: 0.2564\n",
      "249/281, train_loss: 0.0456, step time: 0.2595\n",
      "250/281, train_loss: 0.0398, step time: 0.2494\n",
      "251/281, train_loss: 0.0612, step time: 0.2506\n",
      "252/281, train_loss: 0.0595, step time: 0.2531\n",
      "253/281, train_loss: 0.0703, step time: 0.2570\n",
      "254/281, train_loss: 0.0640, step time: 0.2538\n",
      "255/281, train_loss: 0.0985, step time: 0.2506\n",
      "256/281, train_loss: 0.0514, step time: 0.2510\n",
      "257/281, train_loss: 0.0861, step time: 0.2556\n",
      "258/281, train_loss: 0.0376, step time: 0.2530\n",
      "259/281, train_loss: 0.2146, step time: 0.2528\n",
      "260/281, train_loss: 0.0674, step time: 0.2596\n",
      "261/281, train_loss: 0.0808, step time: 0.2531\n",
      "262/281, train_loss: 0.0502, step time: 0.2580\n",
      "263/281, train_loss: 0.0656, step time: 0.2587\n",
      "264/281, train_loss: 0.0571, step time: 0.2554\n",
      "265/281, train_loss: 0.0810, step time: 0.2574\n",
      "266/281, train_loss: 0.2369, step time: 0.2543\n",
      "267/281, train_loss: 0.0782, step time: 0.2544\n",
      "268/281, train_loss: 0.2005, step time: 0.2546\n",
      "269/281, train_loss: 0.0606, step time: 0.2548\n",
      "270/281, train_loss: 0.0833, step time: 0.2536\n",
      "271/281, train_loss: 0.0635, step time: 0.2605\n",
      "272/281, train_loss: 0.0602, step time: 0.2537\n",
      "273/281, train_loss: 0.0624, step time: 0.2615\n",
      "274/281, train_loss: 0.2194, step time: 0.2571\n",
      "275/281, train_loss: 0.0869, step time: 0.2579\n",
      "276/281, train_loss: 0.0431, step time: 0.2510\n",
      "277/281, train_loss: 0.2278, step time: 0.2520\n",
      "278/281, train_loss: 0.0421, step time: 0.2623\n",
      "279/281, train_loss: 0.0722, step time: 0.2529\n",
      "280/281, train_loss: 0.0502, step time: 0.2556\n",
      "281/281, train_loss: 0.0883, step time: 0.2585\n",
      "282/281, train_loss: 0.0784, step time: 0.1501\n",
      "epoch 108 average loss: 0.0864\n",
      "saved new best metric model\n",
      "current epoch: 108 current mean dice: 0.9007 tc: 0.8931 wt: 0.9272 et: 0.8918\n",
      "best mean dice: 0.9007 at epoch: 108\n",
      "time consuming of epoch 108 is: 385.0585\n",
      "----------\n",
      "epoch 109/200\n",
      "1/281, train_loss: 0.0400, step time: 0.2595\n",
      "2/281, train_loss: 0.0388, step time: 0.2567\n",
      "3/281, train_loss: 0.0527, step time: 0.2519\n",
      "4/281, train_loss: 0.2273, step time: 0.2535\n",
      "5/281, train_loss: 0.0668, step time: 0.2522\n",
      "6/281, train_loss: 0.2440, step time: 0.2556\n",
      "7/281, train_loss: 0.0516, step time: 0.2537\n",
      "8/281, train_loss: 0.0851, step time: 0.2508\n",
      "9/281, train_loss: 0.0455, step time: 0.2549\n",
      "10/281, train_loss: 0.0492, step time: 0.2548\n",
      "11/281, train_loss: 0.0471, step time: 0.2543\n",
      "12/281, train_loss: 0.0603, step time: 0.2511\n",
      "13/281, train_loss: 0.2092, step time: 0.2587\n",
      "14/281, train_loss: 0.0679, step time: 0.2543\n",
      "15/281, train_loss: 0.0467, step time: 0.2563\n",
      "16/281, train_loss: 0.0683, step time: 0.2555\n",
      "17/281, train_loss: 0.2088, step time: 0.2621\n",
      "18/281, train_loss: 0.0603, step time: 0.2582\n",
      "19/281, train_loss: 0.0517, step time: 0.2713\n",
      "20/281, train_loss: 0.0570, step time: 0.2704\n",
      "21/281, train_loss: 0.0699, step time: 0.2579\n",
      "22/281, train_loss: 0.0973, step time: 0.2593\n",
      "23/281, train_loss: 0.0584, step time: 0.2615\n",
      "24/281, train_loss: 0.0601, step time: 0.2626\n",
      "25/281, train_loss: 0.0553, step time: 0.2590\n",
      "26/281, train_loss: 0.0507, step time: 0.2582\n",
      "27/281, train_loss: 0.0430, step time: 0.2591\n",
      "28/281, train_loss: 0.0471, step time: 0.2558\n",
      "29/281, train_loss: 0.0516, step time: 0.2652\n",
      "30/281, train_loss: 0.0802, step time: 0.2608\n",
      "31/281, train_loss: 0.0753, step time: 0.2582\n",
      "32/281, train_loss: 0.0787, step time: 0.2578\n",
      "33/281, train_loss: 0.0621, step time: 0.2556\n",
      "34/281, train_loss: 0.1013, step time: 0.2601\n",
      "35/281, train_loss: 0.0706, step time: 0.2549\n",
      "36/281, train_loss: 0.0655, step time: 0.2500\n",
      "37/281, train_loss: 0.0687, step time: 0.2528\n",
      "38/281, train_loss: 0.0831, step time: 0.2527\n",
      "39/281, train_loss: 0.2415, step time: 0.2539\n",
      "40/281, train_loss: 0.0322, step time: 0.2546\n",
      "41/281, train_loss: 0.0537, step time: 0.2555\n",
      "42/281, train_loss: 0.0719, step time: 0.2588\n",
      "43/281, train_loss: 0.0659, step time: 0.2613\n",
      "44/281, train_loss: 0.0533, step time: 0.2636\n",
      "45/281, train_loss: 0.0532, step time: 0.2559\n",
      "46/281, train_loss: 0.0438, step time: 0.2590\n",
      "47/281, train_loss: 0.0530, step time: 0.2651\n",
      "48/281, train_loss: 0.0518, step time: 0.2591\n",
      "49/281, train_loss: 0.0734, step time: 0.2552\n",
      "50/281, train_loss: 0.0637, step time: 0.2590\n",
      "51/281, train_loss: 0.0500, step time: 0.2707\n",
      "52/281, train_loss: 0.2402, step time: 0.2571\n",
      "53/281, train_loss: 0.0820, step time: 0.2552\n",
      "54/281, train_loss: 0.0751, step time: 0.2562\n",
      "55/281, train_loss: 0.2311, step time: 0.2508\n",
      "56/281, train_loss: 0.0831, step time: 0.2547\n",
      "57/281, train_loss: 0.0844, step time: 0.2549\n",
      "58/281, train_loss: 0.0466, step time: 0.2561\n",
      "59/281, train_loss: 0.2278, step time: 0.2522\n",
      "60/281, train_loss: 0.0428, step time: 0.2568\n",
      "61/281, train_loss: 0.0578, step time: 0.2658\n",
      "62/281, train_loss: 0.2342, step time: 0.2546\n",
      "63/281, train_loss: 0.0625, step time: 0.2543\n",
      "64/281, train_loss: 0.0984, step time: 0.2632\n",
      "65/281, train_loss: 0.0404, step time: 0.2599\n",
      "66/281, train_loss: 0.0406, step time: 0.2595\n",
      "67/281, train_loss: 0.0439, step time: 0.2564\n",
      "68/281, train_loss: 0.0628, step time: 0.2539\n",
      "69/281, train_loss: 0.0674, step time: 0.2556\n",
      "70/281, train_loss: 0.0647, step time: 0.2543\n",
      "71/281, train_loss: 0.0734, step time: 0.2516\n",
      "72/281, train_loss: 0.0587, step time: 0.2509\n",
      "73/281, train_loss: 0.0784, step time: 0.2551\n",
      "74/281, train_loss: 0.2396, step time: 0.2549\n",
      "75/281, train_loss: 0.0581, step time: 0.2505\n",
      "76/281, train_loss: 0.0765, step time: 0.2535\n",
      "77/281, train_loss: 0.0538, step time: 0.2607\n",
      "78/281, train_loss: 0.0597, step time: 0.2576\n",
      "79/281, train_loss: 0.0731, step time: 0.2566\n",
      "80/281, train_loss: 0.0449, step time: 0.2497\n",
      "81/281, train_loss: 0.0370, step time: 0.2572\n",
      "82/281, train_loss: 0.2120, step time: 0.2535\n",
      "83/281, train_loss: 0.0891, step time: 0.2496\n",
      "84/281, train_loss: 0.0726, step time: 0.2496\n",
      "85/281, train_loss: 0.0602, step time: 0.2570\n",
      "86/281, train_loss: 0.0761, step time: 0.2576\n",
      "87/281, train_loss: 0.0729, step time: 0.2581\n",
      "88/281, train_loss: 0.0774, step time: 0.2537\n",
      "89/281, train_loss: 0.2031, step time: 0.2649\n",
      "90/281, train_loss: 0.0700, step time: 0.2554\n",
      "91/281, train_loss: 0.0496, step time: 0.2565\n",
      "92/281, train_loss: 0.0653, step time: 0.2566\n",
      "93/281, train_loss: 0.2642, step time: 0.2587\n",
      "94/281, train_loss: 0.0518, step time: 0.2545\n",
      "95/281, train_loss: 0.2030, step time: 0.2555\n",
      "96/281, train_loss: 0.2364, step time: 0.2573\n",
      "97/281, train_loss: 0.0534, step time: 0.2581\n",
      "98/281, train_loss: 0.2140, step time: 0.2596\n",
      "99/281, train_loss: 0.0506, step time: 0.2581\n",
      "100/281, train_loss: 0.0678, step time: 0.2603\n",
      "101/281, train_loss: 0.0706, step time: 0.2562\n",
      "102/281, train_loss: 0.0461, step time: 0.2623\n",
      "103/281, train_loss: 0.0518, step time: 0.2573\n",
      "104/281, train_loss: 0.0695, step time: 0.2585\n",
      "105/281, train_loss: 0.2125, step time: 0.2551\n",
      "106/281, train_loss: 0.0450, step time: 0.2594\n",
      "107/281, train_loss: 0.0845, step time: 0.2574\n",
      "108/281, train_loss: 0.1950, step time: 0.2551\n",
      "109/281, train_loss: 0.0733, step time: 0.2503\n",
      "110/281, train_loss: 0.0900, step time: 0.2519\n",
      "111/281, train_loss: 0.0492, step time: 0.2510\n",
      "112/281, train_loss: 0.2417, step time: 0.2528\n",
      "113/281, train_loss: 0.0975, step time: 0.2713\n",
      "114/281, train_loss: 0.0697, step time: 0.2536\n",
      "115/281, train_loss: 0.0768, step time: 0.2509\n",
      "116/281, train_loss: 0.0803, step time: 0.2527\n",
      "117/281, train_loss: 0.3821, step time: 0.2559\n",
      "118/281, train_loss: 0.0846, step time: 0.2605\n",
      "119/281, train_loss: 0.0527, step time: 0.2535\n",
      "120/281, train_loss: 0.0375, step time: 0.2574\n",
      "121/281, train_loss: 0.0648, step time: 0.2516\n",
      "122/281, train_loss: 0.0434, step time: 0.2526\n",
      "123/281, train_loss: 0.0622, step time: 0.2515\n",
      "124/281, train_loss: 0.2193, step time: 0.2536\n",
      "125/281, train_loss: 0.0757, step time: 0.2533\n",
      "126/281, train_loss: 0.0314, step time: 0.2539\n",
      "127/281, train_loss: 0.0634, step time: 0.2497\n",
      "128/281, train_loss: 0.0477, step time: 0.2495\n",
      "129/281, train_loss: 0.0909, step time: 0.2527\n",
      "130/281, train_loss: 0.0679, step time: 0.2511\n",
      "131/281, train_loss: 0.0623, step time: 0.2495\n",
      "132/281, train_loss: 0.0514, step time: 0.2527\n",
      "133/281, train_loss: 0.0616, step time: 0.2522\n",
      "134/281, train_loss: 0.0571, step time: 0.2576\n",
      "135/281, train_loss: 0.0466, step time: 0.2525\n",
      "136/281, train_loss: 0.0558, step time: 0.2539\n",
      "137/281, train_loss: 0.0811, step time: 0.2619\n",
      "138/281, train_loss: 0.0475, step time: 0.2554\n",
      "139/281, train_loss: 0.0495, step time: 0.2543\n",
      "140/281, train_loss: 0.2110, step time: 0.2504\n",
      "141/281, train_loss: 0.0502, step time: 0.2552\n",
      "142/281, train_loss: 0.0720, step time: 0.2512\n",
      "143/281, train_loss: 0.0869, step time: 0.2478\n",
      "144/281, train_loss: 0.0655, step time: 0.2520\n",
      "145/281, train_loss: 0.0657, step time: 0.2539\n",
      "146/281, train_loss: 0.0507, step time: 0.2514\n",
      "147/281, train_loss: 0.0400, step time: 0.2552\n",
      "148/281, train_loss: 0.2612, step time: 0.2550\n",
      "149/281, train_loss: 0.2369, step time: 0.2531\n",
      "150/281, train_loss: 0.0512, step time: 0.2534\n",
      "151/281, train_loss: 0.0498, step time: 0.2514\n",
      "152/281, train_loss: 0.0330, step time: 0.2485\n",
      "153/281, train_loss: 0.0761, step time: 0.2549\n",
      "154/281, train_loss: 0.0597, step time: 0.2474\n",
      "155/281, train_loss: 0.0424, step time: 0.2483\n",
      "156/281, train_loss: 0.0605, step time: 0.2539\n",
      "157/281, train_loss: 0.0384, step time: 0.2554\n",
      "158/281, train_loss: 0.0526, step time: 0.2549\n",
      "159/281, train_loss: 0.0449, step time: 0.2510\n",
      "160/281, train_loss: 0.0392, step time: 0.2536\n",
      "161/281, train_loss: 0.0556, step time: 0.2537\n",
      "162/281, train_loss: 0.0866, step time: 0.2563\n",
      "163/281, train_loss: 0.0519, step time: 0.2606\n",
      "164/281, train_loss: 0.0592, step time: 0.2564\n",
      "165/281, train_loss: 0.2251, step time: 0.2534\n",
      "166/281, train_loss: 0.0427, step time: 0.2547\n",
      "167/281, train_loss: 0.0501, step time: 0.2536\n",
      "168/281, train_loss: 0.0560, step time: 0.2539\n",
      "169/281, train_loss: 0.0644, step time: 0.2538\n",
      "170/281, train_loss: 0.2293, step time: 0.2496\n",
      "171/281, train_loss: 0.0534, step time: 0.2565\n",
      "172/281, train_loss: 0.0962, step time: 0.2492\n",
      "173/281, train_loss: 0.2021, step time: 0.2505\n",
      "174/281, train_loss: 0.0704, step time: 0.2508\n",
      "175/281, train_loss: 0.0647, step time: 0.2533\n",
      "176/281, train_loss: 0.0501, step time: 0.2540\n",
      "177/281, train_loss: 0.0731, step time: 0.2488\n",
      "178/281, train_loss: 0.0483, step time: 0.2473\n",
      "179/281, train_loss: 0.0456, step time: 0.2560\n",
      "180/281, train_loss: 0.1021, step time: 0.2536\n",
      "181/281, train_loss: 0.0586, step time: 0.2497\n",
      "182/281, train_loss: 0.2070, step time: 0.2499\n",
      "183/281, train_loss: 0.0596, step time: 0.2590\n",
      "184/281, train_loss: 0.0704, step time: 0.2558\n",
      "185/281, train_loss: 0.0800, step time: 0.2569\n",
      "186/281, train_loss: 0.0715, step time: 0.2572\n",
      "187/281, train_loss: 0.0540, step time: 0.2584\n",
      "188/281, train_loss: 0.0765, step time: 0.2566\n",
      "189/281, train_loss: 0.0704, step time: 0.2486\n",
      "190/281, train_loss: 0.0760, step time: 0.2510\n",
      "191/281, train_loss: 0.0639, step time: 0.2527\n",
      "192/281, train_loss: 0.0664, step time: 0.2550\n",
      "193/281, train_loss: 0.0759, step time: 0.2538\n",
      "194/281, train_loss: 0.0454, step time: 0.2504\n",
      "195/281, train_loss: 0.0543, step time: 0.2530\n",
      "196/281, train_loss: 0.0348, step time: 0.2541\n",
      "197/281, train_loss: 0.0557, step time: 0.2531\n",
      "198/281, train_loss: 0.0485, step time: 0.2479\n",
      "199/281, train_loss: 0.0561, step time: 0.2615\n",
      "200/281, train_loss: 0.0805, step time: 0.2556\n",
      "201/281, train_loss: 0.0676, step time: 0.2543\n",
      "202/281, train_loss: 0.0378, step time: 0.2537\n",
      "203/281, train_loss: 0.0586, step time: 0.2522\n",
      "204/281, train_loss: 0.0591, step time: 0.2499\n",
      "205/281, train_loss: 0.0383, step time: 0.2512\n",
      "206/281, train_loss: 0.0442, step time: 0.2482\n",
      "207/281, train_loss: 0.0748, step time: 0.2495\n",
      "208/281, train_loss: 0.0739, step time: 0.2545\n",
      "209/281, train_loss: 0.0725, step time: 0.2511\n",
      "210/281, train_loss: 0.0562, step time: 0.2469\n",
      "211/281, train_loss: 0.0750, step time: 0.2517\n",
      "212/281, train_loss: 0.0635, step time: 0.2490\n",
      "213/281, train_loss: 0.0829, step time: 0.2531\n",
      "214/281, train_loss: 0.0621, step time: 0.2532\n",
      "215/281, train_loss: 0.0766, step time: 0.2552\n",
      "216/281, train_loss: 0.0777, step time: 0.2561\n",
      "217/281, train_loss: 0.0808, step time: 0.2561\n",
      "218/281, train_loss: 0.0882, step time: 0.2559\n",
      "219/281, train_loss: 0.0669, step time: 0.2509\n",
      "220/281, train_loss: 0.0467, step time: 0.2577\n",
      "221/281, train_loss: 0.0243, step time: 0.2535\n",
      "222/281, train_loss: 0.1944, step time: 0.2520\n",
      "223/281, train_loss: 0.0675, step time: 0.2515\n",
      "224/281, train_loss: 0.0530, step time: 0.2448\n",
      "225/281, train_loss: 0.0672, step time: 0.2563\n",
      "226/281, train_loss: 0.0683, step time: 0.2499\n",
      "227/281, train_loss: 0.0432, step time: 0.2574\n",
      "228/281, train_loss: 0.0664, step time: 0.2526\n",
      "229/281, train_loss: 0.0567, step time: 0.2575\n",
      "230/281, train_loss: 0.0700, step time: 0.2556\n",
      "231/281, train_loss: 0.0584, step time: 0.2491\n",
      "232/281, train_loss: 0.0755, step time: 0.2536\n",
      "233/281, train_loss: 0.2130, step time: 0.2479\n",
      "234/281, train_loss: 0.0677, step time: 0.2539\n",
      "235/281, train_loss: 0.0568, step time: 0.2552\n",
      "236/281, train_loss: 0.0484, step time: 0.2477\n",
      "237/281, train_loss: 0.0631, step time: 0.2588\n",
      "238/281, train_loss: 0.0527, step time: 0.2494\n",
      "239/281, train_loss: 0.0559, step time: 0.2536\n",
      "240/281, train_loss: 0.2248, step time: 0.2537\n",
      "241/281, train_loss: 0.0476, step time: 0.2539\n",
      "242/281, train_loss: 0.2139, step time: 0.2593\n",
      "243/281, train_loss: 0.0395, step time: 0.2533\n",
      "244/281, train_loss: 0.2110, step time: 0.2545\n",
      "245/281, train_loss: 0.2082, step time: 0.2534\n",
      "246/281, train_loss: 0.0277, step time: 0.2523\n",
      "247/281, train_loss: 0.2416, step time: 0.2579\n",
      "248/281, train_loss: 0.0639, step time: 0.2513\n",
      "249/281, train_loss: 0.0553, step time: 0.2558\n",
      "250/281, train_loss: 0.0555, step time: 0.2539\n",
      "251/281, train_loss: 0.0709, step time: 0.2477\n",
      "252/281, train_loss: 0.2159, step time: 0.2526\n",
      "253/281, train_loss: 0.0510, step time: 0.2560\n",
      "254/281, train_loss: 0.0314, step time: 0.2494\n",
      "255/281, train_loss: 0.0568, step time: 0.2571\n",
      "256/281, train_loss: 0.0490, step time: 0.2516\n",
      "257/281, train_loss: 0.2379, step time: 0.2543\n",
      "258/281, train_loss: 0.0894, step time: 0.2587\n",
      "259/281, train_loss: 0.2137, step time: 0.2539\n",
      "260/281, train_loss: 0.0643, step time: 0.2570\n",
      "261/281, train_loss: 0.0376, step time: 0.2498\n",
      "262/281, train_loss: 0.0717, step time: 0.2551\n",
      "263/281, train_loss: 0.1968, step time: 0.2621\n",
      "264/281, train_loss: 0.0464, step time: 0.2569\n",
      "265/281, train_loss: 0.0839, step time: 0.2558\n",
      "266/281, train_loss: 0.0633, step time: 0.2577\n",
      "267/281, train_loss: 0.2017, step time: 0.2600\n",
      "268/281, train_loss: 0.2504, step time: 0.2500\n",
      "269/281, train_loss: 0.0392, step time: 0.2524\n",
      "270/281, train_loss: 0.0344, step time: 0.2543\n",
      "271/281, train_loss: 0.0395, step time: 0.2567\n",
      "272/281, train_loss: 0.0592, step time: 0.2578\n",
      "273/281, train_loss: 0.0687, step time: 0.2527\n",
      "274/281, train_loss: 0.0501, step time: 0.2547\n",
      "275/281, train_loss: 0.0526, step time: 0.2564\n",
      "276/281, train_loss: 0.0555, step time: 0.2545\n",
      "277/281, train_loss: 0.0442, step time: 0.2557\n",
      "278/281, train_loss: 0.0528, step time: 0.2557\n",
      "279/281, train_loss: 0.0381, step time: 0.2533\n",
      "280/281, train_loss: 0.2104, step time: 0.2563\n",
      "281/281, train_loss: 0.0821, step time: 0.2540\n",
      "282/281, train_loss: 0.0694, step time: 0.1511\n",
      "epoch 109 average loss: 0.0853\n",
      "saved new best metric model\n",
      "current epoch: 109 current mean dice: 0.9020 tc: 0.8953 wt: 0.9266 et: 0.8935\n",
      "best mean dice: 0.9020 at epoch: 109\n",
      "time consuming of epoch 109 is: 407.9498\n",
      "----------\n",
      "epoch 110/200\n",
      "1/281, train_loss: 0.0444, step time: 0.2538\n",
      "2/281, train_loss: 0.0720, step time: 0.2500\n",
      "3/281, train_loss: 0.0485, step time: 0.2475\n",
      "4/281, train_loss: 0.0752, step time: 0.2474\n",
      "5/281, train_loss: 0.0223, step time: 0.2518\n",
      "6/281, train_loss: 0.2457, step time: 0.2589\n",
      "7/281, train_loss: 0.0754, step time: 0.2710\n",
      "8/281, train_loss: 0.0453, step time: 0.2482\n",
      "9/281, train_loss: 0.0843, step time: 0.2611\n",
      "10/281, train_loss: 0.0887, step time: 0.2543\n",
      "11/281, train_loss: 0.0642, step time: 0.2522\n",
      "12/281, train_loss: 0.0839, step time: 0.2536\n",
      "13/281, train_loss: 0.0683, step time: 0.2539\n",
      "14/281, train_loss: 0.0592, step time: 0.2532\n",
      "15/281, train_loss: 0.0412, step time: 0.2493\n",
      "16/281, train_loss: 0.2268, step time: 0.2530\n",
      "17/281, train_loss: 0.0475, step time: 0.2619\n",
      "18/281, train_loss: 0.0445, step time: 0.2549\n",
      "19/281, train_loss: 0.0751, step time: 0.2568\n",
      "20/281, train_loss: 0.0707, step time: 0.2552\n",
      "21/281, train_loss: 0.2302, step time: 0.2494\n",
      "22/281, train_loss: 0.0483, step time: 0.2515\n",
      "23/281, train_loss: 0.2196, step time: 0.2515\n",
      "24/281, train_loss: 0.0713, step time: 0.2530\n",
      "25/281, train_loss: 0.0471, step time: 0.2572\n",
      "26/281, train_loss: 0.0453, step time: 0.2593\n",
      "27/281, train_loss: 0.0439, step time: 0.2607\n",
      "28/281, train_loss: 0.0713, step time: 0.2641\n",
      "29/281, train_loss: 0.0484, step time: 0.2515\n",
      "30/281, train_loss: 0.0522, step time: 0.2574\n",
      "31/281, train_loss: 0.0696, step time: 0.2530\n",
      "32/281, train_loss: 0.0558, step time: 0.2558\n",
      "33/281, train_loss: 0.0453, step time: 0.2515\n",
      "34/281, train_loss: 0.2019, step time: 0.2470\n",
      "35/281, train_loss: 0.0429, step time: 0.2527\n",
      "36/281, train_loss: 0.0655, step time: 0.2565\n",
      "37/281, train_loss: 0.0599, step time: 0.2566\n",
      "38/281, train_loss: 0.0466, step time: 0.2520\n",
      "39/281, train_loss: 0.0443, step time: 0.2478\n",
      "40/281, train_loss: 0.0805, step time: 0.2529\n",
      "41/281, train_loss: 0.0766, step time: 0.2552\n",
      "42/281, train_loss: 0.0374, step time: 0.2536\n",
      "43/281, train_loss: 0.0465, step time: 0.2514\n",
      "44/281, train_loss: 0.2214, step time: 0.2464\n",
      "45/281, train_loss: 0.0451, step time: 0.2521\n",
      "46/281, train_loss: 0.0559, step time: 0.2533\n",
      "47/281, train_loss: 0.0733, step time: 0.2566\n",
      "48/281, train_loss: 0.0693, step time: 0.2572\n",
      "49/281, train_loss: 0.1015, step time: 0.2529\n",
      "50/281, train_loss: 0.0737, step time: 0.2551\n",
      "51/281, train_loss: 0.0640, step time: 0.2542\n",
      "52/281, train_loss: 0.0622, step time: 0.2520\n",
      "53/281, train_loss: 0.0538, step time: 0.2500\n",
      "54/281, train_loss: 0.0532, step time: 0.2516\n",
      "55/281, train_loss: 0.0625, step time: 0.2493\n",
      "56/281, train_loss: 0.0535, step time: 0.2478\n",
      "57/281, train_loss: 0.0682, step time: 0.2553\n",
      "58/281, train_loss: 0.0378, step time: 0.2535\n",
      "59/281, train_loss: 0.0699, step time: 0.2460\n",
      "60/281, train_loss: 0.0820, step time: 0.2603\n",
      "61/281, train_loss: 0.0699, step time: 0.2529\n",
      "62/281, train_loss: 0.0714, step time: 0.2539\n",
      "63/281, train_loss: 0.0532, step time: 0.2526\n",
      "64/281, train_loss: 0.0514, step time: 0.2612\n",
      "65/281, train_loss: 0.2159, step time: 0.2560\n",
      "66/281, train_loss: 0.0716, step time: 0.2496\n",
      "67/281, train_loss: 0.0966, step time: 0.2499\n",
      "68/281, train_loss: 0.2237, step time: 0.2555\n",
      "69/281, train_loss: 0.0779, step time: 0.2508\n",
      "70/281, train_loss: 0.0306, step time: 0.2513\n",
      "71/281, train_loss: 0.0795, step time: 0.2485\n",
      "72/281, train_loss: 0.0910, step time: 0.2528\n",
      "73/281, train_loss: 0.0718, step time: 0.2553\n",
      "74/281, train_loss: 0.2234, step time: 0.2537\n",
      "75/281, train_loss: 0.0589, step time: 0.2513\n",
      "76/281, train_loss: 0.2480, step time: 0.2482\n",
      "77/281, train_loss: 0.2446, step time: 0.2504\n",
      "78/281, train_loss: 0.2040, step time: 0.2512\n",
      "79/281, train_loss: 0.0619, step time: 0.2499\n",
      "80/281, train_loss: 0.0305, step time: 0.2536\n",
      "81/281, train_loss: 0.0592, step time: 0.2588\n",
      "82/281, train_loss: 0.0763, step time: 0.2552\n",
      "83/281, train_loss: 0.0586, step time: 0.2522\n",
      "84/281, train_loss: 0.0511, step time: 0.2454\n",
      "85/281, train_loss: 0.0488, step time: 0.2451\n",
      "86/281, train_loss: 0.0819, step time: 0.2500\n",
      "87/281, train_loss: 0.0676, step time: 0.2505\n",
      "88/281, train_loss: 0.0896, step time: 0.2484\n",
      "89/281, train_loss: 0.0486, step time: 0.2491\n",
      "90/281, train_loss: 0.2285, step time: 0.2537\n",
      "91/281, train_loss: 0.0511, step time: 0.2471\n",
      "92/281, train_loss: 0.0480, step time: 0.2496\n",
      "93/281, train_loss: 0.0517, step time: 0.2537\n",
      "94/281, train_loss: 0.0530, step time: 0.2472\n",
      "95/281, train_loss: 0.0376, step time: 0.2507\n",
      "96/281, train_loss: 0.0582, step time: 0.2516\n",
      "97/281, train_loss: 0.0543, step time: 0.2535\n",
      "98/281, train_loss: 0.2276, step time: 0.2550\n",
      "99/281, train_loss: 0.2281, step time: 0.2542\n",
      "100/281, train_loss: 0.0267, step time: 0.2569\n",
      "101/281, train_loss: 0.0714, step time: 0.2548\n",
      "102/281, train_loss: 0.0690, step time: 0.2562\n",
      "103/281, train_loss: 0.0505, step time: 0.2612\n",
      "104/281, train_loss: 0.1994, step time: 0.2548\n",
      "105/281, train_loss: 0.0402, step time: 0.2507\n",
      "106/281, train_loss: 0.0654, step time: 0.2511\n",
      "107/281, train_loss: 0.0559, step time: 0.2483\n",
      "108/281, train_loss: 0.0388, step time: 0.2573\n",
      "109/281, train_loss: 0.2298, step time: 0.2572\n",
      "110/281, train_loss: 0.1103, step time: 0.2528\n",
      "111/281, train_loss: 0.2291, step time: 0.2474\n",
      "112/281, train_loss: 0.0434, step time: 0.2519\n",
      "113/281, train_loss: 0.0885, step time: 0.2542\n",
      "114/281, train_loss: 0.0575, step time: 0.2486\n",
      "115/281, train_loss: 0.0776, step time: 0.2482\n",
      "116/281, train_loss: 0.0709, step time: 0.2486\n",
      "117/281, train_loss: 0.0805, step time: 0.2497\n",
      "118/281, train_loss: 0.0408, step time: 0.2558\n",
      "119/281, train_loss: 0.0774, step time: 0.2512\n",
      "120/281, train_loss: 0.0508, step time: 0.2506\n",
      "121/281, train_loss: 0.0640, step time: 0.2474\n",
      "122/281, train_loss: 0.0384, step time: 0.2515\n",
      "123/281, train_loss: 0.0624, step time: 0.2506\n",
      "124/281, train_loss: 0.0730, step time: 0.2498\n",
      "125/281, train_loss: 0.0720, step time: 0.2506\n",
      "126/281, train_loss: 0.0465, step time: 0.2553\n",
      "127/281, train_loss: 0.0694, step time: 0.2609\n",
      "128/281, train_loss: 0.0441, step time: 0.2754\n",
      "129/281, train_loss: 0.0556, step time: 0.2540\n",
      "130/281, train_loss: 0.2078, step time: 0.2550\n",
      "131/281, train_loss: 0.0666, step time: 0.2532\n",
      "132/281, train_loss: 0.0581, step time: 0.2528\n",
      "133/281, train_loss: 0.0732, step time: 0.2573\n",
      "134/281, train_loss: 0.0448, step time: 0.2504\n",
      "135/281, train_loss: 0.0498, step time: 0.2526\n",
      "136/281, train_loss: 0.0806, step time: 0.2498\n",
      "137/281, train_loss: 0.0813, step time: 0.2459\n",
      "138/281, train_loss: 0.0723, step time: 0.2488\n",
      "139/281, train_loss: 0.0678, step time: 0.2467\n",
      "140/281, train_loss: 0.0603, step time: 0.2515\n",
      "141/281, train_loss: 0.0681, step time: 0.2545\n",
      "142/281, train_loss: 0.2220, step time: 0.2457\n",
      "143/281, train_loss: 0.0386, step time: 0.2534\n",
      "144/281, train_loss: 0.0446, step time: 0.2462\n",
      "145/281, train_loss: 0.0634, step time: 0.2493\n",
      "146/281, train_loss: 0.0595, step time: 0.2425\n",
      "147/281, train_loss: 0.0610, step time: 0.2444\n",
      "148/281, train_loss: 0.0525, step time: 0.2496\n",
      "149/281, train_loss: 0.0916, step time: 0.2499\n",
      "150/281, train_loss: 0.0306, step time: 0.2491\n",
      "151/281, train_loss: 0.0757, step time: 0.2556\n",
      "152/281, train_loss: 0.0571, step time: 0.2533\n",
      "153/281, train_loss: 0.0611, step time: 0.2513\n",
      "154/281, train_loss: 0.0983, step time: 0.2542\n",
      "155/281, train_loss: 0.0483, step time: 0.2551\n",
      "156/281, train_loss: 0.0542, step time: 0.2533\n",
      "157/281, train_loss: 0.0724, step time: 0.2500\n",
      "158/281, train_loss: 0.0822, step time: 0.2510\n",
      "159/281, train_loss: 0.0737, step time: 0.2492\n",
      "160/281, train_loss: 0.0871, step time: 0.2566\n",
      "161/281, train_loss: 0.0702, step time: 0.2467\n",
      "162/281, train_loss: 0.0578, step time: 0.2516\n",
      "163/281, train_loss: 0.0682, step time: 0.2469\n",
      "164/281, train_loss: 0.0580, step time: 0.2493\n",
      "165/281, train_loss: 0.0778, step time: 0.2481\n",
      "166/281, train_loss: 0.0688, step time: 0.2547\n",
      "167/281, train_loss: 0.0750, step time: 0.2539\n",
      "168/281, train_loss: 0.0372, step time: 0.2507\n",
      "169/281, train_loss: 0.0562, step time: 0.2480\n",
      "170/281, train_loss: 0.0453, step time: 0.2583\n",
      "171/281, train_loss: 0.0721, step time: 0.2532\n",
      "172/281, train_loss: 0.0477, step time: 0.2477\n",
      "173/281, train_loss: 0.0536, step time: 0.2516\n",
      "174/281, train_loss: 0.0544, step time: 0.2487\n",
      "175/281, train_loss: 0.0591, step time: 0.2520\n",
      "176/281, train_loss: 0.0389, step time: 0.2891\n",
      "177/281, train_loss: 0.0627, step time: 0.2578\n",
      "178/281, train_loss: 0.0276, step time: 0.2549\n",
      "179/281, train_loss: 0.0762, step time: 0.2571\n",
      "180/281, train_loss: 0.0568, step time: 0.2578\n",
      "181/281, train_loss: 0.0601, step time: 0.2559\n",
      "182/281, train_loss: 0.0654, step time: 0.2542\n",
      "183/281, train_loss: 0.0404, step time: 0.2519\n",
      "184/281, train_loss: 0.1953, step time: 0.2608\n",
      "185/281, train_loss: 0.0861, step time: 0.2513\n",
      "186/281, train_loss: 0.0574, step time: 0.2530\n",
      "187/281, train_loss: 0.2074, step time: 0.2503\n",
      "188/281, train_loss: 0.0570, step time: 0.2501\n",
      "189/281, train_loss: 0.0730, step time: 0.2553\n",
      "190/281, train_loss: 0.0675, step time: 0.2480\n",
      "191/281, train_loss: 0.0867, step time: 0.2563\n",
      "192/281, train_loss: 0.0518, step time: 0.2554\n",
      "193/281, train_loss: 0.0543, step time: 0.2528\n",
      "194/281, train_loss: 0.2212, step time: 0.2532\n",
      "195/281, train_loss: 0.0644, step time: 0.2555\n",
      "196/281, train_loss: 0.0677, step time: 0.2530\n",
      "197/281, train_loss: 0.0672, step time: 0.2499\n",
      "198/281, train_loss: 0.0683, step time: 0.2499\n",
      "199/281, train_loss: 0.0470, step time: 0.2458\n",
      "200/281, train_loss: 0.0470, step time: 0.2477\n",
      "201/281, train_loss: 0.0253, step time: 0.2512\n",
      "202/281, train_loss: 0.0671, step time: 0.2522\n",
      "203/281, train_loss: 0.0520, step time: 0.2562\n",
      "204/281, train_loss: 0.0987, step time: 0.2511\n",
      "205/281, train_loss: 0.0593, step time: 0.2517\n",
      "206/281, train_loss: 0.0685, step time: 0.2526\n",
      "207/281, train_loss: 0.2094, step time: 0.2711\n",
      "208/281, train_loss: 0.0531, step time: 0.2452\n",
      "209/281, train_loss: 0.2264, step time: 0.2443\n",
      "210/281, train_loss: 0.0665, step time: 0.2489\n",
      "211/281, train_loss: 0.0399, step time: 0.2535\n",
      "212/281, train_loss: 0.0504, step time: 0.2565\n",
      "213/281, train_loss: 0.0474, step time: 0.2525\n",
      "214/281, train_loss: 0.0638, step time: 0.2543\n",
      "215/281, train_loss: 0.0498, step time: 0.2466\n",
      "216/281, train_loss: 0.0500, step time: 0.2541\n",
      "217/281, train_loss: 0.0563, step time: 0.2519\n",
      "218/281, train_loss: 0.0865, step time: 0.2507\n",
      "219/281, train_loss: 0.2023, step time: 0.2543\n",
      "220/281, train_loss: 0.0834, step time: 0.2501\n",
      "221/281, train_loss: 0.0565, step time: 0.2496\n",
      "222/281, train_loss: 0.0509, step time: 0.2512\n",
      "223/281, train_loss: 0.0514, step time: 0.2529\n",
      "224/281, train_loss: 0.0461, step time: 0.2478\n",
      "225/281, train_loss: 0.0589, step time: 0.2537\n",
      "226/281, train_loss: 0.0538, step time: 0.2500\n",
      "227/281, train_loss: 0.0709, step time: 0.2540\n",
      "228/281, train_loss: 0.3785, step time: 0.2561\n",
      "229/281, train_loss: 0.0420, step time: 0.2544\n",
      "230/281, train_loss: 0.3798, step time: 0.2494\n",
      "231/281, train_loss: 0.2104, step time: 0.2490\n",
      "232/281, train_loss: 0.0489, step time: 0.2536\n",
      "233/281, train_loss: 0.0451, step time: 0.2519\n",
      "234/281, train_loss: 0.0619, step time: 0.2545\n",
      "235/281, train_loss: 0.0993, step time: 0.2544\n",
      "236/281, train_loss: 0.0436, step time: 0.2559\n",
      "237/281, train_loss: 0.0428, step time: 0.2553\n",
      "238/281, train_loss: 0.0840, step time: 0.2514\n",
      "239/281, train_loss: 0.0402, step time: 0.2522\n",
      "240/281, train_loss: 0.2147, step time: 0.2513\n",
      "241/281, train_loss: 0.0699, step time: 0.2480\n",
      "242/281, train_loss: 0.2224, step time: 0.2513\n",
      "243/281, train_loss: 0.0633, step time: 0.2491\n",
      "244/281, train_loss: 0.1966, step time: 0.2551\n",
      "245/281, train_loss: 0.0605, step time: 0.2553\n",
      "246/281, train_loss: 0.0684, step time: 0.2693\n",
      "247/281, train_loss: 0.0837, step time: 0.2521\n",
      "248/281, train_loss: 0.0790, step time: 0.2476\n",
      "249/281, train_loss: 0.0549, step time: 0.2494\n",
      "250/281, train_loss: 0.0615, step time: 0.2449\n",
      "251/281, train_loss: 0.0509, step time: 0.2528\n",
      "252/281, train_loss: 0.2141, step time: 0.2485\n",
      "253/281, train_loss: 0.0559, step time: 0.2573\n",
      "254/281, train_loss: 0.0875, step time: 0.2495\n",
      "255/281, train_loss: 0.0669, step time: 0.2516\n",
      "256/281, train_loss: 0.0669, step time: 0.2516\n",
      "257/281, train_loss: 0.0713, step time: 0.2514\n",
      "258/281, train_loss: 0.0287, step time: 0.2633\n",
      "259/281, train_loss: 0.2044, step time: 0.2568\n",
      "260/281, train_loss: 0.0672, step time: 0.2538\n",
      "261/281, train_loss: 0.0765, step time: 0.2540\n",
      "262/281, train_loss: 0.0675, step time: 0.2512\n",
      "263/281, train_loss: 0.0631, step time: 0.2551\n",
      "264/281, train_loss: 0.0515, step time: 0.2525\n",
      "265/281, train_loss: 0.0703, step time: 0.2491\n",
      "266/281, train_loss: 0.2258, step time: 0.2543\n",
      "267/281, train_loss: 0.2312, step time: 0.2564\n",
      "268/281, train_loss: 0.2139, step time: 0.2535\n",
      "269/281, train_loss: 0.0389, step time: 0.2521\n",
      "270/281, train_loss: 0.0412, step time: 0.2537\n",
      "271/281, train_loss: 0.0629, step time: 0.2509\n",
      "272/281, train_loss: 0.0574, step time: 0.2543\n",
      "273/281, train_loss: 0.0780, step time: 0.2529\n",
      "274/281, train_loss: 0.0717, step time: 0.2588\n",
      "275/281, train_loss: 0.0459, step time: 0.2481\n",
      "276/281, train_loss: 0.0678, step time: 0.2498\n",
      "277/281, train_loss: 0.2153, step time: 0.2468\n",
      "278/281, train_loss: 0.2460, step time: 0.2544\n",
      "279/281, train_loss: 0.0892, step time: 0.2564\n",
      "280/281, train_loss: 0.0710, step time: 0.2553\n",
      "281/281, train_loss: 0.2049, step time: 0.2504\n",
      "282/281, train_loss: 0.3930, step time: 0.1531\n",
      "epoch 110 average loss: 0.0859\n",
      "current epoch: 110 current mean dice: 0.9010 tc: 0.8939 wt: 0.9275 et: 0.8913\n",
      "best mean dice: 0.9020 at epoch: 109\n",
      "time consuming of epoch 110 is: 397.6990\n",
      "----------\n",
      "epoch 111/200\n",
      "1/281, train_loss: 0.0537, step time: 0.2697\n",
      "2/281, train_loss: 0.0383, step time: 0.2557\n",
      "3/281, train_loss: 0.0799, step time: 0.2597\n",
      "4/281, train_loss: 0.0621, step time: 0.2564\n",
      "5/281, train_loss: 0.0520, step time: 0.2500\n",
      "6/281, train_loss: 0.0684, step time: 0.2606\n",
      "7/281, train_loss: 0.2192, step time: 0.2566\n",
      "8/281, train_loss: 0.0795, step time: 0.2577\n",
      "9/281, train_loss: 0.0252, step time: 0.2552\n",
      "10/281, train_loss: 0.0823, step time: 0.2538\n",
      "11/281, train_loss: 0.0568, step time: 0.2584\n",
      "12/281, train_loss: 0.2218, step time: 0.2588\n",
      "13/281, train_loss: 0.0549, step time: 0.2553\n",
      "14/281, train_loss: 0.0714, step time: 0.2540\n",
      "15/281, train_loss: 0.0611, step time: 0.2544\n",
      "16/281, train_loss: 0.2251, step time: 0.2569\n",
      "17/281, train_loss: 0.2201, step time: 0.2606\n",
      "18/281, train_loss: 0.0496, step time: 0.2595\n",
      "19/281, train_loss: 0.1158, step time: 0.2593\n",
      "20/281, train_loss: 0.0538, step time: 0.2634\n",
      "21/281, train_loss: 0.0388, step time: 0.2651\n",
      "22/281, train_loss: 0.0469, step time: 0.2547\n",
      "23/281, train_loss: 0.0706, step time: 0.2570\n",
      "24/281, train_loss: 0.0574, step time: 0.2586\n",
      "25/281, train_loss: 0.0674, step time: 0.2583\n",
      "26/281, train_loss: 0.0441, step time: 0.2596\n",
      "27/281, train_loss: 0.0627, step time: 0.2577\n",
      "28/281, train_loss: 0.1145, step time: 0.2600\n",
      "29/281, train_loss: 0.0523, step time: 0.2522\n",
      "30/281, train_loss: 0.0856, step time: 0.2544\n",
      "31/281, train_loss: 0.0450, step time: 0.2507\n",
      "32/281, train_loss: 0.0621, step time: 0.2548\n",
      "33/281, train_loss: 0.0764, step time: 0.2569\n",
      "34/281, train_loss: 0.0590, step time: 0.2579\n",
      "35/281, train_loss: 0.0852, step time: 0.2635\n",
      "36/281, train_loss: 0.0618, step time: 0.2706\n",
      "37/281, train_loss: 0.0646, step time: 0.2583\n",
      "38/281, train_loss: 0.0777, step time: 0.2578\n",
      "39/281, train_loss: 0.0723, step time: 0.2558\n",
      "40/281, train_loss: 0.0834, step time: 0.2628\n",
      "41/281, train_loss: 0.0422, step time: 0.2591\n",
      "42/281, train_loss: 0.0632, step time: 0.2612\n",
      "43/281, train_loss: 0.2130, step time: 0.2599\n",
      "44/281, train_loss: 0.0338, step time: 0.2540\n",
      "45/281, train_loss: 0.0622, step time: 0.2553\n",
      "46/281, train_loss: 0.0474, step time: 0.2528\n",
      "47/281, train_loss: 0.0830, step time: 0.2590\n",
      "48/281, train_loss: 0.0655, step time: 0.2552\n",
      "49/281, train_loss: 0.0354, step time: 0.2570\n",
      "50/281, train_loss: 0.0905, step time: 0.2592\n",
      "51/281, train_loss: 0.0544, step time: 0.2592\n",
      "52/281, train_loss: 0.0634, step time: 0.2596\n",
      "53/281, train_loss: 0.0447, step time: 0.2583\n",
      "54/281, train_loss: 0.0490, step time: 0.2547\n",
      "55/281, train_loss: 0.2241, step time: 0.2588\n",
      "56/281, train_loss: 0.0595, step time: 0.2609\n",
      "57/281, train_loss: 0.0369, step time: 0.2596\n",
      "58/281, train_loss: 0.2125, step time: 0.2565\n",
      "59/281, train_loss: 0.0768, step time: 0.2532\n",
      "60/281, train_loss: 0.3938, step time: 0.2586\n",
      "61/281, train_loss: 0.0828, step time: 0.2599\n",
      "62/281, train_loss: 0.0445, step time: 0.2628\n",
      "63/281, train_loss: 0.0468, step time: 0.2537\n",
      "64/281, train_loss: 0.2063, step time: 0.2644\n",
      "65/281, train_loss: 0.1076, step time: 0.2605\n",
      "66/281, train_loss: 0.0496, step time: 0.2568\n",
      "67/281, train_loss: 0.0543, step time: 0.2552\n",
      "68/281, train_loss: 0.2222, step time: 0.2583\n",
      "69/281, train_loss: 0.0659, step time: 0.2628\n",
      "70/281, train_loss: 0.0705, step time: 0.2633\n",
      "71/281, train_loss: 0.1050, step time: 0.2591\n",
      "72/281, train_loss: 0.2061, step time: 0.2614\n",
      "73/281, train_loss: 0.0484, step time: 0.2653\n",
      "74/281, train_loss: 0.0594, step time: 0.2653\n",
      "75/281, train_loss: 0.0775, step time: 0.2614\n",
      "76/281, train_loss: 0.0577, step time: 0.2614\n",
      "77/281, train_loss: 0.0254, step time: 0.2632\n",
      "78/281, train_loss: 0.0652, step time: 0.2579\n",
      "79/281, train_loss: 0.0627, step time: 0.2541\n",
      "80/281, train_loss: 0.0587, step time: 0.2614\n",
      "81/281, train_loss: 0.0592, step time: 0.2549\n",
      "82/281, train_loss: 0.0655, step time: 0.2538\n",
      "83/281, train_loss: 0.2380, step time: 0.2588\n",
      "84/281, train_loss: 0.0589, step time: 0.2589\n",
      "85/281, train_loss: 0.0599, step time: 0.2557\n",
      "86/281, train_loss: 0.0728, step time: 0.2559\n",
      "87/281, train_loss: 0.0368, step time: 0.2500\n",
      "88/281, train_loss: 0.2020, step time: 0.2554\n",
      "89/281, train_loss: 0.1980, step time: 0.2549\n",
      "90/281, train_loss: 0.0341, step time: 0.2563\n",
      "91/281, train_loss: 0.0605, step time: 0.2565\n",
      "92/281, train_loss: 0.0368, step time: 0.2581\n",
      "93/281, train_loss: 0.0683, step time: 0.2575\n",
      "94/281, train_loss: 0.0568, step time: 0.2560\n",
      "95/281, train_loss: 0.0493, step time: 0.2569\n",
      "96/281, train_loss: 0.2175, step time: 0.2545\n",
      "97/281, train_loss: 0.0612, step time: 0.2636\n",
      "98/281, train_loss: 0.0510, step time: 0.2544\n",
      "99/281, train_loss: 0.0361, step time: 0.2557\n",
      "100/281, train_loss: 0.0487, step time: 0.2600\n",
      "101/281, train_loss: 0.0788, step time: 0.2549\n",
      "102/281, train_loss: 0.0812, step time: 0.2587\n",
      "103/281, train_loss: 0.0589, step time: 0.2556\n",
      "104/281, train_loss: 0.0534, step time: 0.2609\n",
      "105/281, train_loss: 0.0889, step time: 0.2629\n",
      "106/281, train_loss: 0.0369, step time: 0.2563\n",
      "107/281, train_loss: 0.0547, step time: 0.2580\n",
      "108/281, train_loss: 0.0758, step time: 0.2536\n",
      "109/281, train_loss: 0.0695, step time: 0.2559\n",
      "110/281, train_loss: 0.0491, step time: 0.2635\n",
      "111/281, train_loss: 0.2389, step time: 0.2585\n",
      "112/281, train_loss: 0.0681, step time: 0.2647\n",
      "113/281, train_loss: 0.0525, step time: 0.2672\n",
      "114/281, train_loss: 0.2341, step time: 0.2547\n",
      "115/281, train_loss: 0.0936, step time: 0.2556\n",
      "116/281, train_loss: 0.2332, step time: 0.2585\n",
      "117/281, train_loss: 0.3715, step time: 0.2569\n",
      "118/281, train_loss: 0.0602, step time: 0.2541\n",
      "119/281, train_loss: 0.0585, step time: 0.2575\n",
      "120/281, train_loss: 0.0681, step time: 0.2571\n",
      "121/281, train_loss: 0.0771, step time: 0.2531\n",
      "122/281, train_loss: 0.2211, step time: 0.2524\n",
      "123/281, train_loss: 0.0534, step time: 0.2557\n",
      "124/281, train_loss: 0.0606, step time: 0.2572\n",
      "125/281, train_loss: 0.1029, step time: 0.2578\n",
      "126/281, train_loss: 0.0401, step time: 0.2551\n",
      "127/281, train_loss: 0.0486, step time: 0.2541\n",
      "128/281, train_loss: 0.0714, step time: 0.2572\n",
      "129/281, train_loss: 0.2360, step time: 0.2596\n",
      "130/281, train_loss: 0.0435, step time: 0.2656\n",
      "131/281, train_loss: 0.0550, step time: 0.2572\n",
      "132/281, train_loss: 0.0364, step time: 0.2562\n",
      "133/281, train_loss: 0.0455, step time: 0.2553\n",
      "134/281, train_loss: 0.0503, step time: 0.2560\n",
      "135/281, train_loss: 0.0680, step time: 0.2506\n",
      "136/281, train_loss: 0.0353, step time: 0.2794\n",
      "137/281, train_loss: 0.0421, step time: 0.2589\n",
      "138/281, train_loss: 0.0767, step time: 0.2607\n",
      "139/281, train_loss: 0.0507, step time: 0.2577\n",
      "140/281, train_loss: 0.2056, step time: 0.2537\n",
      "141/281, train_loss: 0.0465, step time: 0.2523\n",
      "142/281, train_loss: 0.0333, step time: 0.2539\n",
      "143/281, train_loss: 0.0464, step time: 0.2552\n",
      "144/281, train_loss: 0.0500, step time: 0.2636\n",
      "145/281, train_loss: 0.0557, step time: 0.2569\n",
      "146/281, train_loss: 0.0755, step time: 0.2566\n",
      "147/281, train_loss: 0.0385, step time: 0.2593\n",
      "148/281, train_loss: 0.0489, step time: 0.2722\n",
      "149/281, train_loss: 0.0781, step time: 0.2554\n",
      "150/281, train_loss: 0.0571, step time: 0.2500\n",
      "151/281, train_loss: 0.0509, step time: 0.2477\n",
      "152/281, train_loss: 0.0356, step time: 0.2474\n",
      "153/281, train_loss: 0.0544, step time: 0.2536\n",
      "154/281, train_loss: 0.0690, step time: 0.2590\n",
      "155/281, train_loss: 0.0689, step time: 0.2545\n",
      "156/281, train_loss: 0.0414, step time: 0.2562\n",
      "157/281, train_loss: 0.0414, step time: 0.2552\n",
      "158/281, train_loss: 0.0643, step time: 0.2499\n",
      "159/281, train_loss: 0.0875, step time: 0.2531\n",
      "160/281, train_loss: 0.0400, step time: 0.2530\n",
      "161/281, train_loss: 0.0590, step time: 0.2507\n",
      "162/281, train_loss: 0.0779, step time: 0.2540\n",
      "163/281, train_loss: 0.0645, step time: 0.2510\n",
      "164/281, train_loss: 0.0487, step time: 0.2528\n",
      "165/281, train_loss: 0.0649, step time: 0.2530\n",
      "166/281, train_loss: 0.0660, step time: 0.2539\n",
      "167/281, train_loss: 0.0455, step time: 0.2545\n",
      "168/281, train_loss: 0.0550, step time: 0.2505\n",
      "169/281, train_loss: 0.0446, step time: 0.2533\n",
      "170/281, train_loss: 0.0423, step time: 0.2541\n",
      "171/281, train_loss: 0.2172, step time: 0.2559\n",
      "172/281, train_loss: 0.0613, step time: 0.2617\n",
      "173/281, train_loss: 0.0311, step time: 0.2557\n",
      "174/281, train_loss: 0.0673, step time: 0.2544\n",
      "175/281, train_loss: 0.0478, step time: 0.2513\n",
      "176/281, train_loss: 0.0680, step time: 0.2520\n",
      "177/281, train_loss: 0.0620, step time: 0.2473\n",
      "178/281, train_loss: 0.0344, step time: 0.2528\n",
      "179/281, train_loss: 0.2298, step time: 0.2530\n",
      "180/281, train_loss: 0.0733, step time: 0.2561\n",
      "181/281, train_loss: 0.0504, step time: 0.2517\n",
      "182/281, train_loss: 0.0296, step time: 0.2604\n",
      "183/281, train_loss: 0.0545, step time: 0.2603\n",
      "184/281, train_loss: 0.1030, step time: 0.2536\n",
      "185/281, train_loss: 0.0393, step time: 0.2566\n",
      "186/281, train_loss: 0.2246, step time: 0.2610\n",
      "187/281, train_loss: 0.0688, step time: 0.2551\n",
      "188/281, train_loss: 0.0859, step time: 0.2572\n",
      "189/281, train_loss: 0.0456, step time: 0.2514\n",
      "190/281, train_loss: 0.0593, step time: 0.2581\n",
      "191/281, train_loss: 0.2278, step time: 0.2554\n",
      "192/281, train_loss: 0.2119, step time: 0.2535\n",
      "193/281, train_loss: 0.0784, step time: 0.2513\n",
      "194/281, train_loss: 0.2226, step time: 0.2635\n",
      "195/281, train_loss: 0.0693, step time: 0.2585\n",
      "196/281, train_loss: 0.2101, step time: 0.2566\n",
      "197/281, train_loss: 0.1076, step time: 0.2539\n",
      "198/281, train_loss: 0.0464, step time: 0.2733\n",
      "199/281, train_loss: 0.2288, step time: 0.2568\n",
      "200/281, train_loss: 0.0544, step time: 0.2543\n",
      "201/281, train_loss: 0.2155, step time: 0.2565\n",
      "202/281, train_loss: 0.0986, step time: 0.2567\n",
      "203/281, train_loss: 0.0589, step time: 0.2540\n",
      "204/281, train_loss: 0.2248, step time: 0.2568\n",
      "205/281, train_loss: 0.0697, step time: 0.2580\n",
      "206/281, train_loss: 0.0665, step time: 0.2576\n",
      "207/281, train_loss: 0.2316, step time: 0.2632\n",
      "208/281, train_loss: 0.0788, step time: 0.2588\n",
      "209/281, train_loss: 0.0621, step time: 0.2585\n",
      "210/281, train_loss: 0.0724, step time: 0.2558\n",
      "211/281, train_loss: 0.0437, step time: 0.2522\n",
      "212/281, train_loss: 0.0614, step time: 0.2589\n",
      "213/281, train_loss: 0.0703, step time: 0.2546\n",
      "214/281, train_loss: 0.0683, step time: 0.2540\n",
      "215/281, train_loss: 0.0569, step time: 0.2571\n",
      "216/281, train_loss: 0.0659, step time: 0.2585\n",
      "217/281, train_loss: 0.0556, step time: 0.2536\n",
      "218/281, train_loss: 0.0637, step time: 0.2490\n",
      "219/281, train_loss: 0.0476, step time: 0.2504\n",
      "220/281, train_loss: 0.0402, step time: 0.2493\n",
      "221/281, train_loss: 0.0473, step time: 0.2552\n",
      "222/281, train_loss: 0.0761, step time: 0.2553\n",
      "223/281, train_loss: 0.0623, step time: 0.2584\n",
      "224/281, train_loss: 0.0489, step time: 0.2573\n",
      "225/281, train_loss: 0.0674, step time: 0.2529\n",
      "226/281, train_loss: 0.0553, step time: 0.2563\n",
      "227/281, train_loss: 0.2266, step time: 0.2551\n",
      "228/281, train_loss: 0.0490, step time: 0.2545\n",
      "229/281, train_loss: 0.0447, step time: 0.2608\n",
      "230/281, train_loss: 0.0688, step time: 0.2576\n",
      "231/281, train_loss: 0.0584, step time: 0.2560\n",
      "232/281, train_loss: 0.2328, step time: 0.2589\n",
      "233/281, train_loss: 0.0649, step time: 0.2524\n",
      "234/281, train_loss: 0.0537, step time: 0.2552\n",
      "235/281, train_loss: 0.0498, step time: 0.2613\n",
      "236/281, train_loss: 0.2046, step time: 0.2560\n",
      "237/281, train_loss: 0.0568, step time: 0.2566\n",
      "238/281, train_loss: 0.0807, step time: 0.2601\n",
      "239/281, train_loss: 0.0297, step time: 0.2576\n",
      "240/281, train_loss: 0.0778, step time: 0.2570\n",
      "241/281, train_loss: 0.0516, step time: 0.2582\n",
      "242/281, train_loss: 0.0462, step time: 0.2533\n",
      "243/281, train_loss: 0.0556, step time: 0.2567\n",
      "244/281, train_loss: 0.0582, step time: 0.2584\n",
      "245/281, train_loss: 0.0519, step time: 0.2522\n",
      "246/281, train_loss: 0.1008, step time: 0.2502\n",
      "247/281, train_loss: 0.0463, step time: 0.2540\n",
      "248/281, train_loss: 0.0658, step time: 0.2556\n",
      "249/281, train_loss: 0.0513, step time: 0.2524\n",
      "250/281, train_loss: 0.0823, step time: 0.2528\n",
      "251/281, train_loss: 0.2027, step time: 0.2578\n",
      "252/281, train_loss: 0.0582, step time: 0.2606\n",
      "253/281, train_loss: 0.0624, step time: 0.2650\n",
      "254/281, train_loss: 0.0315, step time: 0.2520\n",
      "255/281, train_loss: 0.0459, step time: 0.2557\n",
      "256/281, train_loss: 0.0695, step time: 0.2570\n",
      "257/281, train_loss: 0.0377, step time: 0.2570\n",
      "258/281, train_loss: 0.0834, step time: 0.2586\n",
      "259/281, train_loss: 0.2167, step time: 0.2553\n",
      "260/281, train_loss: 0.0286, step time: 0.2541\n",
      "261/281, train_loss: 0.0497, step time: 0.2638\n",
      "262/281, train_loss: 0.0449, step time: 0.2679\n",
      "263/281, train_loss: 0.0490, step time: 0.2511\n",
      "264/281, train_loss: 0.0518, step time: 0.2725\n",
      "265/281, train_loss: 0.1892, step time: 0.2526\n",
      "266/281, train_loss: 0.0919, step time: 0.2588\n",
      "267/281, train_loss: 0.0705, step time: 0.2569\n",
      "268/281, train_loss: 0.0717, step time: 0.2562\n",
      "269/281, train_loss: 0.0582, step time: 0.2504\n",
      "270/281, train_loss: 0.2407, step time: 0.2543\n",
      "271/281, train_loss: 0.0726, step time: 0.2525\n",
      "272/281, train_loss: 0.0762, step time: 0.2549\n",
      "273/281, train_loss: 0.0569, step time: 0.2566\n",
      "274/281, train_loss: 0.2187, step time: 0.2509\n",
      "275/281, train_loss: 0.0553, step time: 0.2527\n",
      "276/281, train_loss: 0.0419, step time: 0.2505\n",
      "277/281, train_loss: 0.0350, step time: 0.2606\n",
      "278/281, train_loss: 0.1057, step time: 0.2615\n",
      "279/281, train_loss: 0.0770, step time: 0.2570\n",
      "280/281, train_loss: 0.0650, step time: 0.2527\n",
      "281/281, train_loss: 0.0688, step time: 0.2523\n",
      "282/281, train_loss: 0.1067, step time: 0.1537\n",
      "epoch 111 average loss: 0.0846\n",
      "current epoch: 111 current mean dice: 0.9018 tc: 0.8946 wt: 0.9269 et: 0.8940\n",
      "best mean dice: 0.9020 at epoch: 109\n",
      "time consuming of epoch 111 is: 372.2999\n",
      "----------\n",
      "epoch 112/200\n",
      "1/281, train_loss: 0.0857, step time: 0.2675\n",
      "2/281, train_loss: 0.0394, step time: 0.2570\n",
      "3/281, train_loss: 0.0912, step time: 0.2606\n",
      "4/281, train_loss: 0.0519, step time: 0.2591\n",
      "5/281, train_loss: 0.0606, step time: 0.2589\n",
      "6/281, train_loss: 0.2020, step time: 0.2579\n",
      "7/281, train_loss: 0.2249, step time: 0.2550\n",
      "8/281, train_loss: 0.0621, step time: 0.2608\n",
      "9/281, train_loss: 0.0498, step time: 0.2546\n",
      "10/281, train_loss: 0.0268, step time: 0.2558\n",
      "11/281, train_loss: 0.2410, step time: 0.2550\n",
      "12/281, train_loss: 0.0724, step time: 0.2497\n",
      "13/281, train_loss: 0.0645, step time: 0.2459\n",
      "14/281, train_loss: 0.2205, step time: 0.2516\n",
      "15/281, train_loss: 0.0442, step time: 0.2487\n",
      "16/281, train_loss: 0.0323, step time: 0.2505\n",
      "17/281, train_loss: 0.0451, step time: 0.2643\n",
      "18/281, train_loss: 0.0513, step time: 0.2525\n",
      "19/281, train_loss: 0.0825, step time: 0.2565\n",
      "20/281, train_loss: 0.0496, step time: 0.2532\n",
      "21/281, train_loss: 0.0561, step time: 0.2537\n",
      "22/281, train_loss: 0.0528, step time: 0.2818\n",
      "23/281, train_loss: 0.0335, step time: 0.2670\n",
      "24/281, train_loss: 0.0543, step time: 0.2528\n",
      "25/281, train_loss: 0.0435, step time: 0.2500\n",
      "26/281, train_loss: 0.0497, step time: 0.2585\n",
      "27/281, train_loss: 0.0448, step time: 0.2519\n",
      "28/281, train_loss: 0.0627, step time: 0.2563\n",
      "29/281, train_loss: 0.2197, step time: 0.2496\n",
      "30/281, train_loss: 0.0469, step time: 0.2525\n",
      "31/281, train_loss: 0.0528, step time: 0.2554\n",
      "32/281, train_loss: 0.0431, step time: 0.2492\n",
      "33/281, train_loss: 0.0715, step time: 0.2679\n",
      "34/281, train_loss: 0.2109, step time: 0.2628\n",
      "35/281, train_loss: 0.0497, step time: 0.2477\n",
      "36/281, train_loss: 0.0587, step time: 0.2506\n",
      "37/281, train_loss: 0.2178, step time: 0.2568\n",
      "38/281, train_loss: 0.0529, step time: 0.2590\n",
      "39/281, train_loss: 0.0853, step time: 0.2597\n",
      "40/281, train_loss: 0.0431, step time: 0.2528\n",
      "41/281, train_loss: 0.0461, step time: 0.2575\n",
      "42/281, train_loss: 0.0463, step time: 0.2517\n",
      "43/281, train_loss: 0.0709, step time: 0.2533\n",
      "44/281, train_loss: 0.0715, step time: 0.2599\n",
      "45/281, train_loss: 0.0436, step time: 0.2574\n",
      "46/281, train_loss: 0.2117, step time: 0.2569\n",
      "47/281, train_loss: 0.0627, step time: 0.2567\n",
      "48/281, train_loss: 0.0746, step time: 0.2628\n",
      "49/281, train_loss: 0.0303, step time: 0.2576\n",
      "50/281, train_loss: 0.0433, step time: 0.2570\n",
      "51/281, train_loss: 0.0637, step time: 0.2523\n",
      "52/281, train_loss: 0.0797, step time: 0.2531\n",
      "53/281, train_loss: 0.0734, step time: 0.2531\n",
      "54/281, train_loss: 0.0391, step time: 0.2550\n",
      "55/281, train_loss: 0.0640, step time: 0.2583\n",
      "56/281, train_loss: 0.0359, step time: 0.2547\n",
      "57/281, train_loss: 0.0742, step time: 0.2507\n",
      "58/281, train_loss: 0.2245, step time: 0.2532\n",
      "59/281, train_loss: 0.2349, step time: 0.2516\n",
      "60/281, train_loss: 0.0674, step time: 0.2534\n",
      "61/281, train_loss: 0.0633, step time: 0.2487\n",
      "62/281, train_loss: 0.0643, step time: 0.2588\n",
      "63/281, train_loss: 0.0542, step time: 0.2575\n",
      "64/281, train_loss: 0.0558, step time: 0.2550\n",
      "65/281, train_loss: 0.0984, step time: 0.2510\n",
      "66/281, train_loss: 0.0624, step time: 0.2524\n",
      "67/281, train_loss: 0.2214, step time: 0.2552\n",
      "68/281, train_loss: 0.0415, step time: 0.2511\n",
      "69/281, train_loss: 0.0726, step time: 0.2497\n",
      "70/281, train_loss: 0.0576, step time: 0.2527\n",
      "71/281, train_loss: 0.0642, step time: 0.2551\n",
      "72/281, train_loss: 0.0645, step time: 0.2553\n",
      "73/281, train_loss: 0.2580, step time: 0.2545\n",
      "74/281, train_loss: 0.0362, step time: 0.2585\n",
      "75/281, train_loss: 0.0720, step time: 0.2614\n",
      "76/281, train_loss: 0.0336, step time: 0.2579\n",
      "77/281, train_loss: 0.2136, step time: 0.2595\n",
      "78/281, train_loss: 0.0702, step time: 0.2488\n",
      "79/281, train_loss: 0.0709, step time: 0.2498\n",
      "80/281, train_loss: 0.0523, step time: 0.2579\n",
      "81/281, train_loss: 0.2206, step time: 0.2562\n",
      "82/281, train_loss: 0.0616, step time: 0.2589\n",
      "83/281, train_loss: 0.0605, step time: 0.2578\n",
      "84/281, train_loss: 0.1109, step time: 0.2556\n",
      "85/281, train_loss: 0.0813, step time: 0.2552\n",
      "86/281, train_loss: 0.0282, step time: 0.2559\n",
      "87/281, train_loss: 0.0546, step time: 0.2557\n",
      "88/281, train_loss: 0.1093, step time: 0.2533\n",
      "89/281, train_loss: 0.2091, step time: 0.2518\n",
      "90/281, train_loss: 0.0679, step time: 0.2526\n",
      "91/281, train_loss: 0.0861, step time: 0.2576\n",
      "92/281, train_loss: 0.0575, step time: 0.2709\n",
      "93/281, train_loss: 0.1080, step time: 0.2735\n",
      "94/281, train_loss: 0.0366, step time: 0.2515\n",
      "95/281, train_loss: 0.2251, step time: 0.2578\n",
      "96/281, train_loss: 0.0499, step time: 0.2539\n",
      "97/281, train_loss: 0.0614, step time: 0.2543\n",
      "98/281, train_loss: 0.0774, step time: 0.2568\n",
      "99/281, train_loss: 0.0738, step time: 0.2570\n",
      "100/281, train_loss: 0.0547, step time: 0.2465\n",
      "101/281, train_loss: 0.0632, step time: 0.2482\n",
      "102/281, train_loss: 0.0500, step time: 0.2518\n",
      "103/281, train_loss: 0.0669, step time: 0.2513\n",
      "104/281, train_loss: 0.0716, step time: 0.2540\n",
      "105/281, train_loss: 0.0739, step time: 0.2542\n",
      "106/281, train_loss: 0.0630, step time: 0.2574\n",
      "107/281, train_loss: 0.0624, step time: 0.2567\n",
      "108/281, train_loss: 0.0386, step time: 0.2583\n",
      "109/281, train_loss: 0.2379, step time: 0.2639\n",
      "110/281, train_loss: 0.0547, step time: 0.2586\n",
      "111/281, train_loss: 0.0499, step time: 0.2566\n",
      "112/281, train_loss: 0.0639, step time: 0.2481\n",
      "113/281, train_loss: 0.0659, step time: 0.2570\n",
      "114/281, train_loss: 0.2175, step time: 0.2543\n",
      "115/281, train_loss: 0.0728, step time: 0.2536\n",
      "116/281, train_loss: 0.0720, step time: 0.2561\n",
      "117/281, train_loss: 0.0729, step time: 0.2558\n",
      "118/281, train_loss: 0.0821, step time: 0.2551\n",
      "119/281, train_loss: 0.0634, step time: 0.2587\n",
      "120/281, train_loss: 0.0663, step time: 0.2527\n",
      "121/281, train_loss: 0.0577, step time: 0.2566\n",
      "122/281, train_loss: 0.0686, step time: 0.2526\n",
      "123/281, train_loss: 0.0815, step time: 0.2510\n",
      "124/281, train_loss: 0.0439, step time: 0.2518\n",
      "125/281, train_loss: 0.2270, step time: 0.2550\n",
      "126/281, train_loss: 0.0334, step time: 0.2519\n",
      "127/281, train_loss: 0.1938, step time: 0.2543\n",
      "128/281, train_loss: 0.0622, step time: 0.2526\n",
      "129/281, train_loss: 0.0496, step time: 0.2541\n",
      "130/281, train_loss: 0.0703, step time: 0.2547\n",
      "131/281, train_loss: 0.0782, step time: 0.2567\n",
      "132/281, train_loss: 0.2210, step time: 0.2538\n",
      "133/281, train_loss: 0.0718, step time: 0.2544\n",
      "134/281, train_loss: 0.0473, step time: 0.2537\n",
      "135/281, train_loss: 0.0571, step time: 0.2472\n",
      "136/281, train_loss: 0.0482, step time: 0.2509\n",
      "137/281, train_loss: 0.0675, step time: 0.2574\n",
      "138/281, train_loss: 0.0486, step time: 0.2534\n",
      "139/281, train_loss: 0.0592, step time: 0.2567\n",
      "140/281, train_loss: 0.0621, step time: 0.2522\n",
      "141/281, train_loss: 0.0730, step time: 0.2567\n",
      "142/281, train_loss: 0.0740, step time: 0.2619\n",
      "143/281, train_loss: 0.0595, step time: 0.2539\n",
      "144/281, train_loss: 0.0650, step time: 0.2610\n",
      "145/281, train_loss: 0.2176, step time: 0.2544\n",
      "146/281, train_loss: 0.0442, step time: 0.2708\n",
      "147/281, train_loss: 0.0664, step time: 0.2924\n",
      "148/281, train_loss: 0.2339, step time: 0.2550\n",
      "149/281, train_loss: 0.0941, step time: 0.2721\n",
      "150/281, train_loss: 0.0494, step time: 0.2644\n",
      "151/281, train_loss: 0.0785, step time: 0.2598\n",
      "152/281, train_loss: 0.1010, step time: 0.2554\n",
      "153/281, train_loss: 0.0439, step time: 0.2575\n",
      "154/281, train_loss: 0.0508, step time: 0.2558\n",
      "155/281, train_loss: 0.0415, step time: 0.2553\n",
      "156/281, train_loss: 0.0518, step time: 0.2557\n",
      "157/281, train_loss: 0.2259, step time: 0.2562\n",
      "158/281, train_loss: 0.0481, step time: 0.2529\n",
      "159/281, train_loss: 0.0654, step time: 0.2558\n",
      "160/281, train_loss: 0.3744, step time: 0.2540\n",
      "161/281, train_loss: 0.0392, step time: 0.2513\n",
      "162/281, train_loss: 0.0702, step time: 0.2563\n",
      "163/281, train_loss: 0.0717, step time: 0.2566\n",
      "164/281, train_loss: 0.0611, step time: 0.2605\n",
      "165/281, train_loss: 0.0610, step time: 0.2516\n",
      "166/281, train_loss: 0.0578, step time: 0.2502\n",
      "167/281, train_loss: 0.0725, step time: 0.2514\n",
      "168/281, train_loss: 0.0817, step time: 0.2544\n",
      "169/281, train_loss: 0.0715, step time: 0.2515\n",
      "170/281, train_loss: 0.2214, step time: 0.2755\n",
      "171/281, train_loss: 0.0487, step time: 0.2558\n",
      "172/281, train_loss: 0.0468, step time: 0.2497\n",
      "173/281, train_loss: 0.0635, step time: 0.2515\n",
      "174/281, train_loss: 0.0698, step time: 0.2524\n",
      "175/281, train_loss: 0.2055, step time: 0.2497\n",
      "176/281, train_loss: 0.0488, step time: 0.2527\n",
      "177/281, train_loss: 0.2111, step time: 0.2472\n",
      "178/281, train_loss: 0.2293, step time: 0.2533\n",
      "179/281, train_loss: 0.0659, step time: 0.2495\n",
      "180/281, train_loss: 0.2423, step time: 0.2528\n",
      "181/281, train_loss: 0.2089, step time: 0.2497\n",
      "182/281, train_loss: 0.0623, step time: 0.2552\n",
      "183/281, train_loss: 0.0650, step time: 0.2482\n",
      "184/281, train_loss: 0.0810, step time: 0.2527\n",
      "185/281, train_loss: 0.0288, step time: 0.2471\n",
      "186/281, train_loss: 0.2225, step time: 0.2490\n",
      "187/281, train_loss: 0.0368, step time: 0.2532\n",
      "188/281, train_loss: 0.0573, step time: 0.2536\n",
      "189/281, train_loss: 0.2276, step time: 0.2605\n",
      "190/281, train_loss: 0.0728, step time: 0.2447\n",
      "191/281, train_loss: 0.0532, step time: 0.2482\n",
      "192/281, train_loss: 0.0556, step time: 0.2533\n",
      "193/281, train_loss: 0.0437, step time: 0.2453\n",
      "194/281, train_loss: 0.2477, step time: 0.2631\n",
      "195/281, train_loss: 0.0583, step time: 0.2563\n",
      "196/281, train_loss: 0.0704, step time: 0.2549\n",
      "197/281, train_loss: 0.0325, step time: 0.2530\n",
      "198/281, train_loss: 0.0696, step time: 0.2495\n",
      "199/281, train_loss: 0.0514, step time: 0.2491\n",
      "200/281, train_loss: 0.0364, step time: 0.2487\n",
      "201/281, train_loss: 0.2097, step time: 0.2551\n",
      "202/281, train_loss: 0.0453, step time: 0.2553\n",
      "203/281, train_loss: 0.0812, step time: 0.2528\n",
      "204/281, train_loss: 0.2130, step time: 0.2509\n",
      "205/281, train_loss: 0.0594, step time: 0.2503\n",
      "206/281, train_loss: 0.0581, step time: 0.2532\n",
      "207/281, train_loss: 0.0460, step time: 0.2517\n",
      "208/281, train_loss: 0.2213, step time: 0.2502\n",
      "209/281, train_loss: 0.0728, step time: 0.2494\n",
      "210/281, train_loss: 0.0866, step time: 0.2498\n",
      "211/281, train_loss: 0.0608, step time: 0.2479\n",
      "212/281, train_loss: 0.0464, step time: 0.2604\n",
      "213/281, train_loss: 0.0529, step time: 0.2493\n",
      "214/281, train_loss: 0.0549, step time: 0.2501\n",
      "215/281, train_loss: 0.0544, step time: 0.2491\n",
      "216/281, train_loss: 0.0539, step time: 0.2591\n",
      "217/281, train_loss: 0.2122, step time: 0.2506\n",
      "218/281, train_loss: 0.0487, step time: 0.2535\n",
      "219/281, train_loss: 0.0561, step time: 0.2522\n",
      "220/281, train_loss: 0.0609, step time: 0.2576\n",
      "221/281, train_loss: 0.0331, step time: 0.2525\n",
      "222/281, train_loss: 0.0570, step time: 0.2511\n",
      "223/281, train_loss: 0.0374, step time: 0.2520\n",
      "224/281, train_loss: 0.0666, step time: 0.2567\n",
      "225/281, train_loss: 0.0616, step time: 0.2546\n",
      "226/281, train_loss: 0.0739, step time: 0.2572\n",
      "227/281, train_loss: 0.0840, step time: 0.2487\n",
      "228/281, train_loss: 0.0527, step time: 0.2547\n",
      "229/281, train_loss: 0.0609, step time: 0.2526\n",
      "230/281, train_loss: 0.0529, step time: 0.2539\n",
      "231/281, train_loss: 0.0803, step time: 0.2504\n",
      "232/281, train_loss: 0.0750, step time: 0.2486\n",
      "233/281, train_loss: 0.0504, step time: 0.2587\n",
      "234/281, train_loss: 0.0656, step time: 0.2518\n",
      "235/281, train_loss: 0.0663, step time: 0.2534\n",
      "236/281, train_loss: 0.0487, step time: 0.2498\n",
      "237/281, train_loss: 0.0685, step time: 0.2576\n",
      "238/281, train_loss: 0.0914, step time: 0.2564\n",
      "239/281, train_loss: 0.0522, step time: 0.2542\n",
      "240/281, train_loss: 0.0565, step time: 0.2543\n",
      "241/281, train_loss: 0.0290, step time: 0.2530\n",
      "242/281, train_loss: 0.2151, step time: 0.2524\n",
      "243/281, train_loss: 0.0800, step time: 0.2482\n",
      "244/281, train_loss: 0.0364, step time: 0.2519\n",
      "245/281, train_loss: 0.0517, step time: 0.2479\n",
      "246/281, train_loss: 0.0455, step time: 0.2519\n",
      "247/281, train_loss: 0.2033, step time: 0.2479\n",
      "248/281, train_loss: 0.0442, step time: 0.2488\n",
      "249/281, train_loss: 0.0357, step time: 0.2528\n",
      "250/281, train_loss: 0.0686, step time: 0.2526\n",
      "251/281, train_loss: 0.0528, step time: 0.2540\n",
      "252/281, train_loss: 0.0561, step time: 0.2512\n",
      "253/281, train_loss: 0.2198, step time: 0.2501\n",
      "254/281, train_loss: 0.0709, step time: 0.2481\n",
      "255/281, train_loss: 0.0630, step time: 0.2471\n",
      "256/281, train_loss: 0.0631, step time: 0.2521\n",
      "257/281, train_loss: 0.0674, step time: 0.2546\n",
      "258/281, train_loss: 0.0517, step time: 0.2566\n",
      "259/281, train_loss: 0.0518, step time: 0.2469\n",
      "260/281, train_loss: 0.0715, step time: 0.2567\n",
      "261/281, train_loss: 0.0670, step time: 0.2961\n",
      "262/281, train_loss: 0.0690, step time: 0.2482\n",
      "263/281, train_loss: 0.0691, step time: 0.2500\n",
      "264/281, train_loss: 0.0771, step time: 0.2518\n",
      "265/281, train_loss: 0.0501, step time: 0.2497\n",
      "266/281, train_loss: 0.0588, step time: 0.2490\n",
      "267/281, train_loss: 0.0615, step time: 0.2468\n",
      "268/281, train_loss: 0.0529, step time: 0.2513\n",
      "269/281, train_loss: 0.0428, step time: 0.2515\n",
      "270/281, train_loss: 0.0588, step time: 0.2508\n",
      "271/281, train_loss: 0.0448, step time: 0.2501\n",
      "272/281, train_loss: 0.0351, step time: 0.2474\n",
      "273/281, train_loss: 0.0377, step time: 0.2495\n",
      "274/281, train_loss: 0.1051, step time: 0.2445\n",
      "275/281, train_loss: 0.2390, step time: 0.2437\n",
      "276/281, train_loss: 0.0673, step time: 0.2453\n",
      "277/281, train_loss: 0.0907, step time: 0.2515\n",
      "278/281, train_loss: 0.0685, step time: 0.2555\n",
      "279/281, train_loss: 0.0700, step time: 0.2605\n",
      "280/281, train_loss: 0.0436, step time: 0.2514\n",
      "281/281, train_loss: 0.0492, step time: 0.2506\n",
      "282/281, train_loss: 0.0534, step time: 0.1472\n",
      "epoch 112 average loss: 0.0845\n",
      "saved new best metric model\n",
      "current epoch: 112 current mean dice: 0.9020 tc: 0.8948 wt: 0.9277 et: 0.8931\n",
      "best mean dice: 0.9020 at epoch: 112\n",
      "time consuming of epoch 112 is: 379.0060\n",
      "----------\n",
      "epoch 113/200\n",
      "1/281, train_loss: 0.0471, step time: 0.2659\n",
      "2/281, train_loss: 0.0702, step time: 0.2564\n",
      "3/281, train_loss: 0.0697, step time: 0.2595\n",
      "4/281, train_loss: 0.0610, step time: 0.2714\n",
      "5/281, train_loss: 0.0553, step time: 0.2539\n",
      "6/281, train_loss: 0.0401, step time: 0.2555\n",
      "7/281, train_loss: 0.2128, step time: 0.2628\n",
      "8/281, train_loss: 0.0807, step time: 0.2585\n",
      "9/281, train_loss: 0.0512, step time: 0.2466\n",
      "10/281, train_loss: 0.3710, step time: 0.2519\n",
      "11/281, train_loss: 0.0597, step time: 0.2521\n",
      "12/281, train_loss: 0.0398, step time: 0.2492\n",
      "13/281, train_loss: 0.0539, step time: 0.2561\n",
      "14/281, train_loss: 0.0914, step time: 0.2517\n",
      "15/281, train_loss: 0.0516, step time: 0.2577\n",
      "16/281, train_loss: 0.2302, step time: 0.2585\n",
      "17/281, train_loss: 0.0938, step time: 0.2548\n",
      "18/281, train_loss: 0.0578, step time: 0.2527\n",
      "19/281, train_loss: 0.0660, step time: 0.2518\n",
      "20/281, train_loss: 0.0623, step time: 0.2547\n",
      "21/281, train_loss: 0.0471, step time: 0.2556\n",
      "22/281, train_loss: 0.0644, step time: 0.2644\n",
      "23/281, train_loss: 0.0597, step time: 0.2634\n",
      "24/281, train_loss: 0.0477, step time: 0.2593\n",
      "25/281, train_loss: 0.0901, step time: 0.2565\n",
      "26/281, train_loss: 0.0788, step time: 0.2562\n",
      "27/281, train_loss: 0.0669, step time: 0.2585\n",
      "28/281, train_loss: 0.0647, step time: 0.2570\n",
      "29/281, train_loss: 0.0611, step time: 0.2561\n",
      "30/281, train_loss: 0.2114, step time: 0.2526\n",
      "31/281, train_loss: 0.0381, step time: 0.2852\n",
      "32/281, train_loss: 0.0341, step time: 0.2655\n",
      "33/281, train_loss: 0.0546, step time: 0.2628\n",
      "34/281, train_loss: 0.1216, step time: 0.2510\n",
      "35/281, train_loss: 0.2139, step time: 0.2510\n",
      "36/281, train_loss: 0.0634, step time: 0.2459\n",
      "37/281, train_loss: 0.0861, step time: 0.2442\n",
      "38/281, train_loss: 0.0766, step time: 0.2515\n",
      "39/281, train_loss: 0.0652, step time: 0.2546\n",
      "40/281, train_loss: 0.0669, step time: 0.2568\n",
      "41/281, train_loss: 0.1132, step time: 0.2566\n",
      "42/281, train_loss: 0.2185, step time: 0.2480\n",
      "43/281, train_loss: 0.0691, step time: 0.2503\n",
      "44/281, train_loss: 0.0496, step time: 0.2509\n",
      "45/281, train_loss: 0.2039, step time: 0.2505\n",
      "46/281, train_loss: 0.0578, step time: 0.2605\n",
      "47/281, train_loss: 0.0703, step time: 0.2489\n",
      "48/281, train_loss: 0.0506, step time: 0.2525\n",
      "49/281, train_loss: 0.2223, step time: 0.2527\n",
      "50/281, train_loss: 0.0625, step time: 0.2478\n",
      "51/281, train_loss: 0.0820, step time: 0.2477\n",
      "52/281, train_loss: 0.0627, step time: 0.2513\n",
      "53/281, train_loss: 0.0642, step time: 0.2500\n",
      "54/281, train_loss: 0.0370, step time: 0.2536\n",
      "55/281, train_loss: 0.2255, step time: 0.2557\n",
      "56/281, train_loss: 0.0659, step time: 0.2502\n",
      "57/281, train_loss: 0.0339, step time: 0.2506\n",
      "58/281, train_loss: 0.2315, step time: 0.2566\n",
      "59/281, train_loss: 0.0548, step time: 0.2551\n",
      "60/281, train_loss: 0.0615, step time: 0.2522\n",
      "61/281, train_loss: 0.0382, step time: 0.2526\n",
      "62/281, train_loss: 0.0428, step time: 0.2576\n",
      "63/281, train_loss: 0.0541, step time: 0.2534\n",
      "64/281, train_loss: 0.0574, step time: 0.2497\n",
      "65/281, train_loss: 0.0536, step time: 0.2464\n",
      "66/281, train_loss: 0.0669, step time: 0.2551\n",
      "67/281, train_loss: 0.2145, step time: 0.2551\n",
      "68/281, train_loss: 0.0649, step time: 0.2514\n",
      "69/281, train_loss: 0.0369, step time: 0.2485\n",
      "70/281, train_loss: 0.2503, step time: 0.2531\n",
      "71/281, train_loss: 0.0812, step time: 0.2473\n",
      "72/281, train_loss: 0.0355, step time: 0.2554\n",
      "73/281, train_loss: 0.2207, step time: 0.2513\n",
      "74/281, train_loss: 0.0694, step time: 0.2614\n",
      "75/281, train_loss: 0.0673, step time: 0.2568\n",
      "76/281, train_loss: 0.0942, step time: 0.2564\n",
      "77/281, train_loss: 0.0431, step time: 0.2555\n",
      "78/281, train_loss: 0.0318, step time: 0.2535\n",
      "79/281, train_loss: 0.2282, step time: 0.2594\n",
      "80/281, train_loss: 0.0707, step time: 0.2574\n",
      "81/281, train_loss: 0.0414, step time: 0.2557\n",
      "82/281, train_loss: 0.0488, step time: 0.2539\n",
      "83/281, train_loss: 0.2307, step time: 0.2582\n",
      "84/281, train_loss: 0.0418, step time: 0.2589\n",
      "85/281, train_loss: 0.0469, step time: 0.2551\n",
      "86/281, train_loss: 0.0379, step time: 0.2584\n",
      "87/281, train_loss: 0.0869, step time: 0.2582\n",
      "88/281, train_loss: 0.2131, step time: 0.2242\n",
      "89/281, train_loss: 0.0604, step time: 0.2592\n",
      "90/281, train_loss: 0.0650, step time: 0.2597\n",
      "91/281, train_loss: 0.0480, step time: 0.2535\n",
      "92/281, train_loss: 0.0386, step time: 0.2534\n",
      "93/281, train_loss: 0.0607, step time: 0.2509\n",
      "94/281, train_loss: 0.0589, step time: 0.2550\n",
      "95/281, train_loss: 0.0372, step time: 0.2560\n",
      "96/281, train_loss: 0.0409, step time: 0.2513\n",
      "97/281, train_loss: 0.0415, step time: 0.2549\n",
      "98/281, train_loss: 0.0616, step time: 0.2559\n",
      "99/281, train_loss: 0.0625, step time: 0.2554\n",
      "100/281, train_loss: 0.2140, step time: 0.2516\n",
      "101/281, train_loss: 0.0481, step time: 0.2513\n",
      "102/281, train_loss: 0.2378, step time: 0.2553\n",
      "103/281, train_loss: 0.0731, step time: 0.2481\n",
      "104/281, train_loss: 0.0393, step time: 0.2492\n",
      "105/281, train_loss: 0.0550, step time: 0.2515\n",
      "106/281, train_loss: 0.0598, step time: 0.2521\n",
      "107/281, train_loss: 0.0545, step time: 0.2538\n",
      "108/281, train_loss: 0.0384, step time: 0.2497\n",
      "109/281, train_loss: 0.0458, step time: 0.2509\n",
      "110/281, train_loss: 0.3753, step time: 0.2514\n",
      "111/281, train_loss: 0.0659, step time: 0.2475\n",
      "112/281, train_loss: 0.0649, step time: 0.2518\n",
      "113/281, train_loss: 0.0595, step time: 0.2461\n",
      "114/281, train_loss: 0.0424, step time: 0.2568\n",
      "115/281, train_loss: 0.0450, step time: 0.2489\n",
      "116/281, train_loss: 0.2102, step time: 0.2549\n",
      "117/281, train_loss: 0.0343, step time: 0.2529\n",
      "118/281, train_loss: 0.0698, step time: 0.2521\n",
      "119/281, train_loss: 0.0443, step time: 0.2549\n",
      "120/281, train_loss: 0.2160, step time: 0.2599\n",
      "121/281, train_loss: 0.0515, step time: 0.2531\n",
      "122/281, train_loss: 0.0365, step time: 0.2554\n",
      "123/281, train_loss: 0.0814, step time: 0.2653\n",
      "124/281, train_loss: 0.0996, step time: 0.2495\n",
      "125/281, train_loss: 0.2093, step time: 0.2522\n",
      "126/281, train_loss: 0.0618, step time: 0.2501\n",
      "127/281, train_loss: 0.0378, step time: 0.2553\n",
      "128/281, train_loss: 0.0470, step time: 0.2544\n",
      "129/281, train_loss: 0.0584, step time: 0.2530\n",
      "130/281, train_loss: 0.0845, step time: 0.2534\n",
      "131/281, train_loss: 0.2386, step time: 0.2533\n",
      "132/281, train_loss: 0.2304, step time: 0.2203\n",
      "133/281, train_loss: 0.0772, step time: 0.2497\n",
      "134/281, train_loss: 0.0758, step time: 0.2534\n",
      "135/281, train_loss: 0.0589, step time: 0.2500\n",
      "136/281, train_loss: 0.0496, step time: 0.2483\n",
      "137/281, train_loss: 0.0324, step time: 0.2473\n",
      "138/281, train_loss: 0.0505, step time: 0.2578\n",
      "139/281, train_loss: 0.0518, step time: 0.2476\n",
      "140/281, train_loss: 0.1060, step time: 0.2526\n",
      "141/281, train_loss: 0.2323, step time: 0.2482\n",
      "142/281, train_loss: 0.2240, step time: 0.2604\n",
      "143/281, train_loss: 0.0880, step time: 0.2566\n",
      "144/281, train_loss: 0.0326, step time: 0.2532\n",
      "145/281, train_loss: 0.0448, step time: 0.2498\n",
      "146/281, train_loss: 0.0568, step time: 0.2627\n",
      "147/281, train_loss: 0.0397, step time: 0.2534\n",
      "148/281, train_loss: 0.0412, step time: 0.2534\n",
      "149/281, train_loss: 0.0470, step time: 0.2510\n",
      "150/281, train_loss: 0.0606, step time: 0.2517\n",
      "151/281, train_loss: 0.0458, step time: 0.2556\n",
      "152/281, train_loss: 0.0613, step time: 0.2567\n",
      "153/281, train_loss: 0.0467, step time: 0.2518\n",
      "154/281, train_loss: 0.0396, step time: 0.2551\n",
      "155/281, train_loss: 0.2261, step time: 0.2578\n",
      "156/281, train_loss: 0.0570, step time: 0.2602\n",
      "157/281, train_loss: 0.0610, step time: 0.2566\n",
      "158/281, train_loss: 0.2149, step time: 0.2545\n",
      "159/281, train_loss: 0.0423, step time: 0.2560\n",
      "160/281, train_loss: 0.0534, step time: 0.2521\n",
      "161/281, train_loss: 0.0702, step time: 0.2619\n",
      "162/281, train_loss: 0.0463, step time: 0.2566\n",
      "163/281, train_loss: 0.0766, step time: 0.2594\n",
      "164/281, train_loss: 0.0489, step time: 0.2586\n",
      "165/281, train_loss: 0.0446, step time: 0.2557\n",
      "166/281, train_loss: 0.0522, step time: 0.2562\n",
      "167/281, train_loss: 0.2439, step time: 0.2495\n",
      "168/281, train_loss: 0.0359, step time: 0.2514\n",
      "169/281, train_loss: 0.0737, step time: 0.2519\n",
      "170/281, train_loss: 0.0488, step time: 0.2548\n",
      "171/281, train_loss: 0.0325, step time: 0.2567\n",
      "172/281, train_loss: 0.0473, step time: 0.2586\n",
      "173/281, train_loss: 0.0703, step time: 0.2563\n",
      "174/281, train_loss: 0.0666, step time: 0.2572\n",
      "175/281, train_loss: 0.0612, step time: 0.2560\n",
      "176/281, train_loss: 0.0333, step time: 0.2548\n",
      "177/281, train_loss: 0.0578, step time: 0.2588\n",
      "178/281, train_loss: 0.0551, step time: 0.2555\n",
      "179/281, train_loss: 0.0448, step time: 0.2479\n",
      "180/281, train_loss: 0.2089, step time: 0.2514\n",
      "181/281, train_loss: 0.0809, step time: 0.2495\n",
      "182/281, train_loss: 0.0593, step time: 0.2560\n",
      "183/281, train_loss: 0.2241, step time: 0.2596\n",
      "184/281, train_loss: 0.0541, step time: 0.2607\n",
      "185/281, train_loss: 0.0566, step time: 0.2516\n",
      "186/281, train_loss: 0.0680, step time: 0.2553\n",
      "187/281, train_loss: 0.0755, step time: 0.2506\n",
      "188/281, train_loss: 0.0573, step time: 0.2580\n",
      "189/281, train_loss: 0.0623, step time: 0.2526\n",
      "190/281, train_loss: 0.0568, step time: 0.2567\n",
      "191/281, train_loss: 0.0298, step time: 0.2559\n",
      "192/281, train_loss: 0.0414, step time: 0.2602\n",
      "193/281, train_loss: 0.0380, step time: 0.2546\n",
      "194/281, train_loss: 0.0592, step time: 0.2540\n",
      "195/281, train_loss: 0.0489, step time: 0.2564\n",
      "196/281, train_loss: 0.0552, step time: 0.2569\n",
      "197/281, train_loss: 0.0511, step time: 0.2523\n",
      "198/281, train_loss: 0.0700, step time: 0.2537\n",
      "199/281, train_loss: 0.2301, step time: 0.2639\n",
      "200/281, train_loss: 0.0752, step time: 0.2586\n",
      "201/281, train_loss: 0.0629, step time: 0.2544\n",
      "202/281, train_loss: 0.1041, step time: 0.2501\n",
      "203/281, train_loss: 0.2166, step time: 0.2582\n",
      "204/281, train_loss: 0.0554, step time: 0.2596\n",
      "205/281, train_loss: 0.0514, step time: 0.2591\n",
      "206/281, train_loss: 0.0631, step time: 0.2594\n",
      "207/281, train_loss: 0.0706, step time: 0.2551\n",
      "208/281, train_loss: 0.0710, step time: 0.2532\n",
      "209/281, train_loss: 0.0843, step time: 0.2481\n",
      "210/281, train_loss: 0.0621, step time: 0.2483\n",
      "211/281, train_loss: 0.0609, step time: 0.2463\n",
      "212/281, train_loss: 0.0514, step time: 0.2505\n",
      "213/281, train_loss: 0.0693, step time: 0.2513\n",
      "214/281, train_loss: 0.0506, step time: 0.2488\n",
      "215/281, train_loss: 0.0396, step time: 0.2527\n",
      "216/281, train_loss: 0.0864, step time: 0.2557\n",
      "217/281, train_loss: 0.2064, step time: 0.2483\n",
      "218/281, train_loss: 0.0417, step time: 0.2492\n",
      "219/281, train_loss: 0.0726, step time: 0.2510\n",
      "220/281, train_loss: 0.2030, step time: 0.2540\n",
      "221/281, train_loss: 0.0541, step time: 0.2518\n",
      "222/281, train_loss: 0.0579, step time: 0.2516\n",
      "223/281, train_loss: 0.0300, step time: 0.2535\n",
      "224/281, train_loss: 0.0426, step time: 0.2540\n",
      "225/281, train_loss: 0.0444, step time: 0.2471\n",
      "226/281, train_loss: 0.0784, step time: 0.2485\n",
      "227/281, train_loss: 0.0619, step time: 0.2601\n",
      "228/281, train_loss: 0.0489, step time: 0.2513\n",
      "229/281, train_loss: 0.0673, step time: 0.2570\n",
      "230/281, train_loss: 0.0457, step time: 0.2535\n",
      "231/281, train_loss: 0.0441, step time: 0.2547\n",
      "232/281, train_loss: 0.0819, step time: 0.2499\n",
      "233/281, train_loss: 0.0559, step time: 0.2528\n",
      "234/281, train_loss: 0.2387, step time: 0.2556\n",
      "235/281, train_loss: 0.0437, step time: 0.2485\n",
      "236/281, train_loss: 0.0353, step time: 0.2436\n",
      "237/281, train_loss: 0.0720, step time: 0.2493\n",
      "238/281, train_loss: 0.0923, step time: 0.2451\n",
      "239/281, train_loss: 0.0836, step time: 0.2541\n",
      "240/281, train_loss: 0.0700, step time: 0.2534\n",
      "241/281, train_loss: 0.0550, step time: 0.2543\n",
      "242/281, train_loss: 0.0912, step time: 0.2573\n",
      "243/281, train_loss: 0.0724, step time: 0.2615\n",
      "244/281, train_loss: 0.0264, step time: 0.2575\n",
      "245/281, train_loss: 0.0518, step time: 0.2563\n",
      "246/281, train_loss: 0.2349, step time: 0.2579\n",
      "247/281, train_loss: 0.0713, step time: 0.2591\n",
      "248/281, train_loss: 0.0364, step time: 0.2573\n",
      "249/281, train_loss: 0.0634, step time: 0.2523\n",
      "250/281, train_loss: 0.0677, step time: 0.2565\n",
      "251/281, train_loss: 0.0882, step time: 0.2553\n",
      "252/281, train_loss: 0.0601, step time: 0.2509\n",
      "253/281, train_loss: 0.2181, step time: 0.2533\n",
      "254/281, train_loss: 0.0609, step time: 0.2568\n",
      "255/281, train_loss: 0.0754, step time: 0.2524\n",
      "256/281, train_loss: 0.2533, step time: 0.2527\n",
      "257/281, train_loss: 0.0570, step time: 0.2599\n",
      "258/281, train_loss: 0.0412, step time: 0.2572\n",
      "259/281, train_loss: 0.0507, step time: 0.2586\n",
      "260/281, train_loss: 0.0547, step time: 0.2565\n",
      "261/281, train_loss: 0.0694, step time: 0.2594\n",
      "262/281, train_loss: 0.0627, step time: 0.2586\n",
      "263/281, train_loss: 0.0444, step time: 0.2596\n",
      "264/281, train_loss: 0.0523, step time: 0.2559\n",
      "265/281, train_loss: 0.0476, step time: 0.2545\n",
      "266/281, train_loss: 0.0905, step time: 0.2560\n",
      "267/281, train_loss: 0.0568, step time: 0.2533\n",
      "268/281, train_loss: 0.0773, step time: 0.2522\n",
      "269/281, train_loss: 0.0536, step time: 0.2586\n",
      "270/281, train_loss: 0.0495, step time: 0.2585\n",
      "271/281, train_loss: 0.0745, step time: 0.2524\n",
      "272/281, train_loss: 0.0500, step time: 0.2528\n",
      "273/281, train_loss: 0.0611, step time: 0.2513\n",
      "274/281, train_loss: 0.0754, step time: 0.2521\n",
      "275/281, train_loss: 0.0613, step time: 0.2593\n",
      "276/281, train_loss: 0.2331, step time: 0.2573\n",
      "277/281, train_loss: 0.0595, step time: 0.2577\n",
      "278/281, train_loss: 0.0616, step time: 0.2565\n",
      "279/281, train_loss: 0.0613, step time: 0.2523\n",
      "280/281, train_loss: 0.2167, step time: 0.2563\n",
      "281/281, train_loss: 0.0966, step time: 0.2502\n",
      "282/281, train_loss: 0.0551, step time: 0.1512\n",
      "epoch 113 average loss: 0.0838\n",
      "current epoch: 113 current mean dice: 0.8991 tc: 0.8947 wt: 0.9244 et: 0.8874\n",
      "best mean dice: 0.9020 at epoch: 112\n",
      "time consuming of epoch 113 is: 391.0487\n",
      "----------\n",
      "epoch 114/200\n",
      "1/281, train_loss: 0.0763, step time: 0.2596\n",
      "2/281, train_loss: 0.0493, step time: 0.2540\n",
      "3/281, train_loss: 0.1963, step time: 0.2545\n",
      "4/281, train_loss: 0.0600, step time: 0.2582\n",
      "5/281, train_loss: 0.0669, step time: 0.2600\n",
      "6/281, train_loss: 0.0539, step time: 0.2612\n",
      "7/281, train_loss: 0.0879, step time: 0.2538\n",
      "8/281, train_loss: 0.0601, step time: 0.2576\n",
      "9/281, train_loss: 0.0646, step time: 0.2580\n",
      "10/281, train_loss: 0.0807, step time: 0.2661\n",
      "11/281, train_loss: 0.0517, step time: 0.2650\n",
      "12/281, train_loss: 0.2857, step time: 0.2599\n",
      "13/281, train_loss: 0.0750, step time: 0.2596\n",
      "14/281, train_loss: 0.0505, step time: 0.2568\n",
      "15/281, train_loss: 0.0393, step time: 0.2593\n",
      "16/281, train_loss: 0.2215, step time: 0.2580\n",
      "17/281, train_loss: 0.0472, step time: 0.2593\n",
      "18/281, train_loss: 0.0739, step time: 0.2556\n",
      "19/281, train_loss: 0.0884, step time: 0.2554\n",
      "20/281, train_loss: 0.2168, step time: 0.2552\n",
      "21/281, train_loss: 0.0453, step time: 0.2516\n",
      "22/281, train_loss: 0.0595, step time: 0.2492\n",
      "23/281, train_loss: 0.0984, step time: 0.2570\n",
      "24/281, train_loss: 0.0858, step time: 0.2567\n",
      "25/281, train_loss: 0.0452, step time: 0.2554\n",
      "26/281, train_loss: 0.0263, step time: 0.2574\n",
      "27/281, train_loss: 0.2164, step time: 0.2561\n",
      "28/281, train_loss: 0.0879, step time: 0.2582\n",
      "29/281, train_loss: 0.0301, step time: 0.2518\n",
      "30/281, train_loss: 0.0401, step time: 0.2561\n",
      "31/281, train_loss: 0.0580, step time: 0.2570\n",
      "32/281, train_loss: 0.2092, step time: 0.2592\n",
      "33/281, train_loss: 0.0717, step time: 0.2570\n",
      "34/281, train_loss: 0.0791, step time: 0.2574\n",
      "35/281, train_loss: 0.0331, step time: 0.2587\n",
      "36/281, train_loss: 0.0337, step time: 0.2572\n",
      "37/281, train_loss: 0.0459, step time: 0.2520\n",
      "38/281, train_loss: 0.0751, step time: 0.2537\n",
      "39/281, train_loss: 0.0346, step time: 0.2560\n",
      "40/281, train_loss: 0.0627, step time: 0.2582\n",
      "41/281, train_loss: 0.0645, step time: 0.2582\n",
      "42/281, train_loss: 0.0872, step time: 0.2587\n",
      "43/281, train_loss: 0.0528, step time: 0.2581\n",
      "44/281, train_loss: 0.0874, step time: 0.2565\n",
      "45/281, train_loss: 0.0564, step time: 0.2651\n",
      "46/281, train_loss: 0.0826, step time: 0.2572\n",
      "47/281, train_loss: 0.0694, step time: 0.2542\n",
      "48/281, train_loss: 0.0842, step time: 0.2586\n",
      "49/281, train_loss: 0.2264, step time: 0.2527\n",
      "50/281, train_loss: 0.0634, step time: 0.2581\n",
      "51/281, train_loss: 0.0795, step time: 0.2551\n",
      "52/281, train_loss: 0.0453, step time: 0.2586\n",
      "53/281, train_loss: 0.0458, step time: 0.2582\n",
      "54/281, train_loss: 0.0703, step time: 0.2575\n",
      "55/281, train_loss: 0.0676, step time: 0.2612\n",
      "56/281, train_loss: 0.0791, step time: 0.2559\n",
      "57/281, train_loss: 0.0388, step time: 0.2575\n",
      "58/281, train_loss: 0.0673, step time: 0.2582\n",
      "59/281, train_loss: 0.0612, step time: 0.2563\n",
      "60/281, train_loss: 0.0478, step time: 0.2622\n",
      "61/281, train_loss: 0.0541, step time: 0.2650\n",
      "62/281, train_loss: 0.0546, step time: 0.2577\n",
      "63/281, train_loss: 0.0382, step time: 0.2600\n",
      "64/281, train_loss: 0.0735, step time: 0.2678\n",
      "65/281, train_loss: 0.2271, step time: 0.2543\n",
      "66/281, train_loss: 0.0688, step time: 0.2571\n",
      "67/281, train_loss: 0.0317, step time: 0.2554\n",
      "68/281, train_loss: 0.0782, step time: 0.2543\n",
      "69/281, train_loss: 0.2085, step time: 0.2586\n",
      "70/281, train_loss: 0.0621, step time: 0.2538\n",
      "71/281, train_loss: 0.0358, step time: 0.2539\n",
      "72/281, train_loss: 0.0483, step time: 0.2533\n",
      "73/281, train_loss: 0.0456, step time: 0.2495\n",
      "74/281, train_loss: 0.0837, step time: 0.2578\n",
      "75/281, train_loss: 0.0401, step time: 0.2570\n",
      "76/281, train_loss: 0.0669, step time: 0.2854\n",
      "77/281, train_loss: 0.3800, step time: 0.2523\n",
      "78/281, train_loss: 0.2027, step time: 0.2563\n",
      "79/281, train_loss: 0.0735, step time: 0.2515\n",
      "80/281, train_loss: 0.0448, step time: 0.2570\n",
      "81/281, train_loss: 0.2139, step time: 0.2731\n",
      "82/281, train_loss: 0.0717, step time: 0.2525\n",
      "83/281, train_loss: 0.0660, step time: 0.2558\n",
      "84/281, train_loss: 0.0672, step time: 0.2614\n",
      "85/281, train_loss: 0.0780, step time: 0.2528\n",
      "86/281, train_loss: 0.2137, step time: 0.2497\n",
      "87/281, train_loss: 0.0678, step time: 0.2508\n",
      "88/281, train_loss: 0.2245, step time: 0.2593\n",
      "89/281, train_loss: 0.0350, step time: 0.2527\n",
      "90/281, train_loss: 0.0304, step time: 0.2523\n",
      "91/281, train_loss: 0.0309, step time: 0.2575\n",
      "92/281, train_loss: 0.2221, step time: 0.2631\n",
      "93/281, train_loss: 0.0622, step time: 0.2489\n",
      "94/281, train_loss: 0.0507, step time: 0.2519\n",
      "95/281, train_loss: 0.0561, step time: 0.2474\n",
      "96/281, train_loss: 0.0606, step time: 0.2540\n",
      "97/281, train_loss: 0.2022, step time: 0.2537\n",
      "98/281, train_loss: 0.0467, step time: 0.2533\n",
      "99/281, train_loss: 0.0513, step time: 0.2525\n",
      "100/281, train_loss: 0.0674, step time: 0.2527\n",
      "101/281, train_loss: 0.0877, step time: 0.2565\n",
      "102/281, train_loss: 0.0667, step time: 0.2678\n",
      "103/281, train_loss: 0.0607, step time: 0.2483\n",
      "104/281, train_loss: 0.0491, step time: 0.2618\n",
      "105/281, train_loss: 0.0496, step time: 0.2551\n",
      "106/281, train_loss: 0.0802, step time: 0.2500\n",
      "107/281, train_loss: 0.0806, step time: 0.2523\n",
      "108/281, train_loss: 0.0700, step time: 0.2553\n",
      "109/281, train_loss: 0.0690, step time: 0.2480\n",
      "110/281, train_loss: 0.0576, step time: 0.2526\n",
      "111/281, train_loss: 0.0633, step time: 0.2513\n",
      "112/281, train_loss: 0.0613, step time: 0.2596\n",
      "113/281, train_loss: 0.0672, step time: 0.2530\n",
      "114/281, train_loss: 0.0479, step time: 0.2496\n",
      "115/281, train_loss: 0.0541, step time: 0.2519\n",
      "116/281, train_loss: 0.0601, step time: 0.2501\n",
      "117/281, train_loss: 0.2439, step time: 0.2506\n",
      "118/281, train_loss: 0.0542, step time: 0.2515\n",
      "119/281, train_loss: 0.0484, step time: 0.2538\n",
      "120/281, train_loss: 0.0662, step time: 0.2536\n",
      "121/281, train_loss: 0.0633, step time: 0.2575\n",
      "122/281, train_loss: 0.0566, step time: 0.2554\n",
      "123/281, train_loss: 0.0494, step time: 0.2531\n",
      "124/281, train_loss: 0.2201, step time: 0.2563\n",
      "125/281, train_loss: 0.2303, step time: 0.2525\n",
      "126/281, train_loss: 0.0787, step time: 0.2548\n",
      "127/281, train_loss: 0.0501, step time: 0.2527\n",
      "128/281, train_loss: 0.0714, step time: 0.2610\n",
      "129/281, train_loss: 0.0702, step time: 0.2569\n",
      "130/281, train_loss: 0.1085, step time: 0.2570\n",
      "131/281, train_loss: 0.0731, step time: 0.2508\n",
      "132/281, train_loss: 0.0501, step time: 0.2525\n",
      "133/281, train_loss: 0.0631, step time: 0.2586\n",
      "134/281, train_loss: 0.0521, step time: 0.2591\n",
      "135/281, train_loss: 0.0836, step time: 0.2534\n",
      "136/281, train_loss: 0.0522, step time: 0.2590\n",
      "137/281, train_loss: 0.0646, step time: 0.2529\n",
      "138/281, train_loss: 0.0459, step time: 0.2518\n",
      "139/281, train_loss: 0.2421, step time: 0.2532\n",
      "140/281, train_loss: 0.0720, step time: 0.2561\n",
      "141/281, train_loss: 0.2183, step time: 0.2555\n",
      "142/281, train_loss: 0.0505, step time: 0.2548\n",
      "143/281, train_loss: 0.0850, step time: 0.2512\n",
      "144/281, train_loss: 0.0597, step time: 0.2554\n",
      "145/281, train_loss: 0.0552, step time: 0.2533\n",
      "146/281, train_loss: 0.2386, step time: 0.2539\n",
      "147/281, train_loss: 0.0754, step time: 0.2550\n",
      "148/281, train_loss: 0.0569, step time: 0.2584\n",
      "149/281, train_loss: 0.0567, step time: 0.2502\n",
      "150/281, train_loss: 0.0575, step time: 0.2561\n",
      "151/281, train_loss: 0.0567, step time: 0.2596\n",
      "152/281, train_loss: 0.0439, step time: 0.2588\n",
      "153/281, train_loss: 0.0424, step time: 0.2544\n",
      "154/281, train_loss: 0.0374, step time: 0.2532\n",
      "155/281, train_loss: 0.0686, step time: 0.2511\n",
      "156/281, train_loss: 0.0431, step time: 0.2499\n",
      "157/281, train_loss: 0.0782, step time: 0.2552\n",
      "158/281, train_loss: 0.0464, step time: 0.2565\n",
      "159/281, train_loss: 0.2334, step time: 0.2561\n",
      "160/281, train_loss: 0.0375, step time: 0.2538\n",
      "161/281, train_loss: 0.0529, step time: 0.2482\n",
      "162/281, train_loss: 0.0732, step time: 0.2509\n",
      "163/281, train_loss: 0.0670, step time: 0.2520\n",
      "164/281, train_loss: 0.0455, step time: 0.2512\n",
      "165/281, train_loss: 0.0958, step time: 0.2486\n",
      "166/281, train_loss: 0.0527, step time: 0.2472\n",
      "167/281, train_loss: 0.0644, step time: 0.2534\n",
      "168/281, train_loss: 0.0487, step time: 0.2503\n",
      "169/281, train_loss: 0.0638, step time: 0.2597\n",
      "170/281, train_loss: 0.0906, step time: 0.2560\n",
      "171/281, train_loss: 0.1027, step time: 0.2590\n",
      "172/281, train_loss: 0.0311, step time: 0.2569\n",
      "173/281, train_loss: 0.0429, step time: 0.2545\n",
      "174/281, train_loss: 0.0753, step time: 0.2529\n",
      "175/281, train_loss: 0.0629, step time: 0.2537\n",
      "176/281, train_loss: 0.0482, step time: 0.2532\n",
      "177/281, train_loss: 0.0661, step time: 0.2562\n",
      "178/281, train_loss: 0.0651, step time: 0.2605\n",
      "179/281, train_loss: 0.0583, step time: 0.2542\n",
      "180/281, train_loss: 0.0640, step time: 0.2531\n",
      "181/281, train_loss: 0.0451, step time: 0.2540\n",
      "182/281, train_loss: 0.0472, step time: 0.2533\n",
      "183/281, train_loss: 0.0619, step time: 0.2506\n",
      "184/281, train_loss: 0.0606, step time: 0.2530\n",
      "185/281, train_loss: 0.0595, step time: 0.2775\n",
      "186/281, train_loss: 0.0560, step time: 0.2718\n",
      "187/281, train_loss: 0.0629, step time: 0.2539\n",
      "188/281, train_loss: 0.0423, step time: 0.2673\n",
      "189/281, train_loss: 0.2074, step time: 0.2511\n",
      "190/281, train_loss: 0.0622, step time: 0.2545\n",
      "191/281, train_loss: 0.0647, step time: 0.2550\n",
      "192/281, train_loss: 0.0961, step time: 0.2508\n",
      "193/281, train_loss: 0.0638, step time: 0.2585\n",
      "194/281, train_loss: 0.0415, step time: 0.2601\n",
      "195/281, train_loss: 0.2374, step time: 0.2210\n",
      "196/281, train_loss: 0.0616, step time: 0.2553\n",
      "197/281, train_loss: 0.0393, step time: 0.2515\n",
      "198/281, train_loss: 0.0743, step time: 0.2536\n",
      "199/281, train_loss: 0.0906, step time: 0.2538\n",
      "200/281, train_loss: 0.2475, step time: 0.2527\n",
      "201/281, train_loss: 0.0576, step time: 0.2535\n",
      "202/281, train_loss: 0.0508, step time: 0.2511\n",
      "203/281, train_loss: 0.0474, step time: 0.2516\n",
      "204/281, train_loss: 0.0687, step time: 0.2485\n",
      "205/281, train_loss: 0.0631, step time: 0.2560\n",
      "206/281, train_loss: 0.0723, step time: 0.2497\n",
      "207/281, train_loss: 0.2498, step time: 0.2492\n",
      "208/281, train_loss: 0.0445, step time: 0.2686\n",
      "209/281, train_loss: 0.0600, step time: 0.2489\n",
      "210/281, train_loss: 0.0770, step time: 0.2475\n",
      "211/281, train_loss: 0.2320, step time: 0.2570\n",
      "212/281, train_loss: 0.3761, step time: 0.2249\n",
      "213/281, train_loss: 0.0641, step time: 0.2502\n",
      "214/281, train_loss: 0.0580, step time: 0.2490\n",
      "215/281, train_loss: 0.0484, step time: 0.2478\n",
      "216/281, train_loss: 0.0859, step time: 0.2546\n",
      "217/281, train_loss: 0.0942, step time: 0.2581\n",
      "218/281, train_loss: 0.0518, step time: 0.2479\n",
      "219/281, train_loss: 0.3820, step time: 0.2468\n",
      "220/281, train_loss: 0.0582, step time: 0.2571\n",
      "221/281, train_loss: 0.0812, step time: 0.2494\n",
      "222/281, train_loss: 0.0571, step time: 0.2551\n",
      "223/281, train_loss: 0.2070, step time: 0.2460\n",
      "224/281, train_loss: 0.2169, step time: 0.2522\n",
      "225/281, train_loss: 0.0607, step time: 0.2540\n",
      "226/281, train_loss: 0.0515, step time: 0.2534\n",
      "227/281, train_loss: 0.0521, step time: 0.2468\n",
      "228/281, train_loss: 0.0532, step time: 0.2480\n",
      "229/281, train_loss: 0.2032, step time: 0.2495\n",
      "230/281, train_loss: 0.0633, step time: 0.2535\n",
      "231/281, train_loss: 0.0694, step time: 0.2531\n",
      "232/281, train_loss: 0.0741, step time: 0.2513\n",
      "233/281, train_loss: 0.0830, step time: 0.2466\n",
      "234/281, train_loss: 0.0882, step time: 0.2501\n",
      "235/281, train_loss: 0.2126, step time: 0.2444\n",
      "236/281, train_loss: 0.0673, step time: 0.2469\n",
      "237/281, train_loss: 0.0634, step time: 0.2460\n",
      "238/281, train_loss: 0.0675, step time: 0.2477\n",
      "239/281, train_loss: 0.0478, step time: 0.2505\n",
      "240/281, train_loss: 0.0608, step time: 0.2484\n",
      "241/281, train_loss: 0.0330, step time: 0.2476\n",
      "242/281, train_loss: 0.0885, step time: 0.2517\n",
      "243/281, train_loss: 0.0542, step time: 0.2453\n",
      "244/281, train_loss: 0.0815, step time: 0.2476\n",
      "245/281, train_loss: 0.0478, step time: 0.2461\n",
      "246/281, train_loss: 0.0447, step time: 0.2501\n",
      "247/281, train_loss: 0.0476, step time: 0.2455\n",
      "248/281, train_loss: 0.0575, step time: 0.2452\n",
      "249/281, train_loss: 0.0478, step time: 0.2509\n",
      "250/281, train_loss: 0.0539, step time: 0.2472\n",
      "251/281, train_loss: 0.0760, step time: 0.2468\n",
      "252/281, train_loss: 0.0606, step time: 0.2523\n",
      "253/281, train_loss: 0.0449, step time: 0.2481\n",
      "254/281, train_loss: 0.0832, step time: 0.2458\n",
      "255/281, train_loss: 0.0605, step time: 0.2591\n",
      "256/281, train_loss: 0.0528, step time: 0.2503\n",
      "257/281, train_loss: 0.0682, step time: 0.2510\n",
      "258/281, train_loss: 0.2130, step time: 0.2450\n",
      "259/281, train_loss: 0.0740, step time: 0.2512\n",
      "260/281, train_loss: 0.0360, step time: 0.2479\n",
      "261/281, train_loss: 0.2435, step time: 0.2496\n",
      "262/281, train_loss: 0.2372, step time: 0.2472\n",
      "263/281, train_loss: 0.0456, step time: 0.2516\n",
      "264/281, train_loss: 0.0824, step time: 0.2506\n",
      "265/281, train_loss: 0.0650, step time: 0.2556\n",
      "266/281, train_loss: 0.0355, step time: 0.2534\n",
      "267/281, train_loss: 0.2047, step time: 0.2576\n",
      "268/281, train_loss: 0.0358, step time: 0.2630\n",
      "269/281, train_loss: 0.0577, step time: 0.2498\n",
      "270/281, train_loss: 0.0673, step time: 0.2485\n",
      "271/281, train_loss: 0.2060, step time: 0.2580\n",
      "272/281, train_loss: 0.0263, step time: 0.2507\n",
      "273/281, train_loss: 0.0589, step time: 0.2452\n",
      "274/281, train_loss: 0.0585, step time: 0.2630\n",
      "275/281, train_loss: 0.0475, step time: 0.2505\n",
      "276/281, train_loss: 0.0523, step time: 0.2464\n",
      "277/281, train_loss: 0.0450, step time: 0.2451\n",
      "278/281, train_loss: 0.0444, step time: 0.2493\n",
      "279/281, train_loss: 0.2201, step time: 0.2488\n",
      "280/281, train_loss: 0.0324, step time: 0.2456\n",
      "281/281, train_loss: 0.0557, step time: 0.2497\n",
      "282/281, train_loss: 0.0724, step time: 0.1495\n",
      "epoch 114 average loss: 0.0851\n",
      "current epoch: 114 current mean dice: 0.9020 tc: 0.8943 wt: 0.9287 et: 0.8920\n",
      "best mean dice: 0.9020 at epoch: 112\n",
      "time consuming of epoch 114 is: 386.5570\n",
      "----------\n",
      "epoch 115/200\n",
      "1/281, train_loss: 0.2224, step time: 0.2561\n",
      "2/281, train_loss: 0.0627, step time: 0.2581\n",
      "3/281, train_loss: 0.0473, step time: 0.2523\n",
      "4/281, train_loss: 0.2190, step time: 0.2457\n",
      "5/281, train_loss: 0.0406, step time: 0.2567\n",
      "6/281, train_loss: 0.0575, step time: 0.2545\n",
      "7/281, train_loss: 0.2019, step time: 0.2482\n",
      "8/281, train_loss: 0.0593, step time: 0.2502\n",
      "9/281, train_loss: 0.0626, step time: 0.2529\n",
      "10/281, train_loss: 0.0504, step time: 0.2520\n",
      "11/281, train_loss: 0.0613, step time: 0.2485\n",
      "12/281, train_loss: 0.0690, step time: 0.2440\n",
      "13/281, train_loss: 0.0720, step time: 0.2502\n",
      "14/281, train_loss: 0.0899, step time: 0.2489\n",
      "15/281, train_loss: 0.2132, step time: 0.2479\n",
      "16/281, train_loss: 0.0673, step time: 0.2525\n",
      "17/281, train_loss: 0.0411, step time: 0.2494\n",
      "18/281, train_loss: 0.0822, step time: 0.2425\n",
      "19/281, train_loss: 0.0719, step time: 0.2456\n",
      "20/281, train_loss: 0.0587, step time: 0.2486\n",
      "21/281, train_loss: 0.0709, step time: 0.2466\n",
      "22/281, train_loss: 0.0397, step time: 0.2467\n",
      "23/281, train_loss: 0.0500, step time: 0.2468\n",
      "24/281, train_loss: 0.0662, step time: 0.2517\n",
      "25/281, train_loss: 0.0418, step time: 0.2498\n",
      "26/281, train_loss: 0.0414, step time: 0.2518\n",
      "27/281, train_loss: 0.2121, step time: 0.2505\n",
      "28/281, train_loss: 0.0647, step time: 0.2587\n",
      "29/281, train_loss: 0.0942, step time: 0.2508\n",
      "30/281, train_loss: 0.0614, step time: 0.2516\n",
      "31/281, train_loss: 0.0705, step time: 0.2517\n",
      "32/281, train_loss: 0.0515, step time: 0.2478\n",
      "33/281, train_loss: 0.0614, step time: 0.2529\n",
      "34/281, train_loss: 0.0759, step time: 0.2537\n",
      "35/281, train_loss: 0.0395, step time: 0.2489\n",
      "36/281, train_loss: 0.0355, step time: 0.2478\n",
      "37/281, train_loss: 0.0620, step time: 0.2491\n",
      "38/281, train_loss: 0.0577, step time: 0.2559\n",
      "39/281, train_loss: 0.2069, step time: 0.2531\n",
      "40/281, train_loss: 0.0529, step time: 0.2579\n",
      "41/281, train_loss: 0.2123, step time: 0.2565\n",
      "42/281, train_loss: 0.0847, step time: 0.2513\n",
      "43/281, train_loss: 0.0528, step time: 0.2474\n",
      "44/281, train_loss: 0.0561, step time: 0.2509\n",
      "45/281, train_loss: 0.0605, step time: 0.2503\n",
      "46/281, train_loss: 0.0588, step time: 0.2501\n",
      "47/281, train_loss: 0.0324, step time: 0.2532\n",
      "48/281, train_loss: 0.0713, step time: 0.2526\n",
      "49/281, train_loss: 0.0437, step time: 0.2520\n",
      "50/281, train_loss: 0.0611, step time: 0.2527\n",
      "51/281, train_loss: 0.0523, step time: 0.2537\n",
      "52/281, train_loss: 0.2265, step time: 0.2564\n",
      "53/281, train_loss: 0.0411, step time: 0.2506\n",
      "54/281, train_loss: 0.0499, step time: 0.2518\n",
      "55/281, train_loss: 0.0543, step time: 0.2520\n",
      "56/281, train_loss: 0.2183, step time: 0.2466\n",
      "57/281, train_loss: 0.0292, step time: 0.2585\n",
      "58/281, train_loss: 0.0557, step time: 0.2555\n",
      "59/281, train_loss: 0.0519, step time: 0.2526\n",
      "60/281, train_loss: 0.0881, step time: 0.2517\n",
      "61/281, train_loss: 0.0529, step time: 0.2510\n",
      "62/281, train_loss: 0.0609, step time: 0.2501\n",
      "63/281, train_loss: 0.0772, step time: 0.2447\n",
      "64/281, train_loss: 0.2635, step time: 0.2449\n",
      "65/281, train_loss: 0.0638, step time: 0.2495\n",
      "66/281, train_loss: 0.1984, step time: 0.2526\n",
      "67/281, train_loss: 0.0603, step time: 0.2539\n",
      "68/281, train_loss: 0.0790, step time: 0.2511\n",
      "69/281, train_loss: 0.2178, step time: 0.2488\n",
      "70/281, train_loss: 0.0710, step time: 0.2508\n",
      "71/281, train_loss: 0.0575, step time: 0.2517\n",
      "72/281, train_loss: 0.0521, step time: 0.2489\n",
      "73/281, train_loss: 0.0830, step time: 0.2531\n",
      "74/281, train_loss: 0.0457, step time: 0.2504\n",
      "75/281, train_loss: 0.0526, step time: 0.2522\n",
      "76/281, train_loss: 0.0716, step time: 0.2537\n",
      "77/281, train_loss: 0.0594, step time: 0.2541\n",
      "78/281, train_loss: 0.0770, step time: 0.2529\n",
      "79/281, train_loss: 0.2208, step time: 0.2464\n",
      "80/281, train_loss: 0.0784, step time: 0.2488\n",
      "81/281, train_loss: 0.0601, step time: 0.2492\n",
      "82/281, train_loss: 0.2067, step time: 0.2539\n",
      "83/281, train_loss: 0.2106, step time: 0.2586\n",
      "84/281, train_loss: 0.1041, step time: 0.2584\n",
      "85/281, train_loss: 0.0408, step time: 0.2495\n",
      "86/281, train_loss: 0.0501, step time: 0.2527\n",
      "87/281, train_loss: 0.2043, step time: 0.2506\n",
      "88/281, train_loss: 0.0598, step time: 0.2510\n",
      "89/281, train_loss: 0.0503, step time: 0.2477\n",
      "90/281, train_loss: 0.0715, step time: 0.2498\n",
      "91/281, train_loss: 0.0431, step time: 0.2520\n",
      "92/281, train_loss: 0.0493, step time: 0.2533\n",
      "93/281, train_loss: 0.0563, step time: 0.2540\n",
      "94/281, train_loss: 0.0598, step time: 0.2484\n",
      "95/281, train_loss: 0.0657, step time: 0.2514\n",
      "96/281, train_loss: 0.0543, step time: 0.2578\n",
      "97/281, train_loss: 0.0547, step time: 0.2513\n",
      "98/281, train_loss: 0.0775, step time: 0.2513\n",
      "99/281, train_loss: 0.0503, step time: 0.2520\n",
      "100/281, train_loss: 0.0552, step time: 0.2486\n",
      "101/281, train_loss: 0.0662, step time: 0.2474\n",
      "102/281, train_loss: 0.0558, step time: 0.2481\n",
      "103/281, train_loss: 0.0274, step time: 0.2559\n",
      "104/281, train_loss: 0.0565, step time: 0.2545\n",
      "105/281, train_loss: 0.0514, step time: 0.2542\n",
      "106/281, train_loss: 0.2346, step time: 0.2469\n",
      "107/281, train_loss: 0.0676, step time: 0.2503\n",
      "108/281, train_loss: 0.0278, step time: 0.2530\n",
      "109/281, train_loss: 0.0747, step time: 0.2486\n",
      "110/281, train_loss: 0.0321, step time: 0.2452\n",
      "111/281, train_loss: 0.0534, step time: 0.2551\n",
      "112/281, train_loss: 0.0722, step time: 0.2558\n",
      "113/281, train_loss: 0.0394, step time: 0.2521\n",
      "114/281, train_loss: 0.0260, step time: 0.2510\n",
      "115/281, train_loss: 0.0549, step time: 0.2565\n",
      "116/281, train_loss: 0.0736, step time: 0.2560\n",
      "117/281, train_loss: 0.0569, step time: 0.2584\n",
      "118/281, train_loss: 0.0872, step time: 0.2589\n",
      "119/281, train_loss: 0.0559, step time: 0.2522\n",
      "120/281, train_loss: 0.0804, step time: 0.2542\n",
      "121/281, train_loss: 0.0494, step time: 0.2533\n",
      "122/281, train_loss: 0.0871, step time: 0.2559\n",
      "123/281, train_loss: 0.0659, step time: 0.2504\n",
      "124/281, train_loss: 0.2206, step time: 0.2514\n",
      "125/281, train_loss: 0.0614, step time: 0.2474\n",
      "126/281, train_loss: 0.0421, step time: 0.2545\n",
      "127/281, train_loss: 0.0614, step time: 0.2488\n",
      "128/281, train_loss: 0.0428, step time: 0.2559\n",
      "129/281, train_loss: 0.0540, step time: 0.2557\n",
      "130/281, train_loss: 0.0502, step time: 0.2561\n",
      "131/281, train_loss: 0.2210, step time: 0.2523\n",
      "132/281, train_loss: 0.0610, step time: 0.2525\n",
      "133/281, train_loss: 0.0586, step time: 0.2516\n",
      "134/281, train_loss: 0.0666, step time: 0.2533\n",
      "135/281, train_loss: 0.0667, step time: 0.2521\n",
      "136/281, train_loss: 0.0795, step time: 0.2517\n",
      "137/281, train_loss: 0.0708, step time: 0.2502\n",
      "138/281, train_loss: 0.0329, step time: 0.2534\n",
      "139/281, train_loss: 0.0566, step time: 0.2557\n",
      "140/281, train_loss: 0.0673, step time: 0.2598\n",
      "141/281, train_loss: 0.0300, step time: 0.2517\n",
      "142/281, train_loss: 0.0822, step time: 0.2500\n",
      "143/281, train_loss: 0.0433, step time: 0.2526\n",
      "144/281, train_loss: 0.0763, step time: 0.2477\n",
      "145/281, train_loss: 0.0832, step time: 0.2495\n",
      "146/281, train_loss: 0.2170, step time: 0.2509\n",
      "147/281, train_loss: 0.0545, step time: 0.2587\n",
      "148/281, train_loss: 0.0580, step time: 0.2533\n",
      "149/281, train_loss: 0.2318, step time: 0.2521\n",
      "150/281, train_loss: 0.0684, step time: 0.2493\n",
      "151/281, train_loss: 0.0586, step time: 0.2503\n",
      "152/281, train_loss: 0.0586, step time: 0.2576\n",
      "153/281, train_loss: 0.0906, step time: 0.2528\n",
      "154/281, train_loss: 0.0697, step time: 0.2471\n",
      "155/281, train_loss: 0.0558, step time: 0.2544\n",
      "156/281, train_loss: 0.0805, step time: 0.2553\n",
      "157/281, train_loss: 0.2447, step time: 0.2540\n",
      "158/281, train_loss: 0.2183, step time: 0.2552\n",
      "159/281, train_loss: 0.0490, step time: 0.2506\n",
      "160/281, train_loss: 0.0801, step time: 0.2519\n",
      "161/281, train_loss: 0.0508, step time: 0.2487\n",
      "162/281, train_loss: 0.0645, step time: 0.2500\n",
      "163/281, train_loss: 0.0356, step time: 0.2535\n",
      "164/281, train_loss: 0.0637, step time: 0.2568\n",
      "165/281, train_loss: 0.0636, step time: 0.2546\n",
      "166/281, train_loss: 0.0527, step time: 0.2585\n",
      "167/281, train_loss: 0.0488, step time: 0.2550\n",
      "168/281, train_loss: 0.0710, step time: 0.2539\n",
      "169/281, train_loss: 0.0421, step time: 0.2489\n",
      "170/281, train_loss: 0.0438, step time: 0.2528\n",
      "171/281, train_loss: 0.0471, step time: 0.2529\n",
      "172/281, train_loss: 0.0603, step time: 0.2519\n",
      "173/281, train_loss: 0.0581, step time: 0.2646\n",
      "174/281, train_loss: 0.3713, step time: 0.2626\n",
      "175/281, train_loss: 0.1929, step time: 0.2530\n",
      "176/281, train_loss: 0.0372, step time: 0.2524\n",
      "177/281, train_loss: 0.0450, step time: 0.2511\n",
      "178/281, train_loss: 0.0742, step time: 0.2472\n",
      "179/281, train_loss: 0.0448, step time: 0.2549\n",
      "180/281, train_loss: 0.0687, step time: 0.2514\n",
      "181/281, train_loss: 0.0563, step time: 0.2483\n",
      "182/281, train_loss: 0.2129, step time: 0.2531\n",
      "183/281, train_loss: 0.0539, step time: 0.2575\n",
      "184/281, train_loss: 0.0524, step time: 0.2535\n",
      "185/281, train_loss: 0.0513, step time: 0.2499\n",
      "186/281, train_loss: 0.0412, step time: 0.2533\n",
      "187/281, train_loss: 0.0816, step time: 0.2579\n",
      "188/281, train_loss: 0.0697, step time: 0.2530\n",
      "189/281, train_loss: 0.0380, step time: 0.2512\n",
      "190/281, train_loss: 0.2117, step time: 0.2556\n",
      "191/281, train_loss: 0.0558, step time: 0.2572\n",
      "192/281, train_loss: 0.0499, step time: 0.2538\n",
      "193/281, train_loss: 0.0468, step time: 0.2803\n",
      "194/281, train_loss: 0.0313, step time: 0.2667\n",
      "195/281, train_loss: 0.0541, step time: 0.2566\n",
      "196/281, train_loss: 0.2125, step time: 0.2539\n",
      "197/281, train_loss: 0.2415, step time: 0.2494\n",
      "198/281, train_loss: 0.0618, step time: 0.2543\n",
      "199/281, train_loss: 0.0512, step time: 0.2624\n",
      "200/281, train_loss: 0.0613, step time: 0.2599\n",
      "201/281, train_loss: 0.0536, step time: 0.2563\n",
      "202/281, train_loss: 0.0443, step time: 0.2555\n",
      "203/281, train_loss: 0.0799, step time: 0.2533\n",
      "204/281, train_loss: 0.0702, step time: 0.2526\n",
      "205/281, train_loss: 0.2452, step time: 0.2575\n",
      "206/281, train_loss: 0.0781, step time: 0.2538\n",
      "207/281, train_loss: 0.0597, step time: 0.2593\n",
      "208/281, train_loss: 0.0565, step time: 0.2574\n",
      "209/281, train_loss: 0.0669, step time: 0.2554\n",
      "210/281, train_loss: 0.1975, step time: 0.2528\n",
      "211/281, train_loss: 0.0448, step time: 0.2585\n",
      "212/281, train_loss: 0.0513, step time: 0.2547\n",
      "213/281, train_loss: 0.0470, step time: 0.2473\n",
      "214/281, train_loss: 0.0509, step time: 0.2611\n",
      "215/281, train_loss: 0.0473, step time: 0.2492\n",
      "216/281, train_loss: 0.0958, step time: 0.2540\n",
      "217/281, train_loss: 0.0479, step time: 0.2509\n",
      "218/281, train_loss: 0.0762, step time: 0.2554\n",
      "219/281, train_loss: 0.0657, step time: 0.2589\n",
      "220/281, train_loss: 0.0582, step time: 0.2554\n",
      "221/281, train_loss: 0.2212, step time: 0.2558\n",
      "222/281, train_loss: 0.2168, step time: 0.2834\n",
      "223/281, train_loss: 0.2029, step time: 0.2495\n",
      "224/281, train_loss: 0.0730, step time: 0.2567\n",
      "225/281, train_loss: 0.0575, step time: 0.2721\n",
      "226/281, train_loss: 0.0486, step time: 0.2525\n",
      "227/281, train_loss: 0.0518, step time: 0.2558\n",
      "228/281, train_loss: 0.0545, step time: 0.2504\n",
      "229/281, train_loss: 0.0517, step time: 0.2548\n",
      "230/281, train_loss: 0.0395, step time: 0.2563\n",
      "231/281, train_loss: 0.0652, step time: 0.2553\n",
      "232/281, train_loss: 0.0645, step time: 0.2580\n",
      "233/281, train_loss: 0.0426, step time: 0.2546\n",
      "234/281, train_loss: 0.0423, step time: 0.2491\n",
      "235/281, train_loss: 0.2115, step time: 0.2536\n",
      "236/281, train_loss: 0.0799, step time: 0.2542\n",
      "237/281, train_loss: 0.0640, step time: 0.2597\n",
      "238/281, train_loss: 0.0373, step time: 0.2512\n",
      "239/281, train_loss: 0.0422, step time: 0.2537\n",
      "240/281, train_loss: 0.2539, step time: 0.2505\n",
      "241/281, train_loss: 0.0559, step time: 0.2550\n",
      "242/281, train_loss: 0.0469, step time: 0.2591\n",
      "243/281, train_loss: 0.0671, step time: 0.2558\n",
      "244/281, train_loss: 0.0688, step time: 0.2511\n",
      "245/281, train_loss: 0.0581, step time: 0.2550\n",
      "246/281, train_loss: 0.0580, step time: 0.2486\n",
      "247/281, train_loss: 0.2196, step time: 0.2519\n",
      "248/281, train_loss: 0.0811, step time: 0.2590\n",
      "249/281, train_loss: 0.2151, step time: 0.2568\n",
      "250/281, train_loss: 0.0745, step time: 0.2512\n",
      "251/281, train_loss: 0.0941, step time: 0.2548\n",
      "252/281, train_loss: 0.0657, step time: 0.2513\n",
      "253/281, train_loss: 0.2123, step time: 0.2237\n",
      "254/281, train_loss: 0.0510, step time: 0.2476\n",
      "255/281, train_loss: 0.0521, step time: 0.2508\n",
      "256/281, train_loss: 0.2486, step time: 0.2498\n",
      "257/281, train_loss: 0.2038, step time: 0.2508\n",
      "258/281, train_loss: 0.0392, step time: 0.2505\n",
      "259/281, train_loss: 0.0561, step time: 0.2490\n",
      "260/281, train_loss: 0.0460, step time: 0.2513\n",
      "261/281, train_loss: 0.0381, step time: 0.2502\n",
      "262/281, train_loss: 0.0538, step time: 0.2559\n",
      "263/281, train_loss: 0.0572, step time: 0.2466\n",
      "264/281, train_loss: 0.0664, step time: 0.2478\n",
      "265/281, train_loss: 0.0539, step time: 0.2473\n",
      "266/281, train_loss: 0.0707, step time: 0.2577\n",
      "267/281, train_loss: 0.0636, step time: 0.2510\n",
      "268/281, train_loss: 0.0456, step time: 0.2433\n",
      "269/281, train_loss: 0.0936, step time: 0.2501\n",
      "270/281, train_loss: 0.0748, step time: 0.2505\n",
      "271/281, train_loss: 0.0825, step time: 0.2502\n",
      "272/281, train_loss: 0.0695, step time: 0.2530\n",
      "273/281, train_loss: 0.0644, step time: 0.2502\n",
      "274/281, train_loss: 0.0727, step time: 0.2527\n",
      "275/281, train_loss: 0.0628, step time: 0.2527\n",
      "276/281, train_loss: 0.0982, step time: 0.2562\n",
      "277/281, train_loss: 0.0735, step time: 0.2542\n",
      "278/281, train_loss: 0.2793, step time: 0.2520\n",
      "279/281, train_loss: 0.0443, step time: 0.2569\n",
      "280/281, train_loss: 0.0699, step time: 0.2506\n",
      "281/281, train_loss: 0.0443, step time: 0.2496\n",
      "282/281, train_loss: 0.0464, step time: 0.1513\n",
      "epoch 115 average loss: 0.0837\n",
      "saved new best metric model\n",
      "current epoch: 115 current mean dice: 0.9030 tc: 0.8958 wt: 0.9291 et: 0.8934\n",
      "best mean dice: 0.9030 at epoch: 115\n",
      "time consuming of epoch 115 is: 401.5819\n",
      "----------\n",
      "epoch 116/200\n",
      "1/281, train_loss: 0.0734, step time: 0.2611\n",
      "2/281, train_loss: 0.0639, step time: 0.2608\n",
      "3/281, train_loss: 0.0887, step time: 0.2572\n",
      "4/281, train_loss: 0.0703, step time: 0.2583\n",
      "5/281, train_loss: 0.0818, step time: 0.2587\n",
      "6/281, train_loss: 0.0556, step time: 0.2573\n",
      "7/281, train_loss: 0.0427, step time: 0.2616\n",
      "8/281, train_loss: 0.0404, step time: 0.2562\n",
      "9/281, train_loss: 0.0734, step time: 0.2599\n",
      "10/281, train_loss: 0.0448, step time: 0.2588\n",
      "11/281, train_loss: 0.2144, step time: 0.2526\n",
      "12/281, train_loss: 0.0448, step time: 0.2578\n",
      "13/281, train_loss: 0.0799, step time: 0.2558\n",
      "14/281, train_loss: 0.0397, step time: 0.2564\n",
      "15/281, train_loss: 0.0668, step time: 0.2550\n",
      "16/281, train_loss: 0.0485, step time: 0.2503\n",
      "17/281, train_loss: 0.2149, step time: 0.2565\n",
      "18/281, train_loss: 0.0467, step time: 0.2579\n",
      "19/281, train_loss: 0.0530, step time: 0.2486\n",
      "20/281, train_loss: 0.0755, step time: 0.2584\n",
      "21/281, train_loss: 0.0712, step time: 0.2542\n",
      "22/281, train_loss: 0.0512, step time: 0.2540\n",
      "23/281, train_loss: 0.0850, step time: 0.2453\n",
      "24/281, train_loss: 0.0308, step time: 0.2478\n",
      "25/281, train_loss: 0.0418, step time: 0.2610\n",
      "26/281, train_loss: 0.0565, step time: 0.2486\n",
      "27/281, train_loss: 0.2118, step time: 0.2495\n",
      "28/281, train_loss: 0.0506, step time: 0.2550\n",
      "29/281, train_loss: 0.0618, step time: 0.2516\n",
      "30/281, train_loss: 0.0572, step time: 0.2471\n",
      "31/281, train_loss: 0.0614, step time: 0.2689\n",
      "32/281, train_loss: 0.0377, step time: 0.2651\n",
      "33/281, train_loss: 0.0699, step time: 0.2558\n",
      "34/281, train_loss: 0.2234, step time: 0.2551\n",
      "35/281, train_loss: 0.0597, step time: 0.2576\n",
      "36/281, train_loss: 0.0541, step time: 0.2487\n",
      "37/281, train_loss: 0.0469, step time: 0.2501\n",
      "38/281, train_loss: 0.0495, step time: 0.2528\n",
      "39/281, train_loss: 0.2338, step time: 0.2489\n",
      "40/281, train_loss: 0.2047, step time: 0.2495\n",
      "41/281, train_loss: 0.0469, step time: 0.2516\n",
      "42/281, train_loss: 0.0689, step time: 0.2513\n",
      "43/281, train_loss: 0.0465, step time: 0.2515\n",
      "44/281, train_loss: 0.0714, step time: 0.2506\n",
      "45/281, train_loss: 0.2182, step time: 0.2529\n",
      "46/281, train_loss: 0.0416, step time: 0.2536\n",
      "47/281, train_loss: 0.0397, step time: 0.2562\n",
      "48/281, train_loss: 0.0466, step time: 0.2526\n",
      "49/281, train_loss: 0.0610, step time: 0.2568\n",
      "50/281, train_loss: 0.0496, step time: 0.2708\n",
      "51/281, train_loss: 0.0666, step time: 0.2588\n",
      "52/281, train_loss: 0.2229, step time: 0.2555\n",
      "53/281, train_loss: 0.0716, step time: 0.2589\n",
      "54/281, train_loss: 0.0568, step time: 0.2553\n",
      "55/281, train_loss: 0.0655, step time: 0.2581\n",
      "56/281, train_loss: 0.0277, step time: 0.2584\n",
      "57/281, train_loss: 0.2245, step time: 0.2517\n",
      "58/281, train_loss: 0.0463, step time: 0.2581\n",
      "59/281, train_loss: 0.1013, step time: 0.2540\n",
      "60/281, train_loss: 0.0444, step time: 0.2534\n",
      "61/281, train_loss: 0.1941, step time: 0.2544\n",
      "62/281, train_loss: 0.0597, step time: 0.2510\n",
      "63/281, train_loss: 0.0544, step time: 0.2546\n",
      "64/281, train_loss: 0.1042, step time: 0.2572\n",
      "65/281, train_loss: 0.0644, step time: 0.2574\n",
      "66/281, train_loss: 0.0840, step time: 0.2647\n",
      "67/281, train_loss: 0.0511, step time: 0.2556\n",
      "68/281, train_loss: 0.0396, step time: 0.2530\n",
      "69/281, train_loss: 0.0465, step time: 0.2534\n",
      "70/281, train_loss: 0.0664, step time: 0.2566\n",
      "71/281, train_loss: 0.0506, step time: 0.2607\n",
      "72/281, train_loss: 0.2199, step time: 0.2501\n",
      "73/281, train_loss: 0.0451, step time: 0.2538\n",
      "74/281, train_loss: 0.0504, step time: 0.2556\n",
      "75/281, train_loss: 0.0476, step time: 0.2555\n",
      "76/281, train_loss: 0.0805, step time: 0.2512\n",
      "77/281, train_loss: 0.0341, step time: 0.2560\n",
      "78/281, train_loss: 0.0430, step time: 0.2581\n",
      "79/281, train_loss: 0.0840, step time: 0.2572\n",
      "80/281, train_loss: 0.0632, step time: 0.2543\n",
      "81/281, train_loss: 0.0591, step time: 0.2532\n",
      "82/281, train_loss: 0.0653, step time: 0.2550\n",
      "83/281, train_loss: 0.0508, step time: 0.2501\n",
      "84/281, train_loss: 0.0401, step time: 0.2496\n",
      "85/281, train_loss: 0.0695, step time: 0.2503\n",
      "86/281, train_loss: 0.2181, step time: 0.2550\n",
      "87/281, train_loss: 0.0360, step time: 0.2574\n",
      "88/281, train_loss: 0.0511, step time: 0.2501\n",
      "89/281, train_loss: 0.0458, step time: 0.2564\n",
      "90/281, train_loss: 0.0547, step time: 0.2465\n",
      "91/281, train_loss: 0.0286, step time: 0.2551\n",
      "92/281, train_loss: 0.2292, step time: 0.2556\n",
      "93/281, train_loss: 0.0580, step time: 0.2521\n",
      "94/281, train_loss: 0.2483, step time: 0.2547\n",
      "95/281, train_loss: 0.0592, step time: 0.2560\n",
      "96/281, train_loss: 0.2066, step time: 0.2556\n",
      "97/281, train_loss: 0.0565, step time: 0.2557\n",
      "98/281, train_loss: 0.0320, step time: 0.2542\n",
      "99/281, train_loss: 0.0438, step time: 0.2517\n",
      "100/281, train_loss: 0.2326, step time: 0.2524\n",
      "101/281, train_loss: 0.0689, step time: 0.2514\n",
      "102/281, train_loss: 0.0582, step time: 0.2563\n",
      "103/281, train_loss: 0.0766, step time: 0.2519\n",
      "104/281, train_loss: 0.0610, step time: 0.2549\n",
      "105/281, train_loss: 0.2097, step time: 0.2574\n",
      "106/281, train_loss: 0.0687, step time: 0.2512\n",
      "107/281, train_loss: 0.0471, step time: 0.2540\n",
      "108/281, train_loss: 0.0292, step time: 0.2517\n",
      "109/281, train_loss: 0.0617, step time: 0.2494\n",
      "110/281, train_loss: 0.0825, step time: 0.2562\n",
      "111/281, train_loss: 0.0904, step time: 0.2517\n",
      "112/281, train_loss: 0.0451, step time: 0.2522\n",
      "113/281, train_loss: 0.0610, step time: 0.2483\n",
      "114/281, train_loss: 0.0443, step time: 0.2482\n",
      "115/281, train_loss: 0.2319, step time: 0.2546\n",
      "116/281, train_loss: 0.0637, step time: 0.2528\n",
      "117/281, train_loss: 0.0494, step time: 0.2499\n",
      "118/281, train_loss: 0.0545, step time: 0.2558\n",
      "119/281, train_loss: 0.0510, step time: 0.2547\n",
      "120/281, train_loss: 0.2034, step time: 0.2574\n",
      "121/281, train_loss: 0.0678, step time: 0.2526\n",
      "122/281, train_loss: 0.0559, step time: 0.2531\n",
      "123/281, train_loss: 0.0431, step time: 0.2549\n",
      "124/281, train_loss: 0.2219, step time: 0.2557\n",
      "125/281, train_loss: 0.1056, step time: 0.2510\n",
      "126/281, train_loss: 0.0459, step time: 0.2543\n",
      "127/281, train_loss: 0.0934, step time: 0.2509\n",
      "128/281, train_loss: 0.0471, step time: 0.2515\n",
      "129/281, train_loss: 0.0436, step time: 0.2453\n",
      "130/281, train_loss: 0.0593, step time: 0.2488\n",
      "131/281, train_loss: 0.2072, step time: 0.2540\n",
      "132/281, train_loss: 0.0625, step time: 0.2525\n",
      "133/281, train_loss: 0.2241, step time: 0.2540\n",
      "134/281, train_loss: 0.0831, step time: 0.2539\n",
      "135/281, train_loss: 0.0637, step time: 0.2572\n",
      "136/281, train_loss: 0.0549, step time: 0.2515\n",
      "137/281, train_loss: 0.0730, step time: 0.2530\n",
      "138/281, train_loss: 0.0607, step time: 0.2581\n",
      "139/281, train_loss: 0.0488, step time: 0.2523\n",
      "140/281, train_loss: 0.0671, step time: 0.2513\n",
      "141/281, train_loss: 0.0714, step time: 0.2498\n",
      "142/281, train_loss: 0.0556, step time: 0.2502\n",
      "143/281, train_loss: 0.0386, step time: 0.2495\n",
      "144/281, train_loss: 0.0538, step time: 0.2549\n",
      "145/281, train_loss: 0.0520, step time: 0.2558\n",
      "146/281, train_loss: 0.2079, step time: 0.2574\n",
      "147/281, train_loss: 0.0495, step time: 0.2542\n",
      "148/281, train_loss: 0.0398, step time: 0.2571\n",
      "149/281, train_loss: 0.0555, step time: 0.2531\n",
      "150/281, train_loss: 0.0543, step time: 0.2486\n",
      "151/281, train_loss: 0.0536, step time: 0.2531\n",
      "152/281, train_loss: 0.0714, step time: 0.2555\n",
      "153/281, train_loss: 0.0564, step time: 0.2498\n",
      "154/281, train_loss: 0.0279, step time: 0.2525\n",
      "155/281, train_loss: 0.0564, step time: 0.2580\n",
      "156/281, train_loss: 0.2197, step time: 0.2558\n",
      "157/281, train_loss: 0.0450, step time: 0.2540\n",
      "158/281, train_loss: 0.0525, step time: 0.2466\n",
      "159/281, train_loss: 0.0638, step time: 0.2550\n",
      "160/281, train_loss: 0.2230, step time: 0.2564\n",
      "161/281, train_loss: 0.0593, step time: 0.2579\n",
      "162/281, train_loss: 0.0421, step time: 0.2472\n",
      "163/281, train_loss: 0.0718, step time: 0.2513\n",
      "164/281, train_loss: 0.0729, step time: 0.2545\n",
      "165/281, train_loss: 0.0763, step time: 0.2552\n",
      "166/281, train_loss: 0.0351, step time: 0.2529\n",
      "167/281, train_loss: 0.0590, step time: 0.2524\n",
      "168/281, train_loss: 0.0428, step time: 0.2529\n",
      "169/281, train_loss: 0.1174, step time: 0.2533\n",
      "170/281, train_loss: 0.0542, step time: 0.2573\n",
      "171/281, train_loss: 0.0637, step time: 0.2532\n",
      "172/281, train_loss: 0.0872, step time: 0.2548\n",
      "173/281, train_loss: 0.0302, step time: 0.2517\n",
      "174/281, train_loss: 0.0763, step time: 0.2449\n",
      "175/281, train_loss: 0.2075, step time: 0.2483\n",
      "176/281, train_loss: 0.0686, step time: 0.2503\n",
      "177/281, train_loss: 0.0475, step time: 0.2541\n",
      "178/281, train_loss: 0.0560, step time: 0.2488\n",
      "179/281, train_loss: 0.0694, step time: 0.2545\n",
      "180/281, train_loss: 0.0525, step time: 0.2472\n",
      "181/281, train_loss: 0.0699, step time: 0.2444\n",
      "182/281, train_loss: 0.2183, step time: 0.2444\n",
      "183/281, train_loss: 0.2101, step time: 0.2532\n",
      "184/281, train_loss: 0.0416, step time: 0.2495\n",
      "185/281, train_loss: 0.0274, step time: 0.2470\n",
      "186/281, train_loss: 0.3795, step time: 0.2530\n",
      "187/281, train_loss: 0.0751, step time: 0.2505\n",
      "188/281, train_loss: 0.0812, step time: 0.2497\n",
      "189/281, train_loss: 0.0831, step time: 0.2459\n",
      "190/281, train_loss: 0.0559, step time: 0.2467\n",
      "191/281, train_loss: 0.0330, step time: 0.2522\n",
      "192/281, train_loss: 0.0605, step time: 0.2468\n",
      "193/281, train_loss: 0.3746, step time: 0.2478\n",
      "194/281, train_loss: 0.0477, step time: 0.2466\n",
      "195/281, train_loss: 0.0355, step time: 0.2447\n",
      "196/281, train_loss: 0.0532, step time: 0.2477\n",
      "197/281, train_loss: 0.0692, step time: 0.2473\n",
      "198/281, train_loss: 0.0439, step time: 0.2465\n",
      "199/281, train_loss: 0.1027, step time: 0.2500\n",
      "200/281, train_loss: 0.0669, step time: 0.2494\n",
      "201/281, train_loss: 0.2135, step time: 0.2499\n",
      "202/281, train_loss: 0.0732, step time: 0.2505\n",
      "203/281, train_loss: 0.1084, step time: 0.2505\n",
      "204/281, train_loss: 0.0644, step time: 0.2472\n",
      "205/281, train_loss: 0.0819, step time: 0.2476\n",
      "206/281, train_loss: 0.0383, step time: 0.2520\n",
      "207/281, train_loss: 0.0445, step time: 0.2520\n",
      "208/281, train_loss: 0.0787, step time: 0.2464\n",
      "209/281, train_loss: 0.0510, step time: 0.2483\n",
      "210/281, train_loss: 0.2164, step time: 0.2477\n",
      "211/281, train_loss: 0.0563, step time: 0.2477\n",
      "212/281, train_loss: 0.0686, step time: 0.2537\n",
      "213/281, train_loss: 0.0495, step time: 0.2516\n",
      "214/281, train_loss: 0.0617, step time: 0.2509\n",
      "215/281, train_loss: 0.0449, step time: 0.2595\n",
      "216/281, train_loss: 0.0605, step time: 0.2607\n",
      "217/281, train_loss: 0.0453, step time: 0.2755\n",
      "218/281, train_loss: 0.0523, step time: 0.2537\n",
      "219/281, train_loss: 0.0503, step time: 0.2547\n",
      "220/281, train_loss: 0.0489, step time: 0.2548\n",
      "221/281, train_loss: 0.0200, step time: 0.2502\n",
      "222/281, train_loss: 0.0641, step time: 0.2538\n",
      "223/281, train_loss: 0.0512, step time: 0.2528\n",
      "224/281, train_loss: 0.0772, step time: 0.2562\n",
      "225/281, train_loss: 0.0503, step time: 0.2512\n",
      "226/281, train_loss: 0.0669, step time: 0.2531\n",
      "227/281, train_loss: 0.0693, step time: 0.2504\n",
      "228/281, train_loss: 0.0447, step time: 0.2490\n",
      "229/281, train_loss: 0.0449, step time: 0.2477\n",
      "230/281, train_loss: 0.0586, step time: 0.2476\n",
      "231/281, train_loss: 0.0485, step time: 0.2642\n",
      "232/281, train_loss: 0.0856, step time: 0.2489\n",
      "233/281, train_loss: 0.0423, step time: 0.2526\n",
      "234/281, train_loss: 0.0294, step time: 0.2480\n",
      "235/281, train_loss: 0.0692, step time: 0.2587\n",
      "236/281, train_loss: 0.0366, step time: 0.2538\n",
      "237/281, train_loss: 0.1143, step time: 0.2507\n",
      "238/281, train_loss: 0.0938, step time: 0.2510\n",
      "239/281, train_loss: 0.1881, step time: 0.2508\n",
      "240/281, train_loss: 0.0645, step time: 0.2521\n",
      "241/281, train_loss: 0.0426, step time: 0.2480\n",
      "242/281, train_loss: 0.2232, step time: 0.2485\n",
      "243/281, train_loss: 0.0624, step time: 0.2498\n",
      "244/281, train_loss: 0.1024, step time: 0.2561\n",
      "245/281, train_loss: 0.0735, step time: 0.2538\n",
      "246/281, train_loss: 0.0469, step time: 0.2592\n",
      "247/281, train_loss: 0.0727, step time: 0.2563\n",
      "248/281, train_loss: 0.0466, step time: 0.2550\n",
      "249/281, train_loss: 0.0500, step time: 0.2545\n",
      "250/281, train_loss: 0.0511, step time: 0.2578\n",
      "251/281, train_loss: 0.0476, step time: 0.2533\n",
      "252/281, train_loss: 0.2196, step time: 0.2515\n",
      "253/281, train_loss: 0.0523, step time: 0.2494\n",
      "254/281, train_loss: 0.2103, step time: 0.2534\n",
      "255/281, train_loss: 0.0641, step time: 0.2547\n",
      "256/281, train_loss: 0.0591, step time: 0.2531\n",
      "257/281, train_loss: 0.2100, step time: 0.2535\n",
      "258/281, train_loss: 0.0405, step time: 0.2485\n",
      "259/281, train_loss: 0.0610, step time: 0.2492\n",
      "260/281, train_loss: 0.0780, step time: 0.2506\n",
      "261/281, train_loss: 0.0640, step time: 0.2534\n",
      "262/281, train_loss: 0.0808, step time: 0.2457\n",
      "263/281, train_loss: 0.0645, step time: 0.2478\n",
      "264/281, train_loss: 0.0416, step time: 0.2514\n",
      "265/281, train_loss: 0.0650, step time: 0.2512\n",
      "266/281, train_loss: 0.0787, step time: 0.2464\n",
      "267/281, train_loss: 0.0713, step time: 0.2463\n",
      "268/281, train_loss: 0.0626, step time: 0.2462\n",
      "269/281, train_loss: 0.2142, step time: 0.2460\n",
      "270/281, train_loss: 0.0517, step time: 0.2553\n",
      "271/281, train_loss: 0.0686, step time: 0.2494\n",
      "272/281, train_loss: 0.0578, step time: 0.2487\n",
      "273/281, train_loss: 0.0545, step time: 0.2508\n",
      "274/281, train_loss: 0.0728, step time: 0.2500\n",
      "275/281, train_loss: 0.0573, step time: 0.2468\n",
      "276/281, train_loss: 0.0399, step time: 0.2506\n",
      "277/281, train_loss: 0.2323, step time: 0.2524\n",
      "278/281, train_loss: 0.0620, step time: 0.2824\n",
      "279/281, train_loss: 0.0537, step time: 0.2694\n",
      "280/281, train_loss: 0.2040, step time: 0.2234\n",
      "281/281, train_loss: 0.0666, step time: 0.2544\n",
      "282/281, train_loss: 0.3847, step time: 0.1513\n",
      "epoch 116 average loss: 0.0834\n",
      "current epoch: 116 current mean dice: 0.9010 tc: 0.8936 wt: 0.9278 et: 0.8914\n",
      "best mean dice: 0.9030 at epoch: 115\n",
      "time consuming of epoch 116 is: 409.3873\n",
      "----------\n",
      "epoch 117/200\n",
      "1/281, train_loss: 0.0634, step time: 0.2654\n",
      "2/281, train_loss: 0.0437, step time: 0.2550\n",
      "3/281, train_loss: 0.0690, step time: 0.2474\n",
      "4/281, train_loss: 0.0830, step time: 0.2410\n",
      "5/281, train_loss: 0.0621, step time: 0.2584\n",
      "6/281, train_loss: 0.0490, step time: 0.2561\n",
      "7/281, train_loss: 0.1981, step time: 0.2521\n",
      "8/281, train_loss: 0.0347, step time: 0.2576\n",
      "9/281, train_loss: 0.0429, step time: 0.2483\n",
      "10/281, train_loss: 0.0349, step time: 0.2520\n",
      "11/281, train_loss: 0.0517, step time: 0.2498\n",
      "12/281, train_loss: 0.0673, step time: 0.2489\n",
      "13/281, train_loss: 0.2083, step time: 0.2483\n",
      "14/281, train_loss: 0.0587, step time: 0.2594\n",
      "15/281, train_loss: 0.0502, step time: 0.2506\n",
      "16/281, train_loss: 0.2139, step time: 0.2461\n",
      "17/281, train_loss: 0.0867, step time: 0.2528\n",
      "18/281, train_loss: 0.1953, step time: 0.2570\n",
      "19/281, train_loss: 0.0703, step time: 0.2571\n",
      "20/281, train_loss: 0.0608, step time: 0.2530\n",
      "21/281, train_loss: 0.0721, step time: 0.2503\n",
      "22/281, train_loss: 0.0447, step time: 0.2510\n",
      "23/281, train_loss: 0.0580, step time: 0.2520\n",
      "24/281, train_loss: 0.0475, step time: 0.2563\n",
      "25/281, train_loss: 0.0468, step time: 0.2546\n",
      "26/281, train_loss: 0.0351, step time: 0.2510\n",
      "27/281, train_loss: 0.0468, step time: 0.2505\n",
      "28/281, train_loss: 0.0479, step time: 0.2465\n",
      "29/281, train_loss: 0.0821, step time: 0.2470\n",
      "30/281, train_loss: 0.0440, step time: 0.2477\n",
      "31/281, train_loss: 0.0424, step time: 0.2510\n",
      "32/281, train_loss: 0.0691, step time: 0.2478\n",
      "33/281, train_loss: 0.0401, step time: 0.2481\n",
      "34/281, train_loss: 0.0634, step time: 0.2570\n",
      "35/281, train_loss: 0.0638, step time: 0.2496\n",
      "36/281, train_loss: 0.0625, step time: 0.2529\n",
      "37/281, train_loss: 0.0726, step time: 0.2483\n",
      "38/281, train_loss: 0.0528, step time: 0.2506\n",
      "39/281, train_loss: 0.0890, step time: 0.2523\n",
      "40/281, train_loss: 0.0529, step time: 0.2553\n",
      "41/281, train_loss: 0.0668, step time: 0.2524\n",
      "42/281, train_loss: 0.0407, step time: 0.2488\n",
      "43/281, train_loss: 0.0758, step time: 0.2512\n",
      "44/281, train_loss: 0.0417, step time: 0.2496\n",
      "45/281, train_loss: 0.0625, step time: 0.2515\n",
      "46/281, train_loss: 0.0399, step time: 0.2530\n",
      "47/281, train_loss: 0.0600, step time: 0.2501\n",
      "48/281, train_loss: 0.0562, step time: 0.2501\n",
      "49/281, train_loss: 0.2104, step time: 0.2473\n",
      "50/281, train_loss: 0.0632, step time: 0.2522\n",
      "51/281, train_loss: 0.0372, step time: 0.2514\n",
      "52/281, train_loss: 0.0718, step time: 0.2497\n",
      "53/281, train_loss: 0.0976, step time: 0.2475\n",
      "54/281, train_loss: 0.0521, step time: 0.2500\n",
      "55/281, train_loss: 0.0808, step time: 0.2549\n",
      "56/281, train_loss: 0.0555, step time: 0.2456\n",
      "57/281, train_loss: 0.0622, step time: 0.2456\n",
      "58/281, train_loss: 0.0742, step time: 0.2595\n",
      "59/281, train_loss: 0.0931, step time: 0.2525\n",
      "60/281, train_loss: 0.0612, step time: 0.2417\n",
      "61/281, train_loss: 0.0486, step time: 0.2444\n",
      "62/281, train_loss: 0.0514, step time: 0.2531\n",
      "63/281, train_loss: 0.0414, step time: 0.2576\n",
      "64/281, train_loss: 0.0683, step time: 0.2492\n",
      "65/281, train_loss: 0.0549, step time: 0.2577\n",
      "66/281, train_loss: 0.0489, step time: 0.2521\n",
      "67/281, train_loss: 0.0417, step time: 0.2529\n",
      "68/281, train_loss: 0.0724, step time: 0.2556\n",
      "69/281, train_loss: 0.0491, step time: 0.2567\n",
      "70/281, train_loss: 0.0346, step time: 0.2521\n",
      "71/281, train_loss: 0.2035, step time: 0.2573\n",
      "72/281, train_loss: 0.0866, step time: 0.2545\n",
      "73/281, train_loss: 0.0513, step time: 0.2507\n",
      "74/281, train_loss: 0.0652, step time: 0.2533\n",
      "75/281, train_loss: 0.3905, step time: 0.2469\n",
      "76/281, train_loss: 0.1040, step time: 0.2557\n",
      "77/281, train_loss: 0.0497, step time: 0.2501\n",
      "78/281, train_loss: 0.0402, step time: 0.2573\n",
      "79/281, train_loss: 0.1882, step time: 0.2264\n",
      "80/281, train_loss: 0.0404, step time: 0.2482\n",
      "81/281, train_loss: 0.0703, step time: 0.2509\n",
      "82/281, train_loss: 0.0667, step time: 0.2522\n",
      "83/281, train_loss: 0.1602, step time: 0.2186\n",
      "84/281, train_loss: 0.0345, step time: 0.2493\n",
      "85/281, train_loss: 0.0582, step time: 0.2496\n",
      "86/281, train_loss: 0.0664, step time: 0.2581\n",
      "87/281, train_loss: 0.0387, step time: 0.2509\n",
      "88/281, train_loss: 0.2203, step time: 0.2529\n",
      "89/281, train_loss: 0.0458, step time: 0.2486\n",
      "90/281, train_loss: 0.0780, step time: 0.2453\n",
      "91/281, train_loss: 0.0749, step time: 0.2459\n",
      "92/281, train_loss: 0.2258, step time: 0.2492\n",
      "93/281, train_loss: 0.1893, step time: 0.2489\n",
      "94/281, train_loss: 0.0449, step time: 0.2520\n",
      "95/281, train_loss: 0.0486, step time: 0.2452\n",
      "96/281, train_loss: 0.0704, step time: 0.2514\n",
      "97/281, train_loss: 0.0727, step time: 0.2470\n",
      "98/281, train_loss: 0.0917, step time: 0.2517\n",
      "99/281, train_loss: 0.0467, step time: 0.2473\n",
      "100/281, train_loss: 0.0659, step time: 0.2532\n",
      "101/281, train_loss: 0.0471, step time: 0.2450\n",
      "102/281, train_loss: 0.0986, step time: 0.2428\n",
      "103/281, train_loss: 0.0394, step time: 0.2441\n",
      "104/281, train_loss: 0.0667, step time: 0.2489\n",
      "105/281, train_loss: 0.2274, step time: 0.2501\n",
      "106/281, train_loss: 0.0560, step time: 0.2502\n",
      "107/281, train_loss: 0.0488, step time: 0.2459\n",
      "108/281, train_loss: 0.1992, step time: 0.2477\n",
      "109/281, train_loss: 0.0694, step time: 0.2487\n",
      "110/281, train_loss: 0.0559, step time: 0.2517\n",
      "111/281, train_loss: 0.0648, step time: 0.2498\n",
      "112/281, train_loss: 0.2313, step time: 0.2429\n",
      "113/281, train_loss: 0.0523, step time: 0.2493\n",
      "114/281, train_loss: 0.0724, step time: 0.2503\n",
      "115/281, train_loss: 0.2006, step time: 0.2521\n",
      "116/281, train_loss: 0.0541, step time: 0.2575\n",
      "117/281, train_loss: 0.0765, step time: 0.2486\n",
      "118/281, train_loss: 0.0426, step time: 0.2506\n",
      "119/281, train_loss: 0.0507, step time: 0.2457\n",
      "120/281, train_loss: 0.0560, step time: 0.2511\n",
      "121/281, train_loss: 0.0700, step time: 0.2554\n",
      "122/281, train_loss: 0.0574, step time: 0.2548\n",
      "123/281, train_loss: 0.0483, step time: 0.2560\n",
      "124/281, train_loss: 0.2544, step time: 0.2526\n",
      "125/281, train_loss: 0.2282, step time: 0.2694\n",
      "126/281, train_loss: 0.0575, step time: 0.2613\n",
      "127/281, train_loss: 0.0832, step time: 0.2502\n",
      "128/281, train_loss: 0.0833, step time: 0.2496\n",
      "129/281, train_loss: 0.2224, step time: 0.2490\n",
      "130/281, train_loss: 0.0596, step time: 0.2556\n",
      "131/281, train_loss: 0.0762, step time: 0.2520\n",
      "132/281, train_loss: 0.0810, step time: 0.2499\n",
      "133/281, train_loss: 0.0400, step time: 0.2532\n",
      "134/281, train_loss: 0.0530, step time: 0.2525\n",
      "135/281, train_loss: 0.0566, step time: 0.2545\n",
      "136/281, train_loss: 0.0447, step time: 0.2514\n",
      "137/281, train_loss: 0.0399, step time: 0.2555\n",
      "138/281, train_loss: 0.0315, step time: 0.2521\n",
      "139/281, train_loss: 0.0615, step time: 0.2546\n",
      "140/281, train_loss: 0.0724, step time: 0.2571\n",
      "141/281, train_loss: 0.0884, step time: 0.2567\n",
      "142/281, train_loss: 0.0552, step time: 0.2508\n",
      "143/281, train_loss: 0.0501, step time: 0.2541\n",
      "144/281, train_loss: 0.0361, step time: 0.2516\n",
      "145/281, train_loss: 0.0660, step time: 0.2486\n",
      "146/281, train_loss: 0.0624, step time: 0.2491\n",
      "147/281, train_loss: 0.0696, step time: 0.2535\n",
      "148/281, train_loss: 0.0562, step time: 0.2540\n",
      "149/281, train_loss: 0.0340, step time: 0.2556\n",
      "150/281, train_loss: 0.0312, step time: 0.2577\n",
      "151/281, train_loss: 0.2058, step time: 0.2618\n",
      "152/281, train_loss: 0.0302, step time: 0.2572\n",
      "153/281, train_loss: 0.2182, step time: 0.2563\n",
      "154/281, train_loss: 0.0757, step time: 0.2472\n",
      "155/281, train_loss: 0.0648, step time: 0.2516\n",
      "156/281, train_loss: 0.0391, step time: 0.2509\n",
      "157/281, train_loss: 0.0719, step time: 0.2527\n",
      "158/281, train_loss: 0.0363, step time: 0.2491\n",
      "159/281, train_loss: 0.0488, step time: 0.2540\n",
      "160/281, train_loss: 0.0628, step time: 0.2528\n",
      "161/281, train_loss: 0.0657, step time: 0.2507\n",
      "162/281, train_loss: 0.0585, step time: 0.2519\n",
      "163/281, train_loss: 0.2026, step time: 0.2461\n",
      "164/281, train_loss: 0.0240, step time: 0.2499\n",
      "165/281, train_loss: 0.0476, step time: 0.2488\n",
      "166/281, train_loss: 0.0566, step time: 0.2529\n",
      "167/281, train_loss: 0.0809, step time: 0.2542\n",
      "168/281, train_loss: 0.2106, step time: 0.2475\n",
      "169/281, train_loss: 0.0488, step time: 0.2456\n",
      "170/281, train_loss: 0.0439, step time: 0.2480\n",
      "171/281, train_loss: 0.2173, step time: 0.2510\n",
      "172/281, train_loss: 0.0423, step time: 0.2556\n",
      "173/281, train_loss: 0.0570, step time: 0.2510\n",
      "174/281, train_loss: 0.0507, step time: 0.2531\n",
      "175/281, train_loss: 0.0534, step time: 0.2504\n",
      "176/281, train_loss: 0.2158, step time: 0.2562\n",
      "177/281, train_loss: 0.2340, step time: 0.2524\n",
      "178/281, train_loss: 0.0554, step time: 0.2469\n",
      "179/281, train_loss: 0.0683, step time: 0.2458\n",
      "180/281, train_loss: 0.0508, step time: 0.2509\n",
      "181/281, train_loss: 0.0454, step time: 0.2534\n",
      "182/281, train_loss: 0.0673, step time: 0.2457\n",
      "183/281, train_loss: 0.0721, step time: 0.2487\n",
      "184/281, train_loss: 0.0503, step time: 0.2512\n",
      "185/281, train_loss: 0.2304, step time: 0.2508\n",
      "186/281, train_loss: 0.2240, step time: 0.2480\n",
      "187/281, train_loss: 0.0355, step time: 0.2509\n",
      "188/281, train_loss: 0.0671, step time: 0.2595\n",
      "189/281, train_loss: 0.0480, step time: 0.2536\n",
      "190/281, train_loss: 0.0947, step time: 0.2478\n",
      "191/281, train_loss: 0.0385, step time: 0.2506\n",
      "192/281, train_loss: 0.0653, step time: 0.2534\n",
      "193/281, train_loss: 0.2111, step time: 0.2521\n",
      "194/281, train_loss: 0.0475, step time: 0.2507\n",
      "195/281, train_loss: 0.0558, step time: 0.2528\n",
      "196/281, train_loss: 0.2236, step time: 0.2509\n",
      "197/281, train_loss: 0.0861, step time: 0.2491\n",
      "198/281, train_loss: 0.0950, step time: 0.2569\n",
      "199/281, train_loss: 0.0706, step time: 0.2536\n",
      "200/281, train_loss: 0.0803, step time: 0.2470\n",
      "201/281, train_loss: 0.0704, step time: 0.2505\n",
      "202/281, train_loss: 0.0757, step time: 0.2523\n",
      "203/281, train_loss: 0.0685, step time: 0.2493\n",
      "204/281, train_loss: 0.0372, step time: 0.2500\n",
      "205/281, train_loss: 0.0479, step time: 0.2508\n",
      "206/281, train_loss: 0.0367, step time: 0.2536\n",
      "207/281, train_loss: 0.0544, step time: 0.2535\n",
      "208/281, train_loss: 0.0731, step time: 0.2526\n",
      "209/281, train_loss: 0.0589, step time: 0.2517\n",
      "210/281, train_loss: 0.0757, step time: 0.2552\n",
      "211/281, train_loss: 0.0499, step time: 0.2514\n",
      "212/281, train_loss: 0.0518, step time: 0.2525\n",
      "213/281, train_loss: 0.0739, step time: 0.2502\n",
      "214/281, train_loss: 0.0306, step time: 0.2535\n",
      "215/281, train_loss: 0.2307, step time: 0.2497\n",
      "216/281, train_loss: 0.0375, step time: 0.2545\n",
      "217/281, train_loss: 0.0518, step time: 0.2520\n",
      "218/281, train_loss: 0.0860, step time: 0.2509\n",
      "219/281, train_loss: 0.0533, step time: 0.2558\n",
      "220/281, train_loss: 0.2229, step time: 0.2545\n",
      "221/281, train_loss: 0.0945, step time: 0.2465\n",
      "222/281, train_loss: 0.0766, step time: 0.2592\n",
      "223/281, train_loss: 0.0374, step time: 0.2539\n",
      "224/281, train_loss: 0.0886, step time: 0.2528\n",
      "225/281, train_loss: 0.0509, step time: 0.2558\n",
      "226/281, train_loss: 0.0531, step time: 0.2562\n",
      "227/281, train_loss: 0.0500, step time: 0.2509\n",
      "228/281, train_loss: 0.0552, step time: 0.2498\n",
      "229/281, train_loss: 0.0629, step time: 0.2543\n",
      "230/281, train_loss: 0.0628, step time: 0.2589\n",
      "231/281, train_loss: 0.0532, step time: 0.2550\n",
      "232/281, train_loss: 0.0448, step time: 0.2523\n",
      "233/281, train_loss: 0.0542, step time: 0.2528\n",
      "234/281, train_loss: 0.0710, step time: 0.2499\n",
      "235/281, train_loss: 0.0585, step time: 0.2566\n",
      "236/281, train_loss: 0.0712, step time: 0.2580\n",
      "237/281, train_loss: 0.2004, step time: 0.2565\n",
      "238/281, train_loss: 0.2161, step time: 0.2558\n",
      "239/281, train_loss: 0.0480, step time: 0.2581\n",
      "240/281, train_loss: 0.0543, step time: 0.2564\n",
      "241/281, train_loss: 0.0444, step time: 0.2564\n",
      "242/281, train_loss: 0.0987, step time: 0.2538\n",
      "243/281, train_loss: 0.0360, step time: 0.2524\n",
      "244/281, train_loss: 0.0643, step time: 0.2535\n",
      "245/281, train_loss: 0.0434, step time: 0.2517\n",
      "246/281, train_loss: 0.0534, step time: 0.2550\n",
      "247/281, train_loss: 0.0659, step time: 0.2615\n",
      "248/281, train_loss: 0.0578, step time: 0.2620\n",
      "249/281, train_loss: 0.0565, step time: 0.2571\n",
      "250/281, train_loss: 0.2074, step time: 0.2564\n",
      "251/281, train_loss: 0.0537, step time: 0.2536\n",
      "252/281, train_loss: 0.0727, step time: 0.2596\n",
      "253/281, train_loss: 0.2175, step time: 0.2590\n",
      "254/281, train_loss: 0.0444, step time: 0.2489\n",
      "255/281, train_loss: 0.0372, step time: 0.2498\n",
      "256/281, train_loss: 0.0499, step time: 0.2510\n",
      "257/281, train_loss: 0.0852, step time: 0.2526\n",
      "258/281, train_loss: 0.0565, step time: 0.2501\n",
      "259/281, train_loss: 0.0321, step time: 0.2514\n",
      "260/281, train_loss: 0.2085, step time: 0.2536\n",
      "261/281, train_loss: 0.2370, step time: 0.2514\n",
      "262/281, train_loss: 0.0583, step time: 0.2569\n",
      "263/281, train_loss: 0.0594, step time: 0.2572\n",
      "264/281, train_loss: 0.0575, step time: 0.2536\n",
      "265/281, train_loss: 0.2237, step time: 0.2595\n",
      "266/281, train_loss: 0.0586, step time: 0.2600\n",
      "267/281, train_loss: 0.2415, step time: 0.2591\n",
      "268/281, train_loss: 0.0356, step time: 0.2567\n",
      "269/281, train_loss: 0.0680, step time: 0.2551\n",
      "270/281, train_loss: 0.0384, step time: 0.2522\n",
      "271/281, train_loss: 0.0869, step time: 0.2563\n",
      "272/281, train_loss: 0.0516, step time: 0.2556\n",
      "273/281, train_loss: 0.0800, step time: 0.2500\n",
      "274/281, train_loss: 0.0388, step time: 0.2548\n",
      "275/281, train_loss: 0.3716, step time: 0.2529\n",
      "276/281, train_loss: 0.0364, step time: 0.2535\n",
      "277/281, train_loss: 0.0976, step time: 0.2525\n",
      "278/281, train_loss: 0.0647, step time: 0.2556\n",
      "279/281, train_loss: 0.0945, step time: 0.2543\n",
      "280/281, train_loss: 0.0322, step time: 0.2523\n",
      "281/281, train_loss: 0.0621, step time: 0.2482\n",
      "282/281, train_loss: 0.1013, step time: 0.1498\n",
      "epoch 117 average loss: 0.0826\n",
      "saved new best metric model\n",
      "current epoch: 117 current mean dice: 0.9035 tc: 0.8965 wt: 0.9279 et: 0.8956\n",
      "best mean dice: 0.9035 at epoch: 117\n",
      "time consuming of epoch 117 is: 387.6140\n",
      "----------\n",
      "epoch 118/200\n",
      "1/281, train_loss: 0.0281, step time: 0.2581\n",
      "2/281, train_loss: 0.0569, step time: 0.2562\n",
      "3/281, train_loss: 0.0475, step time: 0.2633\n",
      "4/281, train_loss: 0.2315, step time: 0.2573\n",
      "5/281, train_loss: 0.0483, step time: 0.2573\n",
      "6/281, train_loss: 0.0542, step time: 0.2542\n",
      "7/281, train_loss: 0.0458, step time: 0.2521\n",
      "8/281, train_loss: 0.0488, step time: 0.2510\n",
      "9/281, train_loss: 0.2225, step time: 0.2541\n",
      "10/281, train_loss: 0.0487, step time: 0.2569\n",
      "11/281, train_loss: 0.0681, step time: 0.2594\n",
      "12/281, train_loss: 0.0325, step time: 0.2579\n",
      "13/281, train_loss: 0.0515, step time: 0.2548\n",
      "14/281, train_loss: 0.2253, step time: 0.2583\n",
      "15/281, train_loss: 0.0363, step time: 0.2536\n",
      "16/281, train_loss: 0.0615, step time: 0.2602\n",
      "17/281, train_loss: 0.2205, step time: 0.2528\n",
      "18/281, train_loss: 0.0562, step time: 0.2569\n",
      "19/281, train_loss: 0.0517, step time: 0.2580\n",
      "20/281, train_loss: 0.2216, step time: 0.2577\n",
      "21/281, train_loss: 0.0360, step time: 0.2524\n",
      "22/281, train_loss: 0.0696, step time: 0.2563\n",
      "23/281, train_loss: 0.2353, step time: 0.2572\n",
      "24/281, train_loss: 0.0608, step time: 0.2524\n",
      "25/281, train_loss: 0.0690, step time: 0.2556\n",
      "26/281, train_loss: 0.0592, step time: 0.2581\n",
      "27/281, train_loss: 0.2174, step time: 0.2562\n",
      "28/281, train_loss: 0.0903, step time: 0.2575\n",
      "29/281, train_loss: 0.0698, step time: 0.2578\n",
      "30/281, train_loss: 0.0412, step time: 0.2604\n",
      "31/281, train_loss: 0.0320, step time: 0.2552\n",
      "32/281, train_loss: 0.0467, step time: 0.2555\n",
      "33/281, train_loss: 0.2288, step time: 0.2556\n",
      "34/281, train_loss: 0.0594, step time: 0.2553\n",
      "35/281, train_loss: 0.0535, step time: 0.2585\n",
      "36/281, train_loss: 0.0414, step time: 0.2614\n",
      "37/281, train_loss: 0.0826, step time: 0.2595\n",
      "38/281, train_loss: 0.0535, step time: 0.2596\n",
      "39/281, train_loss: 0.0335, step time: 0.2521\n",
      "40/281, train_loss: 0.0529, step time: 0.2511\n",
      "41/281, train_loss: 0.0499, step time: 0.2551\n",
      "42/281, train_loss: 0.0325, step time: 0.2524\n",
      "43/281, train_loss: 0.0820, step time: 0.2505\n",
      "44/281, train_loss: 0.0374, step time: 0.2594\n",
      "45/281, train_loss: 0.0496, step time: 0.2522\n",
      "46/281, train_loss: 0.0712, step time: 0.2570\n",
      "47/281, train_loss: 0.0471, step time: 0.2558\n",
      "48/281, train_loss: 0.0420, step time: 0.2534\n",
      "49/281, train_loss: 0.2105, step time: 0.2545\n",
      "50/281, train_loss: 0.0736, step time: 0.2557\n",
      "51/281, train_loss: 0.0689, step time: 0.2570\n",
      "52/281, train_loss: 0.2214, step time: 0.2596\n",
      "53/281, train_loss: 0.0593, step time: 0.2574\n",
      "54/281, train_loss: 0.2077, step time: 0.2549\n",
      "55/281, train_loss: 0.3663, step time: 0.2565\n",
      "56/281, train_loss: 0.0618, step time: 0.2525\n",
      "57/281, train_loss: 0.2170, step time: 0.2509\n",
      "58/281, train_loss: 0.0677, step time: 0.2537\n",
      "59/281, train_loss: 0.0523, step time: 0.2515\n",
      "60/281, train_loss: 0.0630, step time: 0.2466\n",
      "61/281, train_loss: 0.0430, step time: 0.2511\n",
      "62/281, train_loss: 0.0506, step time: 0.2492\n",
      "63/281, train_loss: 0.2320, step time: 0.2501\n",
      "64/281, train_loss: 0.2082, step time: 0.2494\n",
      "65/281, train_loss: 0.2059, step time: 0.2611\n",
      "66/281, train_loss: 0.0570, step time: 0.2512\n",
      "67/281, train_loss: 0.0771, step time: 0.2488\n",
      "68/281, train_loss: 0.0413, step time: 0.2495\n",
      "69/281, train_loss: 0.0419, step time: 0.2512\n",
      "70/281, train_loss: 0.0709, step time: 0.2501\n",
      "71/281, train_loss: 0.0622, step time: 0.2541\n",
      "72/281, train_loss: 0.0554, step time: 0.2587\n",
      "73/281, train_loss: 0.0676, step time: 0.2575\n",
      "74/281, train_loss: 0.0418, step time: 0.2549\n",
      "75/281, train_loss: 0.0526, step time: 0.2583\n",
      "76/281, train_loss: 0.0760, step time: 0.2547\n",
      "77/281, train_loss: 0.1877, step time: 0.2575\n",
      "78/281, train_loss: 0.0358, step time: 0.2535\n",
      "79/281, train_loss: 0.2066, step time: 0.2550\n",
      "80/281, train_loss: 0.0585, step time: 0.2540\n",
      "81/281, train_loss: 0.0280, step time: 0.2564\n",
      "82/281, train_loss: 0.2129, step time: 0.2559\n",
      "83/281, train_loss: 0.0898, step time: 0.2527\n",
      "84/281, train_loss: 0.0532, step time: 0.2498\n",
      "85/281, train_loss: 0.0521, step time: 0.2579\n",
      "86/281, train_loss: 0.0975, step time: 0.2578\n",
      "87/281, train_loss: 0.2111, step time: 0.2577\n",
      "88/281, train_loss: 0.0503, step time: 0.2577\n",
      "89/281, train_loss: 0.0673, step time: 0.2648\n",
      "90/281, train_loss: 0.0531, step time: 0.2556\n",
      "91/281, train_loss: 0.0839, step time: 0.2575\n",
      "92/281, train_loss: 0.0590, step time: 0.2587\n",
      "93/281, train_loss: 0.0515, step time: 0.2535\n",
      "94/281, train_loss: 0.0545, step time: 0.2575\n",
      "95/281, train_loss: 0.0527, step time: 0.2536\n",
      "96/281, train_loss: 0.1011, step time: 0.2261\n",
      "97/281, train_loss: 0.0640, step time: 0.2530\n",
      "98/281, train_loss: 0.0751, step time: 0.2515\n",
      "99/281, train_loss: 0.0671, step time: 0.2579\n",
      "100/281, train_loss: 0.0634, step time: 0.2547\n",
      "101/281, train_loss: 0.0592, step time: 0.2533\n",
      "102/281, train_loss: 0.2033, step time: 0.2564\n",
      "103/281, train_loss: 0.0709, step time: 0.2530\n",
      "104/281, train_loss: 0.0301, step time: 0.2559\n",
      "105/281, train_loss: 0.0804, step time: 0.2550\n",
      "106/281, train_loss: 0.0608, step time: 0.2545\n",
      "107/281, train_loss: 0.0358, step time: 0.2567\n",
      "108/281, train_loss: 0.0436, step time: 0.2544\n",
      "109/281, train_loss: 0.0624, step time: 0.2514\n",
      "110/281, train_loss: 0.0453, step time: 0.2531\n",
      "111/281, train_loss: 0.0452, step time: 0.2548\n",
      "112/281, train_loss: 0.0438, step time: 0.2549\n",
      "113/281, train_loss: 0.0450, step time: 0.2571\n",
      "114/281, train_loss: 0.0557, step time: 0.2552\n",
      "115/281, train_loss: 0.2227, step time: 0.2535\n",
      "116/281, train_loss: 0.0570, step time: 0.2595\n",
      "117/281, train_loss: 0.0750, step time: 0.2521\n",
      "118/281, train_loss: 0.2486, step time: 0.2671\n",
      "119/281, train_loss: 0.0620, step time: 0.2551\n",
      "120/281, train_loss: 0.0649, step time: 0.2563\n",
      "121/281, train_loss: 0.0501, step time: 0.2542\n",
      "122/281, train_loss: 0.2081, step time: 0.2571\n",
      "123/281, train_loss: 0.2155, step time: 0.2558\n",
      "124/281, train_loss: 0.0633, step time: 0.2561\n",
      "125/281, train_loss: 0.0937, step time: 0.2521\n",
      "126/281, train_loss: 0.0552, step time: 0.2587\n",
      "127/281, train_loss: 0.0517, step time: 0.2536\n",
      "128/281, train_loss: 0.0505, step time: 0.2537\n",
      "129/281, train_loss: 0.0448, step time: 0.2546\n",
      "130/281, train_loss: 0.0541, step time: 0.2505\n",
      "131/281, train_loss: 0.0449, step time: 0.2516\n",
      "132/281, train_loss: 0.0747, step time: 0.2487\n",
      "133/281, train_loss: 0.0782, step time: 0.2492\n",
      "134/281, train_loss: 0.2203, step time: 0.2489\n",
      "135/281, train_loss: 0.0380, step time: 0.2591\n",
      "136/281, train_loss: 0.0711, step time: 0.2593\n",
      "137/281, train_loss: 0.0863, step time: 0.2563\n",
      "138/281, train_loss: 0.0554, step time: 0.2607\n",
      "139/281, train_loss: 0.0351, step time: 0.2565\n",
      "140/281, train_loss: 0.0756, step time: 0.2582\n",
      "141/281, train_loss: 0.0438, step time: 0.2508\n",
      "142/281, train_loss: 0.0809, step time: 0.2497\n",
      "143/281, train_loss: 0.0526, step time: 0.2553\n",
      "144/281, train_loss: 0.0555, step time: 0.2576\n",
      "145/281, train_loss: 0.0594, step time: 0.2559\n",
      "146/281, train_loss: 0.0921, step time: 0.2582\n",
      "147/281, train_loss: 0.0541, step time: 0.2546\n",
      "148/281, train_loss: 0.0439, step time: 0.2573\n",
      "149/281, train_loss: 0.2306, step time: 0.2576\n",
      "150/281, train_loss: 0.0571, step time: 0.2566\n",
      "151/281, train_loss: 0.0663, step time: 0.2508\n",
      "152/281, train_loss: 0.0536, step time: 0.2458\n",
      "153/281, train_loss: 0.0752, step time: 0.2491\n",
      "154/281, train_loss: 0.0744, step time: 0.2457\n",
      "155/281, train_loss: 0.0512, step time: 0.2492\n",
      "156/281, train_loss: 0.0701, step time: 0.2532\n",
      "157/281, train_loss: 0.0984, step time: 0.2503\n",
      "158/281, train_loss: 0.0786, step time: 0.2501\n",
      "159/281, train_loss: 0.0386, step time: 0.2585\n",
      "160/281, train_loss: 0.0642, step time: 0.2499\n",
      "161/281, train_loss: 0.2069, step time: 0.2487\n",
      "162/281, train_loss: 0.0613, step time: 0.2483\n",
      "163/281, train_loss: 0.0692, step time: 0.2533\n",
      "164/281, train_loss: 0.0583, step time: 0.2522\n",
      "165/281, train_loss: 0.0809, step time: 0.2584\n",
      "166/281, train_loss: 0.2193, step time: 0.2607\n",
      "167/281, train_loss: 0.0547, step time: 0.2528\n",
      "168/281, train_loss: 0.0415, step time: 0.2515\n",
      "169/281, train_loss: 0.0417, step time: 0.2496\n",
      "170/281, train_loss: 0.0516, step time: 0.2481\n",
      "171/281, train_loss: 0.0408, step time: 0.2554\n",
      "172/281, train_loss: 0.0372, step time: 0.2557\n",
      "173/281, train_loss: 0.0459, step time: 0.2553\n",
      "174/281, train_loss: 0.0603, step time: 0.2580\n",
      "175/281, train_loss: 0.0767, step time: 0.2514\n",
      "176/281, train_loss: 0.0331, step time: 0.2557\n",
      "177/281, train_loss: 0.0678, step time: 0.2564\n",
      "178/281, train_loss: 0.0594, step time: 0.2538\n",
      "179/281, train_loss: 0.0525, step time: 0.2575\n",
      "180/281, train_loss: 0.0508, step time: 0.2552\n",
      "181/281, train_loss: 0.2257, step time: 0.2528\n",
      "182/281, train_loss: 0.0616, step time: 0.2508\n",
      "183/281, train_loss: 0.0683, step time: 0.2526\n",
      "184/281, train_loss: 0.0776, step time: 0.2521\n",
      "185/281, train_loss: 0.0570, step time: 0.2528\n",
      "186/281, train_loss: 0.0468, step time: 0.2539\n",
      "187/281, train_loss: 0.0542, step time: 0.2562\n",
      "188/281, train_loss: 0.0767, step time: 0.2554\n",
      "189/281, train_loss: 0.0724, step time: 0.2543\n",
      "190/281, train_loss: 0.0749, step time: 0.2508\n",
      "191/281, train_loss: 0.0692, step time: 0.2493\n",
      "192/281, train_loss: 0.0534, step time: 0.2514\n",
      "193/281, train_loss: 0.0281, step time: 0.2476\n",
      "194/281, train_loss: 0.0356, step time: 0.2531\n",
      "195/281, train_loss: 0.0653, step time: 0.2525\n",
      "196/281, train_loss: 0.0839, step time: 0.2507\n",
      "197/281, train_loss: 0.0656, step time: 0.2479\n",
      "198/281, train_loss: 0.0275, step time: 0.2455\n",
      "199/281, train_loss: 0.0385, step time: 0.2509\n",
      "200/281, train_loss: 0.0784, step time: 0.2515\n",
      "201/281, train_loss: 0.0369, step time: 0.2523\n",
      "202/281, train_loss: 0.1132, step time: 0.2492\n",
      "203/281, train_loss: 0.0512, step time: 0.2482\n",
      "204/281, train_loss: 0.0475, step time: 0.2491\n",
      "205/281, train_loss: 0.0776, step time: 0.2531\n",
      "206/281, train_loss: 0.0381, step time: 0.2502\n",
      "207/281, train_loss: 0.0277, step time: 0.2503\n",
      "208/281, train_loss: 0.0626, step time: 0.2502\n",
      "209/281, train_loss: 0.0428, step time: 0.2532\n",
      "210/281, train_loss: 0.0497, step time: 0.2505\n",
      "211/281, train_loss: 0.0622, step time: 0.2546\n",
      "212/281, train_loss: 0.0622, step time: 0.2569\n",
      "213/281, train_loss: 0.0485, step time: 0.2515\n",
      "214/281, train_loss: 0.0539, step time: 0.2515\n",
      "215/281, train_loss: 0.0727, step time: 0.2538\n",
      "216/281, train_loss: 0.2297, step time: 0.2532\n",
      "217/281, train_loss: 0.0811, step time: 0.2559\n",
      "218/281, train_loss: 0.0512, step time: 0.2483\n",
      "219/281, train_loss: 0.0481, step time: 0.2566\n",
      "220/281, train_loss: 0.0386, step time: 0.2511\n",
      "221/281, train_loss: 0.2207, step time: 0.2489\n",
      "222/281, train_loss: 0.0448, step time: 0.2470\n",
      "223/281, train_loss: 0.2311, step time: 0.2505\n",
      "224/281, train_loss: 0.0575, step time: 0.2506\n",
      "225/281, train_loss: 0.0466, step time: 0.2525\n",
      "226/281, train_loss: 0.0429, step time: 0.2487\n",
      "227/281, train_loss: 0.0592, step time: 0.2488\n",
      "228/281, train_loss: 0.0621, step time: 0.2529\n",
      "229/281, train_loss: 0.0498, step time: 0.2515\n",
      "230/281, train_loss: 0.0623, step time: 0.2507\n",
      "231/281, train_loss: 0.0775, step time: 0.2497\n",
      "232/281, train_loss: 0.0607, step time: 0.2511\n",
      "233/281, train_loss: 0.0581, step time: 0.2521\n",
      "234/281, train_loss: 0.0937, step time: 0.2483\n",
      "235/281, train_loss: 0.0841, step time: 0.2565\n",
      "236/281, train_loss: 0.0263, step time: 0.2562\n",
      "237/281, train_loss: 0.0561, step time: 0.2532\n",
      "238/281, train_loss: 0.0851, step time: 0.2529\n",
      "239/281, train_loss: 0.0619, step time: 0.2607\n",
      "240/281, train_loss: 0.2014, step time: 0.2748\n",
      "241/281, train_loss: 0.0822, step time: 0.2510\n",
      "242/281, train_loss: 0.0785, step time: 0.2453\n",
      "243/281, train_loss: 0.0732, step time: 0.2501\n",
      "244/281, train_loss: 0.0442, step time: 0.2553\n",
      "245/281, train_loss: 0.0575, step time: 0.2482\n",
      "246/281, train_loss: 0.0638, step time: 0.2469\n",
      "247/281, train_loss: 0.0707, step time: 0.2519\n",
      "248/281, train_loss: 0.0437, step time: 0.2559\n",
      "249/281, train_loss: 0.0780, step time: 0.2454\n",
      "250/281, train_loss: 0.0528, step time: 0.2513\n",
      "251/281, train_loss: 0.0492, step time: 0.2522\n",
      "252/281, train_loss: 0.0474, step time: 0.2494\n",
      "253/281, train_loss: 0.0653, step time: 0.2550\n",
      "254/281, train_loss: 0.0627, step time: 0.2545\n",
      "255/281, train_loss: 0.2205, step time: 0.2530\n",
      "256/281, train_loss: 0.0467, step time: 0.2556\n",
      "257/281, train_loss: 0.0818, step time: 0.2563\n",
      "258/281, train_loss: 0.2086, step time: 0.2605\n",
      "259/281, train_loss: 0.0568, step time: 0.2497\n",
      "260/281, train_loss: 0.0645, step time: 0.2526\n",
      "261/281, train_loss: 0.0587, step time: 0.2561\n",
      "262/281, train_loss: 0.0536, step time: 0.2569\n",
      "263/281, train_loss: 0.0590, step time: 0.2504\n",
      "264/281, train_loss: 0.2193, step time: 0.2527\n",
      "265/281, train_loss: 0.2387, step time: 0.2495\n",
      "266/281, train_loss: 0.0353, step time: 0.2535\n",
      "267/281, train_loss: 0.0391, step time: 0.2558\n",
      "268/281, train_loss: 0.0635, step time: 0.2565\n",
      "269/281, train_loss: 0.0522, step time: 0.2511\n",
      "270/281, train_loss: 0.1954, step time: 0.2534\n",
      "271/281, train_loss: 0.2155, step time: 0.2473\n",
      "272/281, train_loss: 0.1997, step time: 0.2496\n",
      "273/281, train_loss: 0.0557, step time: 0.2489\n",
      "274/281, train_loss: 0.0359, step time: 0.2670\n",
      "275/281, train_loss: 0.0949, step time: 0.2468\n",
      "276/281, train_loss: 0.0468, step time: 0.2541\n",
      "277/281, train_loss: 0.0590, step time: 0.2563\n",
      "278/281, train_loss: 0.0771, step time: 0.2515\n",
      "279/281, train_loss: 0.0784, step time: 0.2445\n",
      "280/281, train_loss: 0.0472, step time: 0.2493\n",
      "281/281, train_loss: 0.0610, step time: 0.2452\n",
      "282/281, train_loss: 0.0430, step time: 0.1450\n",
      "epoch 118 average loss: 0.0818\n",
      "saved new best metric model\n",
      "current epoch: 118 current mean dice: 0.9048 tc: 0.8977 wt: 0.9306 et: 0.8950\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 118 is: 431.6266\n",
      "----------\n",
      "epoch 119/200\n",
      "1/281, train_loss: 0.2180, step time: 0.2555\n",
      "2/281, train_loss: 0.0486, step time: 0.2530\n",
      "3/281, train_loss: 0.0484, step time: 0.2479\n",
      "4/281, train_loss: 0.0629, step time: 0.2637\n",
      "5/281, train_loss: 0.0405, step time: 0.2451\n",
      "6/281, train_loss: 0.0574, step time: 0.2461\n",
      "7/281, train_loss: 0.1010, step time: 0.2499\n",
      "8/281, train_loss: 0.0632, step time: 0.2532\n",
      "9/281, train_loss: 0.0563, step time: 0.2482\n",
      "10/281, train_loss: 0.0508, step time: 0.2433\n",
      "11/281, train_loss: 0.2047, step time: 0.2442\n",
      "12/281, train_loss: 0.0786, step time: 0.2497\n",
      "13/281, train_loss: 0.0452, step time: 0.2514\n",
      "14/281, train_loss: 0.0391, step time: 0.2557\n",
      "15/281, train_loss: 0.2116, step time: 0.2490\n",
      "16/281, train_loss: 0.0399, step time: 0.2529\n",
      "17/281, train_loss: 0.0710, step time: 0.2500\n",
      "18/281, train_loss: 0.0578, step time: 0.2515\n",
      "19/281, train_loss: 0.0672, step time: 0.2527\n",
      "20/281, train_loss: 0.0695, step time: 0.2521\n",
      "21/281, train_loss: 0.0727, step time: 0.2456\n",
      "22/281, train_loss: 0.1030, step time: 0.2466\n",
      "23/281, train_loss: 0.0587, step time: 0.2452\n",
      "24/281, train_loss: 0.2135, step time: 0.2488\n",
      "25/281, train_loss: 0.0636, step time: 0.2539\n",
      "26/281, train_loss: 0.0400, step time: 0.2493\n",
      "27/281, train_loss: 0.0755, step time: 0.2518\n",
      "28/281, train_loss: 0.2350, step time: 0.2575\n",
      "29/281, train_loss: 0.0586, step time: 0.2496\n",
      "30/281, train_loss: 0.0654, step time: 0.2484\n",
      "31/281, train_loss: 0.0420, step time: 0.2478\n",
      "32/281, train_loss: 0.0478, step time: 0.2439\n",
      "33/281, train_loss: 0.2097, step time: 0.2526\n",
      "34/281, train_loss: 0.0687, step time: 0.2494\n",
      "35/281, train_loss: 0.0237, step time: 0.2527\n",
      "36/281, train_loss: 0.0588, step time: 0.2541\n",
      "37/281, train_loss: 0.2366, step time: 0.2498\n",
      "38/281, train_loss: 0.0526, step time: 0.2537\n",
      "39/281, train_loss: 0.0533, step time: 0.2532\n",
      "40/281, train_loss: 0.0627, step time: 0.2481\n",
      "41/281, train_loss: 0.0768, step time: 0.2487\n",
      "42/281, train_loss: 0.0543, step time: 0.2457\n",
      "43/281, train_loss: 0.0748, step time: 0.2433\n",
      "44/281, train_loss: 0.0494, step time: 0.2552\n",
      "45/281, train_loss: 0.2094, step time: 0.2545\n",
      "46/281, train_loss: 0.0485, step time: 0.2618\n",
      "47/281, train_loss: 0.0785, step time: 0.2651\n",
      "48/281, train_loss: 0.0497, step time: 0.2452\n",
      "49/281, train_loss: 0.0435, step time: 0.2456\n",
      "50/281, train_loss: 0.0609, step time: 0.2532\n",
      "51/281, train_loss: 0.0806, step time: 0.2571\n",
      "52/281, train_loss: 0.0362, step time: 0.2492\n",
      "53/281, train_loss: 0.0587, step time: 0.2492\n",
      "54/281, train_loss: 0.0482, step time: 0.2413\n",
      "55/281, train_loss: 0.2302, step time: 0.2466\n",
      "56/281, train_loss: 0.0633, step time: 0.2464\n",
      "57/281, train_loss: 0.0507, step time: 0.2462\n",
      "58/281, train_loss: 0.0567, step time: 0.2449\n",
      "59/281, train_loss: 0.0626, step time: 0.2483\n",
      "60/281, train_loss: 0.0708, step time: 0.2459\n",
      "61/281, train_loss: 0.0605, step time: 0.2428\n",
      "62/281, train_loss: 0.0471, step time: 0.2626\n",
      "63/281, train_loss: 0.0524, step time: 0.2442\n",
      "64/281, train_loss: 0.0817, step time: 0.2435\n",
      "65/281, train_loss: 0.0724, step time: 0.2438\n",
      "66/281, train_loss: 0.0605, step time: 0.2455\n",
      "67/281, train_loss: 0.0537, step time: 0.2504\n",
      "68/281, train_loss: 0.0423, step time: 0.2470\n",
      "69/281, train_loss: 0.0720, step time: 0.2410\n",
      "70/281, train_loss: 0.0513, step time: 0.2493\n",
      "71/281, train_loss: 0.0414, step time: 0.2489\n",
      "72/281, train_loss: 0.0529, step time: 0.2457\n",
      "73/281, train_loss: 0.0453, step time: 0.2515\n",
      "74/281, train_loss: 0.0780, step time: 0.2538\n",
      "75/281, train_loss: 0.0652, step time: 0.2571\n",
      "76/281, train_loss: 0.0574, step time: 0.2552\n",
      "77/281, train_loss: 0.0624, step time: 0.2589\n",
      "78/281, train_loss: 0.0984, step time: 0.2525\n",
      "79/281, train_loss: 0.0659, step time: 0.2464\n",
      "80/281, train_loss: 0.0586, step time: 0.2480\n",
      "81/281, train_loss: 0.0400, step time: 0.2509\n",
      "82/281, train_loss: 0.0562, step time: 0.2480\n",
      "83/281, train_loss: 0.0650, step time: 0.2517\n",
      "84/281, train_loss: 0.0769, step time: 0.2558\n",
      "85/281, train_loss: 0.0481, step time: 0.2667\n",
      "86/281, train_loss: 0.0563, step time: 0.2666\n",
      "87/281, train_loss: 0.0664, step time: 0.2585\n",
      "88/281, train_loss: 0.0549, step time: 0.2531\n",
      "89/281, train_loss: 0.0467, step time: 0.2469\n",
      "90/281, train_loss: 0.2361, step time: 0.2497\n",
      "91/281, train_loss: 0.0634, step time: 0.2506\n",
      "92/281, train_loss: 0.0824, step time: 0.2478\n",
      "93/281, train_loss: 0.0533, step time: 0.2512\n",
      "94/281, train_loss: 0.0774, step time: 0.2488\n",
      "95/281, train_loss: 0.0580, step time: 0.2465\n",
      "96/281, train_loss: 0.0766, step time: 0.2553\n",
      "97/281, train_loss: 0.0442, step time: 0.2522\n",
      "98/281, train_loss: 0.0660, step time: 0.2522\n",
      "99/281, train_loss: 0.0556, step time: 0.2471\n",
      "100/281, train_loss: 0.0599, step time: 0.2568\n",
      "101/281, train_loss: 0.0694, step time: 0.2506\n",
      "102/281, train_loss: 0.0666, step time: 0.2541\n",
      "103/281, train_loss: 0.0519, step time: 0.2491\n",
      "104/281, train_loss: 0.0584, step time: 0.2487\n",
      "105/281, train_loss: 0.0608, step time: 0.2511\n",
      "106/281, train_loss: 0.0209, step time: 0.2508\n",
      "107/281, train_loss: 0.0648, step time: 0.2533\n",
      "108/281, train_loss: 0.0639, step time: 0.2479\n",
      "109/281, train_loss: 0.0619, step time: 0.2499\n",
      "110/281, train_loss: 0.0679, step time: 0.2447\n",
      "111/281, train_loss: 0.0517, step time: 0.2546\n",
      "112/281, train_loss: 0.3833, step time: 0.2558\n",
      "113/281, train_loss: 0.0450, step time: 0.2485\n",
      "114/281, train_loss: 0.0378, step time: 0.2475\n",
      "115/281, train_loss: 0.0920, step time: 0.2500\n",
      "116/281, train_loss: 0.2346, step time: 0.2532\n",
      "117/281, train_loss: 0.0292, step time: 0.2559\n",
      "118/281, train_loss: 0.0526, step time: 0.2509\n",
      "119/281, train_loss: 0.2135, step time: 0.2485\n",
      "120/281, train_loss: 0.0590, step time: 0.2563\n",
      "121/281, train_loss: 0.0744, step time: 0.2484\n",
      "122/281, train_loss: 0.0518, step time: 0.2506\n",
      "123/281, train_loss: 0.2103, step time: 0.2471\n",
      "124/281, train_loss: 0.0790, step time: 0.2492\n",
      "125/281, train_loss: 0.1224, step time: 0.2511\n",
      "126/281, train_loss: 0.0688, step time: 0.2476\n",
      "127/281, train_loss: 0.0619, step time: 0.2421\n",
      "128/281, train_loss: 0.0448, step time: 0.2505\n",
      "129/281, train_loss: 0.0558, step time: 0.2481\n",
      "130/281, train_loss: 0.0410, step time: 0.2460\n",
      "131/281, train_loss: 0.0701, step time: 0.2460\n",
      "132/281, train_loss: 0.2502, step time: 0.2475\n",
      "133/281, train_loss: 0.0781, step time: 0.2462\n",
      "134/281, train_loss: 0.2014, step time: 0.2519\n",
      "135/281, train_loss: 0.0695, step time: 0.2541\n",
      "136/281, train_loss: 0.0328, step time: 0.2493\n",
      "137/281, train_loss: 0.0650, step time: 0.2511\n",
      "138/281, train_loss: 0.0645, step time: 0.2479\n",
      "139/281, train_loss: 0.0555, step time: 0.2494\n",
      "140/281, train_loss: 0.0549, step time: 0.2511\n",
      "141/281, train_loss: 0.0589, step time: 0.2443\n",
      "142/281, train_loss: 0.0575, step time: 0.2469\n",
      "143/281, train_loss: 0.0502, step time: 0.2506\n",
      "144/281, train_loss: 0.0668, step time: 0.2448\n",
      "145/281, train_loss: 0.0724, step time: 0.2432\n",
      "146/281, train_loss: 0.0194, step time: 0.2493\n",
      "147/281, train_loss: 0.2444, step time: 0.2499\n",
      "148/281, train_loss: 0.0529, step time: 0.2535\n",
      "149/281, train_loss: 0.0374, step time: 0.2557\n",
      "150/281, train_loss: 0.0680, step time: 0.2556\n",
      "151/281, train_loss: 0.0482, step time: 0.2491\n",
      "152/281, train_loss: 0.0577, step time: 0.2479\n",
      "153/281, train_loss: 0.0554, step time: 0.2502\n",
      "154/281, train_loss: 0.0684, step time: 0.2449\n",
      "155/281, train_loss: 0.0402, step time: 0.2467\n",
      "156/281, train_loss: 0.0320, step time: 0.2527\n",
      "157/281, train_loss: 0.0643, step time: 0.2489\n",
      "158/281, train_loss: 0.0428, step time: 0.2498\n",
      "159/281, train_loss: 0.0463, step time: 0.2466\n",
      "160/281, train_loss: 0.0270, step time: 0.2468\n",
      "161/281, train_loss: 0.0701, step time: 0.2450\n",
      "162/281, train_loss: 0.0705, step time: 0.2499\n",
      "163/281, train_loss: 0.2152, step time: 0.2502\n",
      "164/281, train_loss: 0.0717, step time: 0.2472\n",
      "165/281, train_loss: 0.0470, step time: 0.2501\n",
      "166/281, train_loss: 0.0602, step time: 0.2466\n",
      "167/281, train_loss: 0.0432, step time: 0.2525\n",
      "168/281, train_loss: 0.0523, step time: 0.2506\n",
      "169/281, train_loss: 0.3735, step time: 0.2477\n",
      "170/281, train_loss: 0.0449, step time: 0.2458\n",
      "171/281, train_loss: 0.0494, step time: 0.2518\n",
      "172/281, train_loss: 0.2006, step time: 0.2533\n",
      "173/281, train_loss: 0.0445, step time: 0.2505\n",
      "174/281, train_loss: 0.0675, step time: 0.2473\n",
      "175/281, train_loss: 0.0780, step time: 0.2494\n",
      "176/281, train_loss: 0.0683, step time: 0.2525\n",
      "177/281, train_loss: 0.0445, step time: 0.2440\n",
      "178/281, train_loss: 0.0611, step time: 0.2437\n",
      "179/281, train_loss: 0.0377, step time: 0.2441\n",
      "180/281, train_loss: 0.0344, step time: 0.2483\n",
      "181/281, train_loss: 0.0726, step time: 0.2501\n",
      "182/281, train_loss: 0.0466, step time: 0.2481\n",
      "183/281, train_loss: 0.0529, step time: 0.2479\n",
      "184/281, train_loss: 0.1847, step time: 0.2221\n",
      "185/281, train_loss: 0.0514, step time: 0.2499\n",
      "186/281, train_loss: 0.0668, step time: 0.2537\n",
      "187/281, train_loss: 0.2194, step time: 0.2500\n",
      "188/281, train_loss: 0.0785, step time: 0.2493\n",
      "189/281, train_loss: 0.0439, step time: 0.2512\n",
      "190/281, train_loss: 0.0410, step time: 0.2462\n",
      "191/281, train_loss: 0.0815, step time: 0.2482\n",
      "192/281, train_loss: 0.2423, step time: 0.2537\n",
      "193/281, train_loss: 0.0673, step time: 0.2487\n",
      "194/281, train_loss: 0.0574, step time: 0.2492\n",
      "195/281, train_loss: 0.0698, step time: 0.2476\n",
      "196/281, train_loss: 0.2184, step time: 0.2533\n",
      "197/281, train_loss: 0.0572, step time: 0.2515\n",
      "198/281, train_loss: 0.0573, step time: 0.2529\n",
      "199/281, train_loss: 0.0706, step time: 0.2474\n",
      "200/281, train_loss: 0.0751, step time: 0.2475\n",
      "201/281, train_loss: 0.0483, step time: 0.2534\n",
      "202/281, train_loss: 0.0685, step time: 0.2510\n",
      "203/281, train_loss: 0.0408, step time: 0.2520\n",
      "204/281, train_loss: 0.0574, step time: 0.2530\n",
      "205/281, train_loss: 0.0631, step time: 0.2548\n",
      "206/281, train_loss: 0.0574, step time: 0.2487\n",
      "207/281, train_loss: 0.2058, step time: 0.2533\n",
      "208/281, train_loss: 0.0597, step time: 0.2563\n",
      "209/281, train_loss: 0.0462, step time: 0.2494\n",
      "210/281, train_loss: 0.0591, step time: 0.2470\n",
      "211/281, train_loss: 0.0482, step time: 0.2436\n",
      "212/281, train_loss: 0.0484, step time: 0.2588\n",
      "213/281, train_loss: 0.2106, step time: 0.2543\n",
      "214/281, train_loss: 0.0369, step time: 0.2513\n",
      "215/281, train_loss: 0.0455, step time: 0.2485\n",
      "216/281, train_loss: 0.0677, step time: 0.2561\n",
      "217/281, train_loss: 0.0734, step time: 0.2482\n",
      "218/281, train_loss: 0.0649, step time: 0.2454\n",
      "219/281, train_loss: 0.2174, step time: 0.2492\n",
      "220/281, train_loss: 0.2115, step time: 0.2509\n",
      "221/281, train_loss: 0.0326, step time: 0.2506\n",
      "222/281, train_loss: 0.0616, step time: 0.2586\n",
      "223/281, train_loss: 0.0301, step time: 0.2465\n",
      "224/281, train_loss: 0.0590, step time: 0.2539\n",
      "225/281, train_loss: 0.0670, step time: 0.2498\n",
      "226/281, train_loss: 0.0378, step time: 0.2524\n",
      "227/281, train_loss: 0.0431, step time: 0.2502\n",
      "228/281, train_loss: 0.0498, step time: 0.2511\n",
      "229/281, train_loss: 0.2247, step time: 0.2480\n",
      "230/281, train_loss: 0.0556, step time: 0.2514\n",
      "231/281, train_loss: 0.0500, step time: 0.2545\n",
      "232/281, train_loss: 0.0599, step time: 0.2547\n",
      "233/281, train_loss: 0.0484, step time: 0.2528\n",
      "234/281, train_loss: 0.0483, step time: 0.2506\n",
      "235/281, train_loss: 0.3742, step time: 0.2512\n",
      "236/281, train_loss: 0.0441, step time: 0.2515\n",
      "237/281, train_loss: 0.0280, step time: 0.2487\n",
      "238/281, train_loss: 0.2434, step time: 0.2687\n",
      "239/281, train_loss: 0.0422, step time: 0.2556\n",
      "240/281, train_loss: 0.0725, step time: 0.2493\n",
      "241/281, train_loss: 0.0376, step time: 0.2512\n",
      "242/281, train_loss: 0.2169, step time: 0.2605\n",
      "243/281, train_loss: 0.0453, step time: 0.2520\n",
      "244/281, train_loss: 0.0348, step time: 0.2536\n",
      "245/281, train_loss: 0.0591, step time: 0.2516\n",
      "246/281, train_loss: 0.2119, step time: 0.2521\n",
      "247/281, train_loss: 0.0523, step time: 0.2485\n",
      "248/281, train_loss: 0.0619, step time: 0.2566\n",
      "249/281, train_loss: 0.0448, step time: 0.2616\n",
      "250/281, train_loss: 0.2218, step time: 0.2516\n",
      "251/281, train_loss: 0.0663, step time: 0.2469\n",
      "252/281, train_loss: 0.0661, step time: 0.2597\n",
      "253/281, train_loss: 0.2159, step time: 0.2573\n",
      "254/281, train_loss: 0.0485, step time: 0.2549\n",
      "255/281, train_loss: 0.2333, step time: 0.2471\n",
      "256/281, train_loss: 0.2193, step time: 0.2576\n",
      "257/281, train_loss: 0.0564, step time: 0.2541\n",
      "258/281, train_loss: 0.0882, step time: 0.2522\n",
      "259/281, train_loss: 0.0662, step time: 0.2517\n",
      "260/281, train_loss: 0.2192, step time: 0.2622\n",
      "261/281, train_loss: 0.0562, step time: 0.2542\n",
      "262/281, train_loss: 0.0483, step time: 0.2567\n",
      "263/281, train_loss: 0.0460, step time: 0.2491\n",
      "264/281, train_loss: 0.0748, step time: 0.2549\n",
      "265/281, train_loss: 0.2319, step time: 0.2567\n",
      "266/281, train_loss: 0.0384, step time: 0.2509\n",
      "267/281, train_loss: 0.0455, step time: 0.2513\n",
      "268/281, train_loss: 0.0552, step time: 0.2512\n",
      "269/281, train_loss: 0.0538, step time: 0.2531\n",
      "270/281, train_loss: 0.0486, step time: 0.2576\n",
      "271/281, train_loss: 0.0592, step time: 0.2522\n",
      "272/281, train_loss: 0.0959, step time: 0.2530\n",
      "273/281, train_loss: 0.0637, step time: 0.2543\n",
      "274/281, train_loss: 0.2157, step time: 0.2545\n",
      "275/281, train_loss: 0.0713, step time: 0.2549\n",
      "276/281, train_loss: 0.0518, step time: 0.2453\n",
      "277/281, train_loss: 0.0642, step time: 0.2516\n",
      "278/281, train_loss: 0.0507, step time: 0.2512\n",
      "279/281, train_loss: 0.0517, step time: 0.2509\n",
      "280/281, train_loss: 0.0392, step time: 0.2490\n",
      "281/281, train_loss: 0.0424, step time: 0.2485\n",
      "282/281, train_loss: 0.0436, step time: 0.1471\n",
      "epoch 119 average loss: 0.0820\n",
      "current epoch: 119 current mean dice: 0.9044 tc: 0.8971 wt: 0.9292 et: 0.8964\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 119 is: 397.2777\n",
      "----------\n",
      "epoch 120/200\n",
      "1/281, train_loss: 0.0523, step time: 0.2582\n",
      "2/281, train_loss: 0.0621, step time: 0.2554\n",
      "3/281, train_loss: 0.0677, step time: 0.2565\n",
      "4/281, train_loss: 0.0561, step time: 0.2547\n",
      "5/281, train_loss: 0.0601, step time: 0.2513\n",
      "6/281, train_loss: 0.0680, step time: 0.2503\n",
      "7/281, train_loss: 0.0587, step time: 0.2551\n",
      "8/281, train_loss: 0.0440, step time: 0.2527\n",
      "9/281, train_loss: 0.0494, step time: 0.2520\n",
      "10/281, train_loss: 0.0614, step time: 0.2559\n",
      "11/281, train_loss: 0.0875, step time: 0.2564\n",
      "12/281, train_loss: 0.0583, step time: 0.2749\n",
      "13/281, train_loss: 0.2068, step time: 0.2518\n",
      "14/281, train_loss: 0.0537, step time: 0.2579\n",
      "15/281, train_loss: 0.0663, step time: 0.2567\n",
      "16/281, train_loss: 0.0715, step time: 0.2571\n",
      "17/281, train_loss: 0.0704, step time: 0.2505\n",
      "18/281, train_loss: 0.0687, step time: 0.2528\n",
      "19/281, train_loss: 0.0642, step time: 0.2516\n",
      "20/281, train_loss: 0.0449, step time: 0.2496\n",
      "21/281, train_loss: 0.2289, step time: 0.2533\n",
      "22/281, train_loss: 0.0533, step time: 0.2544\n",
      "23/281, train_loss: 0.0727, step time: 0.2540\n",
      "24/281, train_loss: 0.0787, step time: 0.2496\n",
      "25/281, train_loss: 0.2079, step time: 0.2506\n",
      "26/281, train_loss: 0.0450, step time: 0.2572\n",
      "27/281, train_loss: 0.0719, step time: 0.2625\n",
      "28/281, train_loss: 0.0392, step time: 0.2590\n",
      "29/281, train_loss: 0.0541, step time: 0.2614\n",
      "30/281, train_loss: 0.0712, step time: 0.2526\n",
      "31/281, train_loss: 0.2156, step time: 0.2613\n",
      "32/281, train_loss: 0.0896, step time: 0.2595\n",
      "33/281, train_loss: 0.0694, step time: 0.2587\n",
      "34/281, train_loss: 0.2024, step time: 0.2562\n",
      "35/281, train_loss: 0.0599, step time: 0.2510\n",
      "36/281, train_loss: 0.0688, step time: 0.2516\n",
      "37/281, train_loss: 0.0773, step time: 0.2592\n",
      "38/281, train_loss: 0.0938, step time: 0.2540\n",
      "39/281, train_loss: 0.0484, step time: 0.2518\n",
      "40/281, train_loss: 0.2330, step time: 0.2545\n",
      "41/281, train_loss: 0.2096, step time: 0.2536\n",
      "42/281, train_loss: 0.2245, step time: 0.2585\n",
      "43/281, train_loss: 0.0404, step time: 0.2590\n",
      "44/281, train_loss: 0.0223, step time: 0.2558\n",
      "45/281, train_loss: 0.0538, step time: 0.2528\n",
      "46/281, train_loss: 0.2248, step time: 0.2598\n",
      "47/281, train_loss: 0.0645, step time: 0.2582\n",
      "48/281, train_loss: 0.0544, step time: 0.2626\n",
      "49/281, train_loss: 0.0635, step time: 0.2536\n",
      "50/281, train_loss: 0.0879, step time: 0.2570\n",
      "51/281, train_loss: 0.0562, step time: 0.2575\n",
      "52/281, train_loss: 0.0467, step time: 0.2575\n",
      "53/281, train_loss: 0.0658, step time: 0.2643\n",
      "54/281, train_loss: 0.0452, step time: 0.2539\n",
      "55/281, train_loss: 0.0656, step time: 0.2535\n",
      "56/281, train_loss: 0.0477, step time: 0.2523\n",
      "57/281, train_loss: 0.0582, step time: 0.2534\n",
      "58/281, train_loss: 0.0675, step time: 0.2517\n",
      "59/281, train_loss: 0.0690, step time: 0.2549\n",
      "60/281, train_loss: 0.2025, step time: 0.2631\n",
      "61/281, train_loss: 0.0574, step time: 0.2555\n",
      "62/281, train_loss: 0.0418, step time: 0.2572\n",
      "63/281, train_loss: 0.0539, step time: 0.2509\n",
      "64/281, train_loss: 0.0348, step time: 0.2669\n",
      "65/281, train_loss: 0.2074, step time: 0.2503\n",
      "66/281, train_loss: 0.0566, step time: 0.2506\n",
      "67/281, train_loss: 0.0502, step time: 0.2508\n",
      "68/281, train_loss: 0.0292, step time: 0.2514\n",
      "69/281, train_loss: 0.0700, step time: 0.2523\n",
      "70/281, train_loss: 0.0505, step time: 0.2589\n",
      "71/281, train_loss: 0.0342, step time: 0.2593\n",
      "72/281, train_loss: 0.0428, step time: 0.2594\n",
      "73/281, train_loss: 0.0701, step time: 0.2537\n",
      "74/281, train_loss: 0.2055, step time: 0.2571\n",
      "75/281, train_loss: 0.0695, step time: 0.2566\n",
      "76/281, train_loss: 0.2251, step time: 0.2582\n",
      "77/281, train_loss: 0.0591, step time: 0.2509\n",
      "78/281, train_loss: 0.0651, step time: 0.2537\n",
      "79/281, train_loss: 0.0653, step time: 0.2550\n",
      "80/281, train_loss: 0.0635, step time: 0.2525\n",
      "81/281, train_loss: 0.0419, step time: 0.2614\n",
      "82/281, train_loss: 0.0684, step time: 0.2445\n",
      "83/281, train_loss: 0.0675, step time: 0.2467\n",
      "84/281, train_loss: 0.0469, step time: 0.2466\n",
      "85/281, train_loss: 0.0475, step time: 0.2521\n",
      "86/281, train_loss: 0.0281, step time: 0.2528\n",
      "87/281, train_loss: 0.2245, step time: 0.2524\n",
      "88/281, train_loss: 0.0507, step time: 0.2470\n",
      "89/281, train_loss: 0.0487, step time: 0.2505\n",
      "90/281, train_loss: 0.0416, step time: 0.2539\n",
      "91/281, train_loss: 0.1000, step time: 0.2556\n",
      "92/281, train_loss: 0.0687, step time: 0.2723\n",
      "93/281, train_loss: 0.0512, step time: 0.2589\n",
      "94/281, train_loss: 0.0387, step time: 0.2533\n",
      "95/281, train_loss: 0.0532, step time: 0.2565\n",
      "96/281, train_loss: 0.0615, step time: 0.2527\n",
      "97/281, train_loss: 0.0571, step time: 0.2601\n",
      "98/281, train_loss: 0.0446, step time: 0.2523\n",
      "99/281, train_loss: 0.0259, step time: 0.2543\n",
      "100/281, train_loss: 0.0510, step time: 0.2538\n",
      "101/281, train_loss: 0.0638, step time: 0.2600\n",
      "102/281, train_loss: 0.2379, step time: 0.2617\n",
      "103/281, train_loss: 0.0469, step time: 0.2590\n",
      "104/281, train_loss: 0.0695, step time: 0.2578\n",
      "105/281, train_loss: 0.0670, step time: 0.2593\n",
      "106/281, train_loss: 0.0779, step time: 0.2538\n",
      "107/281, train_loss: 0.0349, step time: 0.2544\n",
      "108/281, train_loss: 0.0432, step time: 0.2516\n",
      "109/281, train_loss: 0.0385, step time: 0.2537\n",
      "110/281, train_loss: 0.0498, step time: 0.2616\n",
      "111/281, train_loss: 0.0360, step time: 0.2586\n",
      "112/281, train_loss: 0.0549, step time: 0.2547\n",
      "113/281, train_loss: 0.1548, step time: 0.2254\n",
      "114/281, train_loss: 0.2267, step time: 0.2575\n",
      "115/281, train_loss: 0.2110, step time: 0.2475\n",
      "116/281, train_loss: 0.0461, step time: 0.2506\n",
      "117/281, train_loss: 0.1996, step time: 0.2513\n",
      "118/281, train_loss: 0.0424, step time: 0.2547\n",
      "119/281, train_loss: 0.2148, step time: 0.2559\n",
      "120/281, train_loss: 0.0530, step time: 0.2543\n",
      "121/281, train_loss: 0.0356, step time: 0.2529\n",
      "122/281, train_loss: 0.0448, step time: 0.2548\n",
      "123/281, train_loss: 0.0518, step time: 0.2585\n",
      "124/281, train_loss: 0.0378, step time: 0.2570\n",
      "125/281, train_loss: 0.0562, step time: 0.2560\n",
      "126/281, train_loss: 0.0568, step time: 0.2588\n",
      "127/281, train_loss: 0.0688, step time: 0.2561\n",
      "128/281, train_loss: 0.0605, step time: 0.2561\n",
      "129/281, train_loss: 0.0385, step time: 0.2481\n",
      "130/281, train_loss: 0.0412, step time: 0.2545\n",
      "131/281, train_loss: 0.0711, step time: 0.2553\n",
      "132/281, train_loss: 0.2140, step time: 0.2522\n",
      "133/281, train_loss: 0.0464, step time: 0.2557\n",
      "134/281, train_loss: 0.0374, step time: 0.2620\n",
      "135/281, train_loss: 0.0287, step time: 0.2617\n",
      "136/281, train_loss: 0.0664, step time: 0.2532\n",
      "137/281, train_loss: 0.1604, step time: 0.2279\n",
      "138/281, train_loss: 0.0703, step time: 0.2588\n",
      "139/281, train_loss: 0.2438, step time: 0.2545\n",
      "140/281, train_loss: 0.0600, step time: 0.2548\n",
      "141/281, train_loss: 0.0720, step time: 0.2560\n",
      "142/281, train_loss: 0.0467, step time: 0.2588\n",
      "143/281, train_loss: 0.0787, step time: 0.2516\n",
      "144/281, train_loss: 0.0252, step time: 0.2480\n",
      "145/281, train_loss: 0.2280, step time: 0.2561\n",
      "146/281, train_loss: 0.0847, step time: 0.2545\n",
      "147/281, train_loss: 0.0574, step time: 0.2542\n",
      "148/281, train_loss: 0.0825, step time: 0.2543\n",
      "149/281, train_loss: 0.1009, step time: 0.2526\n",
      "150/281, train_loss: 0.0460, step time: 0.2507\n",
      "151/281, train_loss: 0.0704, step time: 0.2552\n",
      "152/281, train_loss: 0.0555, step time: 0.2558\n",
      "153/281, train_loss: 0.0411, step time: 0.2550\n",
      "154/281, train_loss: 0.1981, step time: 0.2565\n",
      "155/281, train_loss: 0.0870, step time: 0.2509\n",
      "156/281, train_loss: 0.0543, step time: 0.2506\n",
      "157/281, train_loss: 0.0646, step time: 0.2511\n",
      "158/281, train_loss: 0.0488, step time: 0.2469\n",
      "159/281, train_loss: 0.0436, step time: 0.2531\n",
      "160/281, train_loss: 0.0478, step time: 0.2523\n",
      "161/281, train_loss: 0.2181, step time: 0.2554\n",
      "162/281, train_loss: 0.0604, step time: 0.2554\n",
      "163/281, train_loss: 0.0447, step time: 0.2532\n",
      "164/281, train_loss: 0.0523, step time: 0.2535\n",
      "165/281, train_loss: 0.0591, step time: 0.2503\n",
      "166/281, train_loss: 0.2199, step time: 0.2549\n",
      "167/281, train_loss: 0.0431, step time: 0.2536\n",
      "168/281, train_loss: 0.0345, step time: 0.2533\n",
      "169/281, train_loss: 0.0460, step time: 0.2515\n",
      "170/281, train_loss: 0.0782, step time: 0.2485\n",
      "171/281, train_loss: 0.0338, step time: 0.2555\n",
      "172/281, train_loss: 0.2281, step time: 0.2523\n",
      "173/281, train_loss: 0.0579, step time: 0.2491\n",
      "174/281, train_loss: 0.0564, step time: 0.2484\n",
      "175/281, train_loss: 0.3751, step time: 0.2474\n",
      "176/281, train_loss: 0.0707, step time: 0.2455\n",
      "177/281, train_loss: 0.0793, step time: 0.2497\n",
      "178/281, train_loss: 0.0562, step time: 0.2445\n",
      "179/281, train_loss: 0.0590, step time: 0.2477\n",
      "180/281, train_loss: 0.0744, step time: 0.2469\n",
      "181/281, train_loss: 0.0644, step time: 0.2437\n",
      "182/281, train_loss: 0.0397, step time: 0.2456\n",
      "183/281, train_loss: 0.0520, step time: 0.2469\n",
      "184/281, train_loss: 0.0757, step time: 0.2489\n",
      "185/281, train_loss: 0.0419, step time: 0.2504\n",
      "186/281, train_loss: 0.3731, step time: 0.2432\n",
      "187/281, train_loss: 0.0340, step time: 0.2456\n",
      "188/281, train_loss: 0.2000, step time: 0.2622\n",
      "189/281, train_loss: 0.2167, step time: 0.2701\n",
      "190/281, train_loss: 0.2132, step time: 0.2515\n",
      "191/281, train_loss: 0.0398, step time: 0.2504\n",
      "192/281, train_loss: 0.0379, step time: 0.2517\n",
      "193/281, train_loss: 0.0562, step time: 0.2503\n",
      "194/281, train_loss: 0.0518, step time: 0.2489\n",
      "195/281, train_loss: 0.0539, step time: 0.2467\n",
      "196/281, train_loss: 0.0845, step time: 0.2534\n",
      "197/281, train_loss: 0.0610, step time: 0.2477\n",
      "198/281, train_loss: 0.3683, step time: 0.2512\n",
      "199/281, train_loss: 0.0495, step time: 0.2446\n",
      "200/281, train_loss: 0.0422, step time: 0.2535\n",
      "201/281, train_loss: 0.0549, step time: 0.2495\n",
      "202/281, train_loss: 0.2017, step time: 0.2471\n",
      "203/281, train_loss: 0.0524, step time: 0.2496\n",
      "204/281, train_loss: 0.0449, step time: 0.2530\n",
      "205/281, train_loss: 0.0572, step time: 0.2481\n",
      "206/281, train_loss: 0.0580, step time: 0.2508\n",
      "207/281, train_loss: 0.0267, step time: 0.2545\n",
      "208/281, train_loss: 0.0537, step time: 0.2566\n",
      "209/281, train_loss: 0.0454, step time: 0.2558\n",
      "210/281, train_loss: 0.0649, step time: 0.2520\n",
      "211/281, train_loss: 0.0504, step time: 0.2483\n",
      "212/281, train_loss: 0.1379, step time: 0.2530\n",
      "213/281, train_loss: 0.0261, step time: 0.2502\n",
      "214/281, train_loss: 0.0672, step time: 0.2491\n",
      "215/281, train_loss: 0.2412, step time: 0.2530\n",
      "216/281, train_loss: 0.0411, step time: 0.2498\n",
      "217/281, train_loss: 0.0664, step time: 0.2535\n",
      "218/281, train_loss: 0.0755, step time: 0.2576\n",
      "219/281, train_loss: 0.0543, step time: 0.2477\n",
      "220/281, train_loss: 0.0409, step time: 0.2507\n",
      "221/281, train_loss: 0.0355, step time: 0.2541\n",
      "222/281, train_loss: 0.2170, step time: 0.2477\n",
      "223/281, train_loss: 0.0744, step time: 0.2511\n",
      "224/281, train_loss: 0.0793, step time: 0.2515\n",
      "225/281, train_loss: 0.0484, step time: 0.2532\n",
      "226/281, train_loss: 0.0441, step time: 0.2449\n",
      "227/281, train_loss: 0.0608, step time: 0.2468\n",
      "228/281, train_loss: 0.0991, step time: 0.2560\n",
      "229/281, train_loss: 0.0999, step time: 0.2530\n",
      "230/281, train_loss: 0.0452, step time: 0.2456\n",
      "231/281, train_loss: 0.0504, step time: 0.2533\n",
      "232/281, train_loss: 0.0546, step time: 0.2484\n",
      "233/281, train_loss: 0.0741, step time: 0.2502\n",
      "234/281, train_loss: 0.0393, step time: 0.2489\n",
      "235/281, train_loss: 0.0501, step time: 0.2486\n",
      "236/281, train_loss: 0.0896, step time: 0.2530\n",
      "237/281, train_loss: 0.0821, step time: 0.2523\n",
      "238/281, train_loss: 0.0735, step time: 0.2502\n",
      "239/281, train_loss: 0.0483, step time: 0.2510\n",
      "240/281, train_loss: 0.0483, step time: 0.2510\n",
      "241/281, train_loss: 0.0700, step time: 0.2490\n",
      "242/281, train_loss: 0.0457, step time: 0.2516\n",
      "243/281, train_loss: 0.0363, step time: 0.2438\n",
      "244/281, train_loss: 0.0638, step time: 0.2528\n",
      "245/281, train_loss: 0.0557, step time: 0.2547\n",
      "246/281, train_loss: 0.0626, step time: 0.2460\n",
      "247/281, train_loss: 0.0461, step time: 0.2469\n",
      "248/281, train_loss: 0.0594, step time: 0.2494\n",
      "249/281, train_loss: 0.0591, step time: 0.2479\n",
      "250/281, train_loss: 0.0465, step time: 0.2442\n",
      "251/281, train_loss: 0.2028, step time: 0.2497\n",
      "252/281, train_loss: 0.0818, step time: 0.2477\n",
      "253/281, train_loss: 0.0429, step time: 0.2612\n",
      "254/281, train_loss: 0.0750, step time: 0.2491\n",
      "255/281, train_loss: 0.0446, step time: 0.2477\n",
      "256/281, train_loss: 0.0589, step time: 0.2525\n",
      "257/281, train_loss: 0.0631, step time: 0.2529\n",
      "258/281, train_loss: 0.0496, step time: 0.2493\n",
      "259/281, train_loss: 0.0947, step time: 0.2609\n",
      "260/281, train_loss: 0.0649, step time: 0.2472\n",
      "261/281, train_loss: 0.2250, step time: 0.2550\n",
      "262/281, train_loss: 0.2066, step time: 0.2503\n",
      "263/281, train_loss: 0.0591, step time: 0.2439\n",
      "264/281, train_loss: 0.0728, step time: 0.2475\n",
      "265/281, train_loss: 0.0564, step time: 0.2566\n",
      "266/281, train_loss: 0.0634, step time: 0.2564\n",
      "267/281, train_loss: 0.0660, step time: 0.2548\n",
      "268/281, train_loss: 0.0653, step time: 0.2493\n",
      "269/281, train_loss: 0.0674, step time: 0.2482\n",
      "270/281, train_loss: 0.0502, step time: 0.2528\n",
      "271/281, train_loss: 0.0488, step time: 0.2570\n",
      "272/281, train_loss: 0.0649, step time: 0.2828\n",
      "273/281, train_loss: 0.0246, step time: 0.2490\n",
      "274/281, train_loss: 0.0546, step time: 0.2493\n",
      "275/281, train_loss: 0.0542, step time: 0.2478\n",
      "276/281, train_loss: 0.0843, step time: 0.2490\n",
      "277/281, train_loss: 0.0612, step time: 0.2495\n",
      "278/281, train_loss: 0.0412, step time: 0.2498\n",
      "279/281, train_loss: 0.0454, step time: 0.2515\n",
      "280/281, train_loss: 0.0548, step time: 0.2532\n",
      "281/281, train_loss: 0.0487, step time: 0.2523\n",
      "282/281, train_loss: 0.0548, step time: 0.1513\n",
      "epoch 120 average loss: 0.0811\n",
      "current epoch: 120 current mean dice: 0.9048 tc: 0.8985 wt: 0.9281 et: 0.8971\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 120 is: 377.4514\n",
      "----------\n",
      "epoch 121/200\n",
      "1/281, train_loss: 0.0583, step time: 0.2459\n",
      "2/281, train_loss: 0.0584, step time: 0.2421\n",
      "3/281, train_loss: 0.0495, step time: 0.2464\n",
      "4/281, train_loss: 0.0482, step time: 0.2500\n",
      "5/281, train_loss: 0.0783, step time: 0.2457\n",
      "6/281, train_loss: 0.0320, step time: 0.2469\n",
      "7/281, train_loss: 0.0557, step time: 0.2408\n",
      "8/281, train_loss: 0.0680, step time: 0.2447\n",
      "9/281, train_loss: 0.0696, step time: 0.2558\n",
      "10/281, train_loss: 0.0397, step time: 0.2489\n",
      "11/281, train_loss: 0.2277, step time: 0.2476\n",
      "12/281, train_loss: 0.0384, step time: 0.2480\n",
      "13/281, train_loss: 0.0327, step time: 0.2524\n",
      "14/281, train_loss: 0.2165, step time: 0.2535\n",
      "15/281, train_loss: 0.0426, step time: 0.2460\n",
      "16/281, train_loss: 0.0602, step time: 0.2418\n",
      "17/281, train_loss: 0.0564, step time: 0.2502\n",
      "18/281, train_loss: 0.0479, step time: 0.2590\n",
      "19/281, train_loss: 0.0436, step time: 0.2504\n",
      "20/281, train_loss: 0.0406, step time: 0.2459\n",
      "21/281, train_loss: 0.0712, step time: 0.2533\n",
      "22/281, train_loss: 0.0932, step time: 0.2534\n",
      "23/281, train_loss: 0.2208, step time: 0.2504\n",
      "24/281, train_loss: 0.0556, step time: 0.2472\n",
      "25/281, train_loss: 0.0361, step time: 0.2463\n",
      "26/281, train_loss: 0.0483, step time: 0.2473\n",
      "27/281, train_loss: 0.0585, step time: 0.2505\n",
      "28/281, train_loss: 0.0596, step time: 0.2446\n",
      "29/281, train_loss: 0.2344, step time: 0.2552\n",
      "30/281, train_loss: 0.3709, step time: 0.2538\n",
      "31/281, train_loss: 0.0943, step time: 0.2461\n",
      "32/281, train_loss: 0.0681, step time: 0.2476\n",
      "33/281, train_loss: 0.0404, step time: 0.2481\n",
      "34/281, train_loss: 0.0432, step time: 0.2509\n",
      "35/281, train_loss: 0.0406, step time: 0.2576\n",
      "36/281, train_loss: 0.0469, step time: 0.2517\n",
      "37/281, train_loss: 0.0578, step time: 0.2467\n",
      "38/281, train_loss: 0.0637, step time: 0.2502\n",
      "39/281, train_loss: 0.0512, step time: 0.2506\n",
      "40/281, train_loss: 0.0627, step time: 0.2481\n",
      "41/281, train_loss: 0.0643, step time: 0.2429\n",
      "42/281, train_loss: 0.0433, step time: 0.2458\n",
      "43/281, train_loss: 0.0503, step time: 0.2501\n",
      "44/281, train_loss: 0.0408, step time: 0.2480\n",
      "45/281, train_loss: 0.0533, step time: 0.2509\n",
      "46/281, train_loss: 0.0772, step time: 0.2473\n",
      "47/281, train_loss: 0.0847, step time: 0.2433\n",
      "48/281, train_loss: 0.0509, step time: 0.2442\n",
      "49/281, train_loss: 0.0925, step time: 0.2444\n",
      "50/281, train_loss: 0.0550, step time: 0.2502\n",
      "51/281, train_loss: 0.0437, step time: 0.2453\n",
      "52/281, train_loss: 0.0675, step time: 0.2516\n",
      "53/281, train_loss: 0.0504, step time: 0.2535\n",
      "54/281, train_loss: 0.0731, step time: 0.2460\n",
      "55/281, train_loss: 0.0604, step time: 0.2467\n",
      "56/281, train_loss: 0.0597, step time: 0.2427\n",
      "57/281, train_loss: 0.0405, step time: 0.2459\n",
      "58/281, train_loss: 0.2036, step time: 0.2687\n",
      "59/281, train_loss: 0.0483, step time: 0.2506\n",
      "60/281, train_loss: 0.0308, step time: 0.2404\n",
      "61/281, train_loss: 0.0328, step time: 0.2416\n",
      "62/281, train_loss: 0.0668, step time: 0.2370\n",
      "63/281, train_loss: 0.0571, step time: 0.2397\n",
      "64/281, train_loss: 0.0477, step time: 0.2430\n",
      "65/281, train_loss: 0.0423, step time: 0.2452\n",
      "66/281, train_loss: 0.0527, step time: 0.2461\n",
      "67/281, train_loss: 0.0680, step time: 0.2440\n",
      "68/281, train_loss: 0.0465, step time: 0.2549\n",
      "69/281, train_loss: 0.2083, step time: 0.2502\n",
      "70/281, train_loss: 0.0740, step time: 0.2455\n",
      "71/281, train_loss: 0.0393, step time: 0.2442\n",
      "72/281, train_loss: 0.0737, step time: 0.2434\n",
      "73/281, train_loss: 0.0712, step time: 0.2482\n",
      "74/281, train_loss: 0.0394, step time: 0.2537\n",
      "75/281, train_loss: 0.0542, step time: 0.2505\n",
      "76/281, train_loss: 0.0719, step time: 0.2484\n",
      "77/281, train_loss: 0.0591, step time: 0.2525\n",
      "78/281, train_loss: 0.1142, step time: 0.2464\n",
      "79/281, train_loss: 0.0640, step time: 0.2506\n",
      "80/281, train_loss: 0.0650, step time: 0.2498\n",
      "81/281, train_loss: 0.0624, step time: 0.2591\n",
      "82/281, train_loss: 0.0615, step time: 0.2516\n",
      "83/281, train_loss: 0.0484, step time: 0.2445\n",
      "84/281, train_loss: 0.0635, step time: 0.2514\n",
      "85/281, train_loss: 0.0483, step time: 0.2506\n",
      "86/281, train_loss: 0.0437, step time: 0.2488\n",
      "87/281, train_loss: 0.0707, step time: 0.2520\n",
      "88/281, train_loss: 0.0791, step time: 0.2573\n",
      "89/281, train_loss: 0.0611, step time: 0.2467\n",
      "90/281, train_loss: 0.0370, step time: 0.2515\n",
      "91/281, train_loss: 0.0509, step time: 0.2475\n",
      "92/281, train_loss: 0.0538, step time: 0.2424\n",
      "93/281, train_loss: 0.2223, step time: 0.2515\n",
      "94/281, train_loss: 0.0843, step time: 0.2507\n",
      "95/281, train_loss: 0.0607, step time: 0.2465\n",
      "96/281, train_loss: 0.0585, step time: 0.2457\n",
      "97/281, train_loss: 0.0687, step time: 0.2516\n",
      "98/281, train_loss: 0.0405, step time: 0.2555\n",
      "99/281, train_loss: 0.2298, step time: 0.2585\n",
      "100/281, train_loss: 0.0300, step time: 0.2542\n",
      "101/281, train_loss: 0.0798, step time: 0.2525\n",
      "102/281, train_loss: 0.0383, step time: 0.2514\n",
      "103/281, train_loss: 0.0565, step time: 0.2514\n",
      "104/281, train_loss: 0.0677, step time: 0.2479\n",
      "105/281, train_loss: 0.0530, step time: 0.2441\n",
      "106/281, train_loss: 0.0524, step time: 0.2450\n",
      "107/281, train_loss: 0.0536, step time: 0.2436\n",
      "108/281, train_loss: 0.0470, step time: 0.2488\n",
      "109/281, train_loss: 0.2242, step time: 0.2481\n",
      "110/281, train_loss: 0.2241, step time: 0.2457\n",
      "111/281, train_loss: 0.2230, step time: 0.2470\n",
      "112/281, train_loss: 0.2553, step time: 0.2430\n",
      "113/281, train_loss: 0.0731, step time: 0.2455\n",
      "114/281, train_loss: 0.0623, step time: 0.2441\n",
      "115/281, train_loss: 0.0496, step time: 0.2610\n",
      "116/281, train_loss: 0.0376, step time: 0.2561\n",
      "117/281, train_loss: 0.0650, step time: 0.2439\n",
      "118/281, train_loss: 0.0681, step time: 0.2442\n",
      "119/281, train_loss: 0.0526, step time: 0.2444\n",
      "120/281, train_loss: 0.0468, step time: 0.2472\n",
      "121/281, train_loss: 0.0646, step time: 0.2410\n",
      "122/281, train_loss: 0.0359, step time: 0.2440\n",
      "123/281, train_loss: 0.3854, step time: 0.2461\n",
      "124/281, train_loss: 0.0731, step time: 0.2492\n",
      "125/281, train_loss: 0.0404, step time: 0.2470\n",
      "126/281, train_loss: 0.0556, step time: 0.2423\n",
      "127/281, train_loss: 0.0913, step time: 0.2423\n",
      "128/281, train_loss: 0.0369, step time: 0.2414\n",
      "129/281, train_loss: 0.0293, step time: 0.2433\n",
      "130/281, train_loss: 0.0790, step time: 0.2482\n",
      "131/281, train_loss: 0.0622, step time: 0.2417\n",
      "132/281, train_loss: 0.0382, step time: 0.2419\n",
      "133/281, train_loss: 0.2184, step time: 0.2433\n",
      "134/281, train_loss: 0.0478, step time: 0.2456\n",
      "135/281, train_loss: 0.0407, step time: 0.2489\n",
      "136/281, train_loss: 0.0617, step time: 0.2507\n",
      "137/281, train_loss: 0.0470, step time: 0.2475\n",
      "138/281, train_loss: 0.0450, step time: 0.2497\n",
      "139/281, train_loss: 0.0709, step time: 0.2488\n",
      "140/281, train_loss: 0.0492, step time: 0.2446\n",
      "141/281, train_loss: 0.0583, step time: 0.2468\n",
      "142/281, train_loss: 0.0846, step time: 0.2547\n",
      "143/281, train_loss: 0.0563, step time: 0.2531\n",
      "144/281, train_loss: 0.0550, step time: 0.2456\n",
      "145/281, train_loss: 0.0591, step time: 0.2517\n",
      "146/281, train_loss: 0.0578, step time: 0.2507\n",
      "147/281, train_loss: 0.0338, step time: 0.2517\n",
      "148/281, train_loss: 0.0863, step time: 0.2471\n",
      "149/281, train_loss: 0.0386, step time: 0.2513\n",
      "150/281, train_loss: 0.0721, step time: 0.2515\n",
      "151/281, train_loss: 0.0281, step time: 0.2437\n",
      "152/281, train_loss: 0.0438, step time: 0.2461\n",
      "153/281, train_loss: 0.0959, step time: 0.2466\n",
      "154/281, train_loss: 0.0472, step time: 0.2498\n",
      "155/281, train_loss: 0.0830, step time: 0.2431\n",
      "156/281, train_loss: 0.0322, step time: 0.2465\n",
      "157/281, train_loss: 0.0504, step time: 0.2477\n",
      "158/281, train_loss: 0.2236, step time: 0.2499\n",
      "159/281, train_loss: 0.0403, step time: 0.2513\n",
      "160/281, train_loss: 0.0597, step time: 0.2433\n",
      "161/281, train_loss: 0.0541, step time: 0.2451\n",
      "162/281, train_loss: 0.0332, step time: 0.2550\n",
      "163/281, train_loss: 0.0306, step time: 0.2498\n",
      "164/281, train_loss: 0.0521, step time: 0.2499\n",
      "165/281, train_loss: 0.0481, step time: 0.2516\n",
      "166/281, train_loss: 0.2053, step time: 0.2504\n",
      "167/281, train_loss: 0.0450, step time: 0.2464\n",
      "168/281, train_loss: 0.0535, step time: 0.2545\n",
      "169/281, train_loss: 0.0742, step time: 0.2567\n",
      "170/281, train_loss: 0.0449, step time: 0.2494\n",
      "171/281, train_loss: 0.0547, step time: 0.2486\n",
      "172/281, train_loss: 0.0670, step time: 0.2494\n",
      "173/281, train_loss: 0.0737, step time: 0.2481\n",
      "174/281, train_loss: 0.0411, step time: 0.2510\n",
      "175/281, train_loss: 0.2006, step time: 0.2528\n",
      "176/281, train_loss: 0.0645, step time: 0.2473\n",
      "177/281, train_loss: 0.0659, step time: 0.2448\n",
      "178/281, train_loss: 0.0544, step time: 0.2487\n",
      "179/281, train_loss: 0.0447, step time: 0.2567\n",
      "180/281, train_loss: 0.0430, step time: 0.2575\n",
      "181/281, train_loss: 0.0439, step time: 0.2476\n",
      "182/281, train_loss: 0.2122, step time: 0.2478\n",
      "183/281, train_loss: 0.0395, step time: 0.2546\n",
      "184/281, train_loss: 0.0558, step time: 0.2528\n",
      "185/281, train_loss: 0.0506, step time: 0.2506\n",
      "186/281, train_loss: 0.1995, step time: 0.2520\n",
      "187/281, train_loss: 0.0601, step time: 0.2475\n",
      "188/281, train_loss: 0.0689, step time: 0.2506\n",
      "189/281, train_loss: 0.0396, step time: 0.2443\n",
      "190/281, train_loss: 0.0491, step time: 0.2450\n",
      "191/281, train_loss: 0.0757, step time: 0.2509\n",
      "192/281, train_loss: 0.0669, step time: 0.2545\n",
      "193/281, train_loss: 0.0498, step time: 0.2508\n",
      "194/281, train_loss: 0.0497, step time: 0.2484\n",
      "195/281, train_loss: 0.0546, step time: 0.2480\n",
      "196/281, train_loss: 0.0595, step time: 0.2498\n",
      "197/281, train_loss: 0.2169, step time: 0.2458\n",
      "198/281, train_loss: 0.0489, step time: 0.2445\n",
      "199/281, train_loss: 0.0642, step time: 0.2485\n",
      "200/281, train_loss: 0.0470, step time: 0.2467\n",
      "201/281, train_loss: 0.0737, step time: 0.2595\n",
      "202/281, train_loss: 0.2128, step time: 0.2495\n",
      "203/281, train_loss: 0.0626, step time: 0.2499\n",
      "204/281, train_loss: 0.0290, step time: 0.2464\n",
      "205/281, train_loss: 0.2028, step time: 0.2526\n",
      "206/281, train_loss: 0.0694, step time: 0.2505\n",
      "207/281, train_loss: 0.0632, step time: 0.2433\n",
      "208/281, train_loss: 0.0569, step time: 0.2433\n",
      "209/281, train_loss: 0.0566, step time: 0.2429\n",
      "210/281, train_loss: 0.0519, step time: 0.2503\n",
      "211/281, train_loss: 0.1760, step time: 0.2219\n",
      "212/281, train_loss: 0.0566, step time: 0.2477\n",
      "213/281, train_loss: 0.0893, step time: 0.2459\n",
      "214/281, train_loss: 0.2054, step time: 0.2540\n",
      "215/281, train_loss: 0.0892, step time: 0.2508\n",
      "216/281, train_loss: 0.0679, step time: 0.2496\n",
      "217/281, train_loss: 0.0750, step time: 0.2533\n",
      "218/281, train_loss: 0.0501, step time: 0.2497\n",
      "219/281, train_loss: 0.0827, step time: 0.2448\n",
      "220/281, train_loss: 0.0766, step time: 0.2471\n",
      "221/281, train_loss: 0.0772, step time: 0.2478\n",
      "222/281, train_loss: 0.0452, step time: 0.2476\n",
      "223/281, train_loss: 0.0749, step time: 0.2514\n",
      "224/281, train_loss: 0.0654, step time: 0.2464\n",
      "225/281, train_loss: 0.0383, step time: 0.2485\n",
      "226/281, train_loss: 0.0348, step time: 0.2457\n",
      "227/281, train_loss: 0.0365, step time: 0.2561\n",
      "228/281, train_loss: 0.0711, step time: 0.2505\n",
      "229/281, train_loss: 0.1087, step time: 0.2519\n",
      "230/281, train_loss: 0.0706, step time: 0.2523\n",
      "231/281, train_loss: 0.2184, step time: 0.2498\n",
      "232/281, train_loss: 0.0292, step time: 0.2522\n",
      "233/281, train_loss: 0.2101, step time: 0.2477\n",
      "234/281, train_loss: 0.1991, step time: 0.2500\n",
      "235/281, train_loss: 0.0605, step time: 0.2467\n",
      "236/281, train_loss: 0.0430, step time: 0.2454\n",
      "237/281, train_loss: 0.2066, step time: 0.2433\n",
      "238/281, train_loss: 0.2132, step time: 0.2419\n",
      "239/281, train_loss: 0.0350, step time: 0.2496\n",
      "240/281, train_loss: 0.0557, step time: 0.2538\n",
      "241/281, train_loss: 0.0377, step time: 0.2531\n",
      "242/281, train_loss: 0.0512, step time: 0.2559\n",
      "243/281, train_loss: 0.1912, step time: 0.2495\n",
      "244/281, train_loss: 0.0512, step time: 0.2552\n",
      "245/281, train_loss: 0.0741, step time: 0.2510\n",
      "246/281, train_loss: 0.0855, step time: 0.2490\n",
      "247/281, train_loss: 0.0587, step time: 0.2491\n",
      "248/281, train_loss: 0.3947, step time: 0.2541\n",
      "249/281, train_loss: 0.2544, step time: 0.2532\n",
      "250/281, train_loss: 0.2431, step time: 0.2539\n",
      "251/281, train_loss: 0.0861, step time: 0.2510\n",
      "252/281, train_loss: 0.0473, step time: 0.2533\n",
      "253/281, train_loss: 0.0872, step time: 0.2549\n",
      "254/281, train_loss: 0.2471, step time: 0.2523\n",
      "255/281, train_loss: 0.1117, step time: 0.2485\n",
      "256/281, train_loss: 0.2198, step time: 0.2573\n",
      "257/281, train_loss: 0.2294, step time: 0.2528\n",
      "258/281, train_loss: 0.0624, step time: 0.2463\n",
      "259/281, train_loss: 0.1061, step time: 0.2513\n",
      "260/281, train_loss: 0.0919, step time: 0.2568\n",
      "261/281, train_loss: 0.0943, step time: 0.2564\n",
      "262/281, train_loss: 0.1210, step time: 0.2546\n",
      "263/281, train_loss: 0.0503, step time: 0.2527\n",
      "264/281, train_loss: 0.2784, step time: 0.2523\n",
      "265/281, train_loss: 0.2241, step time: 0.2778\n",
      "266/281, train_loss: 0.0949, step time: 0.2640\n",
      "267/281, train_loss: 0.0647, step time: 0.2537\n",
      "268/281, train_loss: 0.0950, step time: 0.2588\n",
      "269/281, train_loss: 0.0385, step time: 0.2863\n",
      "270/281, train_loss: 0.0578, step time: 0.2460\n",
      "271/281, train_loss: 0.2134, step time: 0.2488\n",
      "272/281, train_loss: 0.0675, step time: 0.2472\n",
      "273/281, train_loss: 0.0490, step time: 0.2511\n",
      "274/281, train_loss: 0.0784, step time: 0.2519\n",
      "275/281, train_loss: 0.0904, step time: 0.2562\n",
      "276/281, train_loss: 0.0559, step time: 0.2582\n",
      "277/281, train_loss: 0.0763, step time: 0.2597\n",
      "278/281, train_loss: 0.0710, step time: 0.2564\n",
      "279/281, train_loss: 0.0768, step time: 0.2545\n",
      "280/281, train_loss: 0.0931, step time: 0.2545\n",
      "281/281, train_loss: 0.1092, step time: 0.2548\n",
      "282/281, train_loss: 0.0510, step time: 0.1539\n",
      "epoch 121 average loss: 0.0834\n",
      "current epoch: 121 current mean dice: 0.8931 tc: 0.8883 wt: 0.9194 et: 0.8809\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 121 is: 384.8207\n",
      "----------\n",
      "epoch 122/200\n",
      "1/281, train_loss: 0.0655, step time: 0.2649\n",
      "2/281, train_loss: 0.0452, step time: 0.2416\n",
      "3/281, train_loss: 0.0625, step time: 0.2505\n",
      "4/281, train_loss: 0.1110, step time: 0.2503\n",
      "5/281, train_loss: 0.0518, step time: 0.2521\n",
      "6/281, train_loss: 0.0442, step time: 0.2564\n",
      "7/281, train_loss: 0.0903, step time: 0.2567\n",
      "8/281, train_loss: 0.2240, step time: 0.2539\n",
      "9/281, train_loss: 0.0832, step time: 0.2428\n",
      "10/281, train_loss: 0.0746, step time: 0.2476\n",
      "11/281, train_loss: 0.0558, step time: 0.2486\n",
      "12/281, train_loss: 0.0763, step time: 0.2470\n",
      "13/281, train_loss: 0.0595, step time: 0.2461\n",
      "14/281, train_loss: 0.0394, step time: 0.2480\n",
      "15/281, train_loss: 0.0340, step time: 0.2422\n",
      "16/281, train_loss: 0.0731, step time: 0.2445\n",
      "17/281, train_loss: 0.1025, step time: 0.2490\n",
      "18/281, train_loss: 0.0599, step time: 0.2504\n",
      "19/281, train_loss: 0.0837, step time: 0.2503\n",
      "20/281, train_loss: 0.0502, step time: 0.2512\n",
      "21/281, train_loss: 0.0719, step time: 0.2505\n",
      "22/281, train_loss: 0.0491, step time: 0.2536\n",
      "23/281, train_loss: 0.0543, step time: 0.2493\n",
      "24/281, train_loss: 0.0618, step time: 0.2456\n",
      "25/281, train_loss: 0.0436, step time: 0.2465\n",
      "26/281, train_loss: 0.0858, step time: 0.2516\n",
      "27/281, train_loss: 0.1320, step time: 0.2498\n",
      "28/281, train_loss: 0.0726, step time: 0.2448\n",
      "29/281, train_loss: 0.0931, step time: 0.2439\n",
      "30/281, train_loss: 0.0343, step time: 0.2490\n",
      "31/281, train_loss: 0.0604, step time: 0.2634\n",
      "32/281, train_loss: 0.0620, step time: 0.2586\n",
      "33/281, train_loss: 0.0662, step time: 0.2545\n",
      "34/281, train_loss: 0.0567, step time: 0.2572\n",
      "35/281, train_loss: 0.0878, step time: 0.2587\n",
      "36/281, train_loss: 0.0660, step time: 0.2538\n",
      "37/281, train_loss: 0.0767, step time: 0.2547\n",
      "38/281, train_loss: 0.0506, step time: 0.2510\n",
      "39/281, train_loss: 0.0790, step time: 0.2509\n",
      "40/281, train_loss: 0.0656, step time: 0.2477\n",
      "41/281, train_loss: 0.0639, step time: 0.2478\n",
      "42/281, train_loss: 0.0743, step time: 0.2476\n",
      "43/281, train_loss: 0.0714, step time: 0.2558\n",
      "44/281, train_loss: 0.0648, step time: 0.2608\n",
      "45/281, train_loss: 0.0542, step time: 0.2547\n",
      "46/281, train_loss: 0.0581, step time: 0.2542\n",
      "47/281, train_loss: 0.1082, step time: 0.2610\n",
      "48/281, train_loss: 0.2074, step time: 0.2513\n",
      "49/281, train_loss: 0.0550, step time: 0.2496\n",
      "50/281, train_loss: 0.2207, step time: 0.2515\n",
      "51/281, train_loss: 0.0598, step time: 0.2228\n",
      "52/281, train_loss: 0.0857, step time: 0.2521\n",
      "53/281, train_loss: 0.0490, step time: 0.2533\n",
      "54/281, train_loss: 0.0430, step time: 0.2496\n",
      "55/281, train_loss: 0.0586, step time: 0.2547\n",
      "56/281, train_loss: 0.0626, step time: 0.2534\n",
      "57/281, train_loss: 0.0909, step time: 0.2568\n",
      "58/281, train_loss: 0.0430, step time: 0.2503\n",
      "59/281, train_loss: 0.0493, step time: 0.2526\n",
      "60/281, train_loss: 0.0906, step time: 0.2483\n",
      "61/281, train_loss: 0.1059, step time: 0.2505\n",
      "62/281, train_loss: 0.1014, step time: 0.2521\n",
      "63/281, train_loss: 0.0537, step time: 0.2490\n",
      "64/281, train_loss: 0.0579, step time: 0.2571\n",
      "65/281, train_loss: 0.0460, step time: 0.2669\n",
      "66/281, train_loss: 0.0461, step time: 0.2522\n",
      "67/281, train_loss: 0.2512, step time: 0.2491\n",
      "68/281, train_loss: 0.0579, step time: 0.2505\n",
      "69/281, train_loss: 0.0739, step time: 0.2525\n",
      "70/281, train_loss: 0.0742, step time: 0.2550\n",
      "71/281, train_loss: 0.2122, step time: 0.2532\n",
      "72/281, train_loss: 0.0904, step time: 0.2534\n",
      "73/281, train_loss: 0.0647, step time: 0.2549\n",
      "74/281, train_loss: 0.0691, step time: 0.2529\n",
      "75/281, train_loss: 0.0736, step time: 0.2525\n",
      "76/281, train_loss: 0.0703, step time: 0.2521\n",
      "77/281, train_loss: 0.2377, step time: 0.2532\n",
      "78/281, train_loss: 0.0693, step time: 0.2545\n",
      "79/281, train_loss: 0.2404, step time: 0.2523\n",
      "80/281, train_loss: 0.0649, step time: 0.2530\n",
      "81/281, train_loss: 0.2263, step time: 0.2509\n",
      "82/281, train_loss: 0.0775, step time: 0.2549\n",
      "83/281, train_loss: 0.0513, step time: 0.2482\n",
      "84/281, train_loss: 0.0455, step time: 0.2687\n",
      "85/281, train_loss: 0.0522, step time: 0.2538\n",
      "86/281, train_loss: 0.0522, step time: 0.2572\n",
      "87/281, train_loss: 0.0558, step time: 0.2551\n",
      "88/281, train_loss: 0.0642, step time: 0.2587\n",
      "89/281, train_loss: 0.2143, step time: 0.2584\n",
      "90/281, train_loss: 0.0713, step time: 0.2612\n",
      "91/281, train_loss: 0.0674, step time: 0.2727\n",
      "92/281, train_loss: 0.0419, step time: 0.2750\n",
      "93/281, train_loss: 0.2172, step time: 0.2472\n",
      "94/281, train_loss: 0.0392, step time: 0.2488\n",
      "95/281, train_loss: 0.0465, step time: 0.2526\n",
      "96/281, train_loss: 0.2072, step time: 0.2552\n",
      "97/281, train_loss: 0.0463, step time: 0.2497\n",
      "98/281, train_loss: 0.0837, step time: 0.2520\n",
      "99/281, train_loss: 0.0467, step time: 0.2546\n",
      "100/281, train_loss: 0.1027, step time: 0.2576\n",
      "101/281, train_loss: 0.1165, step time: 0.2537\n",
      "102/281, train_loss: 0.0415, step time: 0.2506\n",
      "103/281, train_loss: 0.0509, step time: 0.2558\n",
      "104/281, train_loss: 0.2007, step time: 0.2500\n",
      "105/281, train_loss: 0.0629, step time: 0.2479\n",
      "106/281, train_loss: 0.0747, step time: 0.2506\n",
      "107/281, train_loss: 0.0598, step time: 0.2487\n",
      "108/281, train_loss: 0.0473, step time: 0.2453\n",
      "109/281, train_loss: 0.0553, step time: 0.2499\n",
      "110/281, train_loss: 0.0527, step time: 0.2563\n",
      "111/281, train_loss: 0.0906, step time: 0.2516\n",
      "112/281, train_loss: 0.2097, step time: 0.2479\n",
      "113/281, train_loss: 0.0689, step time: 0.2537\n",
      "114/281, train_loss: 0.1074, step time: 0.2553\n",
      "115/281, train_loss: 0.0632, step time: 0.2473\n",
      "116/281, train_loss: 0.0458, step time: 0.2504\n",
      "117/281, train_loss: 0.0494, step time: 0.2518\n",
      "118/281, train_loss: 0.0620, step time: 0.2518\n",
      "119/281, train_loss: 0.0708, step time: 0.2479\n",
      "120/281, train_loss: 0.0548, step time: 0.2440\n",
      "121/281, train_loss: 0.0500, step time: 0.2569\n",
      "122/281, train_loss: 0.0712, step time: 0.2543\n",
      "123/281, train_loss: 0.0794, step time: 0.2528\n",
      "124/281, train_loss: 0.0630, step time: 0.2563\n",
      "125/281, train_loss: 0.2192, step time: 0.2489\n",
      "126/281, train_loss: 0.0480, step time: 0.2536\n",
      "127/281, train_loss: 0.0521, step time: 0.2485\n",
      "128/281, train_loss: 0.0688, step time: 0.2551\n",
      "129/281, train_loss: 0.0481, step time: 0.2445\n",
      "130/281, train_loss: 0.0461, step time: 0.2486\n",
      "131/281, train_loss: 0.0325, step time: 0.2429\n",
      "132/281, train_loss: 0.0350, step time: 0.2422\n",
      "133/281, train_loss: 0.2062, step time: 0.2492\n",
      "134/281, train_loss: 0.0456, step time: 0.2465\n",
      "135/281, train_loss: 0.0444, step time: 0.2434\n",
      "136/281, train_loss: 0.2204, step time: 0.2425\n",
      "137/281, train_loss: 0.0676, step time: 0.2441\n",
      "138/281, train_loss: 0.0489, step time: 0.2428\n",
      "139/281, train_loss: 0.0661, step time: 0.2491\n",
      "140/281, train_loss: 0.0532, step time: 0.2475\n",
      "141/281, train_loss: 0.0563, step time: 0.2450\n",
      "142/281, train_loss: 0.0521, step time: 0.2463\n",
      "143/281, train_loss: 0.3899, step time: 0.2487\n",
      "144/281, train_loss: 0.0955, step time: 0.2407\n",
      "145/281, train_loss: 0.0517, step time: 0.2426\n",
      "146/281, train_loss: 0.0684, step time: 0.2407\n",
      "147/281, train_loss: 0.0443, step time: 0.2422\n",
      "148/281, train_loss: 0.2097, step time: 0.2422\n",
      "149/281, train_loss: 0.0627, step time: 0.2522\n",
      "150/281, train_loss: 0.0553, step time: 0.2514\n",
      "151/281, train_loss: 0.0547, step time: 0.2515\n",
      "152/281, train_loss: 0.0485, step time: 0.2491\n",
      "153/281, train_loss: 0.0811, step time: 0.2528\n",
      "154/281, train_loss: 0.0371, step time: 0.2515\n",
      "155/281, train_loss: 0.0641, step time: 0.2497\n",
      "156/281, train_loss: 0.0535, step time: 0.2473\n",
      "157/281, train_loss: 0.0583, step time: 0.2510\n",
      "158/281, train_loss: 0.0354, step time: 0.2496\n",
      "159/281, train_loss: 0.0584, step time: 0.2512\n",
      "160/281, train_loss: 0.2026, step time: 0.2507\n",
      "161/281, train_loss: 0.0518, step time: 0.2463\n",
      "162/281, train_loss: 0.0447, step time: 0.2434\n",
      "163/281, train_loss: 0.0394, step time: 0.2445\n",
      "164/281, train_loss: 0.0603, step time: 0.2498\n",
      "165/281, train_loss: 0.0228, step time: 0.2499\n",
      "166/281, train_loss: 0.0527, step time: 0.2418\n",
      "167/281, train_loss: 0.0665, step time: 0.2440\n",
      "168/281, train_loss: 0.0541, step time: 0.2486\n",
      "169/281, train_loss: 0.0569, step time: 0.2477\n",
      "170/281, train_loss: 0.0834, step time: 0.2441\n",
      "171/281, train_loss: 0.0568, step time: 0.2479\n",
      "172/281, train_loss: 0.1991, step time: 0.2422\n",
      "173/281, train_loss: 0.0591, step time: 0.2473\n",
      "174/281, train_loss: 0.0669, step time: 0.2427\n",
      "175/281, train_loss: 0.2137, step time: 0.2418\n",
      "176/281, train_loss: 0.0694, step time: 0.2497\n",
      "177/281, train_loss: 0.2359, step time: 0.2468\n",
      "178/281, train_loss: 0.0610, step time: 0.2498\n",
      "179/281, train_loss: 0.0699, step time: 0.2478\n",
      "180/281, train_loss: 0.0440, step time: 0.2525\n",
      "181/281, train_loss: 0.0609, step time: 0.2543\n",
      "182/281, train_loss: 0.0530, step time: 0.2503\n",
      "183/281, train_loss: 0.0598, step time: 0.2509\n",
      "184/281, train_loss: 0.0478, step time: 0.2516\n",
      "185/281, train_loss: 0.2003, step time: 0.2479\n",
      "186/281, train_loss: 0.0734, step time: 0.2520\n",
      "187/281, train_loss: 0.0779, step time: 0.2469\n",
      "188/281, train_loss: 0.0456, step time: 0.2610\n",
      "189/281, train_loss: 0.0454, step time: 0.2502\n",
      "190/281, train_loss: 0.2281, step time: 0.2473\n",
      "191/281, train_loss: 0.0512, step time: 0.2464\n",
      "192/281, train_loss: 0.2311, step time: 0.2486\n",
      "193/281, train_loss: 0.0621, step time: 0.2554\n",
      "194/281, train_loss: 0.0949, step time: 0.2567\n",
      "195/281, train_loss: 0.0577, step time: 0.2488\n",
      "196/281, train_loss: 0.3781, step time: 0.2540\n",
      "197/281, train_loss: 0.0397, step time: 0.2544\n",
      "198/281, train_loss: 0.0581, step time: 0.2516\n",
      "199/281, train_loss: 0.0513, step time: 0.2532\n",
      "200/281, train_loss: 0.2147, step time: 0.2549\n",
      "201/281, train_loss: 0.0812, step time: 0.2516\n",
      "202/281, train_loss: 0.0424, step time: 0.2486\n",
      "203/281, train_loss: 0.0464, step time: 0.2473\n",
      "204/281, train_loss: 0.0793, step time: 0.2468\n",
      "205/281, train_loss: 0.0721, step time: 0.2496\n",
      "206/281, train_loss: 0.2130, step time: 0.2449\n",
      "207/281, train_loss: 0.3729, step time: 0.2477\n",
      "208/281, train_loss: 0.0869, step time: 0.2478\n",
      "209/281, train_loss: 0.0630, step time: 0.2548\n",
      "210/281, train_loss: 0.2478, step time: 0.2494\n",
      "211/281, train_loss: 0.0803, step time: 0.2513\n",
      "212/281, train_loss: 0.0980, step time: 0.2546\n",
      "213/281, train_loss: 0.0700, step time: 0.2591\n",
      "214/281, train_loss: 0.0422, step time: 0.2639\n",
      "215/281, train_loss: 0.0762, step time: 0.2524\n",
      "216/281, train_loss: 0.2277, step time: 0.2480\n",
      "217/281, train_loss: 0.0393, step time: 0.2510\n",
      "218/281, train_loss: 0.0602, step time: 0.2541\n",
      "219/281, train_loss: 0.0591, step time: 0.2507\n",
      "220/281, train_loss: 0.0490, step time: 0.2487\n",
      "221/281, train_loss: 0.0599, step time: 0.2445\n",
      "222/281, train_loss: 0.0803, step time: 0.2478\n",
      "223/281, train_loss: 0.0626, step time: 0.2507\n",
      "224/281, train_loss: 0.0529, step time: 0.2504\n",
      "225/281, train_loss: 0.0562, step time: 0.2492\n",
      "226/281, train_loss: 0.0561, step time: 0.2512\n",
      "227/281, train_loss: 0.0582, step time: 0.2475\n",
      "228/281, train_loss: 0.0561, step time: 0.2488\n",
      "229/281, train_loss: 0.0569, step time: 0.2467\n",
      "230/281, train_loss: 0.0525, step time: 0.2533\n",
      "231/281, train_loss: 0.0294, step time: 0.2466\n",
      "232/281, train_loss: 0.2285, step time: 0.2501\n",
      "233/281, train_loss: 0.0838, step time: 0.2494\n",
      "234/281, train_loss: 0.0600, step time: 0.2500\n",
      "235/281, train_loss: 0.0683, step time: 0.2485\n",
      "236/281, train_loss: 0.0588, step time: 0.2490\n",
      "237/281, train_loss: 0.0568, step time: 0.2432\n",
      "238/281, train_loss: 0.0445, step time: 0.2455\n",
      "239/281, train_loss: 0.0525, step time: 0.2443\n",
      "240/281, train_loss: 0.2423, step time: 0.2481\n",
      "241/281, train_loss: 0.2183, step time: 0.2478\n",
      "242/281, train_loss: 0.0451, step time: 0.2460\n",
      "243/281, train_loss: 0.0430, step time: 0.2494\n",
      "244/281, train_loss: 0.0649, step time: 0.2487\n",
      "245/281, train_loss: 0.0732, step time: 0.2453\n",
      "246/281, train_loss: 0.0526, step time: 0.2595\n",
      "247/281, train_loss: 0.0701, step time: 0.2455\n",
      "248/281, train_loss: 0.0719, step time: 0.2501\n",
      "249/281, train_loss: 0.2127, step time: 0.2480\n",
      "250/281, train_loss: 0.0412, step time: 0.2494\n",
      "251/281, train_loss: 0.0462, step time: 0.2512\n",
      "252/281, train_loss: 0.0654, step time: 0.2552\n",
      "253/281, train_loss: 0.0555, step time: 0.2506\n",
      "254/281, train_loss: 0.0541, step time: 0.2467\n",
      "255/281, train_loss: 0.0428, step time: 0.2440\n",
      "256/281, train_loss: 0.0419, step time: 0.2462\n",
      "257/281, train_loss: 0.0297, step time: 0.2504\n",
      "258/281, train_loss: 0.2102, step time: 0.2543\n",
      "259/281, train_loss: 0.0486, step time: 0.2559\n",
      "260/281, train_loss: 0.0687, step time: 0.2472\n",
      "261/281, train_loss: 0.0646, step time: 0.2480\n",
      "262/281, train_loss: 0.0474, step time: 0.2476\n",
      "263/281, train_loss: 0.0591, step time: 0.2448\n",
      "264/281, train_loss: 0.0705, step time: 0.2480\n",
      "265/281, train_loss: 0.0465, step time: 0.2465\n",
      "266/281, train_loss: 0.0743, step time: 0.2475\n",
      "267/281, train_loss: 0.0457, step time: 0.2511\n",
      "268/281, train_loss: 0.0555, step time: 0.2539\n",
      "269/281, train_loss: 0.0787, step time: 0.2472\n",
      "270/281, train_loss: 0.0305, step time: 0.2479\n",
      "271/281, train_loss: 0.0640, step time: 0.2454\n",
      "272/281, train_loss: 0.2228, step time: 0.2458\n",
      "273/281, train_loss: 0.1998, step time: 0.2412\n",
      "274/281, train_loss: 0.0382, step time: 0.2456\n",
      "275/281, train_loss: 0.0572, step time: 0.2485\n",
      "276/281, train_loss: 0.0561, step time: 0.2478\n",
      "277/281, train_loss: 0.1985, step time: 0.2476\n",
      "278/281, train_loss: 0.0576, step time: 0.2448\n",
      "279/281, train_loss: 0.0706, step time: 0.2474\n",
      "280/281, train_loss: 0.0424, step time: 0.2483\n",
      "281/281, train_loss: 0.0386, step time: 0.2482\n",
      "282/281, train_loss: 0.0563, step time: 0.1509\n",
      "epoch 122 average loss: 0.0846\n",
      "current epoch: 122 current mean dice: 0.9036 tc: 0.8986 wt: 0.9270 et: 0.8941\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 122 is: 402.4988\n",
      "----------\n",
      "epoch 123/200\n",
      "1/281, train_loss: 0.0424, step time: 0.2560\n",
      "2/281, train_loss: 0.0521, step time: 0.2493\n",
      "3/281, train_loss: 0.0584, step time: 0.2490\n",
      "4/281, train_loss: 0.0710, step time: 0.2503\n",
      "5/281, train_loss: 0.0691, step time: 0.2510\n",
      "6/281, train_loss: 0.0407, step time: 0.2547\n",
      "7/281, train_loss: 0.0517, step time: 0.2436\n",
      "8/281, train_loss: 0.0771, step time: 0.2480\n",
      "9/281, train_loss: 0.0579, step time: 0.2600\n",
      "10/281, train_loss: 0.0399, step time: 0.2575\n",
      "11/281, train_loss: 0.2251, step time: 0.2522\n",
      "12/281, train_loss: 0.2255, step time: 0.2416\n",
      "13/281, train_loss: 0.0618, step time: 0.2391\n",
      "14/281, train_loss: 0.0726, step time: 0.2442\n",
      "15/281, train_loss: 0.0613, step time: 0.2427\n",
      "16/281, train_loss: 0.0515, step time: 0.2465\n",
      "17/281, train_loss: 0.1014, step time: 0.2501\n",
      "18/281, train_loss: 0.0415, step time: 0.2522\n",
      "19/281, train_loss: 0.0705, step time: 0.2557\n",
      "20/281, train_loss: 0.0525, step time: 0.2498\n",
      "21/281, train_loss: 0.0769, step time: 0.2462\n",
      "22/281, train_loss: 0.0775, step time: 0.2440\n",
      "23/281, train_loss: 0.0767, step time: 0.2499\n",
      "24/281, train_loss: 0.0350, step time: 0.2476\n",
      "25/281, train_loss: 0.0618, step time: 0.2454\n",
      "26/281, train_loss: 0.2068, step time: 0.2522\n",
      "27/281, train_loss: 0.0562, step time: 0.2494\n",
      "28/281, train_loss: 0.0635, step time: 0.2448\n",
      "29/281, train_loss: 0.0583, step time: 0.2413\n",
      "30/281, train_loss: 0.0407, step time: 0.2448\n",
      "31/281, train_loss: 0.0412, step time: 0.2422\n",
      "32/281, train_loss: 0.2630, step time: 0.2472\n",
      "33/281, train_loss: 0.0567, step time: 0.2474\n",
      "34/281, train_loss: 0.1061, step time: 0.2459\n",
      "35/281, train_loss: 0.0724, step time: 0.2476\n",
      "36/281, train_loss: 0.0679, step time: 0.2487\n",
      "37/281, train_loss: 0.0870, step time: 0.2488\n",
      "38/281, train_loss: 0.0815, step time: 0.2469\n",
      "39/281, train_loss: 0.0563, step time: 0.2472\n",
      "40/281, train_loss: 0.0646, step time: 0.2431\n",
      "41/281, train_loss: 0.0745, step time: 0.2412\n",
      "42/281, train_loss: 0.0399, step time: 0.2397\n",
      "43/281, train_loss: 0.0629, step time: 0.2475\n",
      "44/281, train_loss: 0.0477, step time: 0.2517\n",
      "45/281, train_loss: 0.0474, step time: 0.2407\n",
      "46/281, train_loss: 0.0609, step time: 0.2440\n",
      "47/281, train_loss: 0.0445, step time: 0.2456\n",
      "48/281, train_loss: 0.0519, step time: 0.2487\n",
      "49/281, train_loss: 0.2177, step time: 0.2501\n",
      "50/281, train_loss: 0.0550, step time: 0.2480\n",
      "51/281, train_loss: 0.0510, step time: 0.2465\n",
      "52/281, train_loss: 0.0404, step time: 0.2481\n",
      "53/281, train_loss: 0.0497, step time: 0.2501\n",
      "54/281, train_loss: 0.0716, step time: 0.2419\n",
      "55/281, train_loss: 0.0540, step time: 0.2436\n",
      "56/281, train_loss: 0.0497, step time: 0.2452\n",
      "57/281, train_loss: 0.1043, step time: 0.2448\n",
      "58/281, train_loss: 0.0503, step time: 0.2522\n",
      "59/281, train_loss: 0.0563, step time: 0.2609\n",
      "60/281, train_loss: 0.0440, step time: 0.2497\n",
      "61/281, train_loss: 0.0479, step time: 0.2491\n",
      "62/281, train_loss: 0.0628, step time: 0.2535\n",
      "63/281, train_loss: 0.0617, step time: 0.2486\n",
      "64/281, train_loss: 0.0447, step time: 0.2483\n",
      "65/281, train_loss: 0.0525, step time: 0.2452\n",
      "66/281, train_loss: 0.0418, step time: 0.2487\n",
      "67/281, train_loss: 0.0539, step time: 0.2454\n",
      "68/281, train_loss: 0.2153, step time: 0.2436\n",
      "69/281, train_loss: 0.0458, step time: 0.2461\n",
      "70/281, train_loss: 0.0671, step time: 0.2501\n",
      "71/281, train_loss: 0.2157, step time: 0.2498\n",
      "72/281, train_loss: 0.0414, step time: 0.2414\n",
      "73/281, train_loss: 0.0571, step time: 0.2485\n",
      "74/281, train_loss: 0.0436, step time: 0.2501\n",
      "75/281, train_loss: 0.0477, step time: 0.2471\n",
      "76/281, train_loss: 0.0677, step time: 0.2495\n",
      "77/281, train_loss: 0.0348, step time: 0.2524\n",
      "78/281, train_loss: 0.0814, step time: 0.2414\n",
      "79/281, train_loss: 0.0546, step time: 0.2431\n",
      "80/281, train_loss: 0.0412, step time: 0.2419\n",
      "81/281, train_loss: 0.0685, step time: 0.2495\n",
      "82/281, train_loss: 0.0514, step time: 0.2488\n",
      "83/281, train_loss: 0.0424, step time: 0.2457\n",
      "84/281, train_loss: 0.0700, step time: 0.2434\n",
      "85/281, train_loss: 0.0427, step time: 0.2453\n",
      "86/281, train_loss: 0.0437, step time: 0.2458\n",
      "87/281, train_loss: 0.0593, step time: 0.2449\n",
      "88/281, train_loss: 0.0408, step time: 0.2472\n",
      "89/281, train_loss: 0.0523, step time: 0.2468\n",
      "90/281, train_loss: 0.0754, step time: 0.2536\n",
      "91/281, train_loss: 0.0632, step time: 0.2433\n",
      "92/281, train_loss: 0.0362, step time: 0.2468\n",
      "93/281, train_loss: 0.2158, step time: 0.2516\n",
      "94/281, train_loss: 0.0541, step time: 0.2481\n",
      "95/281, train_loss: 0.0330, step time: 0.2495\n",
      "96/281, train_loss: 0.0462, step time: 0.2499\n",
      "97/281, train_loss: 0.0405, step time: 0.2453\n",
      "98/281, train_loss: 0.2059, step time: 0.2499\n",
      "99/281, train_loss: 0.0851, step time: 0.2525\n",
      "100/281, train_loss: 0.0562, step time: 0.2492\n",
      "101/281, train_loss: 0.0529, step time: 0.2474\n",
      "102/281, train_loss: 0.0364, step time: 0.2455\n",
      "103/281, train_loss: 0.0615, step time: 0.2506\n",
      "104/281, train_loss: 0.0542, step time: 0.2508\n",
      "105/281, train_loss: 0.0796, step time: 0.2537\n",
      "106/281, train_loss: 0.2010, step time: 0.2468\n",
      "107/281, train_loss: 0.0326, step time: 0.2471\n",
      "108/281, train_loss: 0.0561, step time: 0.2509\n",
      "109/281, train_loss: 0.0557, step time: 0.2504\n",
      "110/281, train_loss: 0.0637, step time: 0.2461\n",
      "111/281, train_loss: 0.2258, step time: 0.2466\n",
      "112/281, train_loss: 0.0551, step time: 0.2449\n",
      "113/281, train_loss: 0.0632, step time: 0.2458\n",
      "114/281, train_loss: 0.0427, step time: 0.2448\n",
      "115/281, train_loss: 0.0919, step time: 0.2529\n",
      "116/281, train_loss: 0.0577, step time: 0.2481\n",
      "117/281, train_loss: 0.0481, step time: 0.2475\n",
      "118/281, train_loss: 0.0350, step time: 0.2513\n",
      "119/281, train_loss: 0.2389, step time: 0.2463\n",
      "120/281, train_loss: 0.0820, step time: 0.2500\n",
      "121/281, train_loss: 0.0562, step time: 0.2463\n",
      "122/281, train_loss: 0.0555, step time: 0.2458\n",
      "123/281, train_loss: 0.0642, step time: 0.2467\n",
      "124/281, train_loss: 0.2025, step time: 0.2466\n",
      "125/281, train_loss: 0.2194, step time: 0.2457\n",
      "126/281, train_loss: 0.0657, step time: 0.2468\n",
      "127/281, train_loss: 0.0571, step time: 0.2501\n",
      "128/281, train_loss: 0.2294, step time: 0.2503\n",
      "129/281, train_loss: 0.0445, step time: 0.2476\n",
      "130/281, train_loss: 0.0560, step time: 0.2425\n",
      "131/281, train_loss: 0.0515, step time: 0.2575\n",
      "132/281, train_loss: 0.0544, step time: 0.2492\n",
      "133/281, train_loss: 0.2189, step time: 0.2570\n",
      "134/281, train_loss: 0.1064, step time: 0.2499\n",
      "135/281, train_loss: 0.0729, step time: 0.2470\n",
      "136/281, train_loss: 0.0440, step time: 0.2477\n",
      "137/281, train_loss: 0.0591, step time: 0.2511\n",
      "138/281, train_loss: 0.2274, step time: 0.2519\n",
      "139/281, train_loss: 0.0806, step time: 0.2487\n",
      "140/281, train_loss: 0.0366, step time: 0.2467\n",
      "141/281, train_loss: 0.2319, step time: 0.2466\n",
      "142/281, train_loss: 0.0501, step time: 0.2509\n",
      "143/281, train_loss: 0.0636, step time: 0.2462\n",
      "144/281, train_loss: 0.0792, step time: 0.2443\n",
      "145/281, train_loss: 0.0502, step time: 0.2454\n",
      "146/281, train_loss: 0.2148, step time: 0.2478\n",
      "147/281, train_loss: 0.0247, step time: 0.2435\n",
      "148/281, train_loss: 0.0369, step time: 0.2452\n",
      "149/281, train_loss: 0.2218, step time: 0.2400\n",
      "150/281, train_loss: 0.0906, step time: 0.2169\n",
      "151/281, train_loss: 0.0671, step time: 0.2405\n",
      "152/281, train_loss: 0.0471, step time: 0.2474\n",
      "153/281, train_loss: 0.0383, step time: 0.2506\n",
      "154/281, train_loss: 0.2067, step time: 0.2455\n",
      "155/281, train_loss: 0.0432, step time: 0.2455\n",
      "156/281, train_loss: 0.2172, step time: 0.2497\n",
      "157/281, train_loss: 0.0411, step time: 0.2436\n",
      "158/281, train_loss: 0.0578, step time: 0.2420\n",
      "159/281, train_loss: 0.0395, step time: 0.2389\n",
      "160/281, train_loss: 0.0819, step time: 0.2435\n",
      "161/281, train_loss: 0.2291, step time: 0.2436\n",
      "162/281, train_loss: 0.2032, step time: 0.2395\n",
      "163/281, train_loss: 0.0582, step time: 0.2432\n",
      "164/281, train_loss: 0.0298, step time: 0.2433\n",
      "165/281, train_loss: 0.0529, step time: 0.2453\n",
      "166/281, train_loss: 0.0283, step time: 0.2364\n",
      "167/281, train_loss: 0.2099, step time: 0.2424\n",
      "168/281, train_loss: 0.0600, step time: 0.2497\n",
      "169/281, train_loss: 0.0500, step time: 0.2475\n",
      "170/281, train_loss: 0.0489, step time: 0.2444\n",
      "171/281, train_loss: 0.2075, step time: 0.2441\n",
      "172/281, train_loss: 0.0771, step time: 0.2404\n",
      "173/281, train_loss: 0.0786, step time: 0.2416\n",
      "174/281, train_loss: 0.0475, step time: 0.2420\n",
      "175/281, train_loss: 0.0650, step time: 0.2436\n",
      "176/281, train_loss: 0.0501, step time: 0.2477\n",
      "177/281, train_loss: 0.0541, step time: 0.2441\n",
      "178/281, train_loss: 0.1945, step time: 0.2496\n",
      "179/281, train_loss: 0.0353, step time: 0.2468\n",
      "180/281, train_loss: 0.0650, step time: 0.2472\n",
      "181/281, train_loss: 0.0385, step time: 0.2519\n",
      "182/281, train_loss: 0.0690, step time: 0.2460\n",
      "183/281, train_loss: 0.0712, step time: 0.2510\n",
      "184/281, train_loss: 0.0612, step time: 0.2493\n",
      "185/281, train_loss: 0.0365, step time: 0.2488\n",
      "186/281, train_loss: 0.0400, step time: 0.2427\n",
      "187/281, train_loss: 0.0487, step time: 0.2425\n",
      "188/281, train_loss: 0.0428, step time: 0.2451\n",
      "189/281, train_loss: 0.2346, step time: 0.2417\n",
      "190/281, train_loss: 0.0678, step time: 0.2454\n",
      "191/281, train_loss: 0.0441, step time: 0.2488\n",
      "192/281, train_loss: 0.0627, step time: 0.2460\n",
      "193/281, train_loss: 0.0644, step time: 0.2515\n",
      "194/281, train_loss: 0.2137, step time: 0.2710\n",
      "195/281, train_loss: 0.0565, step time: 0.2661\n",
      "196/281, train_loss: 0.0914, step time: 0.2499\n",
      "197/281, train_loss: 0.2248, step time: 0.2521\n",
      "198/281, train_loss: 0.0601, step time: 0.2514\n",
      "199/281, train_loss: 0.2142, step time: 0.2477\n",
      "200/281, train_loss: 0.0654, step time: 0.2517\n",
      "201/281, train_loss: 0.0454, step time: 0.2524\n",
      "202/281, train_loss: 0.0416, step time: 0.2479\n",
      "203/281, train_loss: 0.0369, step time: 0.2495\n",
      "204/281, train_loss: 0.0750, step time: 0.2502\n",
      "205/281, train_loss: 0.0449, step time: 0.2502\n",
      "206/281, train_loss: 0.0446, step time: 0.2518\n",
      "207/281, train_loss: 0.0267, step time: 0.2494\n",
      "208/281, train_loss: 0.0596, step time: 0.2476\n",
      "209/281, train_loss: 0.2144, step time: 0.2443\n",
      "210/281, train_loss: 0.0506, step time: 0.2455\n",
      "211/281, train_loss: 0.2161, step time: 0.2474\n",
      "212/281, train_loss: 0.0504, step time: 0.2489\n",
      "213/281, train_loss: 0.0415, step time: 0.2459\n",
      "214/281, train_loss: 0.0572, step time: 0.2447\n",
      "215/281, train_loss: 0.0719, step time: 0.2527\n",
      "216/281, train_loss: 0.0459, step time: 0.2588\n",
      "217/281, train_loss: 0.0463, step time: 0.2458\n",
      "218/281, train_loss: 0.0395, step time: 0.2436\n",
      "219/281, train_loss: 0.0843, step time: 0.2534\n",
      "220/281, train_loss: 0.0570, step time: 0.2475\n",
      "221/281, train_loss: 0.1984, step time: 0.2449\n",
      "222/281, train_loss: 0.0600, step time: 0.2483\n",
      "223/281, train_loss: 0.1030, step time: 0.2517\n",
      "224/281, train_loss: 0.0436, step time: 0.2459\n",
      "225/281, train_loss: 0.0630, step time: 0.2474\n",
      "226/281, train_loss: 0.0724, step time: 0.2212\n",
      "227/281, train_loss: 0.0526, step time: 0.2486\n",
      "228/281, train_loss: 0.0658, step time: 0.2436\n",
      "229/281, train_loss: 0.2037, step time: 0.2424\n",
      "230/281, train_loss: 0.0386, step time: 0.2438\n",
      "231/281, train_loss: 0.0771, step time: 0.2483\n",
      "232/281, train_loss: 0.2151, step time: 0.2446\n",
      "233/281, train_loss: 0.0373, step time: 0.2422\n",
      "234/281, train_loss: 0.2243, step time: 0.2453\n",
      "235/281, train_loss: 0.0759, step time: 0.2478\n",
      "236/281, train_loss: 0.0646, step time: 0.2474\n",
      "237/281, train_loss: 0.0486, step time: 0.2443\n",
      "238/281, train_loss: 0.0552, step time: 0.2437\n",
      "239/281, train_loss: 0.0740, step time: 0.2513\n",
      "240/281, train_loss: 0.0291, step time: 0.2512\n",
      "241/281, train_loss: 0.2197, step time: 0.2474\n",
      "242/281, train_loss: 0.0655, step time: 0.2477\n",
      "243/281, train_loss: 0.0729, step time: 0.2528\n",
      "244/281, train_loss: 0.0350, step time: 0.2473\n",
      "245/281, train_loss: 0.0630, step time: 0.2537\n",
      "246/281, train_loss: 0.0712, step time: 0.2540\n",
      "247/281, train_loss: 0.0619, step time: 0.2463\n",
      "248/281, train_loss: 0.0623, step time: 0.2483\n",
      "249/281, train_loss: 0.0460, step time: 0.2445\n",
      "250/281, train_loss: 0.0623, step time: 0.2419\n",
      "251/281, train_loss: 0.0300, step time: 0.2459\n",
      "252/281, train_loss: 0.0674, step time: 0.2501\n",
      "253/281, train_loss: 0.0425, step time: 0.2461\n",
      "254/281, train_loss: 0.0444, step time: 0.2515\n",
      "255/281, train_loss: 0.0758, step time: 0.2491\n",
      "256/281, train_loss: 0.0726, step time: 0.2471\n",
      "257/281, train_loss: 0.0490, step time: 0.2463\n",
      "258/281, train_loss: 0.0346, step time: 0.2463\n",
      "259/281, train_loss: 0.0569, step time: 0.2508\n",
      "260/281, train_loss: 0.2115, step time: 0.2464\n",
      "261/281, train_loss: 0.0712, step time: 0.2432\n",
      "262/281, train_loss: 0.0414, step time: 0.2528\n",
      "263/281, train_loss: 0.0562, step time: 0.2477\n",
      "264/281, train_loss: 0.0505, step time: 0.2448\n",
      "265/281, train_loss: 0.2138, step time: 0.2550\n",
      "266/281, train_loss: 0.0446, step time: 0.2513\n",
      "267/281, train_loss: 0.0830, step time: 0.2467\n",
      "268/281, train_loss: 0.0413, step time: 0.2511\n",
      "269/281, train_loss: 0.0762, step time: 0.2451\n",
      "270/281, train_loss: 0.0622, step time: 0.2562\n",
      "271/281, train_loss: 0.0437, step time: 0.2487\n",
      "272/281, train_loss: 0.1983, step time: 0.2498\n",
      "273/281, train_loss: 0.0355, step time: 0.2473\n",
      "274/281, train_loss: 0.0532, step time: 0.2508\n",
      "275/281, train_loss: 0.0428, step time: 0.2481\n",
      "276/281, train_loss: 0.0678, step time: 0.2505\n",
      "277/281, train_loss: 0.0388, step time: 0.2526\n",
      "278/281, train_loss: 0.0396, step time: 0.2444\n",
      "279/281, train_loss: 0.0668, step time: 0.2500\n",
      "280/281, train_loss: 0.0441, step time: 0.2532\n",
      "281/281, train_loss: 0.0349, step time: 0.2497\n",
      "282/281, train_loss: 0.0932, step time: 0.1489\n",
      "epoch 123 average loss: 0.0798\n",
      "current epoch: 123 current mean dice: 0.9035 tc: 0.8971 wt: 0.9271 et: 0.8958\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 123 is: 393.0619\n",
      "----------\n",
      "epoch 124/200\n",
      "1/281, train_loss: 0.0422, step time: 0.2997\n",
      "2/281, train_loss: 0.0458, step time: 0.2713\n",
      "3/281, train_loss: 0.0561, step time: 0.2453\n",
      "4/281, train_loss: 0.0311, step time: 0.2504\n",
      "5/281, train_loss: 0.2089, step time: 0.2530\n",
      "6/281, train_loss: 0.0499, step time: 0.2511\n",
      "7/281, train_loss: 0.0400, step time: 0.2469\n",
      "8/281, train_loss: 0.0246, step time: 0.2476\n",
      "9/281, train_loss: 0.0678, step time: 0.2478\n",
      "10/281, train_loss: 0.0510, step time: 0.2519\n",
      "11/281, train_loss: 0.2206, step time: 0.2531\n",
      "12/281, train_loss: 0.0650, step time: 0.2456\n",
      "13/281, train_loss: 0.0451, step time: 0.2499\n",
      "14/281, train_loss: 0.0454, step time: 0.2486\n",
      "15/281, train_loss: 0.0502, step time: 0.2495\n",
      "16/281, train_loss: 0.0435, step time: 0.2500\n",
      "17/281, train_loss: 0.0617, step time: 0.2531\n",
      "18/281, train_loss: 0.0498, step time: 0.2504\n",
      "19/281, train_loss: 0.0730, step time: 0.2498\n",
      "20/281, train_loss: 0.0680, step time: 0.2472\n",
      "21/281, train_loss: 0.0636, step time: 0.2527\n",
      "22/281, train_loss: 0.2128, step time: 0.2509\n",
      "23/281, train_loss: 0.0439, step time: 0.2456\n",
      "24/281, train_loss: 0.0438, step time: 0.2522\n",
      "25/281, train_loss: 0.0467, step time: 0.2488\n",
      "26/281, train_loss: 0.0411, step time: 0.2586\n",
      "27/281, train_loss: 0.0681, step time: 0.2561\n",
      "28/281, train_loss: 0.1934, step time: 0.2496\n",
      "29/281, train_loss: 0.1971, step time: 0.2489\n",
      "30/281, train_loss: 0.2222, step time: 0.2513\n",
      "31/281, train_loss: 0.2181, step time: 0.2487\n",
      "32/281, train_loss: 0.0350, step time: 0.2525\n",
      "33/281, train_loss: 0.0679, step time: 0.2479\n",
      "34/281, train_loss: 0.0556, step time: 0.2480\n",
      "35/281, train_loss: 0.2050, step time: 0.2471\n",
      "36/281, train_loss: 0.0632, step time: 0.2488\n",
      "37/281, train_loss: 0.2247, step time: 0.2508\n",
      "38/281, train_loss: 0.0597, step time: 0.2545\n",
      "39/281, train_loss: 0.0643, step time: 0.2472\n",
      "40/281, train_loss: 0.0725, step time: 0.2415\n",
      "41/281, train_loss: 0.0447, step time: 0.2521\n",
      "42/281, train_loss: 0.0491, step time: 0.2502\n",
      "43/281, train_loss: 0.0508, step time: 0.2506\n",
      "44/281, train_loss: 0.0391, step time: 0.2491\n",
      "45/281, train_loss: 0.0546, step time: 0.2501\n",
      "46/281, train_loss: 0.0571, step time: 0.2495\n",
      "47/281, train_loss: 0.2073, step time: 0.2448\n",
      "48/281, train_loss: 0.0391, step time: 0.2485\n",
      "49/281, train_loss: 0.0774, step time: 0.2462\n",
      "50/281, train_loss: 0.0474, step time: 0.2487\n",
      "51/281, train_loss: 0.0911, step time: 0.2451\n",
      "52/281, train_loss: 0.0416, step time: 0.2507\n",
      "53/281, train_loss: 0.0668, step time: 0.2480\n",
      "54/281, train_loss: 0.0541, step time: 0.2468\n",
      "55/281, train_loss: 0.2239, step time: 0.2500\n",
      "56/281, train_loss: 0.0730, step time: 0.2497\n",
      "57/281, train_loss: 0.0567, step time: 0.2530\n",
      "58/281, train_loss: 0.0663, step time: 0.2482\n",
      "59/281, train_loss: 0.0774, step time: 0.2483\n",
      "60/281, train_loss: 0.0816, step time: 0.2467\n",
      "61/281, train_loss: 0.2149, step time: 0.2472\n",
      "62/281, train_loss: 0.0672, step time: 0.2449\n",
      "63/281, train_loss: 0.0408, step time: 0.2446\n",
      "64/281, train_loss: 0.1920, step time: 0.2467\n",
      "65/281, train_loss: 0.0442, step time: 0.2530\n",
      "66/281, train_loss: 0.2197, step time: 0.2521\n",
      "67/281, train_loss: 0.0484, step time: 0.2462\n",
      "68/281, train_loss: 0.0685, step time: 0.2522\n",
      "69/281, train_loss: 0.0778, step time: 0.2488\n",
      "70/281, train_loss: 0.0714, step time: 0.2490\n",
      "71/281, train_loss: 0.2102, step time: 0.2434\n",
      "72/281, train_loss: 0.0705, step time: 0.2477\n",
      "73/281, train_loss: 0.0421, step time: 0.2494\n",
      "74/281, train_loss: 0.2145, step time: 0.2534\n",
      "75/281, train_loss: 0.0387, step time: 0.2502\n",
      "76/281, train_loss: 0.0492, step time: 0.2531\n",
      "77/281, train_loss: 0.0640, step time: 0.2536\n",
      "78/281, train_loss: 0.0825, step time: 0.2499\n",
      "79/281, train_loss: 0.0636, step time: 0.2512\n",
      "80/281, train_loss: 0.0403, step time: 0.2492\n",
      "81/281, train_loss: 0.0567, step time: 0.2463\n",
      "82/281, train_loss: 0.0364, step time: 0.2495\n",
      "83/281, train_loss: 0.0438, step time: 0.2494\n",
      "84/281, train_loss: 0.0777, step time: 0.2512\n",
      "85/281, train_loss: 0.0489, step time: 0.2453\n",
      "86/281, train_loss: 0.0365, step time: 0.2487\n",
      "87/281, train_loss: 0.0517, step time: 0.2495\n",
      "88/281, train_loss: 0.0461, step time: 0.2461\n",
      "89/281, train_loss: 0.2478, step time: 0.2507\n",
      "90/281, train_loss: 0.0463, step time: 0.2499\n",
      "91/281, train_loss: 0.0504, step time: 0.2469\n",
      "92/281, train_loss: 0.0824, step time: 0.2471\n",
      "93/281, train_loss: 0.0752, step time: 0.2503\n",
      "94/281, train_loss: 0.3655, step time: 0.2527\n",
      "95/281, train_loss: 0.0606, step time: 0.2532\n",
      "96/281, train_loss: 0.0359, step time: 0.2491\n",
      "97/281, train_loss: 0.0679, step time: 0.2490\n",
      "98/281, train_loss: 0.2093, step time: 0.2463\n",
      "99/281, train_loss: 0.0506, step time: 0.2443\n",
      "100/281, train_loss: 0.0418, step time: 0.2443\n",
      "101/281, train_loss: 0.0644, step time: 0.2524\n",
      "102/281, train_loss: 0.0949, step time: 0.2476\n",
      "103/281, train_loss: 0.0794, step time: 0.2461\n",
      "104/281, train_loss: 0.0370, step time: 0.2540\n",
      "105/281, train_loss: 0.0459, step time: 0.2559\n",
      "106/281, train_loss: 0.0829, step time: 0.2557\n",
      "107/281, train_loss: 0.0431, step time: 0.2570\n",
      "108/281, train_loss: 0.0593, step time: 0.2570\n",
      "109/281, train_loss: 0.0361, step time: 0.2480\n",
      "110/281, train_loss: 0.0477, step time: 0.2499\n",
      "111/281, train_loss: 0.0712, step time: 0.2508\n",
      "112/281, train_loss: 0.0503, step time: 0.2511\n",
      "113/281, train_loss: 0.2257, step time: 0.2502\n",
      "114/281, train_loss: 0.0454, step time: 0.2482\n",
      "115/281, train_loss: 0.0250, step time: 0.2518\n",
      "116/281, train_loss: 0.0706, step time: 0.2644\n",
      "117/281, train_loss: 0.0694, step time: 0.2521\n",
      "118/281, train_loss: 0.0631, step time: 0.2547\n",
      "119/281, train_loss: 0.0466, step time: 0.2495\n",
      "120/281, train_loss: 0.0590, step time: 0.2507\n",
      "121/281, train_loss: 0.0355, step time: 0.2530\n",
      "122/281, train_loss: 0.0433, step time: 0.2584\n",
      "123/281, train_loss: 0.0694, step time: 0.2508\n",
      "124/281, train_loss: 0.0364, step time: 0.2508\n",
      "125/281, train_loss: 0.1203, step time: 0.2537\n",
      "126/281, train_loss: 0.2084, step time: 0.2534\n",
      "127/281, train_loss: 0.2262, step time: 0.2578\n",
      "128/281, train_loss: 0.0375, step time: 0.2557\n",
      "129/281, train_loss: 0.0576, step time: 0.2492\n",
      "130/281, train_loss: 0.0396, step time: 0.2545\n",
      "131/281, train_loss: 0.0563, step time: 0.2521\n",
      "132/281, train_loss: 0.0645, step time: 0.2522\n",
      "133/281, train_loss: 0.0476, step time: 0.2479\n",
      "134/281, train_loss: 0.0430, step time: 0.2448\n",
      "135/281, train_loss: 0.1036, step time: 0.2539\n",
      "136/281, train_loss: 0.0554, step time: 0.2508\n",
      "137/281, train_loss: 0.0676, step time: 0.2535\n",
      "138/281, train_loss: 0.0749, step time: 0.2563\n",
      "139/281, train_loss: 0.0607, step time: 0.2526\n",
      "140/281, train_loss: 0.0723, step time: 0.2566\n",
      "141/281, train_loss: 0.0578, step time: 0.2588\n",
      "142/281, train_loss: 0.0534, step time: 0.2627\n",
      "143/281, train_loss: 0.0616, step time: 0.2496\n",
      "144/281, train_loss: 0.2477, step time: 0.2501\n",
      "145/281, train_loss: 0.0462, step time: 0.2577\n",
      "146/281, train_loss: 0.0468, step time: 0.2512\n",
      "147/281, train_loss: 0.0414, step time: 0.2508\n",
      "148/281, train_loss: 0.0471, step time: 0.2531\n",
      "149/281, train_loss: 0.2071, step time: 0.2539\n",
      "150/281, train_loss: 0.0456, step time: 0.2512\n",
      "151/281, train_loss: 0.0501, step time: 0.2514\n",
      "152/281, train_loss: 0.0673, step time: 0.2477\n",
      "153/281, train_loss: 0.0591, step time: 0.2483\n",
      "154/281, train_loss: 0.0658, step time: 0.2502\n",
      "155/281, train_loss: 0.0525, step time: 0.2434\n",
      "156/281, train_loss: 0.0522, step time: 0.2654\n",
      "157/281, train_loss: 0.0750, step time: 0.2505\n",
      "158/281, train_loss: 0.0562, step time: 0.2460\n",
      "159/281, train_loss: 0.1057, step time: 0.2489\n",
      "160/281, train_loss: 0.0677, step time: 0.2574\n",
      "161/281, train_loss: 0.0839, step time: 0.2492\n",
      "162/281, train_loss: 0.0336, step time: 0.2482\n",
      "163/281, train_loss: 0.0635, step time: 0.2449\n",
      "164/281, train_loss: 0.0758, step time: 0.2419\n",
      "165/281, train_loss: 0.0639, step time: 0.2430\n",
      "166/281, train_loss: 0.2608, step time: 0.2498\n",
      "167/281, train_loss: 0.0849, step time: 0.2497\n",
      "168/281, train_loss: 0.0581, step time: 0.2428\n",
      "169/281, train_loss: 0.0641, step time: 0.2466\n",
      "170/281, train_loss: 0.2195, step time: 0.2434\n",
      "171/281, train_loss: 0.1316, step time: 0.2467\n",
      "172/281, train_loss: 0.0355, step time: 0.2464\n",
      "173/281, train_loss: 0.0687, step time: 0.2486\n",
      "174/281, train_loss: 0.0736, step time: 0.2543\n",
      "175/281, train_loss: 0.0780, step time: 0.2487\n",
      "176/281, train_loss: 0.2171, step time: 0.2520\n",
      "177/281, train_loss: 0.0911, step time: 0.2490\n",
      "178/281, train_loss: 0.0565, step time: 0.2510\n",
      "179/281, train_loss: 0.0672, step time: 0.2682\n",
      "180/281, train_loss: 0.0358, step time: 0.2543\n",
      "181/281, train_loss: 0.0586, step time: 0.2475\n",
      "182/281, train_loss: 0.2071, step time: 0.2469\n",
      "183/281, train_loss: 0.2098, step time: 0.2499\n",
      "184/281, train_loss: 0.2317, step time: 0.2459\n",
      "185/281, train_loss: 0.2173, step time: 0.2505\n",
      "186/281, train_loss: 0.2165, step time: 0.2504\n",
      "187/281, train_loss: 0.0488, step time: 0.2456\n",
      "188/281, train_loss: 0.0292, step time: 0.2479\n",
      "189/281, train_loss: 0.0549, step time: 0.2481\n",
      "190/281, train_loss: 0.0642, step time: 0.2487\n",
      "191/281, train_loss: 0.0541, step time: 0.2469\n",
      "192/281, train_loss: 0.0454, step time: 0.2471\n",
      "193/281, train_loss: 0.0461, step time: 0.2598\n",
      "194/281, train_loss: 0.0516, step time: 0.2518\n",
      "195/281, train_loss: 0.0614, step time: 0.2558\n",
      "196/281, train_loss: 0.0423, step time: 0.2476\n",
      "197/281, train_loss: 0.0623, step time: 0.2507\n",
      "198/281, train_loss: 0.0644, step time: 0.2446\n",
      "199/281, train_loss: 0.3726, step time: 0.2495\n",
      "200/281, train_loss: 0.2067, step time: 0.2467\n",
      "201/281, train_loss: 0.0559, step time: 0.2535\n",
      "202/281, train_loss: 0.0453, step time: 0.2543\n",
      "203/281, train_loss: 0.0619, step time: 0.2520\n",
      "204/281, train_loss: 0.0347, step time: 0.2515\n",
      "205/281, train_loss: 0.0926, step time: 0.2550\n",
      "206/281, train_loss: 0.0481, step time: 0.2553\n",
      "207/281, train_loss: 0.2217, step time: 0.2531\n",
      "208/281, train_loss: 0.0654, step time: 0.2607\n",
      "209/281, train_loss: 0.0853, step time: 0.2746\n",
      "210/281, train_loss: 0.0809, step time: 0.2503\n",
      "211/281, train_loss: 0.0489, step time: 0.2578\n",
      "212/281, train_loss: 0.0781, step time: 0.2552\n",
      "213/281, train_loss: 0.0653, step time: 0.2504\n",
      "214/281, train_loss: 0.0601, step time: 0.2505\n",
      "215/281, train_loss: 0.0454, step time: 0.2522\n",
      "216/281, train_loss: 0.0321, step time: 0.2494\n",
      "217/281, train_loss: 0.0498, step time: 0.2501\n",
      "218/281, train_loss: 0.0641, step time: 0.2502\n",
      "219/281, train_loss: 0.2076, step time: 0.2474\n",
      "220/281, train_loss: 0.2190, step time: 0.2474\n",
      "221/281, train_loss: 0.0814, step time: 0.2501\n",
      "222/281, train_loss: 0.0556, step time: 0.2512\n",
      "223/281, train_loss: 0.0538, step time: 0.2483\n",
      "224/281, train_loss: 0.0726, step time: 0.2524\n",
      "225/281, train_loss: 0.0757, step time: 0.2461\n",
      "226/281, train_loss: 0.0661, step time: 0.2535\n",
      "227/281, train_loss: 0.0440, step time: 0.2487\n",
      "228/281, train_loss: 0.0492, step time: 0.2508\n",
      "229/281, train_loss: 0.0376, step time: 0.2477\n",
      "230/281, train_loss: 0.0690, step time: 0.2502\n",
      "231/281, train_loss: 0.0609, step time: 0.2488\n",
      "232/281, train_loss: 0.0421, step time: 0.2472\n",
      "233/281, train_loss: 0.0441, step time: 0.2425\n",
      "234/281, train_loss: 0.2138, step time: 0.2534\n",
      "235/281, train_loss: 0.0891, step time: 0.2517\n",
      "236/281, train_loss: 0.0694, step time: 0.2466\n",
      "237/281, train_loss: 0.0654, step time: 0.2484\n",
      "238/281, train_loss: 0.0638, step time: 0.2535\n",
      "239/281, train_loss: 0.0688, step time: 0.2475\n",
      "240/281, train_loss: 0.0692, step time: 0.2498\n",
      "241/281, train_loss: 0.0880, step time: 0.2474\n",
      "242/281, train_loss: 0.0405, step time: 0.2474\n",
      "243/281, train_loss: 0.0309, step time: 0.2527\n",
      "244/281, train_loss: 0.0656, step time: 0.2575\n",
      "245/281, train_loss: 0.0382, step time: 0.2541\n",
      "246/281, train_loss: 0.0670, step time: 0.2487\n",
      "247/281, train_loss: 0.0379, step time: 0.2525\n",
      "248/281, train_loss: 0.0761, step time: 0.2468\n",
      "249/281, train_loss: 0.0369, step time: 0.2495\n",
      "250/281, train_loss: 0.0753, step time: 0.2505\n",
      "251/281, train_loss: 0.0646, step time: 0.2487\n",
      "252/281, train_loss: 0.0416, step time: 0.2412\n",
      "253/281, train_loss: 0.0785, step time: 0.2408\n",
      "254/281, train_loss: 0.0467, step time: 0.2479\n",
      "255/281, train_loss: 0.0428, step time: 0.2496\n",
      "256/281, train_loss: 0.2224, step time: 0.2504\n",
      "257/281, train_loss: 0.0825, step time: 0.2518\n",
      "258/281, train_loss: 0.0268, step time: 0.2476\n",
      "259/281, train_loss: 0.0603, step time: 0.2485\n",
      "260/281, train_loss: 0.0650, step time: 0.2500\n",
      "261/281, train_loss: 0.0609, step time: 0.2524\n",
      "262/281, train_loss: 0.0367, step time: 0.2501\n",
      "263/281, train_loss: 0.0484, step time: 0.2462\n",
      "264/281, train_loss: 0.0675, step time: 0.2424\n",
      "265/281, train_loss: 0.0747, step time: 0.2425\n",
      "266/281, train_loss: 0.0501, step time: 0.2547\n",
      "267/281, train_loss: 0.0683, step time: 0.2510\n",
      "268/281, train_loss: 0.0378, step time: 0.2432\n",
      "269/281, train_loss: 0.0314, step time: 0.2472\n",
      "270/281, train_loss: 0.0790, step time: 0.2521\n",
      "271/281, train_loss: 0.0527, step time: 0.2488\n",
      "272/281, train_loss: 0.0239, step time: 0.2469\n",
      "273/281, train_loss: 0.0768, step time: 0.2423\n",
      "274/281, train_loss: 0.0589, step time: 0.2468\n",
      "275/281, train_loss: 0.0625, step time: 0.2503\n",
      "276/281, train_loss: 0.0640, step time: 0.2484\n",
      "277/281, train_loss: 0.0439, step time: 0.2482\n",
      "278/281, train_loss: 0.0258, step time: 0.2474\n",
      "279/281, train_loss: 0.0834, step time: 0.2455\n",
      "280/281, train_loss: 0.0580, step time: 0.2517\n",
      "281/281, train_loss: 0.0739, step time: 0.2472\n",
      "282/281, train_loss: 0.0539, step time: 0.1471\n",
      "epoch 124 average loss: 0.0809\n",
      "current epoch: 124 current mean dice: 0.9026 tc: 0.8979 wt: 0.9255 et: 0.8932\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 124 is: 407.5761\n",
      "----------\n",
      "epoch 125/200\n",
      "1/281, train_loss: 0.0670, step time: 0.2595\n",
      "2/281, train_loss: 0.0761, step time: 0.2492\n",
      "3/281, train_loss: 0.2038, step time: 0.2456\n",
      "4/281, train_loss: 0.0440, step time: 0.2464\n",
      "5/281, train_loss: 0.1262, step time: 0.2508\n",
      "6/281, train_loss: 0.0648, step time: 0.2495\n",
      "7/281, train_loss: 0.0644, step time: 0.2463\n",
      "8/281, train_loss: 0.0456, step time: 0.2505\n",
      "9/281, train_loss: 0.2287, step time: 0.2454\n",
      "10/281, train_loss: 0.0439, step time: 0.2542\n",
      "11/281, train_loss: 0.2226, step time: 0.2632\n",
      "12/281, train_loss: 0.0469, step time: 0.2606\n",
      "13/281, train_loss: 0.2043, step time: 0.2533\n",
      "14/281, train_loss: 0.0475, step time: 0.2481\n",
      "15/281, train_loss: 0.0647, step time: 0.2515\n",
      "16/281, train_loss: 0.0403, step time: 0.2524\n",
      "17/281, train_loss: 0.0415, step time: 0.2477\n",
      "18/281, train_loss: 0.0413, step time: 0.2469\n",
      "19/281, train_loss: 0.0485, step time: 0.2416\n",
      "20/281, train_loss: 0.0495, step time: 0.2507\n",
      "21/281, train_loss: 0.0668, step time: 0.2489\n",
      "22/281, train_loss: 0.0600, step time: 0.2431\n",
      "23/281, train_loss: 0.0738, step time: 0.2422\n",
      "24/281, train_loss: 0.2107, step time: 0.2454\n",
      "25/281, train_loss: 0.0817, step time: 0.2437\n",
      "26/281, train_loss: 0.2217, step time: 0.2472\n",
      "27/281, train_loss: 0.0369, step time: 0.2517\n",
      "28/281, train_loss: 0.0313, step time: 0.2464\n",
      "29/281, train_loss: 0.0651, step time: 0.2471\n",
      "30/281, train_loss: 0.0627, step time: 0.2501\n",
      "31/281, train_loss: 0.0662, step time: 0.2521\n",
      "32/281, train_loss: 0.0759, step time: 0.2560\n",
      "33/281, train_loss: 0.0399, step time: 0.2503\n",
      "34/281, train_loss: 0.0541, step time: 0.2453\n",
      "35/281, train_loss: 0.2144, step time: 0.2428\n",
      "36/281, train_loss: 0.0579, step time: 0.2458\n",
      "37/281, train_loss: 0.0617, step time: 0.2523\n",
      "38/281, train_loss: 0.0681, step time: 0.2487\n",
      "39/281, train_loss: 0.0469, step time: 0.2551\n",
      "40/281, train_loss: 0.0668, step time: 0.2551\n",
      "41/281, train_loss: 0.2308, step time: 0.2528\n",
      "42/281, train_loss: 0.0297, step time: 0.2519\n",
      "43/281, train_loss: 0.0790, step time: 0.2485\n",
      "44/281, train_loss: 0.2210, step time: 0.2557\n",
      "45/281, train_loss: 0.0383, step time: 0.2543\n",
      "46/281, train_loss: 0.0757, step time: 0.2469\n",
      "47/281, train_loss: 0.0637, step time: 0.2445\n",
      "48/281, train_loss: 0.0702, step time: 0.2445\n",
      "49/281, train_loss: 0.0539, step time: 0.2509\n",
      "50/281, train_loss: 0.0381, step time: 0.2504\n",
      "51/281, train_loss: 0.0375, step time: 0.2472\n",
      "52/281, train_loss: 0.0523, step time: 0.2409\n",
      "53/281, train_loss: 0.0595, step time: 0.2401\n",
      "54/281, train_loss: 0.0419, step time: 0.2428\n",
      "55/281, train_loss: 0.0518, step time: 0.2419\n",
      "56/281, train_loss: 0.0850, step time: 0.2441\n",
      "57/281, train_loss: 0.0520, step time: 0.2454\n",
      "58/281, train_loss: 0.0491, step time: 0.2450\n",
      "59/281, train_loss: 0.2178, step time: 0.2468\n",
      "60/281, train_loss: 0.0433, step time: 0.2416\n",
      "61/281, train_loss: 0.0558, step time: 0.2464\n",
      "62/281, train_loss: 0.0751, step time: 0.2468\n",
      "63/281, train_loss: 0.0563, step time: 0.2491\n",
      "64/281, train_loss: 0.0576, step time: 0.2426\n",
      "65/281, train_loss: 0.1954, step time: 0.2499\n",
      "66/281, train_loss: 0.0676, step time: 0.2474\n",
      "67/281, train_loss: 0.0973, step time: 0.2492\n",
      "68/281, train_loss: 0.0512, step time: 0.2508\n",
      "69/281, train_loss: 0.0611, step time: 0.2517\n",
      "70/281, train_loss: 0.0517, step time: 0.2410\n",
      "71/281, train_loss: 0.0289, step time: 0.2427\n",
      "72/281, train_loss: 0.2003, step time: 0.2583\n",
      "73/281, train_loss: 0.0455, step time: 0.2488\n",
      "74/281, train_loss: 0.0450, step time: 0.2494\n",
      "75/281, train_loss: 0.0746, step time: 0.2464\n",
      "76/281, train_loss: 0.0518, step time: 0.2430\n",
      "77/281, train_loss: 0.0617, step time: 0.2486\n",
      "78/281, train_loss: 0.0725, step time: 0.2482\n",
      "79/281, train_loss: 0.0354, step time: 0.2482\n",
      "80/281, train_loss: 0.1226, step time: 0.2489\n",
      "81/281, train_loss: 0.0394, step time: 0.2556\n",
      "82/281, train_loss: 0.2446, step time: 0.2468\n",
      "83/281, train_loss: 0.0445, step time: 0.2492\n",
      "84/281, train_loss: 0.0361, step time: 0.2499\n",
      "85/281, train_loss: 0.0764, step time: 0.2489\n",
      "86/281, train_loss: 0.0803, step time: 0.2445\n",
      "87/281, train_loss: 0.0649, step time: 0.2452\n",
      "88/281, train_loss: 0.0468, step time: 0.2459\n",
      "89/281, train_loss: 0.0780, step time: 0.2432\n",
      "90/281, train_loss: 0.0563, step time: 0.2445\n",
      "91/281, train_loss: 0.0476, step time: 0.2437\n",
      "92/281, train_loss: 0.0385, step time: 0.2421\n",
      "93/281, train_loss: 0.0598, step time: 0.2458\n",
      "94/281, train_loss: 0.0656, step time: 0.2431\n",
      "95/281, train_loss: 0.0550, step time: 0.2405\n",
      "96/281, train_loss: 0.0952, step time: 0.2485\n",
      "97/281, train_loss: 0.0653, step time: 0.2534\n",
      "98/281, train_loss: 0.0430, step time: 0.2567\n",
      "99/281, train_loss: 0.0551, step time: 0.2469\n",
      "100/281, train_loss: 0.0723, step time: 0.2529\n",
      "101/281, train_loss: 0.0723, step time: 0.2534\n",
      "102/281, train_loss: 0.0734, step time: 0.2512\n",
      "103/281, train_loss: 0.0532, step time: 0.2512\n",
      "104/281, train_loss: 0.0490, step time: 0.2513\n",
      "105/281, train_loss: 0.0821, step time: 0.2539\n",
      "106/281, train_loss: 0.0480, step time: 0.2549\n",
      "107/281, train_loss: 0.0418, step time: 0.2517\n",
      "108/281, train_loss: 0.0645, step time: 0.2473\n",
      "109/281, train_loss: 0.0346, step time: 0.2485\n",
      "110/281, train_loss: 0.0378, step time: 0.2524\n",
      "111/281, train_loss: 0.0423, step time: 0.2467\n",
      "112/281, train_loss: 0.0811, step time: 0.2519\n",
      "113/281, train_loss: 0.0477, step time: 0.2495\n",
      "114/281, train_loss: 0.0471, step time: 0.2459\n",
      "115/281, train_loss: 0.0552, step time: 0.2461\n",
      "116/281, train_loss: 0.0810, step time: 0.2465\n",
      "117/281, train_loss: 0.2063, step time: 0.2519\n",
      "118/281, train_loss: 0.0380, step time: 0.2616\n",
      "119/281, train_loss: 0.0379, step time: 0.2487\n",
      "120/281, train_loss: 0.0877, step time: 0.2542\n",
      "121/281, train_loss: 0.2114, step time: 0.2551\n",
      "122/281, train_loss: 0.0506, step time: 0.2549\n",
      "123/281, train_loss: 0.0625, step time: 0.2474\n",
      "124/281, train_loss: 0.0442, step time: 0.2505\n",
      "125/281, train_loss: 0.0829, step time: 0.2453\n",
      "126/281, train_loss: 0.0741, step time: 0.2486\n",
      "127/281, train_loss: 0.1013, step time: 0.2495\n",
      "128/281, train_loss: 0.0664, step time: 0.2459\n",
      "129/281, train_loss: 0.2413, step time: 0.2504\n",
      "130/281, train_loss: 0.0588, step time: 0.2508\n",
      "131/281, train_loss: 0.0908, step time: 0.2509\n",
      "132/281, train_loss: 0.0771, step time: 0.2488\n",
      "133/281, train_loss: 0.0532, step time: 0.2436\n",
      "134/281, train_loss: 0.2106, step time: 0.2453\n",
      "135/281, train_loss: 0.4148, step time: 0.2468\n",
      "136/281, train_loss: 0.2233, step time: 0.2465\n",
      "137/281, train_loss: 0.0577, step time: 0.2471\n",
      "138/281, train_loss: 0.0528, step time: 0.2550\n",
      "139/281, train_loss: 0.0410, step time: 0.2521\n",
      "140/281, train_loss: 0.0575, step time: 0.2559\n",
      "141/281, train_loss: 0.0704, step time: 0.2476\n",
      "142/281, train_loss: 0.0522, step time: 0.2502\n",
      "143/281, train_loss: 0.0546, step time: 0.2498\n",
      "144/281, train_loss: 0.0460, step time: 0.2493\n",
      "145/281, train_loss: 0.2083, step time: 0.2489\n",
      "146/281, train_loss: 0.0365, step time: 0.2540\n",
      "147/281, train_loss: 0.2202, step time: 0.2479\n",
      "148/281, train_loss: 0.2518, step time: 0.2432\n",
      "149/281, train_loss: 0.0554, step time: 0.2588\n",
      "150/281, train_loss: 0.0257, step time: 0.2464\n",
      "151/281, train_loss: 0.0535, step time: 0.2529\n",
      "152/281, train_loss: 0.0576, step time: 0.2501\n",
      "153/281, train_loss: 0.0913, step time: 0.2475\n",
      "154/281, train_loss: 0.0534, step time: 0.2506\n",
      "155/281, train_loss: 0.0488, step time: 0.2490\n",
      "156/281, train_loss: 0.0483, step time: 0.2439\n",
      "157/281, train_loss: 0.0778, step time: 0.2471\n",
      "158/281, train_loss: 0.0842, step time: 0.2485\n",
      "159/281, train_loss: 0.0585, step time: 0.2529\n",
      "160/281, train_loss: 0.0763, step time: 0.2501\n",
      "161/281, train_loss: 0.0450, step time: 0.2488\n",
      "162/281, train_loss: 0.0534, step time: 0.2448\n",
      "163/281, train_loss: 0.0621, step time: 0.2492\n",
      "164/281, train_loss: 0.2294, step time: 0.2435\n",
      "165/281, train_loss: 0.0569, step time: 0.2404\n",
      "166/281, train_loss: 0.0658, step time: 0.2483\n",
      "167/281, train_loss: 0.1892, step time: 0.2479\n",
      "168/281, train_loss: 0.0572, step time: 0.2446\n",
      "169/281, train_loss: 0.2041, step time: 0.2433\n",
      "170/281, train_loss: 0.0424, step time: 0.2436\n",
      "171/281, train_loss: 0.2144, step time: 0.2532\n",
      "172/281, train_loss: 0.0454, step time: 0.2466\n",
      "173/281, train_loss: 0.0517, step time: 0.2500\n",
      "174/281, train_loss: 0.0425, step time: 0.2473\n",
      "175/281, train_loss: 0.0662, step time: 0.2428\n",
      "176/281, train_loss: 0.0624, step time: 0.2458\n",
      "177/281, train_loss: 0.0452, step time: 0.2520\n",
      "178/281, train_loss: 0.0363, step time: 0.2526\n",
      "179/281, train_loss: 0.0419, step time: 0.2431\n",
      "180/281, train_loss: 0.0711, step time: 0.2476\n",
      "181/281, train_loss: 0.2077, step time: 0.2472\n",
      "182/281, train_loss: 0.0462, step time: 0.2481\n",
      "183/281, train_loss: 0.0577, step time: 0.2509\n",
      "184/281, train_loss: 0.0597, step time: 0.2414\n",
      "185/281, train_loss: 0.0606, step time: 0.2413\n",
      "186/281, train_loss: 0.0562, step time: 0.2437\n",
      "187/281, train_loss: 0.0820, step time: 0.2488\n",
      "188/281, train_loss: 0.0531, step time: 0.2436\n",
      "189/281, train_loss: 0.0934, step time: 0.2460\n",
      "190/281, train_loss: 0.0795, step time: 0.2508\n",
      "191/281, train_loss: 0.0503, step time: 0.2510\n",
      "192/281, train_loss: 0.0539, step time: 0.2526\n",
      "193/281, train_loss: 0.0376, step time: 0.2461\n",
      "194/281, train_loss: 0.0770, step time: 0.2432\n",
      "195/281, train_loss: 0.0473, step time: 0.2494\n",
      "196/281, train_loss: 0.0429, step time: 0.2414\n",
      "197/281, train_loss: 0.0334, step time: 0.2435\n",
      "198/281, train_loss: 0.0495, step time: 0.2431\n",
      "199/281, train_loss: 0.0667, step time: 0.2452\n",
      "200/281, train_loss: 0.0519, step time: 0.2404\n",
      "201/281, train_loss: 0.0732, step time: 0.2484\n",
      "202/281, train_loss: 0.0650, step time: 0.2445\n",
      "203/281, train_loss: 0.0531, step time: 0.2509\n",
      "204/281, train_loss: 0.0654, step time: 0.2505\n",
      "205/281, train_loss: 0.0718, step time: 0.2522\n",
      "206/281, train_loss: 0.0439, step time: 0.2482\n",
      "207/281, train_loss: 0.0601, step time: 0.2485\n",
      "208/281, train_loss: 0.0482, step time: 0.2484\n",
      "209/281, train_loss: 0.0476, step time: 0.2480\n",
      "210/281, train_loss: 0.0391, step time: 0.2420\n",
      "211/281, train_loss: 0.0608, step time: 0.2462\n",
      "212/281, train_loss: 0.0452, step time: 0.2481\n",
      "213/281, train_loss: 0.2243, step time: 0.2501\n",
      "214/281, train_loss: 0.0576, step time: 0.2473\n",
      "215/281, train_loss: 0.0493, step time: 0.2435\n",
      "216/281, train_loss: 0.0395, step time: 0.2496\n",
      "217/281, train_loss: 0.0541, step time: 0.2492\n",
      "218/281, train_loss: 0.0675, step time: 0.2489\n",
      "219/281, train_loss: 0.0416, step time: 0.2438\n",
      "220/281, train_loss: 0.0705, step time: 0.2437\n",
      "221/281, train_loss: 0.0213, step time: 0.2488\n",
      "222/281, train_loss: 0.0398, step time: 0.2466\n",
      "223/281, train_loss: 0.0498, step time: 0.2461\n",
      "224/281, train_loss: 0.0473, step time: 0.2426\n",
      "225/281, train_loss: 0.2151, step time: 0.2467\n",
      "226/281, train_loss: 0.0696, step time: 0.2483\n",
      "227/281, train_loss: 0.0488, step time: 0.2451\n",
      "228/281, train_loss: 0.0423, step time: 0.2481\n",
      "229/281, train_loss: 0.0928, step time: 0.2423\n",
      "230/281, train_loss: 0.2147, step time: 0.2494\n",
      "231/281, train_loss: 0.0655, step time: 0.2492\n",
      "232/281, train_loss: 0.0494, step time: 0.2479\n",
      "233/281, train_loss: 0.0342, step time: 0.2420\n",
      "234/281, train_loss: 0.0599, step time: 0.2427\n",
      "235/281, train_loss: 0.0726, step time: 0.2440\n",
      "236/281, train_loss: 0.0762, step time: 0.2471\n",
      "237/281, train_loss: 0.0724, step time: 0.2505\n",
      "238/281, train_loss: 0.0488, step time: 0.2486\n",
      "239/281, train_loss: 0.0514, step time: 0.2465\n",
      "240/281, train_loss: 0.0873, step time: 0.2438\n",
      "241/281, train_loss: 0.0547, step time: 0.2479\n",
      "242/281, train_loss: 0.0303, step time: 0.2465\n",
      "243/281, train_loss: 0.0472, step time: 0.2418\n",
      "244/281, train_loss: 0.0492, step time: 0.2485\n",
      "245/281, train_loss: 0.2335, step time: 0.2534\n",
      "246/281, train_loss: 0.0729, step time: 0.2446\n",
      "247/281, train_loss: 0.0569, step time: 0.2519\n",
      "248/281, train_loss: 0.2071, step time: 0.2467\n",
      "249/281, train_loss: 0.0395, step time: 0.2491\n",
      "250/281, train_loss: 0.0314, step time: 0.2494\n",
      "251/281, train_loss: 0.0648, step time: 0.2498\n",
      "252/281, train_loss: 0.0319, step time: 0.2468\n",
      "253/281, train_loss: 0.0744, step time: 0.2474\n",
      "254/281, train_loss: 0.0535, step time: 0.2498\n",
      "255/281, train_loss: 0.0744, step time: 0.2489\n",
      "256/281, train_loss: 0.2083, step time: 0.2469\n",
      "257/281, train_loss: 0.0587, step time: 0.2495\n",
      "258/281, train_loss: 0.0515, step time: 0.2437\n",
      "259/281, train_loss: 0.2050, step time: 0.2499\n",
      "260/281, train_loss: 0.3837, step time: 0.2465\n",
      "261/281, train_loss: 0.0602, step time: 0.2473\n",
      "262/281, train_loss: 0.2201, step time: 0.2442\n",
      "263/281, train_loss: 0.0693, step time: 0.2519\n",
      "264/281, train_loss: 0.2273, step time: 0.2496\n",
      "265/281, train_loss: 0.0742, step time: 0.2472\n",
      "266/281, train_loss: 0.0369, step time: 0.2480\n",
      "267/281, train_loss: 0.0637, step time: 0.2465\n",
      "268/281, train_loss: 0.0532, step time: 0.2486\n",
      "269/281, train_loss: 0.2169, step time: 0.2499\n",
      "270/281, train_loss: 0.0850, step time: 0.2468\n",
      "271/281, train_loss: 0.0614, step time: 0.2497\n",
      "272/281, train_loss: 0.0900, step time: 0.2470\n",
      "273/281, train_loss: 0.0534, step time: 0.2439\n",
      "274/281, train_loss: 0.0566, step time: 0.2509\n",
      "275/281, train_loss: 0.0443, step time: 0.2475\n",
      "276/281, train_loss: 0.0883, step time: 0.2477\n",
      "277/281, train_loss: 0.0575, step time: 0.2466\n",
      "278/281, train_loss: 0.1983, step time: 0.2449\n",
      "279/281, train_loss: 0.0626, step time: 0.2497\n",
      "280/281, train_loss: 0.0474, step time: 0.2470\n",
      "281/281, train_loss: 0.0470, step time: 0.2440\n",
      "282/281, train_loss: 0.0677, step time: 0.1462\n",
      "epoch 125 average loss: 0.0811\n",
      "current epoch: 125 current mean dice: 0.9038 tc: 0.8966 wt: 0.9279 et: 0.8967\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 125 is: 362.0203\n",
      "----------\n",
      "epoch 126/200\n",
      "1/281, train_loss: 0.0740, step time: 0.2601\n",
      "2/281, train_loss: 0.0724, step time: 0.2525\n",
      "3/281, train_loss: 0.0806, step time: 0.2506\n",
      "4/281, train_loss: 0.0721, step time: 0.2546\n",
      "5/281, train_loss: 0.0851, step time: 0.2611\n",
      "6/281, train_loss: 0.1021, step time: 0.2654\n",
      "7/281, train_loss: 0.0498, step time: 0.2599\n",
      "8/281, train_loss: 0.0626, step time: 0.2546\n",
      "9/281, train_loss: 0.1011, step time: 0.2617\n",
      "10/281, train_loss: 0.0599, step time: 0.2541\n",
      "11/281, train_loss: 0.0625, step time: 0.2568\n",
      "12/281, train_loss: 0.0686, step time: 0.2540\n",
      "13/281, train_loss: 0.2197, step time: 0.2480\n",
      "14/281, train_loss: 0.0559, step time: 0.2492\n",
      "15/281, train_loss: 0.0469, step time: 0.2534\n",
      "16/281, train_loss: 0.0640, step time: 0.2562\n",
      "17/281, train_loss: 0.2155, step time: 0.2584\n",
      "18/281, train_loss: 0.0605, step time: 0.2536\n",
      "19/281, train_loss: 0.0644, step time: 0.2561\n",
      "20/281, train_loss: 0.0571, step time: 0.2573\n",
      "21/281, train_loss: 0.0589, step time: 0.2593\n",
      "22/281, train_loss: 0.0593, step time: 0.2556\n",
      "23/281, train_loss: 0.0355, step time: 0.2583\n",
      "24/281, train_loss: 0.0641, step time: 0.2593\n",
      "25/281, train_loss: 0.0509, step time: 0.2554\n",
      "26/281, train_loss: 0.1992, step time: 0.2594\n",
      "27/281, train_loss: 0.0748, step time: 0.2571\n",
      "28/281, train_loss: 0.0591, step time: 0.2653\n",
      "29/281, train_loss: 0.0488, step time: 0.2545\n",
      "30/281, train_loss: 0.2101, step time: 0.2557\n",
      "31/281, train_loss: 0.0506, step time: 0.2598\n",
      "32/281, train_loss: 0.0527, step time: 0.2672\n",
      "33/281, train_loss: 0.0365, step time: 0.2575\n",
      "34/281, train_loss: 0.0797, step time: 0.2563\n",
      "35/281, train_loss: 0.0482, step time: 0.2554\n",
      "36/281, train_loss: 0.0442, step time: 0.2556\n",
      "37/281, train_loss: 0.0742, step time: 0.2536\n",
      "38/281, train_loss: 0.0546, step time: 0.2515\n",
      "39/281, train_loss: 0.2194, step time: 0.2512\n",
      "40/281, train_loss: 0.1130, step time: 0.2512\n",
      "41/281, train_loss: 0.0270, step time: 0.2556\n",
      "42/281, train_loss: 0.0471, step time: 0.2592\n",
      "43/281, train_loss: 0.0723, step time: 0.2547\n",
      "44/281, train_loss: 0.2403, step time: 0.2560\n",
      "45/281, train_loss: 0.0706, step time: 0.2451\n",
      "46/281, train_loss: 0.3987, step time: 0.2574\n",
      "47/281, train_loss: 0.0779, step time: 0.2582\n",
      "48/281, train_loss: 0.0910, step time: 0.2587\n",
      "49/281, train_loss: 0.0398, step time: 0.2495\n",
      "50/281, train_loss: 0.0633, step time: 0.2523\n",
      "51/281, train_loss: 0.0445, step time: 0.2551\n",
      "52/281, train_loss: 0.0820, step time: 0.2700\n",
      "53/281, train_loss: 0.0552, step time: 0.2576\n",
      "54/281, train_loss: 0.0638, step time: 0.2554\n",
      "55/281, train_loss: 0.0649, step time: 0.2528\n",
      "56/281, train_loss: 0.0506, step time: 0.2499\n",
      "57/281, train_loss: 0.0609, step time: 0.2549\n",
      "58/281, train_loss: 0.0535, step time: 0.2531\n",
      "59/281, train_loss: 0.0798, step time: 0.2480\n",
      "60/281, train_loss: 0.0777, step time: 0.2519\n",
      "61/281, train_loss: 0.0506, step time: 0.2520\n",
      "62/281, train_loss: 0.0868, step time: 0.2482\n",
      "63/281, train_loss: 0.1075, step time: 0.2546\n",
      "64/281, train_loss: 0.2151, step time: 0.2534\n",
      "65/281, train_loss: 0.2110, step time: 0.2483\n",
      "66/281, train_loss: 0.0329, step time: 0.2572\n",
      "67/281, train_loss: 0.0550, step time: 0.2605\n",
      "68/281, train_loss: 0.0804, step time: 0.2666\n",
      "69/281, train_loss: 0.0624, step time: 0.2529\n",
      "70/281, train_loss: 0.2243, step time: 0.2511\n",
      "71/281, train_loss: 0.2326, step time: 0.2502\n",
      "72/281, train_loss: 0.0531, step time: 0.2498\n",
      "73/281, train_loss: 0.0502, step time: 0.2474\n",
      "74/281, train_loss: 0.0562, step time: 0.2509\n",
      "75/281, train_loss: 0.0442, step time: 0.2587\n",
      "76/281, train_loss: 0.0517, step time: 0.2605\n",
      "77/281, train_loss: 0.0870, step time: 0.2549\n",
      "78/281, train_loss: 0.0589, step time: 0.2495\n",
      "79/281, train_loss: 0.0664, step time: 0.2491\n",
      "80/281, train_loss: 0.0982, step time: 0.2494\n",
      "81/281, train_loss: 0.0510, step time: 0.2533\n",
      "82/281, train_loss: 0.0414, step time: 0.2549\n",
      "83/281, train_loss: 0.2162, step time: 0.2583\n",
      "84/281, train_loss: 0.0824, step time: 0.2534\n",
      "85/281, train_loss: 0.0413, step time: 0.2586\n",
      "86/281, train_loss: 0.0622, step time: 0.2499\n",
      "87/281, train_loss: 0.0598, step time: 0.2559\n",
      "88/281, train_loss: 0.2201, step time: 0.2562\n",
      "89/281, train_loss: 0.0605, step time: 0.2521\n",
      "90/281, train_loss: 0.2281, step time: 0.2591\n",
      "91/281, train_loss: 0.0536, step time: 0.2552\n",
      "92/281, train_loss: 0.0455, step time: 0.2601\n",
      "93/281, train_loss: 0.0525, step time: 0.2507\n",
      "94/281, train_loss: 0.0511, step time: 0.2545\n",
      "95/281, train_loss: 0.0832, step time: 0.2521\n",
      "96/281, train_loss: 0.0613, step time: 0.2525\n",
      "97/281, train_loss: 0.0663, step time: 0.2507\n",
      "98/281, train_loss: 0.0677, step time: 0.2535\n",
      "99/281, train_loss: 0.0499, step time: 0.2515\n",
      "100/281, train_loss: 0.2318, step time: 0.2560\n",
      "101/281, train_loss: 0.0678, step time: 0.2481\n",
      "102/281, train_loss: 0.0615, step time: 0.2520\n",
      "103/281, train_loss: 0.0422, step time: 0.2514\n",
      "104/281, train_loss: 0.0331, step time: 0.2490\n",
      "105/281, train_loss: 0.0624, step time: 0.2457\n",
      "106/281, train_loss: 0.0629, step time: 0.2507\n",
      "107/281, train_loss: 0.0576, step time: 0.2517\n",
      "108/281, train_loss: 0.2330, step time: 0.2512\n",
      "109/281, train_loss: 0.0506, step time: 0.2503\n",
      "110/281, train_loss: 0.0378, step time: 0.2591\n",
      "111/281, train_loss: 0.0918, step time: 0.2483\n",
      "112/281, train_loss: 0.0369, step time: 0.2535\n",
      "113/281, train_loss: 0.2332, step time: 0.2555\n",
      "114/281, train_loss: 0.0654, step time: 0.2509\n",
      "115/281, train_loss: 0.0756, step time: 0.2473\n",
      "116/281, train_loss: 0.0450, step time: 0.2511\n",
      "117/281, train_loss: 0.0576, step time: 0.2459\n",
      "118/281, train_loss: 0.0780, step time: 0.2426\n",
      "119/281, train_loss: 0.0416, step time: 0.2404\n",
      "120/281, train_loss: 0.0574, step time: 0.2490\n",
      "121/281, train_loss: 0.0593, step time: 0.2487\n",
      "122/281, train_loss: 0.0631, step time: 0.2450\n",
      "123/281, train_loss: 0.0503, step time: 0.2518\n",
      "124/281, train_loss: 0.0609, step time: 0.2445\n",
      "125/281, train_loss: 0.2081, step time: 0.2432\n",
      "126/281, train_loss: 0.0721, step time: 0.2417\n",
      "127/281, train_loss: 0.2070, step time: 0.2455\n",
      "128/281, train_loss: 0.0507, step time: 0.2482\n",
      "129/281, train_loss: 0.0335, step time: 0.2502\n",
      "130/281, train_loss: 0.0452, step time: 0.2439\n",
      "131/281, train_loss: 0.0757, step time: 0.2437\n",
      "132/281, train_loss: 0.0551, step time: 0.2492\n",
      "133/281, train_loss: 0.0843, step time: 0.2519\n",
      "134/281, train_loss: 0.2214, step time: 0.2472\n",
      "135/281, train_loss: 0.0536, step time: 0.2495\n",
      "136/281, train_loss: 0.0594, step time: 0.2550\n",
      "137/281, train_loss: 0.0441, step time: 0.2562\n",
      "138/281, train_loss: 0.0334, step time: 0.2547\n",
      "139/281, train_loss: 0.0595, step time: 0.2533\n",
      "140/281, train_loss: 0.2197, step time: 0.2526\n",
      "141/281, train_loss: 0.0425, step time: 0.2611\n",
      "142/281, train_loss: 0.0682, step time: 0.2500\n",
      "143/281, train_loss: 0.0589, step time: 0.2500\n",
      "144/281, train_loss: 0.0387, step time: 0.2471\n",
      "145/281, train_loss: 0.0805, step time: 0.2468\n",
      "146/281, train_loss: 0.0405, step time: 0.2443\n",
      "147/281, train_loss: 0.0462, step time: 0.2494\n",
      "148/281, train_loss: 0.0992, step time: 0.2523\n",
      "149/281, train_loss: 0.2094, step time: 0.2512\n",
      "150/281, train_loss: 0.0434, step time: 0.2467\n",
      "151/281, train_loss: 0.2212, step time: 0.2476\n",
      "152/281, train_loss: 0.0486, step time: 0.2486\n",
      "153/281, train_loss: 0.2104, step time: 0.2506\n",
      "154/281, train_loss: 0.0581, step time: 0.2494\n",
      "155/281, train_loss: 0.2231, step time: 0.2554\n",
      "156/281, train_loss: 0.0514, step time: 0.2483\n",
      "157/281, train_loss: 0.0517, step time: 0.2508\n",
      "158/281, train_loss: 0.2288, step time: 0.2495\n",
      "159/281, train_loss: 0.0367, step time: 0.2541\n",
      "160/281, train_loss: 0.0687, step time: 0.2510\n",
      "161/281, train_loss: 0.0641, step time: 0.2496\n",
      "162/281, train_loss: 0.0596, step time: 0.2510\n",
      "163/281, train_loss: 0.0569, step time: 0.2507\n",
      "164/281, train_loss: 0.0678, step time: 0.2499\n",
      "165/281, train_loss: 0.0348, step time: 0.2485\n",
      "166/281, train_loss: 0.0663, step time: 0.2524\n",
      "167/281, train_loss: 0.0299, step time: 0.2540\n",
      "168/281, train_loss: 0.0419, step time: 0.2567\n",
      "169/281, train_loss: 0.0552, step time: 0.2559\n",
      "170/281, train_loss: 0.0343, step time: 0.2561\n",
      "171/281, train_loss: 0.0744, step time: 0.2507\n",
      "172/281, train_loss: 0.0593, step time: 0.2543\n",
      "173/281, train_loss: 0.0460, step time: 0.2539\n",
      "174/281, train_loss: 0.0791, step time: 0.2559\n",
      "175/281, train_loss: 0.0745, step time: 0.2559\n",
      "176/281, train_loss: 0.2239, step time: 0.2514\n",
      "177/281, train_loss: 0.0713, step time: 0.2538\n",
      "178/281, train_loss: 0.0715, step time: 0.2477\n",
      "179/281, train_loss: 0.2045, step time: 0.2823\n",
      "180/281, train_loss: 0.0359, step time: 0.2530\n",
      "181/281, train_loss: 0.0578, step time: 0.2502\n",
      "182/281, train_loss: 0.0723, step time: 0.2537\n",
      "183/281, train_loss: 0.0603, step time: 0.2511\n",
      "184/281, train_loss: 0.0407, step time: 0.2555\n",
      "185/281, train_loss: 0.2029, step time: 0.2511\n",
      "186/281, train_loss: 0.2049, step time: 0.2493\n",
      "187/281, train_loss: 0.0697, step time: 0.2468\n",
      "188/281, train_loss: 0.0652, step time: 0.2558\n",
      "189/281, train_loss: 0.0389, step time: 0.2553\n",
      "190/281, train_loss: 0.0517, step time: 0.2473\n",
      "191/281, train_loss: 0.0656, step time: 0.2526\n",
      "192/281, train_loss: 0.2364, step time: 0.2562\n",
      "193/281, train_loss: 0.0719, step time: 0.2506\n",
      "194/281, train_loss: 0.0339, step time: 0.2508\n",
      "195/281, train_loss: 0.0460, step time: 0.2506\n",
      "196/281, train_loss: 0.0861, step time: 0.2506\n",
      "197/281, train_loss: 0.0698, step time: 0.2585\n",
      "198/281, train_loss: 0.0609, step time: 0.2593\n",
      "199/281, train_loss: 0.0621, step time: 0.2500\n",
      "200/281, train_loss: 0.0672, step time: 0.2497\n",
      "201/281, train_loss: 0.0503, step time: 0.2507\n",
      "202/281, train_loss: 0.0505, step time: 0.2485\n",
      "203/281, train_loss: 0.2253, step time: 0.2576\n",
      "204/281, train_loss: 0.0365, step time: 0.2521\n",
      "205/281, train_loss: 0.0629, step time: 0.2748\n",
      "206/281, train_loss: 0.0602, step time: 0.2697\n",
      "207/281, train_loss: 0.0460, step time: 0.2476\n",
      "208/281, train_loss: 0.3604, step time: 0.2572\n",
      "209/281, train_loss: 0.0407, step time: 0.2500\n",
      "210/281, train_loss: 0.0308, step time: 0.2514\n",
      "211/281, train_loss: 0.0420, step time: 0.2513\n",
      "212/281, train_loss: 0.0607, step time: 0.2533\n",
      "213/281, train_loss: 0.0724, step time: 0.2602\n",
      "214/281, train_loss: 0.0633, step time: 0.2554\n",
      "215/281, train_loss: 0.0600, step time: 0.2474\n",
      "216/281, train_loss: 0.0867, step time: 0.2570\n",
      "217/281, train_loss: 0.0785, step time: 0.2508\n",
      "218/281, train_loss: 0.0572, step time: 0.2567\n",
      "219/281, train_loss: 0.0492, step time: 0.2529\n",
      "220/281, train_loss: 0.0558, step time: 0.2546\n",
      "221/281, train_loss: 0.0783, step time: 0.2512\n",
      "222/281, train_loss: 0.0592, step time: 0.2481\n",
      "223/281, train_loss: 0.0438, step time: 0.2473\n",
      "224/281, train_loss: 0.0889, step time: 0.2476\n",
      "225/281, train_loss: 0.0539, step time: 0.2499\n",
      "226/281, train_loss: 0.0444, step time: 0.2482\n",
      "227/281, train_loss: 0.1949, step time: 0.2492\n",
      "228/281, train_loss: 0.0651, step time: 0.2531\n",
      "229/281, train_loss: 0.0735, step time: 0.2506\n",
      "230/281, train_loss: 0.0566, step time: 0.2495\n",
      "231/281, train_loss: 0.0420, step time: 0.2492\n",
      "232/281, train_loss: 0.0506, step time: 0.2514\n",
      "233/281, train_loss: 0.0451, step time: 0.2474\n",
      "234/281, train_loss: 0.0416, step time: 0.2481\n",
      "235/281, train_loss: 0.2109, step time: 0.2515\n",
      "236/281, train_loss: 0.0365, step time: 0.2527\n",
      "237/281, train_loss: 0.0573, step time: 0.2574\n",
      "238/281, train_loss: 0.0765, step time: 0.2508\n",
      "239/281, train_loss: 0.0468, step time: 0.2480\n",
      "240/281, train_loss: 0.2007, step time: 0.2497\n",
      "241/281, train_loss: 0.1014, step time: 0.2509\n",
      "242/281, train_loss: 0.0565, step time: 0.2490\n",
      "243/281, train_loss: 0.2215, step time: 0.2509\n",
      "244/281, train_loss: 0.0507, step time: 0.2454\n",
      "245/281, train_loss: 0.0364, step time: 0.2544\n",
      "246/281, train_loss: 0.0816, step time: 0.2505\n",
      "247/281, train_loss: 0.0694, step time: 0.2431\n",
      "248/281, train_loss: 0.2015, step time: 0.2477\n",
      "249/281, train_loss: 0.0838, step time: 0.2510\n",
      "250/281, train_loss: 0.0385, step time: 0.2470\n",
      "251/281, train_loss: 0.0502, step time: 0.2484\n",
      "252/281, train_loss: 0.0602, step time: 0.2480\n",
      "253/281, train_loss: 0.0292, step time: 0.2461\n",
      "254/281, train_loss: 0.0538, step time: 0.2461\n",
      "255/281, train_loss: 0.0690, step time: 0.2470\n",
      "256/281, train_loss: 0.0508, step time: 0.2509\n",
      "257/281, train_loss: 0.0709, step time: 0.2484\n",
      "258/281, train_loss: 0.0558, step time: 0.2468\n",
      "259/281, train_loss: 0.0581, step time: 0.2458\n",
      "260/281, train_loss: 0.0581, step time: 0.2480\n",
      "261/281, train_loss: 0.0799, step time: 0.2241\n",
      "262/281, train_loss: 0.0646, step time: 0.2561\n",
      "263/281, train_loss: 0.0672, step time: 0.2489\n",
      "264/281, train_loss: 0.0465, step time: 0.2537\n",
      "265/281, train_loss: 0.0909, step time: 0.2501\n",
      "266/281, train_loss: 0.0592, step time: 0.2507\n",
      "267/281, train_loss: 0.0634, step time: 0.2530\n",
      "268/281, train_loss: 0.0812, step time: 0.2586\n",
      "269/281, train_loss: 0.0620, step time: 0.2521\n",
      "270/281, train_loss: 0.0755, step time: 0.2531\n",
      "271/281, train_loss: 0.0536, step time: 0.2571\n",
      "272/281, train_loss: 0.0336, step time: 0.2522\n",
      "273/281, train_loss: 0.0587, step time: 0.2502\n",
      "274/281, train_loss: 0.0523, step time: 0.2456\n",
      "275/281, train_loss: 0.0386, step time: 0.2473\n",
      "276/281, train_loss: 0.0478, step time: 0.2599\n",
      "277/281, train_loss: 0.2229, step time: 0.2503\n",
      "278/281, train_loss: 0.0628, step time: 0.2550\n",
      "279/281, train_loss: 0.0628, step time: 0.2512\n",
      "280/281, train_loss: 0.0589, step time: 0.2480\n",
      "281/281, train_loss: 0.0293, step time: 0.2495\n",
      "282/281, train_loss: 0.0396, step time: 0.1494\n",
      "epoch 126 average loss: 0.0823\n",
      "current epoch: 126 current mean dice: 0.9023 tc: 0.8957 wt: 0.9275 et: 0.8933\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 126 is: 364.0103\n",
      "----------\n",
      "epoch 127/200\n",
      "1/281, train_loss: 0.0569, step time: 0.2609\n",
      "2/281, train_loss: 0.0469, step time: 0.2530\n",
      "3/281, train_loss: 0.0429, step time: 0.2560\n",
      "4/281, train_loss: 0.0550, step time: 0.2465\n",
      "5/281, train_loss: 0.0488, step time: 0.2546\n",
      "6/281, train_loss: 0.0410, step time: 0.2510\n",
      "7/281, train_loss: 0.0636, step time: 0.2568\n",
      "8/281, train_loss: 0.0574, step time: 0.2545\n",
      "9/281, train_loss: 0.0663, step time: 0.2522\n",
      "10/281, train_loss: 0.0634, step time: 0.2505\n",
      "11/281, train_loss: 0.2022, step time: 0.2615\n",
      "12/281, train_loss: 0.0851, step time: 0.2539\n",
      "13/281, train_loss: 0.0629, step time: 0.2508\n",
      "14/281, train_loss: 0.0568, step time: 0.2467\n",
      "15/281, train_loss: 0.0348, step time: 0.2587\n",
      "16/281, train_loss: 0.0408, step time: 0.2463\n",
      "17/281, train_loss: 0.0512, step time: 0.2502\n",
      "18/281, train_loss: 0.2194, step time: 0.2498\n",
      "19/281, train_loss: 0.0475, step time: 0.2553\n",
      "20/281, train_loss: 0.0479, step time: 0.2556\n",
      "21/281, train_loss: 0.0551, step time: 0.2501\n",
      "22/281, train_loss: 0.0602, step time: 0.2506\n",
      "23/281, train_loss: 0.0614, step time: 0.2471\n",
      "24/281, train_loss: 0.0649, step time: 0.2484\n",
      "25/281, train_loss: 0.2222, step time: 0.2483\n",
      "26/281, train_loss: 0.2327, step time: 0.2515\n",
      "27/281, train_loss: 0.0816, step time: 0.2481\n",
      "28/281, train_loss: 0.0694, step time: 0.2477\n",
      "29/281, train_loss: 0.0643, step time: 0.2504\n",
      "30/281, train_loss: 0.0253, step time: 0.2499\n",
      "31/281, train_loss: 0.2278, step time: 0.2534\n",
      "32/281, train_loss: 0.0608, step time: 0.2533\n",
      "33/281, train_loss: 0.0454, step time: 0.2546\n",
      "34/281, train_loss: 0.0553, step time: 0.2517\n",
      "35/281, train_loss: 0.0442, step time: 0.2492\n",
      "36/281, train_loss: 0.0506, step time: 0.2482\n",
      "37/281, train_loss: 0.2085, step time: 0.2487\n",
      "38/281, train_loss: 0.0384, step time: 0.2511\n",
      "39/281, train_loss: 0.0657, step time: 0.2532\n",
      "40/281, train_loss: 0.2452, step time: 0.2596\n",
      "41/281, train_loss: 0.0656, step time: 0.2596\n",
      "42/281, train_loss: 0.0502, step time: 0.2490\n",
      "43/281, train_loss: 0.0761, step time: 0.2556\n",
      "44/281, train_loss: 0.0540, step time: 0.2484\n",
      "45/281, train_loss: 0.0381, step time: 0.2468\n",
      "46/281, train_loss: 0.0699, step time: 0.2530\n",
      "47/281, train_loss: 0.0370, step time: 0.2555\n",
      "48/281, train_loss: 0.0214, step time: 0.2509\n",
      "49/281, train_loss: 0.0714, step time: 0.2519\n",
      "50/281, train_loss: 0.0459, step time: 0.2582\n",
      "51/281, train_loss: 0.0479, step time: 0.2746\n",
      "52/281, train_loss: 0.0417, step time: 0.2604\n",
      "53/281, train_loss: 0.0696, step time: 0.2607\n",
      "54/281, train_loss: 0.2015, step time: 0.2492\n",
      "55/281, train_loss: 0.0785, step time: 0.2510\n",
      "56/281, train_loss: 0.0489, step time: 0.2481\n",
      "57/281, train_loss: 0.0462, step time: 0.2499\n",
      "58/281, train_loss: 0.0351, step time: 0.2468\n",
      "59/281, train_loss: 0.0707, step time: 0.2482\n",
      "60/281, train_loss: 0.0470, step time: 0.2504\n",
      "61/281, train_loss: 0.0539, step time: 0.2440\n",
      "62/281, train_loss: 0.0567, step time: 0.2492\n",
      "63/281, train_loss: 0.2271, step time: 0.2539\n",
      "64/281, train_loss: 0.0699, step time: 0.2490\n",
      "65/281, train_loss: 0.0552, step time: 0.2586\n",
      "66/281, train_loss: 0.0566, step time: 0.2529\n",
      "67/281, train_loss: 0.2125, step time: 0.2516\n",
      "68/281, train_loss: 0.0652, step time: 0.2495\n",
      "69/281, train_loss: 0.0603, step time: 0.2503\n",
      "70/281, train_loss: 0.2361, step time: 0.2484\n",
      "71/281, train_loss: 0.0524, step time: 0.2502\n",
      "72/281, train_loss: 0.0295, step time: 0.2486\n",
      "73/281, train_loss: 0.0533, step time: 0.2458\n",
      "74/281, train_loss: 0.2256, step time: 0.2438\n",
      "75/281, train_loss: 0.0910, step time: 0.2466\n",
      "76/281, train_loss: 0.0361, step time: 0.2496\n",
      "77/281, train_loss: 0.0516, step time: 0.2470\n",
      "78/281, train_loss: 0.1305, step time: 0.2686\n",
      "79/281, train_loss: 0.0632, step time: 0.2584\n",
      "80/281, train_loss: 0.0713, step time: 0.2579\n",
      "81/281, train_loss: 0.0668, step time: 0.2532\n",
      "82/281, train_loss: 0.0491, step time: 0.2513\n",
      "83/281, train_loss: 0.0458, step time: 0.2479\n",
      "84/281, train_loss: 0.0521, step time: 0.2481\n",
      "85/281, train_loss: 0.0700, step time: 0.2445\n",
      "86/281, train_loss: 0.0680, step time: 0.2592\n",
      "87/281, train_loss: 0.0865, step time: 0.2478\n",
      "88/281, train_loss: 0.0796, step time: 0.2495\n",
      "89/281, train_loss: 0.0548, step time: 0.2507\n",
      "90/281, train_loss: 0.0560, step time: 0.2537\n",
      "91/281, train_loss: 0.0524, step time: 0.2479\n",
      "92/281, train_loss: 0.0575, step time: 0.2506\n",
      "93/281, train_loss: 0.0616, step time: 0.2477\n",
      "94/281, train_loss: 0.0497, step time: 0.2510\n",
      "95/281, train_loss: 0.0548, step time: 0.2483\n",
      "96/281, train_loss: 0.0622, step time: 0.2504\n",
      "97/281, train_loss: 0.0408, step time: 0.2442\n",
      "98/281, train_loss: 0.0586, step time: 0.2459\n",
      "99/281, train_loss: 0.0469, step time: 0.2501\n",
      "100/281, train_loss: 0.0800, step time: 0.2538\n",
      "101/281, train_loss: 0.0322, step time: 0.2483\n",
      "102/281, train_loss: 0.0668, step time: 0.2460\n",
      "103/281, train_loss: 0.0573, step time: 0.2520\n",
      "104/281, train_loss: 0.0515, step time: 0.2526\n",
      "105/281, train_loss: 0.2235, step time: 0.2504\n",
      "106/281, train_loss: 0.0474, step time: 0.2588\n",
      "107/281, train_loss: 0.0666, step time: 0.2482\n",
      "108/281, train_loss: 0.2252, step time: 0.2536\n",
      "109/281, train_loss: 0.0323, step time: 0.2448\n",
      "110/281, train_loss: 0.2056, step time: 0.2421\n",
      "111/281, train_loss: 0.0561, step time: 0.2533\n",
      "112/281, train_loss: 0.0969, step time: 0.2456\n",
      "113/281, train_loss: 0.1991, step time: 0.2517\n",
      "114/281, train_loss: 0.0560, step time: 0.2504\n",
      "115/281, train_loss: 0.0526, step time: 0.2564\n",
      "116/281, train_loss: 0.0556, step time: 0.2488\n",
      "117/281, train_loss: 0.0563, step time: 0.2538\n",
      "118/281, train_loss: 0.0799, step time: 0.2550\n",
      "119/281, train_loss: 0.0983, step time: 0.2491\n",
      "120/281, train_loss: 0.0366, step time: 0.2499\n",
      "121/281, train_loss: 0.0576, step time: 0.2490\n",
      "122/281, train_loss: 0.0709, step time: 0.2534\n",
      "123/281, train_loss: 0.2157, step time: 0.2534\n",
      "124/281, train_loss: 0.0544, step time: 0.2509\n",
      "125/281, train_loss: 0.0418, step time: 0.2484\n",
      "126/281, train_loss: 0.2201, step time: 0.2487\n",
      "127/281, train_loss: 0.0868, step time: 0.2535\n",
      "128/281, train_loss: 0.2169, step time: 0.2478\n",
      "129/281, train_loss: 0.2355, step time: 0.2526\n",
      "130/281, train_loss: 0.0737, step time: 0.2459\n",
      "131/281, train_loss: 0.0923, step time: 0.2440\n",
      "132/281, train_loss: 0.0301, step time: 0.2458\n",
      "133/281, train_loss: 0.0797, step time: 0.2448\n",
      "134/281, train_loss: 0.2148, step time: 0.2523\n",
      "135/281, train_loss: 0.0687, step time: 0.2549\n",
      "136/281, train_loss: 0.0768, step time: 0.2531\n",
      "137/281, train_loss: 0.0666, step time: 0.2487\n",
      "138/281, train_loss: 0.0482, step time: 0.2557\n",
      "139/281, train_loss: 0.0737, step time: 0.2521\n",
      "140/281, train_loss: 0.0397, step time: 0.2516\n",
      "141/281, train_loss: 0.0609, step time: 0.2533\n",
      "142/281, train_loss: 0.0373, step time: 0.2528\n",
      "143/281, train_loss: 0.0456, step time: 0.2481\n",
      "144/281, train_loss: 0.0526, step time: 0.2537\n",
      "145/281, train_loss: 0.0796, step time: 0.2507\n",
      "146/281, train_loss: 0.0470, step time: 0.2468\n",
      "147/281, train_loss: 0.0568, step time: 0.2494\n",
      "148/281, train_loss: 0.0529, step time: 0.2431\n",
      "149/281, train_loss: 0.0346, step time: 0.2431\n",
      "150/281, train_loss: 0.0648, step time: 0.2420\n",
      "151/281, train_loss: 0.0749, step time: 0.2478\n",
      "152/281, train_loss: 0.2205, step time: 0.2450\n",
      "153/281, train_loss: 0.0412, step time: 0.2500\n",
      "154/281, train_loss: 0.0503, step time: 0.2499\n",
      "155/281, train_loss: 0.2101, step time: 0.2495\n",
      "156/281, train_loss: 0.0514, step time: 0.2409\n",
      "157/281, train_loss: 0.0400, step time: 0.2428\n",
      "158/281, train_loss: 0.0392, step time: 0.2418\n",
      "159/281, train_loss: 0.0373, step time: 0.2429\n",
      "160/281, train_loss: 0.0827, step time: 0.2483\n",
      "161/281, train_loss: 0.0782, step time: 0.2458\n",
      "162/281, train_loss: 0.0759, step time: 0.2413\n",
      "163/281, train_loss: 0.0349, step time: 0.2406\n",
      "164/281, train_loss: 0.0453, step time: 0.2433\n",
      "165/281, train_loss: 0.0640, step time: 0.2422\n",
      "166/281, train_loss: 0.0437, step time: 0.2432\n",
      "167/281, train_loss: 0.0370, step time: 0.2464\n",
      "168/281, train_loss: 0.2150, step time: 0.2485\n",
      "169/281, train_loss: 0.0725, step time: 0.2503\n",
      "170/281, train_loss: 0.2254, step time: 0.2422\n",
      "171/281, train_loss: 0.0691, step time: 0.2435\n",
      "172/281, train_loss: 0.0496, step time: 0.2472\n",
      "173/281, train_loss: 0.0605, step time: 0.2479\n",
      "174/281, train_loss: 0.0439, step time: 0.2439\n",
      "175/281, train_loss: 0.0744, step time: 0.2412\n",
      "176/281, train_loss: 0.2210, step time: 0.2411\n",
      "177/281, train_loss: 0.0549, step time: 0.2459\n",
      "178/281, train_loss: 0.0483, step time: 0.2479\n",
      "179/281, train_loss: 0.0734, step time: 0.2473\n",
      "180/281, train_loss: 0.2058, step time: 0.2449\n",
      "181/281, train_loss: 0.0292, step time: 0.2463\n",
      "182/281, train_loss: 0.0587, step time: 0.2522\n",
      "183/281, train_loss: 0.0306, step time: 0.2483\n",
      "184/281, train_loss: 0.0487, step time: 0.2511\n",
      "185/281, train_loss: 0.2415, step time: 0.2546\n",
      "186/281, train_loss: 0.0663, step time: 0.2459\n",
      "187/281, train_loss: 0.2443, step time: 0.2468\n",
      "188/281, train_loss: 0.0632, step time: 0.2474\n",
      "189/281, train_loss: 0.0463, step time: 0.2477\n",
      "190/281, train_loss: 0.0710, step time: 0.2505\n",
      "191/281, train_loss: 0.0418, step time: 0.2456\n",
      "192/281, train_loss: 0.0675, step time: 0.2435\n",
      "193/281, train_loss: 0.0498, step time: 0.2476\n",
      "194/281, train_loss: 0.0669, step time: 0.2524\n",
      "195/281, train_loss: 0.0487, step time: 0.2366\n",
      "196/281, train_loss: 0.0468, step time: 0.2457\n",
      "197/281, train_loss: 0.0510, step time: 0.2475\n",
      "198/281, train_loss: 0.0490, step time: 0.2448\n",
      "199/281, train_loss: 0.0418, step time: 0.2515\n",
      "200/281, train_loss: 0.0548, step time: 0.2447\n",
      "201/281, train_loss: 0.0605, step time: 0.2485\n",
      "202/281, train_loss: 0.2079, step time: 0.2433\n",
      "203/281, train_loss: 0.0384, step time: 0.2495\n",
      "204/281, train_loss: 0.0683, step time: 0.2501\n",
      "205/281, train_loss: 0.0586, step time: 0.2517\n",
      "206/281, train_loss: 0.0738, step time: 0.2470\n",
      "207/281, train_loss: 0.0397, step time: 0.2510\n",
      "208/281, train_loss: 0.0640, step time: 0.2527\n",
      "209/281, train_loss: 0.2034, step time: 0.2479\n",
      "210/281, train_loss: 0.3899, step time: 0.2548\n",
      "211/281, train_loss: 0.0551, step time: 0.2457\n",
      "212/281, train_loss: 0.0271, step time: 0.2484\n",
      "213/281, train_loss: 0.0545, step time: 0.2504\n",
      "214/281, train_loss: 0.0539, step time: 0.2476\n",
      "215/281, train_loss: 0.0480, step time: 0.2523\n",
      "216/281, train_loss: 0.0549, step time: 0.2467\n",
      "217/281, train_loss: 0.2110, step time: 0.2511\n",
      "218/281, train_loss: 0.0674, step time: 0.2472\n",
      "219/281, train_loss: 0.0697, step time: 0.2460\n",
      "220/281, train_loss: 0.0372, step time: 0.2421\n",
      "221/281, train_loss: 0.0396, step time: 0.2509\n",
      "222/281, train_loss: 0.0520, step time: 0.2436\n",
      "223/281, train_loss: 0.2065, step time: 0.2443\n",
      "224/281, train_loss: 0.0372, step time: 0.2446\n",
      "225/281, train_loss: 0.2564, step time: 0.2511\n",
      "226/281, train_loss: 0.0504, step time: 0.2468\n",
      "227/281, train_loss: 0.0525, step time: 0.2451\n",
      "228/281, train_loss: 0.0297, step time: 0.2458\n",
      "229/281, train_loss: 0.0557, step time: 0.2471\n",
      "230/281, train_loss: 0.0932, step time: 0.2455\n",
      "231/281, train_loss: 0.2025, step time: 0.2443\n",
      "232/281, train_loss: 0.0597, step time: 0.2443\n",
      "233/281, train_loss: 0.0528, step time: 0.2469\n",
      "234/281, train_loss: 0.0676, step time: 0.2463\n",
      "235/281, train_loss: 0.0381, step time: 0.2465\n",
      "236/281, train_loss: 0.0425, step time: 0.2502\n",
      "237/281, train_loss: 0.0829, step time: 0.2448\n",
      "238/281, train_loss: 0.0420, step time: 0.2454\n",
      "239/281, train_loss: 0.0532, step time: 0.2434\n",
      "240/281, train_loss: 0.0468, step time: 0.2497\n",
      "241/281, train_loss: 0.0402, step time: 0.2443\n",
      "242/281, train_loss: 0.0747, step time: 0.2458\n",
      "243/281, train_loss: 0.0753, step time: 0.2497\n",
      "244/281, train_loss: 0.1982, step time: 0.2463\n",
      "245/281, train_loss: 0.0381, step time: 0.2440\n",
      "246/281, train_loss: 0.0735, step time: 0.2510\n",
      "247/281, train_loss: 0.1931, step time: 0.2448\n",
      "248/281, train_loss: 0.0571, step time: 0.2419\n",
      "249/281, train_loss: 0.0481, step time: 0.2439\n",
      "250/281, train_loss: 0.0608, step time: 0.2481\n",
      "251/281, train_loss: 0.0357, step time: 0.2482\n",
      "252/281, train_loss: 0.2213, step time: 0.2438\n",
      "253/281, train_loss: 0.0819, step time: 0.2395\n",
      "254/281, train_loss: 0.0572, step time: 0.2497\n",
      "255/281, train_loss: 0.0318, step time: 0.2451\n",
      "256/281, train_loss: 0.0679, step time: 0.2420\n",
      "257/281, train_loss: 0.0674, step time: 0.2431\n",
      "258/281, train_loss: 0.0722, step time: 0.2494\n",
      "259/281, train_loss: 0.0532, step time: 0.2498\n",
      "260/281, train_loss: 0.0455, step time: 0.2495\n",
      "261/281, train_loss: 0.0435, step time: 0.2442\n",
      "262/281, train_loss: 0.0426, step time: 0.2466\n",
      "263/281, train_loss: 0.0493, step time: 0.2457\n",
      "264/281, train_loss: 0.0643, step time: 0.2401\n",
      "265/281, train_loss: 0.0225, step time: 0.2392\n",
      "266/281, train_loss: 0.0566, step time: 0.2491\n",
      "267/281, train_loss: 0.0363, step time: 0.2522\n",
      "268/281, train_loss: 0.0480, step time: 0.2460\n",
      "269/281, train_loss: 0.0463, step time: 0.2468\n",
      "270/281, train_loss: 0.2073, step time: 0.2443\n",
      "271/281, train_loss: 0.0417, step time: 0.2440\n",
      "272/281, train_loss: 0.0653, step time: 0.2427\n",
      "273/281, train_loss: 0.0616, step time: 0.2407\n",
      "274/281, train_loss: 0.0577, step time: 0.2468\n",
      "275/281, train_loss: 0.0310, step time: 0.2450\n",
      "276/281, train_loss: 0.0674, step time: 0.2450\n",
      "277/281, train_loss: 0.0562, step time: 0.2430\n",
      "278/281, train_loss: 0.0409, step time: 0.2438\n",
      "279/281, train_loss: 0.0554, step time: 0.2438\n",
      "280/281, train_loss: 0.0809, step time: 0.2440\n",
      "281/281, train_loss: 0.0424, step time: 0.2421\n",
      "282/281, train_loss: 0.0280, step time: 0.1440\n",
      "epoch 127 average loss: 0.0795\n",
      "current epoch: 127 current mean dice: 0.9041 tc: 0.8967 wt: 0.9289 et: 0.8964\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 127 is: 385.6637\n",
      "----------\n",
      "epoch 128/200\n",
      "1/281, train_loss: 0.2273, step time: 0.2445\n",
      "2/281, train_loss: 0.0844, step time: 0.2415\n",
      "3/281, train_loss: 0.0535, step time: 0.2418\n",
      "4/281, train_loss: 0.0561, step time: 0.2463\n",
      "5/281, train_loss: 0.2042, step time: 0.2714\n",
      "6/281, train_loss: 0.2285, step time: 0.2485\n",
      "7/281, train_loss: 0.0442, step time: 0.2445\n",
      "8/281, train_loss: 0.0605, step time: 0.2417\n",
      "9/281, train_loss: 0.0643, step time: 0.2431\n",
      "10/281, train_loss: 0.0553, step time: 0.2498\n",
      "11/281, train_loss: 0.0512, step time: 0.2494\n",
      "12/281, train_loss: 0.0483, step time: 0.2481\n",
      "13/281, train_loss: 0.2406, step time: 0.2511\n",
      "14/281, train_loss: 0.0422, step time: 0.2513\n",
      "15/281, train_loss: 0.0540, step time: 0.2496\n",
      "16/281, train_loss: 0.0506, step time: 0.2537\n",
      "17/281, train_loss: 0.0540, step time: 0.2708\n",
      "18/281, train_loss: 0.0484, step time: 0.2520\n",
      "19/281, train_loss: 0.0697, step time: 0.2533\n",
      "20/281, train_loss: 0.0442, step time: 0.2507\n",
      "21/281, train_loss: 0.0613, step time: 0.2513\n",
      "22/281, train_loss: 0.0482, step time: 0.2411\n",
      "23/281, train_loss: 0.0454, step time: 0.2505\n",
      "24/281, train_loss: 0.0611, step time: 0.2562\n",
      "25/281, train_loss: 0.2078, step time: 0.2560\n",
      "26/281, train_loss: 0.0263, step time: 0.2482\n",
      "27/281, train_loss: 0.0484, step time: 0.2562\n",
      "28/281, train_loss: 0.0789, step time: 0.2570\n",
      "29/281, train_loss: 0.0433, step time: 0.2544\n",
      "30/281, train_loss: 0.0508, step time: 0.2432\n",
      "31/281, train_loss: 0.0377, step time: 0.2569\n",
      "32/281, train_loss: 0.2219, step time: 0.2580\n",
      "33/281, train_loss: 0.0710, step time: 0.2555\n",
      "34/281, train_loss: 0.0612, step time: 0.2581\n",
      "35/281, train_loss: 0.0548, step time: 0.2493\n",
      "36/281, train_loss: 0.0576, step time: 0.2459\n",
      "37/281, train_loss: 0.0742, step time: 0.2429\n",
      "38/281, train_loss: 0.0387, step time: 0.2441\n",
      "39/281, train_loss: 0.0681, step time: 0.2440\n",
      "40/281, train_loss: 0.0622, step time: 0.2452\n",
      "41/281, train_loss: 0.2084, step time: 0.2487\n",
      "42/281, train_loss: 0.0591, step time: 0.2455\n",
      "43/281, train_loss: 0.0598, step time: 0.2450\n",
      "44/281, train_loss: 0.0371, step time: 0.2419\n",
      "45/281, train_loss: 0.0479, step time: 0.2445\n",
      "46/281, train_loss: 0.0675, step time: 0.2457\n",
      "47/281, train_loss: 0.0531, step time: 0.2530\n",
      "48/281, train_loss: 0.0561, step time: 0.2454\n",
      "49/281, train_loss: 0.2169, step time: 0.2534\n",
      "50/281, train_loss: 0.0581, step time: 0.2479\n",
      "51/281, train_loss: 0.2087, step time: 0.2558\n",
      "52/281, train_loss: 0.0427, step time: 0.2550\n",
      "53/281, train_loss: 0.0466, step time: 0.2546\n",
      "54/281, train_loss: 0.0294, step time: 0.2675\n",
      "55/281, train_loss: 0.0264, step time: 0.2432\n",
      "56/281, train_loss: 0.2188, step time: 0.2449\n",
      "57/281, train_loss: 0.0577, step time: 0.2486\n",
      "58/281, train_loss: 0.0517, step time: 0.2474\n",
      "59/281, train_loss: 0.0661, step time: 0.2466\n",
      "60/281, train_loss: 0.0387, step time: 0.2434\n",
      "61/281, train_loss: 0.0415, step time: 0.2491\n",
      "62/281, train_loss: 0.0519, step time: 0.2513\n",
      "63/281, train_loss: 0.0428, step time: 0.2479\n",
      "64/281, train_loss: 0.0265, step time: 0.2472\n",
      "65/281, train_loss: 0.0408, step time: 0.2490\n",
      "66/281, train_loss: 0.0872, step time: 0.2542\n",
      "67/281, train_loss: 0.0633, step time: 0.2494\n",
      "68/281, train_loss: 0.0656, step time: 0.2463\n",
      "69/281, train_loss: 0.2135, step time: 0.2456\n",
      "70/281, train_loss: 0.0583, step time: 0.2587\n",
      "71/281, train_loss: 0.0285, step time: 0.2493\n",
      "72/281, train_loss: 0.0401, step time: 0.2632\n",
      "73/281, train_loss: 0.2343, step time: 0.2541\n",
      "74/281, train_loss: 0.0405, step time: 0.2527\n",
      "75/281, train_loss: 0.0541, step time: 0.2432\n",
      "76/281, train_loss: 0.1989, step time: 0.2424\n",
      "77/281, train_loss: 0.0439, step time: 0.2465\n",
      "78/281, train_loss: 0.0715, step time: 0.2472\n",
      "79/281, train_loss: 0.0549, step time: 0.2550\n",
      "80/281, train_loss: 0.0459, step time: 0.2547\n",
      "81/281, train_loss: 0.0581, step time: 0.2569\n",
      "82/281, train_loss: 0.0700, step time: 0.2569\n",
      "83/281, train_loss: 0.0828, step time: 0.2557\n",
      "84/281, train_loss: 0.0765, step time: 0.2475\n",
      "85/281, train_loss: 0.0403, step time: 0.2581\n",
      "86/281, train_loss: 0.0559, step time: 0.2502\n",
      "87/281, train_loss: 0.2208, step time: 0.2532\n",
      "88/281, train_loss: 0.0760, step time: 0.2498\n",
      "89/281, train_loss: 0.0726, step time: 0.2536\n",
      "90/281, train_loss: 0.0413, step time: 0.2474\n",
      "91/281, train_loss: 0.0295, step time: 0.2532\n",
      "92/281, train_loss: 0.0403, step time: 0.2548\n",
      "93/281, train_loss: 0.0615, step time: 0.2544\n",
      "94/281, train_loss: 0.0482, step time: 0.2491\n",
      "95/281, train_loss: 0.0457, step time: 0.2528\n",
      "96/281, train_loss: 0.0525, step time: 0.2474\n",
      "97/281, train_loss: 0.0766, step time: 0.2513\n",
      "98/281, train_loss: 0.0759, step time: 0.2418\n",
      "99/281, train_loss: 0.0331, step time: 0.2441\n",
      "100/281, train_loss: 0.0612, step time: 0.2446\n",
      "101/281, train_loss: 0.0384, step time: 0.2451\n",
      "102/281, train_loss: 0.0840, step time: 0.2576\n",
      "103/281, train_loss: 0.0759, step time: 0.2454\n",
      "104/281, train_loss: 0.0810, step time: 0.2503\n",
      "105/281, train_loss: 0.0322, step time: 0.2474\n",
      "106/281, train_loss: 0.0499, step time: 0.2529\n",
      "107/281, train_loss: 0.0485, step time: 0.2504\n",
      "108/281, train_loss: 0.0916, step time: 0.2523\n",
      "109/281, train_loss: 0.0758, step time: 0.2508\n",
      "110/281, train_loss: 0.0387, step time: 0.2504\n",
      "111/281, train_loss: 0.0410, step time: 0.2490\n",
      "112/281, train_loss: 0.0429, step time: 0.2500\n",
      "113/281, train_loss: 0.0553, step time: 0.2469\n",
      "114/281, train_loss: 0.0671, step time: 0.2553\n",
      "115/281, train_loss: 0.0696, step time: 0.2545\n",
      "116/281, train_loss: 0.0398, step time: 0.2567\n",
      "117/281, train_loss: 0.2554, step time: 0.2516\n",
      "118/281, train_loss: 0.0771, step time: 0.2578\n",
      "119/281, train_loss: 0.0672, step time: 0.2593\n",
      "120/281, train_loss: 0.0426, step time: 0.2542\n",
      "121/281, train_loss: 0.0918, step time: 0.2594\n",
      "122/281, train_loss: 0.2537, step time: 0.2511\n",
      "123/281, train_loss: 0.0413, step time: 0.2548\n",
      "124/281, train_loss: 0.0807, step time: 0.2579\n",
      "125/281, train_loss: 0.2115, step time: 0.2564\n",
      "126/281, train_loss: 0.2065, step time: 0.2491\n",
      "127/281, train_loss: 0.0735, step time: 0.2523\n",
      "128/281, train_loss: 0.0701, step time: 0.2489\n",
      "129/281, train_loss: 0.0434, step time: 0.2522\n",
      "130/281, train_loss: 0.0546, step time: 0.2529\n",
      "131/281, train_loss: 0.0570, step time: 0.2551\n",
      "132/281, train_loss: 0.0485, step time: 0.2557\n",
      "133/281, train_loss: 0.0566, step time: 0.2555\n",
      "134/281, train_loss: 0.0395, step time: 0.2497\n",
      "135/281, train_loss: 0.2245, step time: 0.2473\n",
      "136/281, train_loss: 0.0530, step time: 0.2463\n",
      "137/281, train_loss: 0.0753, step time: 0.2526\n",
      "138/281, train_loss: 0.0410, step time: 0.2542\n",
      "139/281, train_loss: 0.0425, step time: 0.2561\n",
      "140/281, train_loss: 0.0574, step time: 0.2533\n",
      "141/281, train_loss: 0.0608, step time: 0.2561\n",
      "142/281, train_loss: 0.0527, step time: 0.2595\n",
      "143/281, train_loss: 0.0602, step time: 0.2493\n",
      "144/281, train_loss: 0.2107, step time: 0.2497\n",
      "145/281, train_loss: 0.0552, step time: 0.2500\n",
      "146/281, train_loss: 0.0545, step time: 0.2549\n",
      "147/281, train_loss: 0.0603, step time: 0.2514\n",
      "148/281, train_loss: 0.2133, step time: 0.2521\n",
      "149/281, train_loss: 0.0771, step time: 0.2514\n",
      "150/281, train_loss: 0.0613, step time: 0.2488\n",
      "151/281, train_loss: 0.0354, step time: 0.2549\n",
      "152/281, train_loss: 0.0594, step time: 0.2479\n",
      "153/281, train_loss: 0.1962, step time: 0.2467\n",
      "154/281, train_loss: 0.0777, step time: 0.2471\n",
      "155/281, train_loss: 0.0631, step time: 0.2478\n",
      "156/281, train_loss: 0.0369, step time: 0.2511\n",
      "157/281, train_loss: 0.0616, step time: 0.2492\n",
      "158/281, train_loss: 0.0937, step time: 0.2479\n",
      "159/281, train_loss: 0.0545, step time: 0.2535\n",
      "160/281, train_loss: 0.0649, step time: 0.2631\n",
      "161/281, train_loss: 0.0517, step time: 0.2818\n",
      "162/281, train_loss: 0.2093, step time: 0.2573\n",
      "163/281, train_loss: 0.0299, step time: 0.2503\n",
      "164/281, train_loss: 0.0736, step time: 0.2539\n",
      "165/281, train_loss: 0.0719, step time: 0.2553\n",
      "166/281, train_loss: 0.0661, step time: 0.2486\n",
      "167/281, train_loss: 0.0569, step time: 0.2571\n",
      "168/281, train_loss: 0.0778, step time: 0.2557\n",
      "169/281, train_loss: 0.0599, step time: 0.2589\n",
      "170/281, train_loss: 0.0416, step time: 0.2559\n",
      "171/281, train_loss: 0.0339, step time: 0.2492\n",
      "172/281, train_loss: 0.0646, step time: 0.2502\n",
      "173/281, train_loss: 0.0385, step time: 0.2548\n",
      "174/281, train_loss: 0.0359, step time: 0.2523\n",
      "175/281, train_loss: 0.0647, step time: 0.2543\n",
      "176/281, train_loss: 0.0241, step time: 0.2511\n",
      "177/281, train_loss: 0.0317, step time: 0.2542\n",
      "178/281, train_loss: 0.2332, step time: 0.2542\n",
      "179/281, train_loss: 0.0449, step time: 0.2477\n",
      "180/281, train_loss: 0.0632, step time: 0.2460\n",
      "181/281, train_loss: 0.0703, step time: 0.2517\n",
      "182/281, train_loss: 0.2172, step time: 0.2473\n",
      "183/281, train_loss: 0.0816, step time: 0.2525\n",
      "184/281, train_loss: 0.0553, step time: 0.2525\n",
      "185/281, train_loss: 0.0424, step time: 0.2526\n",
      "186/281, train_loss: 0.0504, step time: 0.2535\n",
      "187/281, train_loss: 0.0468, step time: 0.2550\n",
      "188/281, train_loss: 0.2279, step time: 0.2559\n",
      "189/281, train_loss: 0.0510, step time: 0.2542\n",
      "190/281, train_loss: 0.0558, step time: 0.2573\n",
      "191/281, train_loss: 0.0543, step time: 0.2518\n",
      "192/281, train_loss: 0.0525, step time: 0.2519\n",
      "193/281, train_loss: 0.3665, step time: 0.2505\n",
      "194/281, train_loss: 0.0551, step time: 0.2450\n",
      "195/281, train_loss: 0.0539, step time: 0.2447\n",
      "196/281, train_loss: 0.0830, step time: 0.2505\n",
      "197/281, train_loss: 0.0653, step time: 0.2481\n",
      "198/281, train_loss: 0.0352, step time: 0.2463\n",
      "199/281, train_loss: 0.0640, step time: 0.2465\n",
      "200/281, train_loss: 0.0704, step time: 0.2506\n",
      "201/281, train_loss: 0.0851, step time: 0.2483\n",
      "202/281, train_loss: 0.0474, step time: 0.2453\n",
      "203/281, train_loss: 0.0781, step time: 0.2451\n",
      "204/281, train_loss: 0.0406, step time: 0.2496\n",
      "205/281, train_loss: 0.0523, step time: 0.2501\n",
      "206/281, train_loss: 0.0619, step time: 0.2459\n",
      "207/281, train_loss: 0.0450, step time: 0.2528\n",
      "208/281, train_loss: 0.0618, step time: 0.2487\n",
      "209/281, train_loss: 0.0886, step time: 0.2520\n",
      "210/281, train_loss: 0.0607, step time: 0.2485\n",
      "211/281, train_loss: 0.0307, step time: 0.2456\n",
      "212/281, train_loss: 0.0649, step time: 0.2489\n",
      "213/281, train_loss: 0.0628, step time: 0.2441\n",
      "214/281, train_loss: 0.0291, step time: 0.2516\n",
      "215/281, train_loss: 0.0592, step time: 0.2454\n",
      "216/281, train_loss: 0.0630, step time: 0.2551\n",
      "217/281, train_loss: 0.0569, step time: 0.2439\n",
      "218/281, train_loss: 0.0580, step time: 0.2468\n",
      "219/281, train_loss: 0.0456, step time: 0.2468\n",
      "220/281, train_loss: 0.0489, step time: 0.2498\n",
      "221/281, train_loss: 0.0406, step time: 0.2533\n",
      "222/281, train_loss: 0.0780, step time: 0.2503\n",
      "223/281, train_loss: 0.0440, step time: 0.2477\n",
      "224/281, train_loss: 0.0477, step time: 0.2517\n",
      "225/281, train_loss: 0.0332, step time: 0.2480\n",
      "226/281, train_loss: 0.2173, step time: 0.2477\n",
      "227/281, train_loss: 0.0387, step time: 0.2541\n",
      "228/281, train_loss: 0.0381, step time: 0.2477\n",
      "229/281, train_loss: 0.0487, step time: 0.2500\n",
      "230/281, train_loss: 0.1965, step time: 0.2450\n",
      "231/281, train_loss: 0.0548, step time: 0.2465\n",
      "232/281, train_loss: 0.0496, step time: 0.2467\n",
      "233/281, train_loss: 0.0443, step time: 0.2468\n",
      "234/281, train_loss: 0.2039, step time: 0.2543\n",
      "235/281, train_loss: 0.0597, step time: 0.2518\n",
      "236/281, train_loss: 0.0431, step time: 0.2556\n",
      "237/281, train_loss: 0.0514, step time: 0.2541\n",
      "238/281, train_loss: 0.0367, step time: 0.2504\n",
      "239/281, train_loss: 0.1898, step time: 0.2534\n",
      "240/281, train_loss: 0.0732, step time: 0.2535\n",
      "241/281, train_loss: 0.0339, step time: 0.2546\n",
      "242/281, train_loss: 0.0476, step time: 0.2493\n",
      "243/281, train_loss: 0.0724, step time: 0.2525\n",
      "244/281, train_loss: 0.0967, step time: 0.2446\n",
      "245/281, train_loss: 0.0545, step time: 0.2505\n",
      "246/281, train_loss: 0.0633, step time: 0.2542\n",
      "247/281, train_loss: 0.2020, step time: 0.2571\n",
      "248/281, train_loss: 0.0676, step time: 0.2555\n",
      "249/281, train_loss: 0.0820, step time: 0.2498\n",
      "250/281, train_loss: 0.0402, step time: 0.2523\n",
      "251/281, train_loss: 0.2181, step time: 0.2540\n",
      "252/281, train_loss: 0.0594, step time: 0.2553\n",
      "253/281, train_loss: 0.0538, step time: 0.2504\n",
      "254/281, train_loss: 0.0624, step time: 0.2492\n",
      "255/281, train_loss: 0.0494, step time: 0.2596\n",
      "256/281, train_loss: 0.2014, step time: 0.2528\n",
      "257/281, train_loss: 0.1959, step time: 0.2513\n",
      "258/281, train_loss: 0.2222, step time: 0.2556\n",
      "259/281, train_loss: 0.2256, step time: 0.2537\n",
      "260/281, train_loss: 0.0388, step time: 0.2526\n",
      "261/281, train_loss: 0.0519, step time: 0.2469\n",
      "262/281, train_loss: 0.0504, step time: 0.2544\n",
      "263/281, train_loss: 0.0554, step time: 0.2490\n",
      "264/281, train_loss: 0.0595, step time: 0.2477\n",
      "265/281, train_loss: 0.0399, step time: 0.2539\n",
      "266/281, train_loss: 0.0739, step time: 0.2551\n",
      "267/281, train_loss: 0.0558, step time: 0.2535\n",
      "268/281, train_loss: 0.0476, step time: 0.2574\n",
      "269/281, train_loss: 0.0574, step time: 0.2567\n",
      "270/281, train_loss: 0.0374, step time: 0.2543\n",
      "271/281, train_loss: 0.0388, step time: 0.2514\n",
      "272/281, train_loss: 0.2274, step time: 0.2478\n",
      "273/281, train_loss: 0.2128, step time: 0.2493\n",
      "274/281, train_loss: 0.0654, step time: 0.2493\n",
      "275/281, train_loss: 0.0577, step time: 0.2504\n",
      "276/281, train_loss: 0.2172, step time: 0.2532\n",
      "277/281, train_loss: 0.0498, step time: 0.2504\n",
      "278/281, train_loss: 0.0514, step time: 0.2510\n",
      "279/281, train_loss: 0.0616, step time: 0.2465\n",
      "280/281, train_loss: 0.0606, step time: 0.2486\n",
      "281/281, train_loss: 0.0474, step time: 0.2483\n",
      "282/281, train_loss: 0.0629, step time: 0.1519\n",
      "epoch 128 average loss: 0.0787\n",
      "current epoch: 128 current mean dice: 0.9038 tc: 0.8994 wt: 0.9271 et: 0.8939\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 128 is: 345.9003\n",
      "----------\n",
      "epoch 129/200\n",
      "1/281, train_loss: 0.0447, step time: 0.2614\n",
      "2/281, train_loss: 0.0608, step time: 0.2846\n",
      "3/281, train_loss: 0.0476, step time: 0.2573\n",
      "4/281, train_loss: 0.2172, step time: 0.2481\n",
      "5/281, train_loss: 0.0386, step time: 0.2580\n",
      "6/281, train_loss: 0.2109, step time: 0.2568\n",
      "7/281, train_loss: 0.0522, step time: 0.2540\n",
      "8/281, train_loss: 0.0676, step time: 0.2558\n",
      "9/281, train_loss: 0.3898, step time: 0.2512\n",
      "10/281, train_loss: 0.0564, step time: 0.2532\n",
      "11/281, train_loss: 0.2077, step time: 0.2508\n",
      "12/281, train_loss: 0.0754, step time: 0.2512\n",
      "13/281, train_loss: 0.0309, step time: 0.2566\n",
      "14/281, train_loss: 0.2179, step time: 0.2572\n",
      "15/281, train_loss: 0.0649, step time: 0.2557\n",
      "16/281, train_loss: 0.0596, step time: 0.2474\n",
      "17/281, train_loss: 0.0520, step time: 0.2516\n",
      "18/281, train_loss: 0.0837, step time: 0.2552\n",
      "19/281, train_loss: 0.0453, step time: 0.2534\n",
      "20/281, train_loss: 0.2222, step time: 0.2521\n",
      "21/281, train_loss: 0.0489, step time: 0.2507\n",
      "22/281, train_loss: 0.0487, step time: 0.2515\n",
      "23/281, train_loss: 0.0634, step time: 0.2548\n",
      "24/281, train_loss: 0.0559, step time: 0.2471\n",
      "25/281, train_loss: 0.0532, step time: 0.2519\n",
      "26/281, train_loss: 0.0203, step time: 0.2498\n",
      "27/281, train_loss: 0.0519, step time: 0.2473\n",
      "28/281, train_loss: 0.0488, step time: 0.2519\n",
      "29/281, train_loss: 0.0463, step time: 0.2545\n",
      "30/281, train_loss: 0.0657, step time: 0.2489\n",
      "31/281, train_loss: 0.0738, step time: 0.2441\n",
      "32/281, train_loss: 0.0727, step time: 0.2525\n",
      "33/281, train_loss: 0.0406, step time: 0.2484\n",
      "34/281, train_loss: 0.0653, step time: 0.2486\n",
      "35/281, train_loss: 0.1982, step time: 0.2493\n",
      "36/281, train_loss: 0.0532, step time: 0.2491\n",
      "37/281, train_loss: 0.0383, step time: 0.2451\n",
      "38/281, train_loss: 0.0348, step time: 0.2442\n",
      "39/281, train_loss: 0.0506, step time: 0.2468\n",
      "40/281, train_loss: 0.0240, step time: 0.2474\n",
      "41/281, train_loss: 0.0742, step time: 0.2517\n",
      "42/281, train_loss: 0.0485, step time: 0.2577\n",
      "43/281, train_loss: 0.0484, step time: 0.2501\n",
      "44/281, train_loss: 0.0898, step time: 0.2517\n",
      "45/281, train_loss: 0.0973, step time: 0.2484\n",
      "46/281, train_loss: 0.0642, step time: 0.2469\n",
      "47/281, train_loss: 0.2027, step time: 0.2460\n",
      "48/281, train_loss: 0.2000, step time: 0.2454\n",
      "49/281, train_loss: 0.0567, step time: 0.2475\n",
      "50/281, train_loss: 0.0474, step time: 0.2472\n",
      "51/281, train_loss: 0.0549, step time: 0.2481\n",
      "52/281, train_loss: 0.0352, step time: 0.2436\n",
      "53/281, train_loss: 0.0524, step time: 0.2533\n",
      "54/281, train_loss: 0.0645, step time: 0.2623\n",
      "55/281, train_loss: 0.0570, step time: 0.2522\n",
      "56/281, train_loss: 0.2001, step time: 0.2423\n",
      "57/281, train_loss: 0.0640, step time: 0.2489\n",
      "58/281, train_loss: 0.0453, step time: 0.2460\n",
      "59/281, train_loss: 0.0633, step time: 0.2466\n",
      "60/281, train_loss: 0.0374, step time: 0.2499\n",
      "61/281, train_loss: 0.0518, step time: 0.2511\n",
      "62/281, train_loss: 0.2228, step time: 0.2536\n",
      "63/281, train_loss: 0.0682, step time: 0.2526\n",
      "64/281, train_loss: 0.0451, step time: 0.2513\n",
      "65/281, train_loss: 0.0571, step time: 0.2551\n",
      "66/281, train_loss: 0.0440, step time: 0.2580\n",
      "67/281, train_loss: 0.0564, step time: 0.2583\n",
      "68/281, train_loss: 0.0553, step time: 0.2572\n",
      "69/281, train_loss: 0.2182, step time: 0.2497\n",
      "70/281, train_loss: 0.0457, step time: 0.2512\n",
      "71/281, train_loss: 0.0628, step time: 0.2545\n",
      "72/281, train_loss: 0.0497, step time: 0.2530\n",
      "73/281, train_loss: 0.0433, step time: 0.2584\n",
      "74/281, train_loss: 0.2328, step time: 0.2609\n",
      "75/281, train_loss: 0.0539, step time: 0.2517\n",
      "76/281, train_loss: 0.0896, step time: 0.2609\n",
      "77/281, train_loss: 0.2492, step time: 0.2527\n",
      "78/281, train_loss: 0.0411, step time: 0.2552\n",
      "79/281, train_loss: 0.0647, step time: 0.2531\n",
      "80/281, train_loss: 0.0582, step time: 0.2504\n",
      "81/281, train_loss: 0.0421, step time: 0.2518\n",
      "82/281, train_loss: 0.0626, step time: 0.2493\n",
      "83/281, train_loss: 0.0546, step time: 0.2511\n",
      "84/281, train_loss: 0.0456, step time: 0.2503\n",
      "85/281, train_loss: 0.0422, step time: 0.2524\n",
      "86/281, train_loss: 0.0432, step time: 0.2555\n",
      "87/281, train_loss: 0.0491, step time: 0.2568\n",
      "88/281, train_loss: 0.0560, step time: 0.2480\n",
      "89/281, train_loss: 0.0418, step time: 0.2450\n",
      "90/281, train_loss: 0.0743, step time: 0.2582\n",
      "91/281, train_loss: 0.0544, step time: 0.2539\n",
      "92/281, train_loss: 0.0202, step time: 0.2528\n",
      "93/281, train_loss: 0.0742, step time: 0.2559\n",
      "94/281, train_loss: 0.0518, step time: 0.2516\n",
      "95/281, train_loss: 0.0660, step time: 0.2559\n",
      "96/281, train_loss: 0.0499, step time: 0.2538\n",
      "97/281, train_loss: 0.2285, step time: 0.2505\n",
      "98/281, train_loss: 0.0546, step time: 0.2557\n",
      "99/281, train_loss: 0.0317, step time: 0.2518\n",
      "100/281, train_loss: 0.0429, step time: 0.2517\n",
      "101/281, train_loss: 0.0401, step time: 0.2502\n",
      "102/281, train_loss: 0.0957, step time: 0.2498\n",
      "103/281, train_loss: 0.0471, step time: 0.2463\n",
      "104/281, train_loss: 0.2304, step time: 0.2510\n",
      "105/281, train_loss: 0.0418, step time: 0.2517\n",
      "106/281, train_loss: 0.0460, step time: 0.2557\n",
      "107/281, train_loss: 0.0474, step time: 0.2424\n",
      "108/281, train_loss: 0.0467, step time: 0.2433\n",
      "109/281, train_loss: 0.0757, step time: 0.2429\n",
      "110/281, train_loss: 0.0585, step time: 0.2451\n",
      "111/281, train_loss: 0.0565, step time: 0.2482\n",
      "112/281, train_loss: 0.0628, step time: 0.2424\n",
      "113/281, train_loss: 0.0397, step time: 0.2484\n",
      "114/281, train_loss: 0.0564, step time: 0.2514\n",
      "115/281, train_loss: 0.0485, step time: 0.2500\n",
      "116/281, train_loss: 0.0643, step time: 0.2493\n",
      "117/281, train_loss: 0.0496, step time: 0.2667\n",
      "118/281, train_loss: 0.0276, step time: 0.2547\n",
      "119/281, train_loss: 0.0693, step time: 0.2561\n",
      "120/281, train_loss: 0.0747, step time: 0.2557\n",
      "121/281, train_loss: 0.0365, step time: 0.2568\n",
      "122/281, train_loss: 0.0574, step time: 0.2560\n",
      "123/281, train_loss: 0.0464, step time: 0.2548\n",
      "124/281, train_loss: 0.0909, step time: 0.2513\n",
      "125/281, train_loss: 0.0766, step time: 0.2533\n",
      "126/281, train_loss: 0.1041, step time: 0.2581\n",
      "127/281, train_loss: 0.0374, step time: 0.2492\n",
      "128/281, train_loss: 0.2329, step time: 0.2464\n",
      "129/281, train_loss: 0.0733, step time: 0.2435\n",
      "130/281, train_loss: 0.0724, step time: 0.2481\n",
      "131/281, train_loss: 0.0542, step time: 0.2497\n",
      "132/281, train_loss: 0.0479, step time: 0.2513\n",
      "133/281, train_loss: 0.0652, step time: 0.2500\n",
      "134/281, train_loss: 0.0403, step time: 0.2472\n",
      "135/281, train_loss: 0.0579, step time: 0.2426\n",
      "136/281, train_loss: 0.2028, step time: 0.2429\n",
      "137/281, train_loss: 0.0629, step time: 0.2426\n",
      "138/281, train_loss: 0.0791, step time: 0.2422\n",
      "139/281, train_loss: 0.0569, step time: 0.2443\n",
      "140/281, train_loss: 0.0462, step time: 0.2450\n",
      "141/281, train_loss: 0.0755, step time: 0.2396\n",
      "142/281, train_loss: 0.0722, step time: 0.2522\n",
      "143/281, train_loss: 0.0505, step time: 0.2443\n",
      "144/281, train_loss: 0.0406, step time: 0.2443\n",
      "145/281, train_loss: 0.0665, step time: 0.2423\n",
      "146/281, train_loss: 0.2026, step time: 0.2402\n",
      "147/281, train_loss: 0.0177, step time: 0.2414\n",
      "148/281, train_loss: 0.0421, step time: 0.2504\n",
      "149/281, train_loss: 0.0490, step time: 0.2538\n",
      "150/281, train_loss: 0.0453, step time: 0.2461\n",
      "151/281, train_loss: 0.0442, step time: 0.2513\n",
      "152/281, train_loss: 0.0399, step time: 0.2455\n",
      "153/281, train_loss: 0.0759, step time: 0.2453\n",
      "154/281, train_loss: 0.0630, step time: 0.2437\n",
      "155/281, train_loss: 0.0705, step time: 0.2556\n",
      "156/281, train_loss: 0.0300, step time: 0.2467\n",
      "157/281, train_loss: 0.0553, step time: 0.2518\n",
      "158/281, train_loss: 0.0409, step time: 0.2451\n",
      "159/281, train_loss: 0.0616, step time: 0.2581\n",
      "160/281, train_loss: 0.0518, step time: 0.2470\n",
      "161/281, train_loss: 0.0510, step time: 0.2450\n",
      "162/281, train_loss: 0.2026, step time: 0.2531\n",
      "163/281, train_loss: 0.2164, step time: 0.2486\n",
      "164/281, train_loss: 0.0494, step time: 0.2558\n",
      "165/281, train_loss: 0.2162, step time: 0.2538\n",
      "166/281, train_loss: 0.0877, step time: 0.2440\n",
      "167/281, train_loss: 0.0584, step time: 0.2456\n",
      "168/281, train_loss: 0.0574, step time: 0.2452\n",
      "169/281, train_loss: 0.2200, step time: 0.2454\n",
      "170/281, train_loss: 0.0431, step time: 0.2512\n",
      "171/281, train_loss: 0.0675, step time: 0.2474\n",
      "172/281, train_loss: 0.0318, step time: 0.2456\n",
      "173/281, train_loss: 0.0584, step time: 0.2441\n",
      "174/281, train_loss: 0.2161, step time: 0.2519\n",
      "175/281, train_loss: 0.1101, step time: 0.2449\n",
      "176/281, train_loss: 0.0748, step time: 0.2471\n",
      "177/281, train_loss: 0.0388, step time: 0.2493\n",
      "178/281, train_loss: 0.0555, step time: 0.2446\n",
      "179/281, train_loss: 0.2069, step time: 0.2487\n",
      "180/281, train_loss: 0.2297, step time: 0.2480\n",
      "181/281, train_loss: 0.0892, step time: 0.2497\n",
      "182/281, train_loss: 0.0510, step time: 0.2474\n",
      "183/281, train_loss: 0.2149, step time: 0.2420\n",
      "184/281, train_loss: 0.0412, step time: 0.2404\n",
      "185/281, train_loss: 0.0781, step time: 0.2441\n",
      "186/281, train_loss: 0.0644, step time: 0.2459\n",
      "187/281, train_loss: 0.0630, step time: 0.2504\n",
      "188/281, train_loss: 0.0611, step time: 0.2482\n",
      "189/281, train_loss: 0.0355, step time: 0.2562\n",
      "190/281, train_loss: 0.2191, step time: 0.2646\n",
      "191/281, train_loss: 0.0410, step time: 0.2430\n",
      "192/281, train_loss: 0.0364, step time: 0.2462\n",
      "193/281, train_loss: 0.0519, step time: 0.2489\n",
      "194/281, train_loss: 0.1085, step time: 0.2451\n",
      "195/281, train_loss: 0.0455, step time: 0.2456\n",
      "196/281, train_loss: 0.0391, step time: 0.2487\n",
      "197/281, train_loss: 0.0357, step time: 0.2439\n",
      "198/281, train_loss: 0.0686, step time: 0.2471\n",
      "199/281, train_loss: 0.0257, step time: 0.2468\n",
      "200/281, train_loss: 0.0501, step time: 0.2470\n",
      "201/281, train_loss: 0.2405, step time: 0.2431\n",
      "202/281, train_loss: 0.0632, step time: 0.2501\n",
      "203/281, train_loss: 0.2131, step time: 0.2471\n",
      "204/281, train_loss: 0.0602, step time: 0.2495\n",
      "205/281, train_loss: 0.2103, step time: 0.2506\n",
      "206/281, train_loss: 0.0254, step time: 0.2472\n",
      "207/281, train_loss: 0.0619, step time: 0.2542\n",
      "208/281, train_loss: 0.0721, step time: 0.2528\n",
      "209/281, train_loss: 0.0494, step time: 0.2479\n",
      "210/281, train_loss: 0.0325, step time: 0.2511\n",
      "211/281, train_loss: 0.0385, step time: 0.2543\n",
      "212/281, train_loss: 0.0427, step time: 0.2425\n",
      "213/281, train_loss: 0.2118, step time: 0.2444\n",
      "214/281, train_loss: 0.0543, step time: 0.2457\n",
      "215/281, train_loss: 0.0508, step time: 0.2476\n",
      "216/281, train_loss: 0.0552, step time: 0.2458\n",
      "217/281, train_loss: 0.0398, step time: 0.2480\n",
      "218/281, train_loss: 0.0833, step time: 0.2442\n",
      "219/281, train_loss: 0.0427, step time: 0.2464\n",
      "220/281, train_loss: 0.0763, step time: 0.2481\n",
      "221/281, train_loss: 0.0572, step time: 0.2450\n",
      "222/281, train_loss: 0.0448, step time: 0.2436\n",
      "223/281, train_loss: 0.0746, step time: 0.2514\n",
      "224/281, train_loss: 0.0523, step time: 0.2531\n",
      "225/281, train_loss: 0.0526, step time: 0.2486\n",
      "226/281, train_loss: 0.0670, step time: 0.2462\n",
      "227/281, train_loss: 0.0396, step time: 0.2416\n",
      "228/281, train_loss: 0.0525, step time: 0.2524\n",
      "229/281, train_loss: 0.0485, step time: 0.2464\n",
      "230/281, train_loss: 0.0464, step time: 0.2484\n",
      "231/281, train_loss: 0.2132, step time: 0.2431\n",
      "232/281, train_loss: 0.0546, step time: 0.2431\n",
      "233/281, train_loss: 0.0910, step time: 0.2432\n",
      "234/281, train_loss: 0.0578, step time: 0.2460\n",
      "235/281, train_loss: 0.0793, step time: 0.2479\n",
      "236/281, train_loss: 0.0616, step time: 0.2491\n",
      "237/281, train_loss: 0.0355, step time: 0.2413\n",
      "238/281, train_loss: 0.0489, step time: 0.2428\n",
      "239/281, train_loss: 0.0423, step time: 0.2420\n",
      "240/281, train_loss: 0.0699, step time: 0.2415\n",
      "241/281, train_loss: 0.0463, step time: 0.2478\n",
      "242/281, train_loss: 0.0561, step time: 0.2435\n",
      "243/281, train_loss: 0.0470, step time: 0.2432\n",
      "244/281, train_loss: 0.0441, step time: 0.2376\n",
      "245/281, train_loss: 0.0987, step time: 0.2564\n",
      "246/281, train_loss: 0.2060, step time: 0.2479\n",
      "247/281, train_loss: 0.2104, step time: 0.2450\n",
      "248/281, train_loss: 0.0368, step time: 0.2433\n",
      "249/281, train_loss: 0.0745, step time: 0.2438\n",
      "250/281, train_loss: 0.0789, step time: 0.2427\n",
      "251/281, train_loss: 0.0427, step time: 0.2445\n",
      "252/281, train_loss: 0.0319, step time: 0.2471\n",
      "253/281, train_loss: 0.0847, step time: 0.2513\n",
      "254/281, train_loss: 0.0384, step time: 0.2487\n",
      "255/281, train_loss: 0.0405, step time: 0.2456\n",
      "256/281, train_loss: 0.0492, step time: 0.2426\n",
      "257/281, train_loss: 0.2315, step time: 0.2441\n",
      "258/281, train_loss: 0.0391, step time: 0.2459\n",
      "259/281, train_loss: 0.0462, step time: 0.2447\n",
      "260/281, train_loss: 0.0547, step time: 0.2383\n",
      "261/281, train_loss: 0.0474, step time: 0.2483\n",
      "262/281, train_loss: 0.0844, step time: 0.2483\n",
      "263/281, train_loss: 0.0697, step time: 0.2455\n",
      "264/281, train_loss: 0.0557, step time: 0.2525\n",
      "265/281, train_loss: 0.0616, step time: 0.2470\n",
      "266/281, train_loss: 0.0629, step time: 0.2460\n",
      "267/281, train_loss: 0.0338, step time: 0.2483\n",
      "268/281, train_loss: 0.0565, step time: 0.2428\n",
      "269/281, train_loss: 0.0390, step time: 0.2536\n",
      "270/281, train_loss: 0.0752, step time: 0.2565\n",
      "271/281, train_loss: 0.2112, step time: 0.2731\n",
      "272/281, train_loss: 0.0453, step time: 0.2633\n",
      "273/281, train_loss: 0.0436, step time: 0.2498\n",
      "274/281, train_loss: 0.2027, step time: 0.2528\n",
      "275/281, train_loss: 0.0634, step time: 0.2528\n",
      "276/281, train_loss: 0.0770, step time: 0.2497\n",
      "277/281, train_loss: 0.0665, step time: 0.2477\n",
      "278/281, train_loss: 0.0438, step time: 0.2497\n",
      "279/281, train_loss: 0.2070, step time: 0.2528\n",
      "280/281, train_loss: 0.0236, step time: 0.2468\n",
      "281/281, train_loss: 0.0660, step time: 0.2486\n",
      "282/281, train_loss: 0.3832, step time: 0.1486\n",
      "epoch 129 average loss: 0.0791\n",
      "current epoch: 129 current mean dice: 0.9035 tc: 0.8986 wt: 0.9274 et: 0.8938\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 129 is: 363.0804\n",
      "----------\n",
      "epoch 130/200\n",
      "1/281, train_loss: 0.2473, step time: 0.2573\n",
      "2/281, train_loss: 0.0462, step time: 0.2464\n",
      "3/281, train_loss: 0.0410, step time: 0.2556\n",
      "4/281, train_loss: 0.0427, step time: 0.2528\n",
      "5/281, train_loss: 0.0263, step time: 0.2469\n",
      "6/281, train_loss: 0.0497, step time: 0.2552\n",
      "7/281, train_loss: 0.0762, step time: 0.2556\n",
      "8/281, train_loss: 0.0583, step time: 0.2579\n",
      "9/281, train_loss: 0.0837, step time: 0.2547\n",
      "10/281, train_loss: 0.2102, step time: 0.2518\n",
      "11/281, train_loss: 0.0526, step time: 0.2418\n",
      "12/281, train_loss: 0.0447, step time: 0.2489\n",
      "13/281, train_loss: 0.0433, step time: 0.2487\n",
      "14/281, train_loss: 0.1969, step time: 0.2463\n",
      "15/281, train_loss: 0.2037, step time: 0.2576\n",
      "16/281, train_loss: 0.0524, step time: 0.2566\n",
      "17/281, train_loss: 0.0637, step time: 0.2486\n",
      "18/281, train_loss: 0.0475, step time: 0.2535\n",
      "19/281, train_loss: 0.1060, step time: 0.2443\n",
      "20/281, train_loss: 0.0386, step time: 0.2434\n",
      "21/281, train_loss: 0.0688, step time: 0.2501\n",
      "22/281, train_loss: 0.2293, step time: 0.2579\n",
      "23/281, train_loss: 0.1998, step time: 0.2527\n",
      "24/281, train_loss: 0.0678, step time: 0.2474\n",
      "25/281, train_loss: 0.0700, step time: 0.2501\n",
      "26/281, train_loss: 0.0405, step time: 0.2379\n",
      "27/281, train_loss: 0.1118, step time: 0.2430\n",
      "28/281, train_loss: 0.0500, step time: 0.2458\n",
      "29/281, train_loss: 0.0407, step time: 0.2488\n",
      "30/281, train_loss: 0.0262, step time: 0.2419\n",
      "31/281, train_loss: 0.0412, step time: 0.2434\n",
      "32/281, train_loss: 0.0312, step time: 0.2449\n",
      "33/281, train_loss: 0.0523, step time: 0.2492\n",
      "34/281, train_loss: 0.0387, step time: 0.2451\n",
      "35/281, train_loss: 0.0711, step time: 0.2477\n",
      "36/281, train_loss: 0.0673, step time: 0.2512\n",
      "37/281, train_loss: 0.0341, step time: 0.2437\n",
      "38/281, train_loss: 0.0804, step time: 0.2383\n",
      "39/281, train_loss: 0.0463, step time: 0.2409\n",
      "40/281, train_loss: 0.0619, step time: 0.2380\n",
      "41/281, train_loss: 0.0659, step time: 0.2410\n",
      "42/281, train_loss: 0.2080, step time: 0.2450\n",
      "43/281, train_loss: 0.0368, step time: 0.2435\n",
      "44/281, train_loss: 0.0622, step time: 0.2431\n",
      "45/281, train_loss: 0.0376, step time: 0.2431\n",
      "46/281, train_loss: 0.0563, step time: 0.2492\n",
      "47/281, train_loss: 0.0333, step time: 0.2432\n",
      "48/281, train_loss: 0.0403, step time: 0.2425\n",
      "49/281, train_loss: 0.0564, step time: 0.2459\n",
      "50/281, train_loss: 0.0818, step time: 0.2529\n",
      "51/281, train_loss: 0.0782, step time: 0.2502\n",
      "52/281, train_loss: 0.2041, step time: 0.2404\n",
      "53/281, train_loss: 0.0574, step time: 0.2477\n",
      "54/281, train_loss: 0.0872, step time: 0.2436\n",
      "55/281, train_loss: 0.0613, step time: 0.2441\n",
      "56/281, train_loss: 0.0819, step time: 0.2449\n",
      "57/281, train_loss: 0.2261, step time: 0.2507\n",
      "58/281, train_loss: 0.2088, step time: 0.2507\n",
      "59/281, train_loss: 0.2219, step time: 0.2514\n",
      "60/281, train_loss: 0.0647, step time: 0.2440\n",
      "61/281, train_loss: 0.0645, step time: 0.2413\n",
      "62/281, train_loss: 0.0430, step time: 0.2711\n",
      "63/281, train_loss: 0.0430, step time: 0.2550\n",
      "64/281, train_loss: 0.0857, step time: 0.2452\n",
      "65/281, train_loss: 0.0514, step time: 0.2538\n",
      "66/281, train_loss: 0.0791, step time: 0.2512\n",
      "67/281, train_loss: 0.0315, step time: 0.2446\n",
      "68/281, train_loss: 0.0545, step time: 0.2511\n",
      "69/281, train_loss: 0.0275, step time: 0.2444\n",
      "70/281, train_loss: 0.0540, step time: 0.2478\n",
      "71/281, train_loss: 0.0566, step time: 0.2535\n",
      "72/281, train_loss: 0.2230, step time: 0.2479\n",
      "73/281, train_loss: 0.0775, step time: 0.2491\n",
      "74/281, train_loss: 0.0694, step time: 0.2525\n",
      "75/281, train_loss: 0.0443, step time: 0.2515\n",
      "76/281, train_loss: 0.0413, step time: 0.2459\n",
      "77/281, train_loss: 0.0622, step time: 0.2480\n",
      "78/281, train_loss: 0.2118, step time: 0.2422\n",
      "79/281, train_loss: 0.0580, step time: 0.2491\n",
      "80/281, train_loss: 0.0429, step time: 0.2485\n",
      "81/281, train_loss: 0.0436, step time: 0.2518\n",
      "82/281, train_loss: 0.0525, step time: 0.2510\n",
      "83/281, train_loss: 0.2106, step time: 0.2550\n",
      "84/281, train_loss: 0.0338, step time: 0.2408\n",
      "85/281, train_loss: 0.0707, step time: 0.2389\n",
      "86/281, train_loss: 0.0468, step time: 0.2442\n",
      "87/281, train_loss: 0.0545, step time: 0.2442\n",
      "88/281, train_loss: 0.0583, step time: 0.2468\n",
      "89/281, train_loss: 0.0463, step time: 0.2464\n",
      "90/281, train_loss: 0.0573, step time: 0.2482\n",
      "91/281, train_loss: 0.0462, step time: 0.2495\n",
      "92/281, train_loss: 0.0419, step time: 0.2476\n",
      "93/281, train_loss: 0.0703, step time: 0.2625\n",
      "94/281, train_loss: 0.0537, step time: 0.2483\n",
      "95/281, train_loss: 0.0550, step time: 0.2476\n",
      "96/281, train_loss: 0.0655, step time: 0.2478\n",
      "97/281, train_loss: 0.0387, step time: 0.2493\n",
      "98/281, train_loss: 0.0370, step time: 0.2492\n",
      "99/281, train_loss: 0.1050, step time: 0.2501\n",
      "100/281, train_loss: 0.2499, step time: 0.2498\n",
      "101/281, train_loss: 0.0536, step time: 0.2529\n",
      "102/281, train_loss: 0.2014, step time: 0.2425\n",
      "103/281, train_loss: 0.0588, step time: 0.2451\n",
      "104/281, train_loss: 0.0553, step time: 0.2459\n",
      "105/281, train_loss: 0.0535, step time: 0.2478\n",
      "106/281, train_loss: 0.0712, step time: 0.2456\n",
      "107/281, train_loss: 0.0521, step time: 0.2424\n",
      "108/281, train_loss: 0.0930, step time: 0.2480\n",
      "109/281, train_loss: 0.0588, step time: 0.2455\n",
      "110/281, train_loss: 0.0360, step time: 0.2472\n",
      "111/281, train_loss: 0.0803, step time: 0.2475\n",
      "112/281, train_loss: 0.0809, step time: 0.2487\n",
      "113/281, train_loss: 0.0528, step time: 0.2446\n",
      "114/281, train_loss: 0.0626, step time: 0.2469\n",
      "115/281, train_loss: 0.0568, step time: 0.2474\n",
      "116/281, train_loss: 0.0507, step time: 0.2520\n",
      "117/281, train_loss: 0.2149, step time: 0.2493\n",
      "118/281, train_loss: 0.0433, step time: 0.2457\n",
      "119/281, train_loss: 0.0789, step time: 0.2496\n",
      "120/281, train_loss: 0.0444, step time: 0.2458\n",
      "121/281, train_loss: 0.0506, step time: 0.2454\n",
      "122/281, train_loss: 0.0471, step time: 0.2427\n",
      "123/281, train_loss: 0.0788, step time: 0.2516\n",
      "124/281, train_loss: 0.0920, step time: 0.2440\n",
      "125/281, train_loss: 0.0314, step time: 0.2436\n",
      "126/281, train_loss: 0.0373, step time: 0.2471\n",
      "127/281, train_loss: 0.0487, step time: 0.2466\n",
      "128/281, train_loss: 0.0647, step time: 0.2494\n",
      "129/281, train_loss: 0.0499, step time: 0.2484\n",
      "130/281, train_loss: 0.0520, step time: 0.2463\n",
      "131/281, train_loss: 0.0326, step time: 0.2426\n",
      "132/281, train_loss: 0.0540, step time: 0.2439\n",
      "133/281, train_loss: 0.0470, step time: 0.2484\n",
      "134/281, train_loss: 0.2354, step time: 0.2468\n",
      "135/281, train_loss: 0.0808, step time: 0.2430\n",
      "136/281, train_loss: 0.0449, step time: 0.2451\n",
      "137/281, train_loss: 0.0658, step time: 0.2509\n",
      "138/281, train_loss: 0.0490, step time: 0.2414\n",
      "139/281, train_loss: 0.0896, step time: 0.2415\n",
      "140/281, train_loss: 0.2287, step time: 0.2446\n",
      "141/281, train_loss: 0.0421, step time: 0.2533\n",
      "142/281, train_loss: 0.0449, step time: 0.2432\n",
      "143/281, train_loss: 0.0475, step time: 0.2463\n",
      "144/281, train_loss: 0.0341, step time: 0.2493\n",
      "145/281, train_loss: 0.0643, step time: 0.2480\n",
      "146/281, train_loss: 0.0335, step time: 0.2464\n",
      "147/281, train_loss: 0.0630, step time: 0.2530\n",
      "148/281, train_loss: 0.0384, step time: 0.2499\n",
      "149/281, train_loss: 0.0436, step time: 0.2463\n",
      "150/281, train_loss: 0.0488, step time: 0.2417\n",
      "151/281, train_loss: 0.0790, step time: 0.2455\n",
      "152/281, train_loss: 0.2438, step time: 0.2484\n",
      "153/281, train_loss: 0.2059, step time: 0.2489\n",
      "154/281, train_loss: 0.0445, step time: 0.2416\n",
      "155/281, train_loss: 0.0515, step time: 0.2445\n",
      "156/281, train_loss: 0.0325, step time: 0.2520\n",
      "157/281, train_loss: 0.3813, step time: 0.2471\n",
      "158/281, train_loss: 0.0660, step time: 0.2505\n",
      "159/281, train_loss: 0.0561, step time: 0.2418\n",
      "160/281, train_loss: 0.0521, step time: 0.2429\n",
      "161/281, train_loss: 0.0446, step time: 0.2422\n",
      "162/281, train_loss: 0.0788, step time: 0.2514\n",
      "163/281, train_loss: 0.0329, step time: 0.2481\n",
      "164/281, train_loss: 0.0451, step time: 0.2503\n",
      "165/281, train_loss: 0.0526, step time: 0.2518\n",
      "166/281, train_loss: 0.2304, step time: 0.2508\n",
      "167/281, train_loss: 0.0353, step time: 0.2491\n",
      "168/281, train_loss: 0.2256, step time: 0.2518\n",
      "169/281, train_loss: 0.0527, step time: 0.2462\n",
      "170/281, train_loss: 0.0691, step time: 0.2457\n",
      "171/281, train_loss: 0.2167, step time: 0.2466\n",
      "172/281, train_loss: 0.0735, step time: 0.2410\n",
      "173/281, train_loss: 0.0607, step time: 0.2402\n",
      "174/281, train_loss: 0.0745, step time: 0.2531\n",
      "175/281, train_loss: 0.0543, step time: 0.2446\n",
      "176/281, train_loss: 0.0504, step time: 0.2449\n",
      "177/281, train_loss: 0.0419, step time: 0.2471\n",
      "178/281, train_loss: 0.0432, step time: 0.2505\n",
      "179/281, train_loss: 0.0641, step time: 0.2459\n",
      "180/281, train_loss: 0.2096, step time: 0.2437\n",
      "181/281, train_loss: 0.0776, step time: 0.2512\n",
      "182/281, train_loss: 0.0693, step time: 0.2509\n",
      "183/281, train_loss: 0.2049, step time: 0.2526\n",
      "184/281, train_loss: 0.0512, step time: 0.2540\n",
      "185/281, train_loss: 0.0463, step time: 0.2503\n",
      "186/281, train_loss: 0.0446, step time: 0.2465\n",
      "187/281, train_loss: 0.0332, step time: 0.2490\n",
      "188/281, train_loss: 0.0590, step time: 0.2476\n",
      "189/281, train_loss: 0.0475, step time: 0.2501\n",
      "190/281, train_loss: 0.0513, step time: 0.2501\n",
      "191/281, train_loss: 0.0421, step time: 0.2447\n",
      "192/281, train_loss: 0.0622, step time: 0.2487\n",
      "193/281, train_loss: 0.0633, step time: 0.2526\n",
      "194/281, train_loss: 0.0515, step time: 0.2475\n",
      "195/281, train_loss: 0.2314, step time: 0.2438\n",
      "196/281, train_loss: 0.0406, step time: 0.2549\n",
      "197/281, train_loss: 0.0482, step time: 0.2554\n",
      "198/281, train_loss: 0.0562, step time: 0.2524\n",
      "199/281, train_loss: 0.0407, step time: 0.2508\n",
      "200/281, train_loss: 0.2176, step time: 0.2510\n",
      "201/281, train_loss: 0.0466, step time: 0.2564\n",
      "202/281, train_loss: 0.0525, step time: 0.2468\n",
      "203/281, train_loss: 0.0696, step time: 0.2508\n",
      "204/281, train_loss: 0.2109, step time: 0.2485\n",
      "205/281, train_loss: 0.0997, step time: 0.2458\n",
      "206/281, train_loss: 0.0481, step time: 0.2484\n",
      "207/281, train_loss: 0.0556, step time: 0.2502\n",
      "208/281, train_loss: 0.0709, step time: 0.2519\n",
      "209/281, train_loss: 0.2321, step time: 0.2465\n",
      "210/281, train_loss: 0.0405, step time: 0.2525\n",
      "211/281, train_loss: 0.0501, step time: 0.2487\n",
      "212/281, train_loss: 0.0543, step time: 0.2529\n",
      "213/281, train_loss: 0.0611, step time: 0.2452\n",
      "214/281, train_loss: 0.0680, step time: 0.2473\n",
      "215/281, train_loss: 0.1101, step time: 0.2508\n",
      "216/281, train_loss: 0.0629, step time: 0.2503\n",
      "217/281, train_loss: 0.0562, step time: 0.2519\n",
      "218/281, train_loss: 0.0506, step time: 0.2451\n",
      "219/281, train_loss: 0.0366, step time: 0.2456\n",
      "220/281, train_loss: 0.0646, step time: 0.2507\n",
      "221/281, train_loss: 0.0616, step time: 0.2474\n",
      "222/281, train_loss: 0.0541, step time: 0.2437\n",
      "223/281, train_loss: 0.0295, step time: 0.2420\n",
      "224/281, train_loss: 0.0460, step time: 0.2482\n",
      "225/281, train_loss: 0.0344, step time: 0.2452\n",
      "226/281, train_loss: 0.0488, step time: 0.2472\n",
      "227/281, train_loss: 0.0542, step time: 0.2537\n",
      "228/281, train_loss: 0.0429, step time: 0.2473\n",
      "229/281, train_loss: 0.0413, step time: 0.2459\n",
      "230/281, train_loss: 0.0331, step time: 0.2439\n",
      "231/281, train_loss: 0.2069, step time: 0.2489\n",
      "232/281, train_loss: 0.0539, step time: 0.2490\n",
      "233/281, train_loss: 0.0723, step time: 0.2464\n",
      "234/281, train_loss: 0.0333, step time: 0.2419\n",
      "235/281, train_loss: 0.0540, step time: 0.2463\n",
      "236/281, train_loss: 0.0379, step time: 0.2491\n",
      "237/281, train_loss: 0.0569, step time: 0.2468\n",
      "238/281, train_loss: 0.0540, step time: 0.2512\n",
      "239/281, train_loss: 0.0453, step time: 0.2525\n",
      "240/281, train_loss: 0.0575, step time: 0.2512\n",
      "241/281, train_loss: 0.0445, step time: 0.2483\n",
      "242/281, train_loss: 0.0522, step time: 0.2460\n",
      "243/281, train_loss: 0.0466, step time: 0.2482\n",
      "244/281, train_loss: 0.2097, step time: 0.2526\n",
      "245/281, train_loss: 0.2026, step time: 0.2526\n",
      "246/281, train_loss: 0.0512, step time: 0.2509\n",
      "247/281, train_loss: 0.0783, step time: 0.2534\n",
      "248/281, train_loss: 0.0285, step time: 0.2533\n",
      "249/281, train_loss: 0.0406, step time: 0.2509\n",
      "250/281, train_loss: 0.0422, step time: 0.2506\n",
      "251/281, train_loss: 0.0695, step time: 0.2458\n",
      "252/281, train_loss: 0.2218, step time: 0.2561\n",
      "253/281, train_loss: 0.0644, step time: 0.2526\n",
      "254/281, train_loss: 0.0516, step time: 0.2510\n",
      "255/281, train_loss: 0.0394, step time: 0.2490\n",
      "256/281, train_loss: 0.0676, step time: 0.2460\n",
      "257/281, train_loss: 0.0753, step time: 0.2505\n",
      "258/281, train_loss: 0.2100, step time: 0.2467\n",
      "259/281, train_loss: 0.2196, step time: 0.2491\n",
      "260/281, train_loss: 0.0539, step time: 0.2557\n",
      "261/281, train_loss: 0.0431, step time: 0.2543\n",
      "262/281, train_loss: 0.0507, step time: 0.2507\n",
      "263/281, train_loss: 0.0611, step time: 0.2497\n",
      "264/281, train_loss: 0.0488, step time: 0.2540\n",
      "265/281, train_loss: 0.0307, step time: 0.2545\n",
      "266/281, train_loss: 0.0559, step time: 0.2506\n",
      "267/281, train_loss: 0.1940, step time: 0.2518\n",
      "268/281, train_loss: 0.0704, step time: 0.2518\n",
      "269/281, train_loss: 0.2042, step time: 0.2528\n",
      "270/281, train_loss: 0.0583, step time: 0.2533\n",
      "271/281, train_loss: 0.0638, step time: 0.2574\n",
      "272/281, train_loss: 0.0746, step time: 0.2547\n",
      "273/281, train_loss: 0.0675, step time: 0.2497\n",
      "274/281, train_loss: 0.0406, step time: 0.2471\n",
      "275/281, train_loss: 0.0606, step time: 0.2494\n",
      "276/281, train_loss: 0.2215, step time: 0.2508\n",
      "277/281, train_loss: 0.0396, step time: 0.2569\n",
      "278/281, train_loss: 0.0556, step time: 0.2489\n",
      "279/281, train_loss: 0.0582, step time: 0.2469\n",
      "280/281, train_loss: 0.0433, step time: 0.2487\n",
      "281/281, train_loss: 0.0506, step time: 0.2503\n",
      "282/281, train_loss: 0.0418, step time: 0.1491\n",
      "epoch 130 average loss: 0.0784\n",
      "current epoch: 130 current mean dice: 0.9024 tc: 0.8966 wt: 0.9245 et: 0.8949\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 130 is: 368.0047\n",
      "----------\n",
      "epoch 131/200\n",
      "1/281, train_loss: 0.0273, step time: 0.2590\n",
      "2/281, train_loss: 0.0445, step time: 0.2583\n",
      "3/281, train_loss: 0.0502, step time: 0.2534\n",
      "4/281, train_loss: 0.0525, step time: 0.2579\n",
      "5/281, train_loss: 0.0474, step time: 0.2550\n",
      "6/281, train_loss: 0.0763, step time: 0.2605\n",
      "7/281, train_loss: 0.0682, step time: 0.2570\n",
      "8/281, train_loss: 0.0422, step time: 0.2531\n",
      "9/281, train_loss: 0.0796, step time: 0.2573\n",
      "10/281, train_loss: 0.0632, step time: 0.2581\n",
      "11/281, train_loss: 0.0994, step time: 0.2570\n",
      "12/281, train_loss: 0.0568, step time: 0.2628\n",
      "13/281, train_loss: 0.0470, step time: 0.2528\n",
      "14/281, train_loss: 0.0722, step time: 0.2589\n",
      "15/281, train_loss: 0.2068, step time: 0.2532\n",
      "16/281, train_loss: 0.0579, step time: 0.2549\n",
      "17/281, train_loss: 0.0529, step time: 0.2551\n",
      "18/281, train_loss: 0.0709, step time: 0.2623\n",
      "19/281, train_loss: 0.0452, step time: 0.2502\n",
      "20/281, train_loss: 0.0339, step time: 0.2481\n",
      "21/281, train_loss: 0.2124, step time: 0.2504\n",
      "22/281, train_loss: 0.0287, step time: 0.2486\n",
      "23/281, train_loss: 0.0641, step time: 0.2495\n",
      "24/281, train_loss: 0.0423, step time: 0.2486\n",
      "25/281, train_loss: 0.0576, step time: 0.2502\n",
      "26/281, train_loss: 0.0588, step time: 0.2507\n",
      "27/281, train_loss: 0.0650, step time: 0.2544\n",
      "28/281, train_loss: 0.0730, step time: 0.2531\n",
      "29/281, train_loss: 0.0632, step time: 0.2575\n",
      "30/281, train_loss: 0.0666, step time: 0.2549\n",
      "31/281, train_loss: 0.0586, step time: 0.2545\n",
      "32/281, train_loss: 0.0486, step time: 0.2529\n",
      "33/281, train_loss: 0.0347, step time: 0.2574\n",
      "34/281, train_loss: 0.2220, step time: 0.2549\n",
      "35/281, train_loss: 0.0682, step time: 0.2568\n",
      "36/281, train_loss: 0.0457, step time: 0.2539\n",
      "37/281, train_loss: 0.0493, step time: 0.2502\n",
      "38/281, train_loss: 0.0609, step time: 0.2575\n",
      "39/281, train_loss: 0.0618, step time: 0.2662\n",
      "40/281, train_loss: 0.2173, step time: 0.2564\n",
      "41/281, train_loss: 0.0550, step time: 0.2554\n",
      "42/281, train_loss: 0.0834, step time: 0.2531\n",
      "43/281, train_loss: 0.0479, step time: 0.2552\n",
      "44/281, train_loss: 0.2147, step time: 0.2481\n",
      "45/281, train_loss: 0.0311, step time: 0.2554\n",
      "46/281, train_loss: 0.0397, step time: 0.2539\n",
      "47/281, train_loss: 0.0419, step time: 0.2610\n",
      "48/281, train_loss: 0.0380, step time: 0.2549\n",
      "49/281, train_loss: 0.0413, step time: 0.2490\n",
      "50/281, train_loss: 0.0613, step time: 0.2514\n",
      "51/281, train_loss: 0.0551, step time: 0.2491\n",
      "52/281, train_loss: 0.0574, step time: 0.2513\n",
      "53/281, train_loss: 0.0482, step time: 0.2530\n",
      "54/281, train_loss: 0.0393, step time: 0.2506\n",
      "55/281, train_loss: 0.0587, step time: 0.2520\n",
      "56/281, train_loss: 0.0570, step time: 0.2577\n",
      "57/281, train_loss: 0.0746, step time: 0.2571\n",
      "58/281, train_loss: 0.0563, step time: 0.2622\n",
      "59/281, train_loss: 0.0505, step time: 0.2613\n",
      "60/281, train_loss: 0.0604, step time: 0.2516\n",
      "61/281, train_loss: 0.0391, step time: 0.2549\n",
      "62/281, train_loss: 0.0814, step time: 0.2605\n",
      "63/281, train_loss: 0.0568, step time: 0.2565\n",
      "64/281, train_loss: 0.2092, step time: 0.2522\n",
      "65/281, train_loss: 0.0417, step time: 0.2481\n",
      "66/281, train_loss: 0.0458, step time: 0.2525\n",
      "67/281, train_loss: 0.0528, step time: 0.2532\n",
      "68/281, train_loss: 0.0571, step time: 0.2564\n",
      "69/281, train_loss: 0.0354, step time: 0.2579\n",
      "70/281, train_loss: 0.1978, step time: 0.2542\n",
      "71/281, train_loss: 0.0494, step time: 0.2497\n",
      "72/281, train_loss: 0.0726, step time: 0.2485\n",
      "73/281, train_loss: 0.2069, step time: 0.2501\n",
      "74/281, train_loss: 0.0822, step time: 0.2541\n",
      "75/281, train_loss: 0.0494, step time: 0.2519\n",
      "76/281, train_loss: 0.2114, step time: 0.2493\n",
      "77/281, train_loss: 0.0360, step time: 0.2537\n",
      "78/281, train_loss: 0.0532, step time: 0.2520\n",
      "79/281, train_loss: 0.0417, step time: 0.2557\n",
      "80/281, train_loss: 0.0554, step time: 0.2587\n",
      "81/281, train_loss: 0.0419, step time: 0.2586\n",
      "82/281, train_loss: 0.0395, step time: 0.2476\n",
      "83/281, train_loss: 0.0544, step time: 0.2525\n",
      "84/281, train_loss: 0.0288, step time: 0.2483\n",
      "85/281, train_loss: 0.2116, step time: 0.2471\n",
      "86/281, train_loss: 0.0566, step time: 0.2626\n",
      "87/281, train_loss: 0.0612, step time: 0.2589\n",
      "88/281, train_loss: 0.0444, step time: 0.2599\n",
      "89/281, train_loss: 0.0463, step time: 0.2532\n",
      "90/281, train_loss: 0.0468, step time: 0.2507\n",
      "91/281, train_loss: 0.0382, step time: 0.2511\n",
      "92/281, train_loss: 0.0442, step time: 0.2511\n",
      "93/281, train_loss: 0.0589, step time: 0.2500\n",
      "94/281, train_loss: 0.0453, step time: 0.2542\n",
      "95/281, train_loss: 0.2391, step time: 0.2496\n",
      "96/281, train_loss: 0.0471, step time: 0.2500\n",
      "97/281, train_loss: 0.0457, step time: 0.2502\n",
      "98/281, train_loss: 0.2409, step time: 0.2554\n",
      "99/281, train_loss: 0.0784, step time: 0.2501\n",
      "100/281, train_loss: 0.0884, step time: 0.2496\n",
      "101/281, train_loss: 0.0518, step time: 0.2487\n",
      "102/281, train_loss: 0.0541, step time: 0.2500\n",
      "103/281, train_loss: 0.0582, step time: 0.2455\n",
      "104/281, train_loss: 0.0685, step time: 0.2493\n",
      "105/281, train_loss: 0.0423, step time: 0.2505\n",
      "106/281, train_loss: 0.0523, step time: 0.2588\n",
      "107/281, train_loss: 0.2090, step time: 0.2499\n",
      "108/281, train_loss: 0.0348, step time: 0.2445\n",
      "109/281, train_loss: 0.0458, step time: 0.2455\n",
      "110/281, train_loss: 0.0719, step time: 0.2522\n",
      "111/281, train_loss: 0.0841, step time: 0.2444\n",
      "112/281, train_loss: 0.0544, step time: 0.2439\n",
      "113/281, train_loss: 0.0407, step time: 0.2503\n",
      "114/281, train_loss: 0.0332, step time: 0.2501\n",
      "115/281, train_loss: 0.0556, step time: 0.2458\n",
      "116/281, train_loss: 0.0596, step time: 0.2532\n",
      "117/281, train_loss: 0.0668, step time: 0.2511\n",
      "118/281, train_loss: 0.2220, step time: 0.2519\n",
      "119/281, train_loss: 0.0296, step time: 0.2528\n",
      "120/281, train_loss: 0.0391, step time: 0.2490\n",
      "121/281, train_loss: 0.0398, step time: 0.2497\n",
      "122/281, train_loss: 0.0486, step time: 0.2521\n",
      "123/281, train_loss: 0.0508, step time: 0.2554\n",
      "124/281, train_loss: 0.2178, step time: 0.2520\n",
      "125/281, train_loss: 0.0793, step time: 0.2554\n",
      "126/281, train_loss: 0.0937, step time: 0.2624\n",
      "127/281, train_loss: 0.0705, step time: 0.2553\n",
      "128/281, train_loss: 0.2197, step time: 0.2532\n",
      "129/281, train_loss: 0.0756, step time: 0.2536\n",
      "130/281, train_loss: 0.0507, step time: 0.2613\n",
      "131/281, train_loss: 0.2009, step time: 0.2498\n",
      "132/281, train_loss: 0.0488, step time: 0.2514\n",
      "133/281, train_loss: 0.0431, step time: 0.2504\n",
      "134/281, train_loss: 0.2266, step time: 0.2452\n",
      "135/281, train_loss: 0.0732, step time: 0.2500\n",
      "136/281, train_loss: 0.0645, step time: 0.2510\n",
      "137/281, train_loss: 0.0521, step time: 0.2480\n",
      "138/281, train_loss: 0.0573, step time: 0.2482\n",
      "139/281, train_loss: 0.2225, step time: 0.2556\n",
      "140/281, train_loss: 0.0411, step time: 0.2567\n",
      "141/281, train_loss: 0.2054, step time: 0.2550\n",
      "142/281, train_loss: 0.0382, step time: 0.2551\n",
      "143/281, train_loss: 0.0416, step time: 0.2562\n",
      "144/281, train_loss: 0.0760, step time: 0.2556\n",
      "145/281, train_loss: 0.0502, step time: 0.2528\n",
      "146/281, train_loss: 0.1922, step time: 0.2546\n",
      "147/281, train_loss: 0.0512, step time: 0.2553\n",
      "148/281, train_loss: 0.0456, step time: 0.2504\n",
      "149/281, train_loss: 0.0440, step time: 0.2513\n",
      "150/281, train_loss: 0.0631, step time: 0.2488\n",
      "151/281, train_loss: 0.0692, step time: 0.2524\n",
      "152/281, train_loss: 0.0215, step time: 0.2642\n",
      "153/281, train_loss: 0.2107, step time: 0.2515\n",
      "154/281, train_loss: 0.0522, step time: 0.2658\n",
      "155/281, train_loss: 0.0315, step time: 0.2539\n",
      "156/281, train_loss: 0.0558, step time: 0.2519\n",
      "157/281, train_loss: 0.0459, step time: 0.2553\n",
      "158/281, train_loss: 0.0748, step time: 0.2549\n",
      "159/281, train_loss: 0.0529, step time: 0.2559\n",
      "160/281, train_loss: 0.0838, step time: 0.2500\n",
      "161/281, train_loss: 0.0434, step time: 0.2527\n",
      "162/281, train_loss: 0.0492, step time: 0.2603\n",
      "163/281, train_loss: 0.0609, step time: 0.2541\n",
      "164/281, train_loss: 0.0743, step time: 0.2555\n",
      "165/281, train_loss: 0.0761, step time: 0.2540\n",
      "166/281, train_loss: 0.2295, step time: 0.2684\n",
      "167/281, train_loss: 0.0602, step time: 0.2494\n",
      "168/281, train_loss: 0.0301, step time: 0.2532\n",
      "169/281, train_loss: 0.0428, step time: 0.2520\n",
      "170/281, train_loss: 0.0257, step time: 0.2488\n",
      "171/281, train_loss: 0.0575, step time: 0.2570\n",
      "172/281, train_loss: 0.0402, step time: 0.2486\n",
      "173/281, train_loss: 0.0598, step time: 0.2491\n",
      "174/281, train_loss: 0.0526, step time: 0.2514\n",
      "175/281, train_loss: 0.0288, step time: 0.2471\n",
      "176/281, train_loss: 0.0911, step time: 0.2504\n",
      "177/281, train_loss: 0.0470, step time: 0.2495\n",
      "178/281, train_loss: 0.0469, step time: 0.2532\n",
      "179/281, train_loss: 0.0400, step time: 0.2528\n",
      "180/281, train_loss: 0.0426, step time: 0.2545\n",
      "181/281, train_loss: 0.0654, step time: 0.2506\n",
      "182/281, train_loss: 0.0415, step time: 0.2561\n",
      "183/281, train_loss: 0.0852, step time: 0.2562\n",
      "184/281, train_loss: 0.0673, step time: 0.2520\n",
      "185/281, train_loss: 0.2356, step time: 0.2503\n",
      "186/281, train_loss: 0.0700, step time: 0.2476\n",
      "187/281, train_loss: 0.2420, step time: 0.2476\n",
      "188/281, train_loss: 0.0531, step time: 0.2555\n",
      "189/281, train_loss: 0.0440, step time: 0.2508\n",
      "190/281, train_loss: 0.0482, step time: 0.2476\n",
      "191/281, train_loss: 0.0684, step time: 0.2539\n",
      "192/281, train_loss: 0.0389, step time: 0.2818\n",
      "193/281, train_loss: 0.0602, step time: 0.2678\n",
      "194/281, train_loss: 0.0502, step time: 0.2653\n",
      "195/281, train_loss: 0.0740, step time: 0.2495\n",
      "196/281, train_loss: 0.0655, step time: 0.2543\n",
      "197/281, train_loss: 0.2073, step time: 0.2541\n",
      "198/281, train_loss: 0.0745, step time: 0.2561\n",
      "199/281, train_loss: 0.0507, step time: 0.2489\n",
      "200/281, train_loss: 0.0578, step time: 0.2513\n",
      "201/281, train_loss: 0.0508, step time: 0.2564\n",
      "202/281, train_loss: 0.0767, step time: 0.2537\n",
      "203/281, train_loss: 0.0367, step time: 0.2455\n",
      "204/281, train_loss: 0.0659, step time: 0.2479\n",
      "205/281, train_loss: 0.0634, step time: 0.2550\n",
      "206/281, train_loss: 0.0504, step time: 0.2522\n",
      "207/281, train_loss: 0.0781, step time: 0.2504\n",
      "208/281, train_loss: 0.0481, step time: 0.2446\n",
      "209/281, train_loss: 0.2102, step time: 0.2545\n",
      "210/281, train_loss: 0.0424, step time: 0.2567\n",
      "211/281, train_loss: 0.2446, step time: 0.2474\n",
      "212/281, train_loss: 0.0532, step time: 0.2478\n",
      "213/281, train_loss: 0.0586, step time: 0.2525\n",
      "214/281, train_loss: 0.0441, step time: 0.2484\n",
      "215/281, train_loss: 0.0727, step time: 0.2515\n",
      "216/281, train_loss: 0.1911, step time: 0.2468\n",
      "217/281, train_loss: 0.0623, step time: 0.2468\n",
      "218/281, train_loss: 0.0710, step time: 0.2484\n",
      "219/281, train_loss: 0.0531, step time: 0.2492\n",
      "220/281, train_loss: 0.0779, step time: 0.2516\n",
      "221/281, train_loss: 0.0411, step time: 0.2588\n",
      "222/281, train_loss: 0.0862, step time: 0.2681\n",
      "223/281, train_loss: 0.0535, step time: 0.2555\n",
      "224/281, train_loss: 0.2445, step time: 0.2501\n",
      "225/281, train_loss: 0.0399, step time: 0.2513\n",
      "226/281, train_loss: 0.0579, step time: 0.2515\n",
      "227/281, train_loss: 0.0782, step time: 0.2520\n",
      "228/281, train_loss: 0.0974, step time: 0.2492\n",
      "229/281, train_loss: 0.0401, step time: 0.2509\n",
      "230/281, train_loss: 0.0554, step time: 0.2422\n",
      "231/281, train_loss: 0.0571, step time: 0.2465\n",
      "232/281, train_loss: 0.0473, step time: 0.2474\n",
      "233/281, train_loss: 0.0337, step time: 0.2425\n",
      "234/281, train_loss: 0.0601, step time: 0.2477\n",
      "235/281, train_loss: 0.0438, step time: 0.2487\n",
      "236/281, train_loss: 0.0451, step time: 0.2510\n",
      "237/281, train_loss: 0.2190, step time: 0.2438\n",
      "238/281, train_loss: 0.0607, step time: 0.2517\n",
      "239/281, train_loss: 0.2334, step time: 0.2469\n",
      "240/281, train_loss: 0.0446, step time: 0.2463\n",
      "241/281, train_loss: 0.0512, step time: 0.2468\n",
      "242/281, train_loss: 0.0545, step time: 0.2510\n",
      "243/281, train_loss: 0.0445, step time: 0.2475\n",
      "244/281, train_loss: 0.0598, step time: 0.2503\n",
      "245/281, train_loss: 0.0450, step time: 0.2481\n",
      "246/281, train_loss: 0.0470, step time: 0.2502\n",
      "247/281, train_loss: 0.0817, step time: 0.2496\n",
      "248/281, train_loss: 0.0443, step time: 0.2460\n",
      "249/281, train_loss: 0.2179, step time: 0.2476\n",
      "250/281, train_loss: 0.2174, step time: 0.2514\n",
      "251/281, train_loss: 0.0719, step time: 0.2498\n",
      "252/281, train_loss: 0.0373, step time: 0.2476\n",
      "253/281, train_loss: 0.2216, step time: 0.2435\n",
      "254/281, train_loss: 0.0682, step time: 0.2486\n",
      "255/281, train_loss: 0.0408, step time: 0.2504\n",
      "256/281, train_loss: 0.0569, step time: 0.2465\n",
      "257/281, train_loss: 0.2083, step time: 0.2480\n",
      "258/281, train_loss: 0.0740, step time: 0.2446\n",
      "259/281, train_loss: 0.0441, step time: 0.2487\n",
      "260/281, train_loss: 0.2234, step time: 0.2488\n",
      "261/281, train_loss: 0.0597, step time: 0.2486\n",
      "262/281, train_loss: 0.2177, step time: 0.2431\n",
      "263/281, train_loss: 0.0474, step time: 0.2506\n",
      "264/281, train_loss: 0.2126, step time: 0.2503\n",
      "265/281, train_loss: 0.0489, step time: 0.2452\n",
      "266/281, train_loss: 0.0569, step time: 0.2473\n",
      "267/281, train_loss: 0.0494, step time: 0.2517\n",
      "268/281, train_loss: 0.0568, step time: 0.2533\n",
      "269/281, train_loss: 0.0387, step time: 0.2461\n",
      "270/281, train_loss: 0.2060, step time: 0.2442\n",
      "271/281, train_loss: 0.2358, step time: 0.2487\n",
      "272/281, train_loss: 0.0362, step time: 0.2493\n",
      "273/281, train_loss: 0.0442, step time: 0.2432\n",
      "274/281, train_loss: 0.0736, step time: 0.2473\n",
      "275/281, train_loss: 0.0406, step time: 0.2490\n",
      "276/281, train_loss: 0.0821, step time: 0.2536\n",
      "277/281, train_loss: 0.0672, step time: 0.2474\n",
      "278/281, train_loss: 0.0491, step time: 0.2494\n",
      "279/281, train_loss: 0.0427, step time: 0.2458\n",
      "280/281, train_loss: 0.0659, step time: 0.2521\n",
      "281/281, train_loss: 0.0968, step time: 0.2477\n",
      "282/281, train_loss: 0.0696, step time: 0.1469\n",
      "epoch 131 average loss: 0.0787\n",
      "current epoch: 131 current mean dice: 0.9039 tc: 0.8969 wt: 0.9291 et: 0.8958\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 131 is: 350.2825\n",
      "----------\n",
      "epoch 132/200\n",
      "1/281, train_loss: 0.0475, step time: 0.2472\n",
      "2/281, train_loss: 0.2183, step time: 0.2445\n",
      "3/281, train_loss: 0.0435, step time: 0.2508\n",
      "4/281, train_loss: 0.0535, step time: 0.2505\n",
      "5/281, train_loss: 0.0468, step time: 0.2409\n",
      "6/281, train_loss: 0.0770, step time: 0.2404\n",
      "7/281, train_loss: 0.2089, step time: 0.2518\n",
      "8/281, train_loss: 0.3712, step time: 0.2479\n",
      "9/281, train_loss: 0.0508, step time: 0.2462\n",
      "10/281, train_loss: 0.0646, step time: 0.2506\n",
      "11/281, train_loss: 0.0640, step time: 0.2464\n",
      "12/281, train_loss: 0.0510, step time: 0.2471\n",
      "13/281, train_loss: 0.0674, step time: 0.2452\n",
      "14/281, train_loss: 0.2215, step time: 0.2513\n",
      "15/281, train_loss: 0.0688, step time: 0.2563\n",
      "16/281, train_loss: 0.0668, step time: 0.2495\n",
      "17/281, train_loss: 0.0561, step time: 0.2511\n",
      "18/281, train_loss: 0.2141, step time: 0.2520\n",
      "19/281, train_loss: 0.0540, step time: 0.2520\n",
      "20/281, train_loss: 0.0564, step time: 0.2522\n",
      "21/281, train_loss: 0.0325, step time: 0.2565\n",
      "22/281, train_loss: 0.0663, step time: 0.2458\n",
      "23/281, train_loss: 0.1960, step time: 0.2466\n",
      "24/281, train_loss: 0.0614, step time: 0.2528\n",
      "25/281, train_loss: 0.0699, step time: 0.2505\n",
      "26/281, train_loss: 0.0658, step time: 0.2506\n",
      "27/281, train_loss: 0.0998, step time: 0.2489\n",
      "28/281, train_loss: 0.2164, step time: 0.2465\n",
      "29/281, train_loss: 0.0701, step time: 0.2455\n",
      "30/281, train_loss: 0.2124, step time: 0.2504\n",
      "31/281, train_loss: 0.2168, step time: 0.2473\n",
      "32/281, train_loss: 0.0482, step time: 0.2404\n",
      "33/281, train_loss: 0.2135, step time: 0.2412\n",
      "34/281, train_loss: 0.0566, step time: 0.2412\n",
      "35/281, train_loss: 0.0477, step time: 0.2435\n",
      "36/281, train_loss: 0.0428, step time: 0.2428\n",
      "37/281, train_loss: 0.0707, step time: 0.2430\n",
      "38/281, train_loss: 0.0388, step time: 0.2471\n",
      "39/281, train_loss: 0.2089, step time: 0.2433\n",
      "40/281, train_loss: 0.0368, step time: 0.2441\n",
      "41/281, train_loss: 0.0383, step time: 0.2427\n",
      "42/281, train_loss: 0.0545, step time: 0.2445\n",
      "43/281, train_loss: 0.0701, step time: 0.2444\n",
      "44/281, train_loss: 0.0403, step time: 0.2540\n",
      "45/281, train_loss: 0.0560, step time: 0.2505\n",
      "46/281, train_loss: 0.0711, step time: 0.2474\n",
      "47/281, train_loss: 0.0471, step time: 0.2441\n",
      "48/281, train_loss: 0.0830, step time: 0.2422\n",
      "49/281, train_loss: 0.0658, step time: 0.2418\n",
      "50/281, train_loss: 0.0530, step time: 0.2496\n",
      "51/281, train_loss: 0.2675, step time: 0.2497\n",
      "52/281, train_loss: 0.1932, step time: 0.2490\n",
      "53/281, train_loss: 0.1029, step time: 0.2435\n",
      "54/281, train_loss: 0.0404, step time: 0.2511\n",
      "55/281, train_loss: 0.0698, step time: 0.2489\n",
      "56/281, train_loss: 0.0532, step time: 0.2450\n",
      "57/281, train_loss: 0.0473, step time: 0.2468\n",
      "58/281, train_loss: 0.0520, step time: 0.2467\n",
      "59/281, train_loss: 0.0525, step time: 0.2434\n",
      "60/281, train_loss: 0.0686, step time: 0.2535\n",
      "61/281, train_loss: 0.0452, step time: 0.2482\n",
      "62/281, train_loss: 0.0731, step time: 0.2502\n",
      "63/281, train_loss: 0.0551, step time: 0.2507\n",
      "64/281, train_loss: 0.0693, step time: 0.2522\n",
      "65/281, train_loss: 0.0389, step time: 0.2541\n",
      "66/281, train_loss: 0.0564, step time: 0.2493\n",
      "67/281, train_loss: 0.0415, step time: 0.2483\n",
      "68/281, train_loss: 0.0556, step time: 0.2503\n",
      "69/281, train_loss: 0.0964, step time: 0.2517\n",
      "70/281, train_loss: 0.0510, step time: 0.2503\n",
      "71/281, train_loss: 0.0783, step time: 0.2473\n",
      "72/281, train_loss: 0.0520, step time: 0.2472\n",
      "73/281, train_loss: 0.0434, step time: 0.2452\n",
      "74/281, train_loss: 0.0470, step time: 0.2497\n",
      "75/281, train_loss: 0.2247, step time: 0.2429\n",
      "76/281, train_loss: 0.2058, step time: 0.2508\n",
      "77/281, train_loss: 0.0531, step time: 0.2468\n",
      "78/281, train_loss: 0.2226, step time: 0.2474\n",
      "79/281, train_loss: 0.0553, step time: 0.2509\n",
      "80/281, train_loss: 0.0731, step time: 0.2512\n",
      "81/281, train_loss: 0.2148, step time: 0.2562\n",
      "82/281, train_loss: 0.0553, step time: 0.2482\n",
      "83/281, train_loss: 0.0563, step time: 0.2450\n",
      "84/281, train_loss: 0.0385, step time: 0.2420\n",
      "85/281, train_loss: 0.2207, step time: 0.2517\n",
      "86/281, train_loss: 0.0255, step time: 0.2476\n",
      "87/281, train_loss: 0.0693, step time: 0.2508\n",
      "88/281, train_loss: 0.0378, step time: 0.2520\n",
      "89/281, train_loss: 0.0329, step time: 0.2481\n",
      "90/281, train_loss: 0.0614, step time: 0.2498\n",
      "91/281, train_loss: 0.0483, step time: 0.2477\n",
      "92/281, train_loss: 0.0503, step time: 0.2456\n",
      "93/281, train_loss: 0.0451, step time: 0.2455\n",
      "94/281, train_loss: 0.0400, step time: 0.2422\n",
      "95/281, train_loss: 0.0477, step time: 0.2424\n",
      "96/281, train_loss: 0.0337, step time: 0.2454\n",
      "97/281, train_loss: 0.0626, step time: 0.2437\n",
      "98/281, train_loss: 0.0539, step time: 0.2552\n",
      "99/281, train_loss: 0.0625, step time: 0.2513\n",
      "100/281, train_loss: 0.0663, step time: 0.2461\n",
      "101/281, train_loss: 0.0483, step time: 0.2480\n",
      "102/281, train_loss: 0.0440, step time: 0.2486\n",
      "103/281, train_loss: 0.0437, step time: 0.2432\n",
      "104/281, train_loss: 0.0455, step time: 0.2411\n",
      "105/281, train_loss: 0.0852, step time: 0.2461\n",
      "106/281, train_loss: 0.0541, step time: 0.2534\n",
      "107/281, train_loss: 0.0392, step time: 0.2481\n",
      "108/281, train_loss: 0.0524, step time: 0.2456\n",
      "109/281, train_loss: 0.0807, step time: 0.2398\n",
      "110/281, train_loss: 0.0739, step time: 0.2463\n",
      "111/281, train_loss: 0.0479, step time: 0.2461\n",
      "112/281, train_loss: 0.0666, step time: 0.2536\n",
      "113/281, train_loss: 0.0466, step time: 0.2528\n",
      "114/281, train_loss: 0.0781, step time: 0.2434\n",
      "115/281, train_loss: 0.0505, step time: 0.2437\n",
      "116/281, train_loss: 0.0351, step time: 0.2464\n",
      "117/281, train_loss: 0.2139, step time: 0.2460\n",
      "118/281, train_loss: 0.0675, step time: 0.2424\n",
      "119/281, train_loss: 0.0470, step time: 0.2445\n",
      "120/281, train_loss: 0.0612, step time: 0.2439\n",
      "121/281, train_loss: 0.0802, step time: 0.2441\n",
      "122/281, train_loss: 0.0677, step time: 0.2469\n",
      "123/281, train_loss: 0.2222, step time: 0.2469\n",
      "124/281, train_loss: 0.0535, step time: 0.2452\n",
      "125/281, train_loss: 0.0671, step time: 0.2590\n",
      "126/281, train_loss: 0.0649, step time: 0.2447\n",
      "127/281, train_loss: 0.0755, step time: 0.2474\n",
      "128/281, train_loss: 0.0367, step time: 0.2524\n",
      "129/281, train_loss: 0.0539, step time: 0.2542\n",
      "130/281, train_loss: 0.0557, step time: 0.2477\n",
      "131/281, train_loss: 0.0501, step time: 0.2436\n",
      "132/281, train_loss: 0.0489, step time: 0.2437\n",
      "133/281, train_loss: 0.0383, step time: 0.2748\n",
      "134/281, train_loss: 0.0345, step time: 0.2489\n",
      "135/281, train_loss: 0.0594, step time: 0.2609\n",
      "136/281, train_loss: 0.0810, step time: 0.2455\n",
      "137/281, train_loss: 0.0319, step time: 0.2429\n",
      "138/281, train_loss: 0.0738, step time: 0.2424\n",
      "139/281, train_loss: 0.2193, step time: 0.2439\n",
      "140/281, train_loss: 0.0370, step time: 0.2451\n",
      "141/281, train_loss: 0.0733, step time: 0.2495\n",
      "142/281, train_loss: 0.0532, step time: 0.2466\n",
      "143/281, train_loss: 0.0616, step time: 0.2435\n",
      "144/281, train_loss: 0.0769, step time: 0.2413\n",
      "145/281, train_loss: 0.0819, step time: 0.2418\n",
      "146/281, train_loss: 0.0325, step time: 0.2403\n",
      "147/281, train_loss: 0.0489, step time: 0.2459\n",
      "148/281, train_loss: 0.0379, step time: 0.2461\n",
      "149/281, train_loss: 0.0525, step time: 0.2455\n",
      "150/281, train_loss: 0.0531, step time: 0.2490\n",
      "151/281, train_loss: 0.0632, step time: 0.2456\n",
      "152/281, train_loss: 0.2144, step time: 0.2418\n",
      "153/281, train_loss: 0.0381, step time: 0.2450\n",
      "154/281, train_loss: 0.0355, step time: 0.2492\n",
      "155/281, train_loss: 0.0217, step time: 0.2446\n",
      "156/281, train_loss: 0.0476, step time: 0.2403\n",
      "157/281, train_loss: 0.0595, step time: 0.2450\n",
      "158/281, train_loss: 0.0540, step time: 0.2435\n",
      "159/281, train_loss: 0.0606, step time: 0.2433\n",
      "160/281, train_loss: 0.0418, step time: 0.2436\n",
      "161/281, train_loss: 0.0511, step time: 0.2495\n",
      "162/281, train_loss: 0.0879, step time: 0.2440\n",
      "163/281, train_loss: 0.0616, step time: 0.2435\n",
      "164/281, train_loss: 0.0414, step time: 0.2411\n",
      "165/281, train_loss: 0.0611, step time: 0.2389\n",
      "166/281, train_loss: 0.0459, step time: 0.2474\n",
      "167/281, train_loss: 0.2230, step time: 0.2470\n",
      "168/281, train_loss: 0.0269, step time: 0.2439\n",
      "169/281, train_loss: 0.0421, step time: 0.2439\n",
      "170/281, train_loss: 0.0616, step time: 0.2507\n",
      "171/281, train_loss: 0.0514, step time: 0.2532\n",
      "172/281, train_loss: 0.0546, step time: 0.2514\n",
      "173/281, train_loss: 0.0566, step time: 0.2470\n",
      "174/281, train_loss: 0.0504, step time: 0.2465\n",
      "175/281, train_loss: 0.2052, step time: 0.2459\n",
      "176/281, train_loss: 0.0723, step time: 0.2429\n",
      "177/281, train_loss: 0.0682, step time: 0.2454\n",
      "178/281, train_loss: 0.0711, step time: 0.2500\n",
      "179/281, train_loss: 0.0581, step time: 0.2501\n",
      "180/281, train_loss: 0.0527, step time: 0.2476\n",
      "181/281, train_loss: 0.0762, step time: 0.2526\n",
      "182/281, train_loss: 0.0318, step time: 0.2472\n",
      "183/281, train_loss: 0.0370, step time: 0.2536\n",
      "184/281, train_loss: 0.0647, step time: 0.2499\n",
      "185/281, train_loss: 0.0627, step time: 0.2509\n",
      "186/281, train_loss: 0.0762, step time: 0.2526\n",
      "187/281, train_loss: 0.2299, step time: 0.2505\n",
      "188/281, train_loss: 0.0581, step time: 0.2519\n",
      "189/281, train_loss: 0.0578, step time: 0.2458\n",
      "190/281, train_loss: 0.0622, step time: 0.2482\n",
      "191/281, train_loss: 0.0744, step time: 0.2454\n",
      "192/281, train_loss: 0.0505, step time: 0.2477\n",
      "193/281, train_loss: 0.0471, step time: 0.2527\n",
      "194/281, train_loss: 0.0449, step time: 0.2478\n",
      "195/281, train_loss: 0.0497, step time: 0.2467\n",
      "196/281, train_loss: 0.2242, step time: 0.2501\n",
      "197/281, train_loss: 0.0445, step time: 0.2467\n",
      "198/281, train_loss: 0.0660, step time: 0.2616\n",
      "199/281, train_loss: 0.0575, step time: 0.2446\n",
      "200/281, train_loss: 0.0672, step time: 0.2507\n",
      "201/281, train_loss: 0.0547, step time: 0.2485\n",
      "202/281, train_loss: 0.1996, step time: 0.2446\n",
      "203/281, train_loss: 0.0624, step time: 0.2463\n",
      "204/281, train_loss: 0.0704, step time: 0.2490\n",
      "205/281, train_loss: 0.0606, step time: 0.2409\n",
      "206/281, train_loss: 0.0604, step time: 0.2421\n",
      "207/281, train_loss: 0.0472, step time: 0.2404\n",
      "208/281, train_loss: 0.0754, step time: 0.2477\n",
      "209/281, train_loss: 0.0521, step time: 0.2443\n",
      "210/281, train_loss: 0.0578, step time: 0.2516\n",
      "211/281, train_loss: 0.0753, step time: 0.2436\n",
      "212/281, train_loss: 0.0659, step time: 0.2437\n",
      "213/281, train_loss: 0.0567, step time: 0.2520\n",
      "214/281, train_loss: 0.2079, step time: 0.2446\n",
      "215/281, train_loss: 0.0788, step time: 0.2466\n",
      "216/281, train_loss: 0.0588, step time: 0.2500\n",
      "217/281, train_loss: 0.0573, step time: 0.2444\n",
      "218/281, train_loss: 0.0672, step time: 0.2457\n",
      "219/281, train_loss: 0.2212, step time: 0.2511\n",
      "220/281, train_loss: 0.0711, step time: 0.2476\n",
      "221/281, train_loss: 0.0560, step time: 0.2440\n",
      "222/281, train_loss: 0.2241, step time: 0.2421\n",
      "223/281, train_loss: 0.0446, step time: 0.2432\n",
      "224/281, train_loss: 0.2360, step time: 0.2556\n",
      "225/281, train_loss: 0.2256, step time: 0.2578\n",
      "226/281, train_loss: 0.0502, step time: 0.2430\n",
      "227/281, train_loss: 0.0912, step time: 0.2454\n",
      "228/281, train_loss: 0.2182, step time: 0.2464\n",
      "229/281, train_loss: 0.0685, step time: 0.2406\n",
      "230/281, train_loss: 0.2170, step time: 0.2438\n",
      "231/281, train_loss: 0.0528, step time: 0.2422\n",
      "232/281, train_loss: 0.2190, step time: 0.2506\n",
      "233/281, train_loss: 0.0878, step time: 0.2496\n",
      "234/281, train_loss: 0.0587, step time: 0.2509\n",
      "235/281, train_loss: 0.0755, step time: 0.2463\n",
      "236/281, train_loss: 0.0540, step time: 0.2478\n",
      "237/281, train_loss: 0.0457, step time: 0.2466\n",
      "238/281, train_loss: 0.0591, step time: 0.2472\n",
      "239/281, train_loss: 0.0770, step time: 0.2414\n",
      "240/281, train_loss: 0.2195, step time: 0.2410\n",
      "241/281, train_loss: 0.0719, step time: 0.2444\n",
      "242/281, train_loss: 0.0588, step time: 0.2455\n",
      "243/281, train_loss: 0.0682, step time: 0.2457\n",
      "244/281, train_loss: 0.0398, step time: 0.2505\n",
      "245/281, train_loss: 0.0657, step time: 0.2427\n",
      "246/281, train_loss: 0.0399, step time: 0.2439\n",
      "247/281, train_loss: 0.0543, step time: 0.2427\n",
      "248/281, train_loss: 0.0592, step time: 0.2397\n",
      "249/281, train_loss: 0.0690, step time: 0.2447\n",
      "250/281, train_loss: 0.0627, step time: 0.2517\n",
      "251/281, train_loss: 0.0459, step time: 0.2459\n",
      "252/281, train_loss: 0.0650, step time: 0.2418\n",
      "253/281, train_loss: 0.0477, step time: 0.2414\n",
      "254/281, train_loss: 0.0709, step time: 0.2529\n",
      "255/281, train_loss: 0.2367, step time: 0.2467\n",
      "256/281, train_loss: 0.0626, step time: 0.2497\n",
      "257/281, train_loss: 0.0645, step time: 0.2476\n",
      "258/281, train_loss: 0.0588, step time: 0.2480\n",
      "259/281, train_loss: 0.0690, step time: 0.2488\n",
      "260/281, train_loss: 0.0314, step time: 0.2437\n",
      "261/281, train_loss: 0.0413, step time: 0.2451\n",
      "262/281, train_loss: 0.2211, step time: 0.2499\n",
      "263/281, train_loss: 0.0710, step time: 0.2474\n",
      "264/281, train_loss: 0.0609, step time: 0.2484\n",
      "265/281, train_loss: 0.0574, step time: 0.2516\n",
      "266/281, train_loss: 0.0397, step time: 0.2472\n",
      "267/281, train_loss: 0.0597, step time: 0.2515\n",
      "268/281, train_loss: 0.0699, step time: 0.2523\n",
      "269/281, train_loss: 0.0491, step time: 0.2502\n",
      "270/281, train_loss: 0.0602, step time: 0.2488\n",
      "271/281, train_loss: 0.0685, step time: 0.2483\n",
      "272/281, train_loss: 0.0476, step time: 0.2436\n",
      "273/281, train_loss: 0.0542, step time: 0.2598\n",
      "274/281, train_loss: 0.2232, step time: 0.2495\n",
      "275/281, train_loss: 0.0751, step time: 0.2461\n",
      "276/281, train_loss: 0.2394, step time: 0.2452\n",
      "277/281, train_loss: 0.0527, step time: 0.2508\n",
      "278/281, train_loss: 0.0412, step time: 0.2449\n",
      "279/281, train_loss: 0.0455, step time: 0.2447\n",
      "280/281, train_loss: 0.0601, step time: 0.2416\n",
      "281/281, train_loss: 0.0445, step time: 0.2415\n",
      "282/281, train_loss: 0.0306, step time: 0.1450\n",
      "epoch 132 average loss: 0.0804\n",
      "current epoch: 132 current mean dice: 0.8998 tc: 0.8970 wt: 0.9201 et: 0.8913\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 132 is: 373.0466\n",
      "----------\n",
      "epoch 133/200\n",
      "1/281, train_loss: 0.0718, step time: 0.2558\n",
      "2/281, train_loss: 0.0604, step time: 0.2534\n",
      "3/281, train_loss: 0.2339, step time: 0.2510\n",
      "4/281, train_loss: 0.0710, step time: 0.2540\n",
      "5/281, train_loss: 0.2078, step time: 0.2506\n",
      "6/281, train_loss: 0.0562, step time: 0.2495\n",
      "7/281, train_loss: 0.0913, step time: 0.2505\n",
      "8/281, train_loss: 0.0448, step time: 0.2439\n",
      "9/281, train_loss: 0.0549, step time: 0.2528\n",
      "10/281, train_loss: 0.0708, step time: 0.2534\n",
      "11/281, train_loss: 0.0528, step time: 0.2561\n",
      "12/281, train_loss: 0.0764, step time: 0.2542\n",
      "13/281, train_loss: 0.0411, step time: 0.2508\n",
      "14/281, train_loss: 0.0630, step time: 0.2521\n",
      "15/281, train_loss: 0.0430, step time: 0.2556\n",
      "16/281, train_loss: 0.0475, step time: 0.2525\n",
      "17/281, train_loss: 0.0606, step time: 0.2500\n",
      "18/281, train_loss: 0.0599, step time: 0.2464\n",
      "19/281, train_loss: 0.2274, step time: 0.2500\n",
      "20/281, train_loss: 0.0511, step time: 0.2510\n",
      "21/281, train_loss: 0.0598, step time: 0.2527\n",
      "22/281, train_loss: 0.0643, step time: 0.2483\n",
      "23/281, train_loss: 0.0469, step time: 0.2512\n",
      "24/281, train_loss: 0.2109, step time: 0.2522\n",
      "25/281, train_loss: 0.0627, step time: 0.2498\n",
      "26/281, train_loss: 0.1209, step time: 0.2522\n",
      "27/281, train_loss: 0.0990, step time: 0.2522\n",
      "28/281, train_loss: 0.0481, step time: 0.2569\n",
      "29/281, train_loss: 0.0861, step time: 0.2539\n",
      "30/281, train_loss: 0.0591, step time: 0.2503\n",
      "31/281, train_loss: 0.0636, step time: 0.2785\n",
      "32/281, train_loss: 0.2163, step time: 0.2662\n",
      "33/281, train_loss: 0.0473, step time: 0.2515\n",
      "34/281, train_loss: 0.2258, step time: 0.2550\n",
      "35/281, train_loss: 0.0528, step time: 0.2569\n",
      "36/281, train_loss: 0.0769, step time: 0.2541\n",
      "37/281, train_loss: 0.0594, step time: 0.2518\n",
      "38/281, train_loss: 0.0705, step time: 0.2618\n",
      "39/281, train_loss: 0.2086, step time: 0.2540\n",
      "40/281, train_loss: 0.0382, step time: 0.2475\n",
      "41/281, train_loss: 0.0649, step time: 0.2510\n",
      "42/281, train_loss: 0.0510, step time: 0.2532\n",
      "43/281, train_loss: 0.2137, step time: 0.2608\n",
      "44/281, train_loss: 0.0687, step time: 0.2559\n",
      "45/281, train_loss: 0.0878, step time: 0.2466\n",
      "46/281, train_loss: 0.0651, step time: 0.2452\n",
      "47/281, train_loss: 0.0418, step time: 0.2464\n",
      "48/281, train_loss: 0.0527, step time: 0.2429\n",
      "49/281, train_loss: 0.0643, step time: 0.2471\n",
      "50/281, train_loss: 0.0795, step time: 0.2490\n",
      "51/281, train_loss: 0.0510, step time: 0.2453\n",
      "52/281, train_loss: 0.0580, step time: 0.2466\n",
      "53/281, train_loss: 0.0388, step time: 0.2562\n",
      "54/281, train_loss: 0.0354, step time: 0.2559\n",
      "55/281, train_loss: 0.0811, step time: 0.2541\n",
      "56/281, train_loss: 0.0630, step time: 0.2508\n",
      "57/281, train_loss: 0.2403, step time: 0.2479\n",
      "58/281, train_loss: 0.0663, step time: 0.2526\n",
      "59/281, train_loss: 0.0394, step time: 0.2462\n",
      "60/281, train_loss: 0.0459, step time: 0.2437\n",
      "61/281, train_loss: 0.0730, step time: 0.2480\n",
      "62/281, train_loss: 0.0365, step time: 0.2525\n",
      "63/281, train_loss: 0.0957, step time: 0.2523\n",
      "64/281, train_loss: 0.0699, step time: 0.2590\n",
      "65/281, train_loss: 0.0583, step time: 0.2465\n",
      "66/281, train_loss: 0.0414, step time: 0.2472\n",
      "67/281, train_loss: 0.0560, step time: 0.2463\n",
      "68/281, train_loss: 0.0410, step time: 0.2523\n",
      "69/281, train_loss: 0.0697, step time: 0.2480\n",
      "70/281, train_loss: 0.0732, step time: 0.2468\n",
      "71/281, train_loss: 0.0483, step time: 0.2452\n",
      "72/281, train_loss: 0.0613, step time: 0.2437\n",
      "73/281, train_loss: 0.0471, step time: 0.2523\n",
      "74/281, train_loss: 0.0602, step time: 0.2489\n",
      "75/281, train_loss: 0.0712, step time: 0.2427\n",
      "76/281, train_loss: 0.0480, step time: 0.2455\n",
      "77/281, train_loss: 0.0843, step time: 0.2623\n",
      "78/281, train_loss: 0.0553, step time: 0.2537\n",
      "79/281, train_loss: 0.0388, step time: 0.2501\n",
      "80/281, train_loss: 0.0440, step time: 0.2525\n",
      "81/281, train_loss: 0.0607, step time: 0.2461\n",
      "82/281, train_loss: 0.1280, step time: 0.2429\n",
      "83/281, train_loss: 0.0618, step time: 0.2412\n",
      "84/281, train_loss: 0.0807, step time: 0.2468\n",
      "85/281, train_loss: 0.0475, step time: 0.2480\n",
      "86/281, train_loss: 0.0677, step time: 0.2459\n",
      "87/281, train_loss: 0.0698, step time: 0.2453\n",
      "88/281, train_loss: 0.0710, step time: 0.2438\n",
      "89/281, train_loss: 0.0846, step time: 0.2528\n",
      "90/281, train_loss: 0.0723, step time: 0.2440\n",
      "91/281, train_loss: 0.0639, step time: 0.2487\n",
      "92/281, train_loss: 0.2467, step time: 0.2515\n",
      "93/281, train_loss: 0.3761, step time: 0.2474\n",
      "94/281, train_loss: 0.0782, step time: 0.2568\n",
      "95/281, train_loss: 0.2158, step time: 0.2520\n",
      "96/281, train_loss: 0.1017, step time: 0.2501\n",
      "97/281, train_loss: 0.0515, step time: 0.2585\n",
      "98/281, train_loss: 0.0394, step time: 0.2535\n",
      "99/281, train_loss: 0.0527, step time: 0.2488\n",
      "100/281, train_loss: 0.0659, step time: 0.2505\n",
      "101/281, train_loss: 0.0771, step time: 0.2497\n",
      "102/281, train_loss: 0.0562, step time: 0.2537\n",
      "103/281, train_loss: 0.0427, step time: 0.2566\n",
      "104/281, train_loss: 0.0646, step time: 0.2557\n",
      "105/281, train_loss: 0.0718, step time: 0.2563\n",
      "106/281, train_loss: 0.0592, step time: 0.2614\n",
      "107/281, train_loss: 0.0734, step time: 0.2542\n",
      "108/281, train_loss: 0.0767, step time: 0.2601\n",
      "109/281, train_loss: 0.0654, step time: 0.2599\n",
      "110/281, train_loss: 0.2134, step time: 0.2487\n",
      "111/281, train_loss: 0.4014, step time: 0.2494\n",
      "112/281, train_loss: 0.0496, step time: 0.2517\n",
      "113/281, train_loss: 0.0936, step time: 0.2511\n",
      "114/281, train_loss: 0.0647, step time: 0.2504\n",
      "115/281, train_loss: 0.0944, step time: 0.2560\n",
      "116/281, train_loss: 0.0611, step time: 0.2511\n",
      "117/281, train_loss: 0.0819, step time: 0.2535\n",
      "118/281, train_loss: 0.0434, step time: 0.2477\n",
      "119/281, train_loss: 0.0647, step time: 0.2523\n",
      "120/281, train_loss: 0.0842, step time: 0.2512\n",
      "121/281, train_loss: 0.0912, step time: 0.2532\n",
      "122/281, train_loss: 0.2200, step time: 0.2506\n",
      "123/281, train_loss: 0.2343, step time: 0.2538\n",
      "124/281, train_loss: 0.0862, step time: 0.2516\n",
      "125/281, train_loss: 0.2489, step time: 0.2500\n",
      "126/281, train_loss: 0.2360, step time: 0.2517\n",
      "127/281, train_loss: 0.0847, step time: 0.2531\n",
      "128/281, train_loss: 0.0914, step time: 0.2512\n",
      "129/281, train_loss: 0.2450, step time: 0.2513\n",
      "130/281, train_loss: 0.0966, step time: 0.2514\n",
      "131/281, train_loss: 0.0522, step time: 0.2561\n",
      "132/281, train_loss: 0.0891, step time: 0.2504\n",
      "133/281, train_loss: 0.0585, step time: 0.2451\n",
      "134/281, train_loss: 0.1043, step time: 0.2610\n",
      "135/281, train_loss: 0.0592, step time: 0.2521\n",
      "136/281, train_loss: 0.0906, step time: 0.2571\n",
      "137/281, train_loss: 0.0767, step time: 0.2577\n",
      "138/281, train_loss: 0.0888, step time: 0.2504\n",
      "139/281, train_loss: 0.0790, step time: 0.2474\n",
      "140/281, train_loss: 0.0625, step time: 0.2537\n",
      "141/281, train_loss: 0.1454, step time: 0.2619\n",
      "142/281, train_loss: 0.0900, step time: 0.2569\n",
      "143/281, train_loss: 0.0947, step time: 0.2520\n",
      "144/281, train_loss: 0.0683, step time: 0.2518\n",
      "145/281, train_loss: 0.0645, step time: 0.2514\n",
      "146/281, train_loss: 0.0880, step time: 0.2557\n",
      "147/281, train_loss: 0.2455, step time: 0.2580\n",
      "148/281, train_loss: 0.0811, step time: 0.2539\n",
      "149/281, train_loss: 0.0926, step time: 0.2539\n",
      "150/281, train_loss: 0.2447, step time: 0.2512\n",
      "151/281, train_loss: 0.1156, step time: 0.2558\n",
      "152/281, train_loss: 0.1256, step time: 0.2476\n",
      "153/281, train_loss: 0.0635, step time: 0.2567\n",
      "154/281, train_loss: 0.1450, step time: 0.2474\n",
      "155/281, train_loss: 0.1439, step time: 0.2500\n",
      "156/281, train_loss: 0.1419, step time: 0.2511\n",
      "157/281, train_loss: 0.1284, step time: 0.2514\n",
      "158/281, train_loss: 0.1173, step time: 0.2502\n",
      "159/281, train_loss: 0.2633, step time: 0.2474\n",
      "160/281, train_loss: 0.2410, step time: 0.2478\n",
      "161/281, train_loss: 0.0863, step time: 0.2455\n",
      "162/281, train_loss: 0.1052, step time: 0.2555\n",
      "163/281, train_loss: 0.0890, step time: 0.2521\n",
      "164/281, train_loss: 0.1451, step time: 0.2504\n",
      "165/281, train_loss: 0.1053, step time: 0.2506\n",
      "166/281, train_loss: 0.1417, step time: 0.2513\n",
      "167/281, train_loss: 0.1058, step time: 0.2494\n",
      "168/281, train_loss: 0.1007, step time: 0.2517\n",
      "169/281, train_loss: 0.2762, step time: 0.2488\n",
      "170/281, train_loss: 0.1657, step time: 0.2560\n",
      "171/281, train_loss: 0.3051, step time: 0.2530\n",
      "172/281, train_loss: 0.1240, step time: 0.2488\n",
      "173/281, train_loss: 0.1345, step time: 0.2504\n",
      "174/281, train_loss: 0.1435, step time: 0.2489\n",
      "175/281, train_loss: 0.0796, step time: 0.2574\n",
      "176/281, train_loss: 0.0844, step time: 0.2572\n",
      "177/281, train_loss: 0.1042, step time: 0.2519\n",
      "178/281, train_loss: 0.0907, step time: 0.2532\n",
      "179/281, train_loss: 0.0894, step time: 0.2488\n",
      "180/281, train_loss: 0.1686, step time: 0.2475\n",
      "181/281, train_loss: 0.1348, step time: 0.2489\n",
      "182/281, train_loss: 0.1011, step time: 0.2528\n",
      "183/281, train_loss: 0.1676, step time: 0.2499\n",
      "184/281, train_loss: 0.0765, step time: 0.2482\n",
      "185/281, train_loss: 0.0767, step time: 0.2459\n",
      "186/281, train_loss: 0.2594, step time: 0.2452\n",
      "187/281, train_loss: 0.1176, step time: 0.2452\n",
      "188/281, train_loss: 0.0767, step time: 0.2437\n",
      "189/281, train_loss: 0.1143, step time: 0.2472\n",
      "190/281, train_loss: 0.2737, step time: 0.2556\n",
      "191/281, train_loss: 0.0912, step time: 0.2493\n",
      "192/281, train_loss: 0.0901, step time: 0.2480\n",
      "193/281, train_loss: 0.1000, step time: 0.2446\n",
      "194/281, train_loss: 0.1470, step time: 0.2662\n",
      "195/281, train_loss: 0.1361, step time: 0.2479\n",
      "196/281, train_loss: 0.1053, step time: 0.2415\n",
      "197/281, train_loss: 0.0966, step time: 0.2422\n",
      "198/281, train_loss: 0.1407, step time: 0.2446\n",
      "199/281, train_loss: 0.1462, step time: 0.2523\n",
      "200/281, train_loss: 0.0947, step time: 0.2525\n",
      "201/281, train_loss: 0.2753, step time: 0.2494\n",
      "202/281, train_loss: 0.1295, step time: 0.2520\n",
      "203/281, train_loss: 0.1409, step time: 0.2490\n",
      "204/281, train_loss: 0.2655, step time: 0.2483\n",
      "205/281, train_loss: 0.2447, step time: 0.2515\n",
      "206/281, train_loss: 0.1509, step time: 0.2478\n",
      "207/281, train_loss: 0.1931, step time: 0.2535\n",
      "208/281, train_loss: 0.1097, step time: 0.2473\n",
      "209/281, train_loss: 0.1065, step time: 0.2479\n",
      "210/281, train_loss: 0.2458, step time: 0.2519\n",
      "211/281, train_loss: 0.1223, step time: 0.2539\n",
      "212/281, train_loss: 0.1065, step time: 0.2508\n",
      "213/281, train_loss: 0.0741, step time: 0.2538\n",
      "214/281, train_loss: 0.0796, step time: 0.2575\n",
      "215/281, train_loss: 0.1427, step time: 0.2531\n",
      "216/281, train_loss: 0.0722, step time: 0.2498\n",
      "217/281, train_loss: 0.1076, step time: 0.2536\n",
      "218/281, train_loss: 0.1013, step time: 0.2487\n",
      "219/281, train_loss: 0.0938, step time: 0.2532\n",
      "220/281, train_loss: 0.1220, step time: 0.2509\n",
      "221/281, train_loss: 0.0981, step time: 0.2521\n",
      "222/281, train_loss: 0.0964, step time: 0.2528\n",
      "223/281, train_loss: 0.1517, step time: 0.2516\n",
      "224/281, train_loss: 0.1526, step time: 0.2500\n",
      "225/281, train_loss: 0.1364, step time: 0.2550\n",
      "226/281, train_loss: 0.1384, step time: 0.2528\n",
      "227/281, train_loss: 0.1172, step time: 0.2495\n",
      "228/281, train_loss: 0.0911, step time: 0.2524\n",
      "229/281, train_loss: 0.4220, step time: 0.2566\n",
      "230/281, train_loss: 0.1252, step time: 0.2538\n",
      "231/281, train_loss: 0.3072, step time: 0.2585\n",
      "232/281, train_loss: 0.1394, step time: 0.2559\n",
      "233/281, train_loss: 0.1079, step time: 0.2541\n",
      "234/281, train_loss: 0.1650, step time: 0.2522\n",
      "235/281, train_loss: 0.1504, step time: 0.2536\n",
      "236/281, train_loss: 0.2912, step time: 0.2501\n",
      "237/281, train_loss: 0.4180, step time: 0.2559\n",
      "238/281, train_loss: 0.1137, step time: 0.2515\n",
      "239/281, train_loss: 0.1031, step time: 0.2505\n",
      "240/281, train_loss: 0.0988, step time: 0.2588\n",
      "241/281, train_loss: 0.2630, step time: 0.2536\n",
      "242/281, train_loss: 0.2560, step time: 0.2501\n",
      "243/281, train_loss: 0.0699, step time: 0.2471\n",
      "244/281, train_loss: 0.1962, step time: 0.2510\n",
      "245/281, train_loss: 0.1674, step time: 0.2573\n",
      "246/281, train_loss: 0.1138, step time: 0.2539\n",
      "247/281, train_loss: 0.1813, step time: 0.2514\n",
      "248/281, train_loss: 0.0636, step time: 0.2520\n",
      "249/281, train_loss: 0.0949, step time: 0.2524\n",
      "250/281, train_loss: 0.2875, step time: 0.2502\n",
      "251/281, train_loss: 0.1114, step time: 0.2505\n",
      "252/281, train_loss: 0.2236, step time: 0.2506\n",
      "253/281, train_loss: 0.0929, step time: 0.2489\n",
      "254/281, train_loss: 0.1182, step time: 0.2465\n",
      "255/281, train_loss: 0.0899, step time: 0.2541\n",
      "256/281, train_loss: 0.1061, step time: 0.2563\n",
      "257/281, train_loss: 0.0835, step time: 0.2571\n",
      "258/281, train_loss: 0.1692, step time: 0.2524\n",
      "259/281, train_loss: 0.2344, step time: 0.2542\n",
      "260/281, train_loss: 0.1028, step time: 0.2481\n",
      "261/281, train_loss: 0.1129, step time: 0.2494\n",
      "262/281, train_loss: 0.1700, step time: 0.2514\n",
      "263/281, train_loss: 0.0969, step time: 0.2506\n",
      "264/281, train_loss: 0.0907, step time: 0.2591\n",
      "265/281, train_loss: 0.1417, step time: 0.2592\n",
      "266/281, train_loss: 0.0950, step time: 0.2488\n",
      "267/281, train_loss: 0.1083, step time: 0.2538\n",
      "268/281, train_loss: 0.1120, step time: 0.2540\n",
      "269/281, train_loss: 0.0988, step time: 0.2509\n",
      "270/281, train_loss: 0.0790, step time: 0.2518\n",
      "271/281, train_loss: 0.1690, step time: 0.2518\n",
      "272/281, train_loss: 0.2034, step time: 0.2577\n",
      "273/281, train_loss: 0.2642, step time: 0.2585\n",
      "274/281, train_loss: 0.1160, step time: 0.2560\n",
      "275/281, train_loss: 0.1443, step time: 0.2569\n",
      "276/281, train_loss: 0.1465, step time: 0.2504\n",
      "277/281, train_loss: 0.1834, step time: 0.2544\n",
      "278/281, train_loss: 0.1724, step time: 0.2514\n",
      "279/281, train_loss: 0.1140, step time: 0.2546\n",
      "280/281, train_loss: 0.1607, step time: 0.2494\n",
      "281/281, train_loss: 0.2736, step time: 0.2549\n",
      "282/281, train_loss: 0.1148, step time: 0.1542\n",
      "epoch 133 average loss: 0.1168\n",
      "current epoch: 133 current mean dice: 0.7993 tc: 0.8346 wt: 0.7574 et: 0.8151\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 133 is: 377.7771\n",
      "----------\n",
      "epoch 134/200\n",
      "1/281, train_loss: 0.1431, step time: 0.2631\n",
      "2/281, train_loss: 0.0950, step time: 0.2473\n",
      "3/281, train_loss: 0.3465, step time: 0.2446\n",
      "4/281, train_loss: 0.2602, step time: 0.2516\n",
      "5/281, train_loss: 0.1799, step time: 0.2502\n",
      "6/281, train_loss: 0.1066, step time: 0.2540\n",
      "7/281, train_loss: 0.2551, step time: 0.2490\n",
      "8/281, train_loss: 0.1388, step time: 0.2493\n",
      "9/281, train_loss: 0.1696, step time: 0.2513\n",
      "10/281, train_loss: 0.1543, step time: 0.2519\n",
      "11/281, train_loss: 0.1644, step time: 0.2545\n",
      "12/281, train_loss: 0.1247, step time: 0.2509\n",
      "13/281, train_loss: 0.0927, step time: 0.2543\n",
      "14/281, train_loss: 0.1201, step time: 0.2458\n",
      "15/281, train_loss: 0.0943, step time: 0.2543\n",
      "16/281, train_loss: 0.1079, step time: 0.2545\n",
      "17/281, train_loss: 0.1143, step time: 0.2434\n",
      "18/281, train_loss: 0.0826, step time: 0.2481\n",
      "19/281, train_loss: 0.1068, step time: 0.2487\n",
      "20/281, train_loss: 0.0773, step time: 0.2461\n",
      "21/281, train_loss: 0.1597, step time: 0.2461\n",
      "22/281, train_loss: 0.1335, step time: 0.2468\n",
      "23/281, train_loss: 0.0957, step time: 0.2506\n",
      "24/281, train_loss: 0.2587, step time: 0.2469\n",
      "25/281, train_loss: 0.2356, step time: 0.2420\n",
      "26/281, train_loss: 0.0885, step time: 0.2432\n",
      "27/281, train_loss: 0.0982, step time: 0.2453\n",
      "28/281, train_loss: 0.1401, step time: 0.2415\n",
      "29/281, train_loss: 0.1531, step time: 0.2435\n",
      "30/281, train_loss: 0.1207, step time: 0.2443\n",
      "31/281, train_loss: 0.1099, step time: 0.2412\n",
      "32/281, train_loss: 0.3157, step time: 0.2427\n",
      "33/281, train_loss: 0.2109, step time: 0.2472\n",
      "34/281, train_loss: 0.0974, step time: 0.2460\n",
      "35/281, train_loss: 0.2884, step time: 0.2432\n",
      "36/281, train_loss: 0.1041, step time: 0.2430\n",
      "37/281, train_loss: 0.1028, step time: 0.2526\n",
      "38/281, train_loss: 0.0706, step time: 0.2483\n",
      "39/281, train_loss: 0.2893, step time: 0.2446\n",
      "40/281, train_loss: 0.1260, step time: 0.2477\n",
      "41/281, train_loss: 0.1201, step time: 0.2399\n",
      "42/281, train_loss: 0.0943, step time: 0.2485\n",
      "43/281, train_loss: 0.0885, step time: 0.2688\n",
      "44/281, train_loss: 0.0965, step time: 0.2555\n",
      "45/281, train_loss: 0.1080, step time: 0.2413\n",
      "46/281, train_loss: 0.2749, step time: 0.2463\n",
      "47/281, train_loss: 0.1108, step time: 0.2486\n",
      "48/281, train_loss: 0.1434, step time: 0.2450\n",
      "49/281, train_loss: 0.1120, step time: 0.2449\n",
      "50/281, train_loss: 0.0841, step time: 0.2551\n",
      "51/281, train_loss: 0.1395, step time: 0.2477\n",
      "52/281, train_loss: 0.1645, step time: 0.2462\n",
      "53/281, train_loss: 0.2954, step time: 0.2469\n",
      "54/281, train_loss: 0.1033, step time: 0.2554\n",
      "55/281, train_loss: 0.2708, step time: 0.2512\n",
      "56/281, train_loss: 0.2123, step time: 0.2487\n",
      "57/281, train_loss: 0.1473, step time: 0.2444\n",
      "58/281, train_loss: 0.2654, step time: 0.2535\n",
      "59/281, train_loss: 0.1055, step time: 0.2467\n",
      "60/281, train_loss: 0.1726, step time: 0.2486\n",
      "61/281, train_loss: 0.0903, step time: 0.2459\n",
      "62/281, train_loss: 0.2553, step time: 0.2540\n",
      "63/281, train_loss: 0.1456, step time: 0.2538\n",
      "64/281, train_loss: 0.2824, step time: 0.2466\n",
      "65/281, train_loss: 0.0890, step time: 0.2450\n",
      "66/281, train_loss: 0.1025, step time: 0.2587\n",
      "67/281, train_loss: 0.1263, step time: 0.2515\n",
      "68/281, train_loss: 0.0818, step time: 0.2446\n",
      "69/281, train_loss: 0.1337, step time: 0.2482\n",
      "70/281, train_loss: 0.0864, step time: 0.2518\n",
      "71/281, train_loss: 0.0969, step time: 0.2499\n",
      "72/281, train_loss: 0.1065, step time: 0.2527\n",
      "73/281, train_loss: 0.2820, step time: 0.2505\n",
      "74/281, train_loss: 0.0950, step time: 0.2523\n",
      "75/281, train_loss: 0.1715, step time: 0.2527\n",
      "76/281, train_loss: 0.1331, step time: 0.2516\n",
      "77/281, train_loss: 0.0728, step time: 0.2533\n",
      "78/281, train_loss: 0.0897, step time: 0.2490\n",
      "79/281, train_loss: 0.1020, step time: 0.2497\n",
      "80/281, train_loss: 0.1077, step time: 0.2508\n",
      "81/281, train_loss: 0.1195, step time: 0.2471\n",
      "82/281, train_loss: 0.0989, step time: 0.2500\n",
      "83/281, train_loss: 0.1099, step time: 0.2484\n",
      "84/281, train_loss: 0.2054, step time: 0.2561\n",
      "85/281, train_loss: 0.1231, step time: 0.2521\n",
      "86/281, train_loss: 0.0637, step time: 0.2506\n",
      "87/281, train_loss: 0.0927, step time: 0.2498\n",
      "88/281, train_loss: 0.1034, step time: 0.2501\n",
      "89/281, train_loss: 0.2668, step time: 0.2448\n",
      "90/281, train_loss: 0.0768, step time: 0.2493\n",
      "91/281, train_loss: 0.0978, step time: 0.2464\n",
      "92/281, train_loss: 0.1190, step time: 0.2578\n",
      "93/281, train_loss: 0.1027, step time: 0.2465\n",
      "94/281, train_loss: 0.0965, step time: 0.2468\n",
      "95/281, train_loss: 0.1536, step time: 0.2464\n",
      "96/281, train_loss: 0.1050, step time: 0.2494\n",
      "97/281, train_loss: 0.0805, step time: 0.2479\n",
      "98/281, train_loss: 0.3269, step time: 0.2496\n",
      "99/281, train_loss: 0.1499, step time: 0.2508\n",
      "100/281, train_loss: 0.2961, step time: 0.2489\n",
      "101/281, train_loss: 0.1298, step time: 0.2549\n",
      "102/281, train_loss: 0.1698, step time: 0.2501\n",
      "103/281, train_loss: 0.2335, step time: 0.2438\n",
      "104/281, train_loss: 0.3348, step time: 0.2496\n",
      "105/281, train_loss: 0.0722, step time: 0.2457\n",
      "106/281, train_loss: 0.1231, step time: 0.2458\n",
      "107/281, train_loss: 0.0941, step time: 0.2522\n",
      "108/281, train_loss: 0.0838, step time: 0.2494\n",
      "109/281, train_loss: 0.0856, step time: 0.2488\n",
      "110/281, train_loss: 0.1031, step time: 0.2456\n",
      "111/281, train_loss: 0.0879, step time: 0.2465\n",
      "112/281, train_loss: 0.1000, step time: 0.2469\n",
      "113/281, train_loss: 0.1403, step time: 0.2524\n",
      "114/281, train_loss: 0.0991, step time: 0.2458\n",
      "115/281, train_loss: 0.1086, step time: 0.2495\n",
      "116/281, train_loss: 0.0932, step time: 0.2560\n",
      "117/281, train_loss: 0.0775, step time: 0.2500\n",
      "118/281, train_loss: 0.1626, step time: 0.2448\n",
      "119/281, train_loss: 0.4057, step time: 0.2496\n",
      "120/281, train_loss: 0.3195, step time: 0.2486\n",
      "121/281, train_loss: 0.1395, step time: 0.2509\n",
      "122/281, train_loss: 0.1077, step time: 0.2488\n",
      "123/281, train_loss: 0.1022, step time: 0.2515\n",
      "124/281, train_loss: 0.0678, step time: 0.2510\n",
      "125/281, train_loss: 0.1531, step time: 0.2507\n",
      "126/281, train_loss: 0.1554, step time: 0.2462\n",
      "127/281, train_loss: 0.0905, step time: 0.2482\n",
      "128/281, train_loss: 0.2553, step time: 0.2480\n",
      "129/281, train_loss: 0.1094, step time: 0.2446\n",
      "130/281, train_loss: 0.1326, step time: 0.2392\n",
      "131/281, train_loss: 0.1543, step time: 0.2429\n",
      "132/281, train_loss: 0.1077, step time: 0.2451\n",
      "133/281, train_loss: 0.1420, step time: 0.2386\n",
      "134/281, train_loss: 0.0955, step time: 0.2420\n",
      "135/281, train_loss: 0.0830, step time: 0.2463\n",
      "136/281, train_loss: 0.1196, step time: 0.2503\n",
      "137/281, train_loss: 0.1026, step time: 0.2472\n",
      "138/281, train_loss: 0.1152, step time: 0.2489\n",
      "139/281, train_loss: 0.0677, step time: 0.2532\n",
      "140/281, train_loss: 0.2407, step time: 0.2508\n",
      "141/281, train_loss: 0.1341, step time: 0.2476\n",
      "142/281, train_loss: 0.0880, step time: 0.2499\n",
      "143/281, train_loss: 0.1321, step time: 0.2472\n",
      "144/281, train_loss: 0.0878, step time: 0.2441\n",
      "145/281, train_loss: 0.2507, step time: 0.2434\n",
      "146/281, train_loss: 0.1228, step time: 0.2461\n",
      "147/281, train_loss: 0.0888, step time: 0.2523\n",
      "148/281, train_loss: 0.2838, step time: 0.2502\n",
      "149/281, train_loss: 0.0898, step time: 0.2520\n",
      "150/281, train_loss: 0.1150, step time: 0.2458\n",
      "151/281, train_loss: 0.0987, step time: 0.2445\n",
      "152/281, train_loss: 0.1018, step time: 0.2445\n",
      "153/281, train_loss: 0.0856, step time: 0.2450\n",
      "154/281, train_loss: 0.2531, step time: 0.2448\n",
      "155/281, train_loss: 0.1113, step time: 0.2419\n",
      "156/281, train_loss: 0.0906, step time: 0.2552\n",
      "157/281, train_loss: 0.0812, step time: 0.2488\n",
      "158/281, train_loss: 0.2359, step time: 0.2492\n",
      "159/281, train_loss: 0.0799, step time: 0.2486\n",
      "160/281, train_loss: 0.0661, step time: 0.2444\n",
      "161/281, train_loss: 0.1315, step time: 0.2421\n",
      "162/281, train_loss: 0.2811, step time: 0.2459\n",
      "163/281, train_loss: 0.0816, step time: 0.2497\n",
      "164/281, train_loss: 0.1103, step time: 0.2480\n",
      "165/281, train_loss: 0.2437, step time: 0.2471\n",
      "166/281, train_loss: 0.2682, step time: 0.2445\n",
      "167/281, train_loss: 0.0954, step time: 0.2462\n",
      "168/281, train_loss: 0.1505, step time: 0.2560\n",
      "169/281, train_loss: 0.1118, step time: 0.2487\n",
      "170/281, train_loss: 0.2506, step time: 0.2494\n",
      "171/281, train_loss: 0.0633, step time: 0.2450\n",
      "172/281, train_loss: 0.0847, step time: 0.2489\n",
      "173/281, train_loss: 0.0794, step time: 0.2555\n",
      "174/281, train_loss: 0.0895, step time: 0.2453\n",
      "175/281, train_loss: 0.0677, step time: 0.2437\n",
      "176/281, train_loss: 0.0709, step time: 0.2683\n",
      "177/281, train_loss: 0.1166, step time: 0.2450\n",
      "178/281, train_loss: 0.0802, step time: 0.2512\n",
      "179/281, train_loss: 0.1115, step time: 0.2469\n",
      "180/281, train_loss: 0.0769, step time: 0.2502\n",
      "181/281, train_loss: 0.0718, step time: 0.2518\n",
      "182/281, train_loss: 0.1076, step time: 0.2505\n",
      "183/281, train_loss: 0.1144, step time: 0.2499\n",
      "184/281, train_loss: 0.0887, step time: 0.2483\n",
      "185/281, train_loss: 0.1066, step time: 0.2504\n",
      "186/281, train_loss: 0.0821, step time: 0.2500\n",
      "187/281, train_loss: 0.1348, step time: 0.2429\n",
      "188/281, train_loss: 0.1074, step time: 0.2495\n",
      "189/281, train_loss: 0.1074, step time: 0.2465\n",
      "190/281, train_loss: 0.0759, step time: 0.2453\n",
      "191/281, train_loss: 0.0683, step time: 0.2449\n",
      "192/281, train_loss: 0.0905, step time: 0.2481\n",
      "193/281, train_loss: 0.0993, step time: 0.2469\n",
      "194/281, train_loss: 0.2585, step time: 0.2447\n",
      "195/281, train_loss: 0.1412, step time: 0.2461\n",
      "196/281, train_loss: 0.2885, step time: 0.2517\n",
      "197/281, train_loss: 0.1229, step time: 0.2427\n",
      "198/281, train_loss: 0.1676, step time: 0.2437\n",
      "199/281, train_loss: 0.0918, step time: 0.2486\n",
      "200/281, train_loss: 0.1009, step time: 0.2497\n",
      "201/281, train_loss: 0.0968, step time: 0.2502\n",
      "202/281, train_loss: 0.0885, step time: 0.2483\n",
      "203/281, train_loss: 0.1203, step time: 0.2375\n",
      "204/281, train_loss: 0.0818, step time: 0.2488\n",
      "205/281, train_loss: 0.0818, step time: 0.2460\n",
      "206/281, train_loss: 0.0670, step time: 0.2441\n",
      "207/281, train_loss: 0.0765, step time: 0.2491\n",
      "208/281, train_loss: 0.0762, step time: 0.2531\n",
      "209/281, train_loss: 0.0968, step time: 0.2438\n",
      "210/281, train_loss: 0.2389, step time: 0.2461\n",
      "211/281, train_loss: 0.1004, step time: 0.2698\n",
      "212/281, train_loss: 0.0738, step time: 0.2457\n",
      "213/281, train_loss: 0.1113, step time: 0.2434\n",
      "214/281, train_loss: 0.1683, step time: 0.2424\n",
      "215/281, train_loss: 0.0720, step time: 0.2437\n",
      "216/281, train_loss: 0.1490, step time: 0.2480\n",
      "217/281, train_loss: 0.1538, step time: 0.2472\n",
      "218/281, train_loss: 0.0526, step time: 0.2482\n",
      "219/281, train_loss: 0.1002, step time: 0.2467\n",
      "220/281, train_loss: 0.0864, step time: 0.2491\n",
      "221/281, train_loss: 0.0757, step time: 0.2467\n",
      "222/281, train_loss: 0.0948, step time: 0.2511\n",
      "223/281, train_loss: 0.1829, step time: 0.2476\n",
      "224/281, train_loss: 0.0817, step time: 0.2396\n",
      "225/281, train_loss: 0.0726, step time: 0.2421\n",
      "226/281, train_loss: 0.0983, step time: 0.2436\n",
      "227/281, train_loss: 0.1340, step time: 0.2424\n",
      "228/281, train_loss: 0.1674, step time: 0.2470\n",
      "229/281, train_loss: 0.0963, step time: 0.2466\n",
      "230/281, train_loss: 0.1751, step time: 0.2456\n",
      "231/281, train_loss: 0.0993, step time: 0.2475\n",
      "232/281, train_loss: 0.2324, step time: 0.2444\n",
      "233/281, train_loss: 0.0865, step time: 0.2468\n",
      "234/281, train_loss: 0.1429, step time: 0.2459\n",
      "235/281, train_loss: 0.2344, step time: 0.2439\n",
      "236/281, train_loss: 0.2465, step time: 0.2481\n",
      "237/281, train_loss: 0.1142, step time: 0.2610\n",
      "238/281, train_loss: 0.0834, step time: 0.2450\n",
      "239/281, train_loss: 0.0950, step time: 0.2447\n",
      "240/281, train_loss: 0.0871, step time: 0.2464\n",
      "241/281, train_loss: 0.1110, step time: 0.2466\n",
      "242/281, train_loss: 0.0605, step time: 0.2411\n",
      "243/281, train_loss: 0.1708, step time: 0.2398\n",
      "244/281, train_loss: 0.0723, step time: 0.2487\n",
      "245/281, train_loss: 0.1436, step time: 0.2458\n",
      "246/281, train_loss: 0.1302, step time: 0.2470\n",
      "247/281, train_loss: 0.0629, step time: 0.2495\n",
      "248/281, train_loss: 0.0888, step time: 0.2467\n",
      "249/281, train_loss: 0.0911, step time: 0.2442\n",
      "250/281, train_loss: 0.1812, step time: 0.2431\n",
      "251/281, train_loss: 0.2516, step time: 0.2470\n",
      "252/281, train_loss: 0.0819, step time: 0.2438\n",
      "253/281, train_loss: 0.2605, step time: 0.2469\n",
      "254/281, train_loss: 0.1071, step time: 0.2501\n",
      "255/281, train_loss: 0.1138, step time: 0.2505\n",
      "256/281, train_loss: 0.0911, step time: 0.2495\n",
      "257/281, train_loss: 0.1084, step time: 0.2487\n",
      "258/281, train_loss: 0.1036, step time: 0.2466\n",
      "259/281, train_loss: 0.3948, step time: 0.2449\n",
      "260/281, train_loss: 0.1130, step time: 0.2463\n",
      "261/281, train_loss: 0.1196, step time: 0.2468\n",
      "262/281, train_loss: 0.1163, step time: 0.2398\n",
      "263/281, train_loss: 0.1177, step time: 0.2441\n",
      "264/281, train_loss: 0.0689, step time: 0.2444\n",
      "265/281, train_loss: 0.0985, step time: 0.2434\n",
      "266/281, train_loss: 0.2741, step time: 0.2440\n",
      "267/281, train_loss: 0.1137, step time: 0.2417\n",
      "268/281, train_loss: 0.0851, step time: 0.2489\n",
      "269/281, train_loss: 0.0890, step time: 0.2463\n",
      "270/281, train_loss: 0.0991, step time: 0.2477\n",
      "271/281, train_loss: 0.0859, step time: 0.2457\n",
      "272/281, train_loss: 0.0970, step time: 0.2423\n",
      "273/281, train_loss: 0.2287, step time: 0.2436\n",
      "274/281, train_loss: 0.0671, step time: 0.2432\n",
      "275/281, train_loss: 0.0868, step time: 0.2431\n",
      "276/281, train_loss: 0.0598, step time: 0.2463\n",
      "277/281, train_loss: 0.1795, step time: 0.2445\n",
      "278/281, train_loss: 0.0666, step time: 0.2445\n",
      "279/281, train_loss: 0.1009, step time: 0.2481\n",
      "280/281, train_loss: 0.0752, step time: 0.2389\n",
      "281/281, train_loss: 0.2306, step time: 0.2408\n",
      "282/281, train_loss: 0.2071, step time: 0.1455\n",
      "epoch 134 average loss: 0.1345\n",
      "current epoch: 134 current mean dice: 0.8722 tc: 0.8747 wt: 0.8949 et: 0.8606\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 134 is: 387.3844\n",
      "----------\n",
      "epoch 135/200\n",
      "1/281, train_loss: 0.1162, step time: 0.2530\n",
      "2/281, train_loss: 0.0966, step time: 0.2502\n",
      "3/281, train_loss: 0.0877, step time: 0.2486\n",
      "4/281, train_loss: 0.1205, step time: 0.2434\n",
      "5/281, train_loss: 0.0844, step time: 0.2487\n",
      "6/281, train_loss: 0.1265, step time: 0.2475\n",
      "7/281, train_loss: 0.0721, step time: 0.2610\n",
      "8/281, train_loss: 0.0793, step time: 0.2538\n",
      "9/281, train_loss: 0.0789, step time: 0.2468\n",
      "10/281, train_loss: 0.0800, step time: 0.2463\n",
      "11/281, train_loss: 0.1418, step time: 0.2516\n",
      "12/281, train_loss: 0.0867, step time: 0.2434\n",
      "13/281, train_loss: 0.0863, step time: 0.2426\n",
      "14/281, train_loss: 0.1067, step time: 0.2564\n",
      "15/281, train_loss: 0.0683, step time: 0.2545\n",
      "16/281, train_loss: 0.2251, step time: 0.2552\n",
      "17/281, train_loss: 0.2316, step time: 0.2440\n",
      "18/281, train_loss: 0.0684, step time: 0.2441\n",
      "19/281, train_loss: 0.1006, step time: 0.2479\n",
      "20/281, train_loss: 0.0823, step time: 0.2511\n",
      "21/281, train_loss: 0.0865, step time: 0.2527\n",
      "22/281, train_loss: 0.0511, step time: 0.2461\n",
      "23/281, train_loss: 0.1097, step time: 0.2481\n",
      "24/281, train_loss: 0.1165, step time: 0.2461\n",
      "25/281, train_loss: 0.0689, step time: 0.2502\n",
      "26/281, train_loss: 0.0675, step time: 0.2466\n",
      "27/281, train_loss: 0.0682, step time: 0.2501\n",
      "28/281, train_loss: 0.2426, step time: 0.2448\n",
      "29/281, train_loss: 0.1414, step time: 0.2461\n",
      "30/281, train_loss: 0.2435, step time: 0.2547\n",
      "31/281, train_loss: 0.2530, step time: 0.2456\n",
      "32/281, train_loss: 0.0988, step time: 0.2527\n",
      "33/281, train_loss: 0.0858, step time: 0.2564\n",
      "34/281, train_loss: 0.0647, step time: 0.2441\n",
      "35/281, train_loss: 0.0924, step time: 0.2458\n",
      "36/281, train_loss: 0.2205, step time: 0.2444\n",
      "37/281, train_loss: 0.1231, step time: 0.2468\n",
      "38/281, train_loss: 0.0774, step time: 0.2579\n",
      "39/281, train_loss: 0.0857, step time: 0.2459\n",
      "40/281, train_loss: 0.1200, step time: 0.2462\n",
      "41/281, train_loss: 0.0814, step time: 0.2504\n",
      "42/281, train_loss: 0.0777, step time: 0.2498\n",
      "43/281, train_loss: 0.0731, step time: 0.2555\n",
      "44/281, train_loss: 0.0884, step time: 0.2460\n",
      "45/281, train_loss: 0.0903, step time: 0.2523\n",
      "46/281, train_loss: 0.0990, step time: 0.2531\n",
      "47/281, train_loss: 0.1045, step time: 0.2523\n",
      "48/281, train_loss: 0.1319, step time: 0.2484\n",
      "49/281, train_loss: 0.0827, step time: 0.2547\n",
      "50/281, train_loss: 0.0664, step time: 0.2588\n",
      "51/281, train_loss: 0.1329, step time: 0.2886\n",
      "52/281, train_loss: 0.2245, step time: 0.2549\n",
      "53/281, train_loss: 0.2413, step time: 0.2547\n",
      "54/281, train_loss: 0.0650, step time: 0.2538\n",
      "55/281, train_loss: 0.0934, step time: 0.2552\n",
      "56/281, train_loss: 0.3416, step time: 0.2477\n",
      "57/281, train_loss: 0.0640, step time: 0.2515\n",
      "58/281, train_loss: 0.1218, step time: 0.2466\n",
      "59/281, train_loss: 0.0635, step time: 0.2514\n",
      "60/281, train_loss: 0.1257, step time: 0.2512\n",
      "61/281, train_loss: 0.0845, step time: 0.2557\n",
      "62/281, train_loss: 0.2383, step time: 0.2486\n",
      "63/281, train_loss: 0.2374, step time: 0.2462\n",
      "64/281, train_loss: 0.0704, step time: 0.2546\n",
      "65/281, train_loss: 0.2682, step time: 0.2532\n",
      "66/281, train_loss: 0.0592, step time: 0.2562\n",
      "67/281, train_loss: 0.1531, step time: 0.2557\n",
      "68/281, train_loss: 0.0677, step time: 0.2516\n",
      "69/281, train_loss: 0.0904, step time: 0.2515\n",
      "70/281, train_loss: 0.2243, step time: 0.2487\n",
      "71/281, train_loss: 0.0894, step time: 0.2533\n",
      "72/281, train_loss: 0.0831, step time: 0.2481\n",
      "73/281, train_loss: 0.3029, step time: 0.2509\n",
      "74/281, train_loss: 0.0725, step time: 0.2530\n",
      "75/281, train_loss: 0.2556, step time: 0.2478\n",
      "76/281, train_loss: 0.1097, step time: 0.2510\n",
      "77/281, train_loss: 0.1043, step time: 0.2525\n",
      "78/281, train_loss: 0.0675, step time: 0.2480\n",
      "79/281, train_loss: 0.1227, step time: 0.2479\n",
      "80/281, train_loss: 0.1117, step time: 0.2575\n",
      "81/281, train_loss: 0.2218, step time: 0.2542\n",
      "82/281, train_loss: 0.1512, step time: 0.2491\n",
      "83/281, train_loss: 0.1579, step time: 0.2507\n",
      "84/281, train_loss: 0.1378, step time: 0.2517\n",
      "85/281, train_loss: 0.1005, step time: 0.2530\n",
      "86/281, train_loss: 0.1044, step time: 0.2510\n",
      "87/281, train_loss: 0.0524, step time: 0.2510\n",
      "88/281, train_loss: 0.1198, step time: 0.2489\n",
      "89/281, train_loss: 0.2484, step time: 0.2505\n",
      "90/281, train_loss: 0.0958, step time: 0.2489\n",
      "91/281, train_loss: 0.1169, step time: 0.2515\n",
      "92/281, train_loss: 0.1127, step time: 0.2529\n",
      "93/281, train_loss: 0.0918, step time: 0.2527\n",
      "94/281, train_loss: 0.0921, step time: 0.2543\n",
      "95/281, train_loss: 0.0971, step time: 0.2509\n",
      "96/281, train_loss: 0.1034, step time: 0.2557\n",
      "97/281, train_loss: 0.0626, step time: 0.2493\n",
      "98/281, train_loss: 0.1283, step time: 0.2478\n",
      "99/281, train_loss: 0.0736, step time: 0.2500\n",
      "100/281, train_loss: 0.1729, step time: 0.2483\n",
      "101/281, train_loss: 0.2625, step time: 0.2489\n",
      "102/281, train_loss: 0.2383, step time: 0.2521\n",
      "103/281, train_loss: 0.2509, step time: 0.2539\n",
      "104/281, train_loss: 0.0845, step time: 0.2540\n",
      "105/281, train_loss: 0.0836, step time: 0.2483\n",
      "106/281, train_loss: 0.2428, step time: 0.2581\n",
      "107/281, train_loss: 0.0752, step time: 0.2561\n",
      "108/281, train_loss: 0.0862, step time: 0.2528\n",
      "109/281, train_loss: 0.1483, step time: 0.2590\n",
      "110/281, train_loss: 0.0812, step time: 0.2492\n",
      "111/281, train_loss: 0.1173, step time: 0.2511\n",
      "112/281, train_loss: 0.1101, step time: 0.2515\n",
      "113/281, train_loss: 0.1391, step time: 0.2490\n",
      "114/281, train_loss: 0.1122, step time: 0.2529\n",
      "115/281, train_loss: 0.0691, step time: 0.2461\n",
      "116/281, train_loss: 0.1095, step time: 0.2478\n",
      "117/281, train_loss: 0.0636, step time: 0.2457\n",
      "118/281, train_loss: 0.1064, step time: 0.2439\n",
      "119/281, train_loss: 0.1148, step time: 0.2452\n",
      "120/281, train_loss: 0.1193, step time: 0.2410\n",
      "121/281, train_loss: 0.0633, step time: 0.2429\n",
      "122/281, train_loss: 0.0524, step time: 0.2462\n",
      "123/281, train_loss: 0.2634, step time: 0.2519\n",
      "124/281, train_loss: 0.0843, step time: 0.2549\n",
      "125/281, train_loss: 0.1447, step time: 0.2536\n",
      "126/281, train_loss: 0.0607, step time: 0.2478\n",
      "127/281, train_loss: 0.0747, step time: 0.2552\n",
      "128/281, train_loss: 0.0882, step time: 0.2524\n",
      "129/281, train_loss: 0.0544, step time: 0.2468\n",
      "130/281, train_loss: 0.0850, step time: 0.2475\n",
      "131/281, train_loss: 0.0943, step time: 0.2478\n",
      "132/281, train_loss: 0.1094, step time: 0.2456\n",
      "133/281, train_loss: 0.2818, step time: 0.2518\n",
      "134/281, train_loss: 0.0882, step time: 0.2539\n",
      "135/281, train_loss: 0.0870, step time: 0.2580\n",
      "136/281, train_loss: 0.0676, step time: 0.2474\n",
      "137/281, train_loss: 0.0540, step time: 0.2453\n",
      "138/281, train_loss: 0.0809, step time: 0.2485\n",
      "139/281, train_loss: 0.2522, step time: 0.2476\n",
      "140/281, train_loss: 0.1137, step time: 0.2490\n",
      "141/281, train_loss: 0.1134, step time: 0.2470\n",
      "142/281, train_loss: 0.0795, step time: 0.2478\n",
      "143/281, train_loss: 0.0954, step time: 0.2486\n",
      "144/281, train_loss: 0.0942, step time: 0.2478\n",
      "145/281, train_loss: 0.2372, step time: 0.2452\n",
      "146/281, train_loss: 0.0529, step time: 0.2447\n",
      "147/281, train_loss: 0.1049, step time: 0.2457\n",
      "148/281, train_loss: 0.2353, step time: 0.2434\n",
      "149/281, train_loss: 0.0811, step time: 0.2432\n",
      "150/281, train_loss: 0.0667, step time: 0.2516\n",
      "151/281, train_loss: 0.0885, step time: 0.2550\n",
      "152/281, train_loss: 0.0734, step time: 0.2500\n",
      "153/281, train_loss: 0.0871, step time: 0.2572\n",
      "154/281, train_loss: 0.0845, step time: 0.2519\n",
      "155/281, train_loss: 0.0598, step time: 0.2513\n",
      "156/281, train_loss: 0.0727, step time: 0.2532\n",
      "157/281, train_loss: 0.2345, step time: 0.2499\n",
      "158/281, train_loss: 0.0975, step time: 0.2548\n",
      "159/281, train_loss: 0.1226, step time: 0.2505\n",
      "160/281, train_loss: 0.1469, step time: 0.2503\n",
      "161/281, train_loss: 0.0577, step time: 0.2505\n",
      "162/281, train_loss: 0.0821, step time: 0.2492\n",
      "163/281, train_loss: 0.2416, step time: 0.2634\n",
      "164/281, train_loss: 0.0709, step time: 0.2509\n",
      "165/281, train_loss: 0.0999, step time: 0.2525\n",
      "166/281, train_loss: 0.2658, step time: 0.2543\n",
      "167/281, train_loss: 0.0883, step time: 0.2543\n",
      "168/281, train_loss: 0.0641, step time: 0.2535\n",
      "169/281, train_loss: 0.1312, step time: 0.2523\n",
      "170/281, train_loss: 0.1027, step time: 0.2543\n",
      "171/281, train_loss: 0.0734, step time: 0.2505\n",
      "172/281, train_loss: 0.2420, step time: 0.2526\n",
      "173/281, train_loss: 0.0624, step time: 0.2472\n",
      "174/281, train_loss: 0.1077, step time: 0.2500\n",
      "175/281, train_loss: 0.0606, step time: 0.2475\n",
      "176/281, train_loss: 0.0939, step time: 0.2559\n",
      "177/281, train_loss: 0.0673, step time: 0.2517\n",
      "178/281, train_loss: 0.0733, step time: 0.2540\n",
      "179/281, train_loss: 0.2284, step time: 0.2525\n",
      "180/281, train_loss: 0.1001, step time: 0.2568\n",
      "181/281, train_loss: 0.0994, step time: 0.2537\n",
      "182/281, train_loss: 0.1081, step time: 0.2515\n",
      "183/281, train_loss: 0.1178, step time: 0.2514\n",
      "184/281, train_loss: 0.0849, step time: 0.2474\n",
      "185/281, train_loss: 0.3126, step time: 0.2463\n",
      "186/281, train_loss: 0.2315, step time: 0.2528\n",
      "187/281, train_loss: 0.1285, step time: 0.2493\n",
      "188/281, train_loss: 0.0840, step time: 0.2560\n",
      "189/281, train_loss: 0.1076, step time: 0.2510\n",
      "190/281, train_loss: 0.2735, step time: 0.2543\n",
      "191/281, train_loss: 0.0614, step time: 0.2545\n",
      "192/281, train_loss: 0.0873, step time: 0.2552\n",
      "193/281, train_loss: 0.1214, step time: 0.2521\n",
      "194/281, train_loss: 0.1010, step time: 0.2505\n",
      "195/281, train_loss: 0.0827, step time: 0.2484\n",
      "196/281, train_loss: 0.0843, step time: 0.2519\n",
      "197/281, train_loss: 0.0794, step time: 0.2488\n",
      "198/281, train_loss: 0.1211, step time: 0.2465\n",
      "199/281, train_loss: 0.0897, step time: 0.2483\n",
      "200/281, train_loss: 0.2221, step time: 0.2559\n",
      "201/281, train_loss: 0.0904, step time: 0.2529\n",
      "202/281, train_loss: 0.2214, step time: 0.2541\n",
      "203/281, train_loss: 0.0955, step time: 0.2971\n",
      "204/281, train_loss: 0.1370, step time: 0.2555\n",
      "205/281, train_loss: 0.0908, step time: 0.2517\n",
      "206/281, train_loss: 0.0965, step time: 0.2480\n",
      "207/281, train_loss: 0.1028, step time: 0.2528\n",
      "208/281, train_loss: 0.0862, step time: 0.2431\n",
      "209/281, train_loss: 0.0543, step time: 0.2482\n",
      "210/281, train_loss: 0.2144, step time: 0.2485\n",
      "211/281, train_loss: 0.0502, step time: 0.2517\n",
      "212/281, train_loss: 0.0850, step time: 0.2477\n",
      "213/281, train_loss: 0.1027, step time: 0.2524\n",
      "214/281, train_loss: 0.1383, step time: 0.2532\n",
      "215/281, train_loss: 0.2366, step time: 0.2533\n",
      "216/281, train_loss: 0.1190, step time: 0.2509\n",
      "217/281, train_loss: 0.1294, step time: 0.2481\n",
      "218/281, train_loss: 0.2395, step time: 0.2585\n",
      "219/281, train_loss: 0.0734, step time: 0.2544\n",
      "220/281, train_loss: 0.0694, step time: 0.2543\n",
      "221/281, train_loss: 0.2304, step time: 0.2573\n",
      "222/281, train_loss: 0.0710, step time: 0.2566\n",
      "223/281, train_loss: 0.2648, step time: 0.2551\n",
      "224/281, train_loss: 0.0840, step time: 0.2496\n",
      "225/281, train_loss: 0.1076, step time: 0.2465\n",
      "226/281, train_loss: 0.2323, step time: 0.2555\n",
      "227/281, train_loss: 0.0731, step time: 0.2525\n",
      "228/281, train_loss: 0.1040, step time: 0.2507\n",
      "229/281, train_loss: 0.0907, step time: 0.2487\n",
      "230/281, train_loss: 0.0588, step time: 0.2594\n",
      "231/281, train_loss: 0.0658, step time: 0.2547\n",
      "232/281, train_loss: 0.0642, step time: 0.2547\n",
      "233/281, train_loss: 0.0661, step time: 0.2487\n",
      "234/281, train_loss: 0.0639, step time: 0.2438\n",
      "235/281, train_loss: 0.0789, step time: 0.2419\n",
      "236/281, train_loss: 0.0581, step time: 0.2485\n",
      "237/281, train_loss: 0.0928, step time: 0.2459\n",
      "238/281, train_loss: 0.1148, step time: 0.2499\n",
      "239/281, train_loss: 0.0850, step time: 0.2497\n",
      "240/281, train_loss: 0.0765, step time: 0.2625\n",
      "241/281, train_loss: 0.0982, step time: 0.2490\n",
      "242/281, train_loss: 0.0858, step time: 0.2483\n",
      "243/281, train_loss: 0.1043, step time: 0.2451\n",
      "244/281, train_loss: 0.0738, step time: 0.2489\n",
      "245/281, train_loss: 0.0965, step time: 0.2499\n",
      "246/281, train_loss: 0.0535, step time: 0.2537\n",
      "247/281, train_loss: 0.0942, step time: 0.2552\n",
      "248/281, train_loss: 0.0976, step time: 0.2520\n",
      "249/281, train_loss: 0.0991, step time: 0.2466\n",
      "250/281, train_loss: 0.0644, step time: 0.2561\n",
      "251/281, train_loss: 0.0813, step time: 0.2448\n",
      "252/281, train_loss: 0.0771, step time: 0.2499\n",
      "253/281, train_loss: 0.0809, step time: 0.2811\n",
      "254/281, train_loss: 0.1085, step time: 0.2489\n",
      "255/281, train_loss: 0.0800, step time: 0.2488\n",
      "256/281, train_loss: 0.0824, step time: 0.2513\n",
      "257/281, train_loss: 0.0578, step time: 0.2519\n",
      "258/281, train_loss: 0.0655, step time: 0.2494\n",
      "259/281, train_loss: 0.0840, step time: 0.2442\n",
      "260/281, train_loss: 0.1307, step time: 0.2453\n",
      "261/281, train_loss: 0.1429, step time: 0.2442\n",
      "262/281, train_loss: 0.0862, step time: 0.2576\n",
      "263/281, train_loss: 0.1119, step time: 0.2578\n",
      "264/281, train_loss: 0.0692, step time: 0.2743\n",
      "265/281, train_loss: 0.0738, step time: 0.2546\n",
      "266/281, train_loss: 0.0867, step time: 0.2527\n",
      "267/281, train_loss: 0.0652, step time: 0.2526\n",
      "268/281, train_loss: 0.0784, step time: 0.2545\n",
      "269/281, train_loss: 0.0909, step time: 0.2540\n",
      "270/281, train_loss: 0.0937, step time: 0.2505\n",
      "271/281, train_loss: 0.1076, step time: 0.2523\n",
      "272/281, train_loss: 0.0589, step time: 0.2533\n",
      "273/281, train_loss: 0.0819, step time: 0.2528\n",
      "274/281, train_loss: 0.1146, step time: 0.2476\n",
      "275/281, train_loss: 0.0917, step time: 0.2493\n",
      "276/281, train_loss: 0.0976, step time: 0.2448\n",
      "277/281, train_loss: 0.1033, step time: 0.2492\n",
      "278/281, train_loss: 0.0778, step time: 0.2503\n",
      "279/281, train_loss: 0.0588, step time: 0.2511\n",
      "280/281, train_loss: 0.0657, step time: 0.2489\n",
      "281/281, train_loss: 0.2421, step time: 0.2499\n",
      "282/281, train_loss: 0.1005, step time: 0.1494\n",
      "epoch 135 average loss: 0.1151\n",
      "current epoch: 135 current mean dice: 0.8689 tc: 0.8731 wt: 0.8865 et: 0.8614\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 135 is: 394.1157\n",
      "----------\n",
      "epoch 136/200\n",
      "1/281, train_loss: 0.2206, step time: 0.2554\n",
      "2/281, train_loss: 0.2467, step time: 0.2561\n",
      "3/281, train_loss: 0.0866, step time: 0.2549\n",
      "4/281, train_loss: 0.0896, step time: 0.2516\n",
      "5/281, train_loss: 0.0674, step time: 0.2529\n",
      "6/281, train_loss: 0.2550, step time: 0.2590\n",
      "7/281, train_loss: 0.2535, step time: 0.2527\n",
      "8/281, train_loss: 0.1170, step time: 0.2531\n",
      "9/281, train_loss: 0.0870, step time: 0.2512\n",
      "10/281, train_loss: 0.1018, step time: 0.2539\n",
      "11/281, train_loss: 0.0823, step time: 0.2527\n",
      "12/281, train_loss: 0.0713, step time: 0.2448\n",
      "13/281, train_loss: 0.2224, step time: 0.2564\n",
      "14/281, train_loss: 0.0769, step time: 0.2516\n",
      "15/281, train_loss: 0.1027, step time: 0.2538\n",
      "16/281, train_loss: 0.1348, step time: 0.2519\n",
      "17/281, train_loss: 0.0521, step time: 0.2580\n",
      "18/281, train_loss: 0.2694, step time: 0.2547\n",
      "19/281, train_loss: 0.2670, step time: 0.2571\n",
      "20/281, train_loss: 0.1172, step time: 0.2503\n",
      "21/281, train_loss: 0.0911, step time: 0.2558\n",
      "22/281, train_loss: 0.0544, step time: 0.2475\n",
      "23/281, train_loss: 0.0538, step time: 0.2464\n",
      "24/281, train_loss: 0.0942, step time: 0.2523\n",
      "25/281, train_loss: 0.1317, step time: 0.2488\n",
      "26/281, train_loss: 0.2186, step time: 0.2567\n",
      "27/281, train_loss: 0.0985, step time: 0.2589\n",
      "28/281, train_loss: 0.0732, step time: 0.3248\n",
      "29/281, train_loss: 0.0562, step time: 0.2507\n",
      "30/281, train_loss: 0.0962, step time: 0.2534\n",
      "31/281, train_loss: 0.0682, step time: 0.2548\n",
      "32/281, train_loss: 0.0908, step time: 0.2537\n",
      "33/281, train_loss: 0.0746, step time: 0.2539\n",
      "34/281, train_loss: 0.0898, step time: 0.2530\n",
      "35/281, train_loss: 0.0828, step time: 0.2552\n",
      "36/281, train_loss: 0.0993, step time: 0.2527\n",
      "37/281, train_loss: 0.0658, step time: 0.2570\n",
      "38/281, train_loss: 0.0894, step time: 0.2598\n",
      "39/281, train_loss: 0.0811, step time: 0.2560\n",
      "40/281, train_loss: 0.0821, step time: 0.2516\n",
      "41/281, train_loss: 0.0612, step time: 0.2490\n",
      "42/281, train_loss: 0.1490, step time: 0.2467\n",
      "43/281, train_loss: 0.0584, step time: 0.2509\n",
      "44/281, train_loss: 0.0687, step time: 0.2502\n",
      "45/281, train_loss: 0.0611, step time: 0.2516\n",
      "46/281, train_loss: 0.1005, step time: 0.2514\n",
      "47/281, train_loss: 0.0736, step time: 0.2543\n",
      "48/281, train_loss: 0.1135, step time: 0.2518\n",
      "49/281, train_loss: 0.0779, step time: 0.2534\n",
      "50/281, train_loss: 0.0879, step time: 0.2501\n",
      "51/281, train_loss: 0.0493, step time: 0.2514\n",
      "52/281, train_loss: 0.0661, step time: 0.2483\n",
      "53/281, train_loss: 0.0827, step time: 0.2504\n",
      "54/281, train_loss: 0.0558, step time: 0.2795\n",
      "55/281, train_loss: 0.0873, step time: 0.2776\n",
      "56/281, train_loss: 0.1073, step time: 0.2579\n",
      "57/281, train_loss: 0.2846, step time: 0.2703\n",
      "58/281, train_loss: 0.1086, step time: 0.2535\n",
      "59/281, train_loss: 0.1407, step time: 0.2546\n",
      "60/281, train_loss: 0.1064, step time: 0.2528\n",
      "61/281, train_loss: 0.0593, step time: 0.2506\n",
      "62/281, train_loss: 0.4057, step time: 0.2509\n",
      "63/281, train_loss: 0.1319, step time: 0.2496\n",
      "64/281, train_loss: 0.0677, step time: 0.2472\n",
      "65/281, train_loss: 0.0852, step time: 0.2478\n",
      "66/281, train_loss: 0.0892, step time: 0.2576\n",
      "67/281, train_loss: 0.1207, step time: 0.2587\n",
      "68/281, train_loss: 0.0910, step time: 0.2588\n",
      "69/281, train_loss: 0.1185, step time: 0.2536\n",
      "70/281, train_loss: 0.0924, step time: 0.2567\n",
      "71/281, train_loss: 0.0844, step time: 0.2552\n",
      "72/281, train_loss: 0.0837, step time: 0.2513\n",
      "73/281, train_loss: 0.0791, step time: 0.2563\n",
      "74/281, train_loss: 0.0972, step time: 0.2544\n",
      "75/281, train_loss: 0.0935, step time: 0.2519\n",
      "76/281, train_loss: 0.0715, step time: 0.2539\n",
      "77/281, train_loss: 0.0960, step time: 0.2505\n",
      "78/281, train_loss: 0.2374, step time: 0.2512\n",
      "79/281, train_loss: 0.0880, step time: 0.2532\n",
      "80/281, train_loss: 0.0708, step time: 0.2499\n",
      "81/281, train_loss: 0.0811, step time: 0.2622\n",
      "82/281, train_loss: 0.0962, step time: 0.2526\n",
      "83/281, train_loss: 0.0946, step time: 0.2514\n",
      "84/281, train_loss: 0.0507, step time: 0.2519\n",
      "85/281, train_loss: 0.0847, step time: 0.2531\n",
      "86/281, train_loss: 0.0895, step time: 0.2535\n",
      "87/281, train_loss: 0.0836, step time: 0.2545\n",
      "88/281, train_loss: 0.1005, step time: 0.2548\n",
      "89/281, train_loss: 0.0549, step time: 0.2644\n",
      "90/281, train_loss: 0.0710, step time: 0.2666\n",
      "91/281, train_loss: 0.0781, step time: 0.2591\n",
      "92/281, train_loss: 0.2242, step time: 0.2564\n",
      "93/281, train_loss: 0.2377, step time: 0.2555\n",
      "94/281, train_loss: 0.0972, step time: 0.2572\n",
      "95/281, train_loss: 0.0803, step time: 0.2483\n",
      "96/281, train_loss: 0.0678, step time: 0.2529\n",
      "97/281, train_loss: 0.1107, step time: 0.2535\n",
      "98/281, train_loss: 0.1173, step time: 0.2516\n",
      "99/281, train_loss: 0.2436, step time: 0.2507\n",
      "100/281, train_loss: 0.0659, step time: 0.2760\n",
      "101/281, train_loss: 0.0807, step time: 0.2535\n",
      "102/281, train_loss: 0.0747, step time: 0.2554\n",
      "103/281, train_loss: 0.2267, step time: 0.2577\n",
      "104/281, train_loss: 0.0658, step time: 0.2581\n",
      "105/281, train_loss: 0.0552, step time: 0.2568\n",
      "106/281, train_loss: 0.0988, step time: 0.2524\n",
      "107/281, train_loss: 0.0715, step time: 0.2552\n",
      "108/281, train_loss: 0.0829, step time: 0.2491\n",
      "109/281, train_loss: 0.0965, step time: 0.2529\n",
      "110/281, train_loss: 0.0842, step time: 0.2480\n",
      "111/281, train_loss: 0.0549, step time: 0.2467\n",
      "112/281, train_loss: 0.0694, step time: 0.2497\n",
      "113/281, train_loss: 0.0535, step time: 0.2521\n",
      "114/281, train_loss: 0.2325, step time: 0.2498\n",
      "115/281, train_loss: 0.2225, step time: 0.2516\n",
      "116/281, train_loss: 0.1184, step time: 0.2496\n",
      "117/281, train_loss: 0.2383, step time: 0.2504\n",
      "118/281, train_loss: 0.1150, step time: 0.2517\n",
      "119/281, train_loss: 0.0662, step time: 0.2526\n",
      "120/281, train_loss: 0.2284, step time: 0.2506\n",
      "121/281, train_loss: 0.0991, step time: 0.2515\n",
      "122/281, train_loss: 0.0537, step time: 0.2471\n",
      "123/281, train_loss: 0.2412, step time: 0.2506\n",
      "124/281, train_loss: 0.2599, step time: 0.2551\n",
      "125/281, train_loss: 0.1039, step time: 0.2469\n",
      "126/281, train_loss: 0.0739, step time: 0.2537\n",
      "127/281, train_loss: 0.0830, step time: 0.2533\n",
      "128/281, train_loss: 0.0825, step time: 0.2471\n",
      "129/281, train_loss: 0.0711, step time: 0.2482\n",
      "130/281, train_loss: 0.1183, step time: 0.2581\n",
      "131/281, train_loss: 0.0777, step time: 0.2532\n",
      "132/281, train_loss: 0.1215, step time: 0.2546\n",
      "133/281, train_loss: 0.1147, step time: 0.2506\n",
      "134/281, train_loss: 0.2440, step time: 0.2491\n",
      "135/281, train_loss: 0.0542, step time: 0.2481\n",
      "136/281, train_loss: 0.2399, step time: 0.2448\n",
      "137/281, train_loss: 0.0732, step time: 0.2482\n",
      "138/281, train_loss: 0.2398, step time: 0.2459\n",
      "139/281, train_loss: 0.0922, step time: 0.2475\n",
      "140/281, train_loss: 0.0913, step time: 0.2447\n",
      "141/281, train_loss: 0.1009, step time: 0.2453\n",
      "142/281, train_loss: 0.3812, step time: 0.2468\n",
      "143/281, train_loss: 0.0941, step time: 0.2413\n",
      "144/281, train_loss: 0.0659, step time: 0.2457\n",
      "145/281, train_loss: 0.2396, step time: 0.2503\n",
      "146/281, train_loss: 0.0802, step time: 0.2426\n",
      "147/281, train_loss: 0.0965, step time: 0.2426\n",
      "148/281, train_loss: 0.1591, step time: 0.2493\n",
      "149/281, train_loss: 0.1189, step time: 0.2475\n",
      "150/281, train_loss: 0.0947, step time: 0.2452\n",
      "151/281, train_loss: 0.2223, step time: 0.2464\n",
      "152/281, train_loss: 0.0736, step time: 0.2499\n",
      "153/281, train_loss: 0.0802, step time: 0.2497\n",
      "154/281, train_loss: 0.1211, step time: 0.2476\n",
      "155/281, train_loss: 0.0786, step time: 0.2475\n",
      "156/281, train_loss: 0.0613, step time: 0.2496\n",
      "157/281, train_loss: 0.1036, step time: 0.2558\n",
      "158/281, train_loss: 0.3024, step time: 0.2449\n",
      "159/281, train_loss: 0.2705, step time: 0.2473\n",
      "160/281, train_loss: 0.0666, step time: 0.2475\n",
      "161/281, train_loss: 0.0746, step time: 0.2473\n",
      "162/281, train_loss: 0.2208, step time: 0.2496\n",
      "163/281, train_loss: 0.0745, step time: 0.2482\n",
      "164/281, train_loss: 0.0663, step time: 0.2467\n",
      "165/281, train_loss: 0.0613, step time: 0.2449\n",
      "166/281, train_loss: 0.0817, step time: 0.2428\n",
      "167/281, train_loss: 0.1145, step time: 0.2474\n",
      "168/281, train_loss: 0.0808, step time: 0.2477\n",
      "169/281, train_loss: 0.0587, step time: 0.2489\n",
      "170/281, train_loss: 0.1008, step time: 0.2442\n",
      "171/281, train_loss: 0.2375, step time: 0.2503\n",
      "172/281, train_loss: 0.0790, step time: 0.2581\n",
      "173/281, train_loss: 0.0755, step time: 0.2483\n",
      "174/281, train_loss: 0.0774, step time: 0.2482\n",
      "175/281, train_loss: 0.1003, step time: 0.2489\n",
      "176/281, train_loss: 0.0747, step time: 0.2487\n",
      "177/281, train_loss: 0.0732, step time: 0.2478\n",
      "178/281, train_loss: 0.0929, step time: 0.2468\n",
      "179/281, train_loss: 0.2438, step time: 0.2446\n",
      "180/281, train_loss: 0.0836, step time: 0.2456\n",
      "181/281, train_loss: 0.0985, step time: 0.2485\n",
      "182/281, train_loss: 0.0902, step time: 0.2463\n",
      "183/281, train_loss: 0.0776, step time: 0.2455\n",
      "184/281, train_loss: 0.2716, step time: 0.2452\n",
      "185/281, train_loss: 0.2271, step time: 0.2510\n",
      "186/281, train_loss: 0.0781, step time: 0.2475\n",
      "187/281, train_loss: 0.0880, step time: 0.2477\n",
      "188/281, train_loss: 0.1029, step time: 0.2492\n",
      "189/281, train_loss: 0.0988, step time: 0.2536\n",
      "190/281, train_loss: 0.0798, step time: 0.2496\n",
      "191/281, train_loss: 0.2681, step time: 0.2497\n",
      "192/281, train_loss: 0.0886, step time: 0.2588\n",
      "193/281, train_loss: 0.0973, step time: 0.2470\n",
      "194/281, train_loss: 0.0934, step time: 0.2445\n",
      "195/281, train_loss: 0.0971, step time: 0.2503\n",
      "196/281, train_loss: 0.0871, step time: 0.2482\n",
      "197/281, train_loss: 0.0823, step time: 0.2497\n",
      "198/281, train_loss: 0.0854, step time: 0.2513\n",
      "199/281, train_loss: 0.0722, step time: 0.2599\n",
      "200/281, train_loss: 0.0868, step time: 0.2578\n",
      "201/281, train_loss: 0.2617, step time: 0.2452\n",
      "202/281, train_loss: 0.0656, step time: 0.2474\n",
      "203/281, train_loss: 0.2337, step time: 0.2472\n",
      "204/281, train_loss: 0.1322, step time: 0.2466\n",
      "205/281, train_loss: 0.0908, step time: 0.2502\n",
      "206/281, train_loss: 0.0843, step time: 0.2519\n",
      "207/281, train_loss: 0.1081, step time: 0.2523\n",
      "208/281, train_loss: 0.1335, step time: 0.2511\n",
      "209/281, train_loss: 0.0809, step time: 0.2480\n",
      "210/281, train_loss: 0.1042, step time: 0.2491\n",
      "211/281, train_loss: 0.1262, step time: 0.2476\n",
      "212/281, train_loss: 0.0690, step time: 0.2508\n",
      "213/281, train_loss: 0.0877, step time: 0.2493\n",
      "214/281, train_loss: 0.0537, step time: 0.2543\n",
      "215/281, train_loss: 0.0649, step time: 0.2482\n",
      "216/281, train_loss: 0.0888, step time: 0.2453\n",
      "217/281, train_loss: 0.0740, step time: 0.2503\n",
      "218/281, train_loss: 0.0505, step time: 0.2459\n",
      "219/281, train_loss: 0.0731, step time: 0.2512\n",
      "220/281, train_loss: 0.0610, step time: 0.2523\n",
      "221/281, train_loss: 0.0547, step time: 0.2538\n",
      "222/281, train_loss: 0.0692, step time: 0.2445\n",
      "223/281, train_loss: 0.2650, step time: 0.2458\n",
      "224/281, train_loss: 0.2139, step time: 0.2444\n",
      "225/281, train_loss: 0.0887, step time: 0.2456\n",
      "226/281, train_loss: 0.1047, step time: 0.2489\n",
      "227/281, train_loss: 0.2231, step time: 0.2440\n",
      "228/281, train_loss: 0.0739, step time: 0.2487\n",
      "229/281, train_loss: 0.0901, step time: 0.2522\n",
      "230/281, train_loss: 0.0665, step time: 0.2460\n",
      "231/281, train_loss: 0.0629, step time: 0.2505\n",
      "232/281, train_loss: 0.0779, step time: 0.2507\n",
      "233/281, train_loss: 0.0846, step time: 0.2516\n",
      "234/281, train_loss: 0.2268, step time: 0.2514\n",
      "235/281, train_loss: 0.0936, step time: 0.2489\n",
      "236/281, train_loss: 0.0767, step time: 0.2495\n",
      "237/281, train_loss: 0.0901, step time: 0.2530\n",
      "238/281, train_loss: 0.1164, step time: 0.2475\n",
      "239/281, train_loss: 0.1043, step time: 0.2506\n",
      "240/281, train_loss: 0.1316, step time: 0.2463\n",
      "241/281, train_loss: 0.0626, step time: 0.2492\n",
      "242/281, train_loss: 0.0872, step time: 0.2456\n",
      "243/281, train_loss: 0.1006, step time: 0.2481\n",
      "244/281, train_loss: 0.0580, step time: 0.2380\n",
      "245/281, train_loss: 0.0919, step time: 0.2435\n",
      "246/281, train_loss: 0.0815, step time: 0.2462\n",
      "247/281, train_loss: 0.1583, step time: 0.2429\n",
      "248/281, train_loss: 0.1076, step time: 0.2687\n",
      "249/281, train_loss: 0.0609, step time: 0.2649\n",
      "250/281, train_loss: 0.0935, step time: 0.2455\n",
      "251/281, train_loss: 0.0953, step time: 0.2534\n",
      "252/281, train_loss: 0.1250, step time: 0.2469\n",
      "253/281, train_loss: 0.0621, step time: 0.2473\n",
      "254/281, train_loss: 0.1541, step time: 0.2460\n",
      "255/281, train_loss: 0.1048, step time: 0.2803\n",
      "256/281, train_loss: 0.0839, step time: 0.2460\n",
      "257/281, train_loss: 0.1173, step time: 0.2505\n",
      "258/281, train_loss: 0.0521, step time: 0.2437\n",
      "259/281, train_loss: 0.1941, step time: 0.2541\n",
      "260/281, train_loss: 0.0677, step time: 0.2524\n",
      "261/281, train_loss: 0.0869, step time: 0.2559\n",
      "262/281, train_loss: 0.0685, step time: 0.2785\n",
      "263/281, train_loss: 0.0930, step time: 0.2501\n",
      "264/281, train_loss: 0.0861, step time: 0.2540\n",
      "265/281, train_loss: 0.0810, step time: 0.2466\n",
      "266/281, train_loss: 0.1148, step time: 0.2514\n",
      "267/281, train_loss: 0.0906, step time: 0.2459\n",
      "268/281, train_loss: 0.0568, step time: 0.2466\n",
      "269/281, train_loss: 0.0537, step time: 0.2436\n",
      "270/281, train_loss: 0.0642, step time: 0.2474\n",
      "271/281, train_loss: 0.0687, step time: 0.2432\n",
      "272/281, train_loss: 0.0871, step time: 0.2412\n",
      "273/281, train_loss: 0.0506, step time: 0.2431\n",
      "274/281, train_loss: 0.2174, step time: 0.2429\n",
      "275/281, train_loss: 0.0922, step time: 0.2440\n",
      "276/281, train_loss: 0.0945, step time: 0.2431\n",
      "277/281, train_loss: 0.0616, step time: 0.2425\n",
      "278/281, train_loss: 0.0853, step time: 0.2482\n",
      "279/281, train_loss: 0.0645, step time: 0.2417\n",
      "280/281, train_loss: 0.0806, step time: 0.2457\n",
      "281/281, train_loss: 0.0668, step time: 0.2465\n",
      "282/281, train_loss: 0.0935, step time: 0.1476\n",
      "epoch 136 average loss: 0.1108\n",
      "current epoch: 136 current mean dice: 0.8773 tc: 0.8766 wt: 0.8996 et: 0.8699\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 136 is: 351.8678\n",
      "----------\n",
      "epoch 137/200\n",
      "1/281, train_loss: 0.1452, step time: 0.2463\n",
      "2/281, train_loss: 0.0890, step time: 0.2523\n",
      "3/281, train_loss: 0.1210, step time: 0.2473\n",
      "4/281, train_loss: 0.2273, step time: 0.2483\n",
      "5/281, train_loss: 0.0960, step time: 0.2560\n",
      "6/281, train_loss: 0.0890, step time: 0.2521\n",
      "7/281, train_loss: 0.1261, step time: 0.2499\n",
      "8/281, train_loss: 0.1019, step time: 0.2443\n",
      "9/281, train_loss: 0.1140, step time: 0.2493\n",
      "10/281, train_loss: 0.0716, step time: 0.2467\n",
      "11/281, train_loss: 0.0654, step time: 0.2430\n",
      "12/281, train_loss: 0.0798, step time: 0.2403\n",
      "13/281, train_loss: 0.0686, step time: 0.2467\n",
      "14/281, train_loss: 0.0654, step time: 0.2489\n",
      "15/281, train_loss: 0.0826, step time: 0.2447\n",
      "16/281, train_loss: 0.2238, step time: 0.2467\n",
      "17/281, train_loss: 0.2456, step time: 0.2566\n",
      "18/281, train_loss: 0.1013, step time: 0.2533\n",
      "19/281, train_loss: 0.1057, step time: 0.2505\n",
      "20/281, train_loss: 0.1195, step time: 0.2492\n",
      "21/281, train_loss: 0.0672, step time: 0.2538\n",
      "22/281, train_loss: 0.2209, step time: 0.2530\n",
      "23/281, train_loss: 0.0745, step time: 0.2445\n",
      "24/281, train_loss: 0.0837, step time: 0.2473\n",
      "25/281, train_loss: 0.0733, step time: 0.2499\n",
      "26/281, train_loss: 0.2205, step time: 0.2566\n",
      "27/281, train_loss: 0.2369, step time: 0.2526\n",
      "28/281, train_loss: 0.1143, step time: 0.2482\n",
      "29/281, train_loss: 0.1361, step time: 0.2475\n",
      "30/281, train_loss: 0.0782, step time: 0.2502\n",
      "31/281, train_loss: 0.2157, step time: 0.2471\n",
      "32/281, train_loss: 0.0659, step time: 0.2642\n",
      "33/281, train_loss: 0.3815, step time: 0.2575\n",
      "34/281, train_loss: 0.0625, step time: 0.2555\n",
      "35/281, train_loss: 0.0855, step time: 0.2551\n",
      "36/281, train_loss: 0.0592, step time: 0.2485\n",
      "37/281, train_loss: 0.2244, step time: 0.2477\n",
      "38/281, train_loss: 0.0774, step time: 0.2442\n",
      "39/281, train_loss: 0.0956, step time: 0.2507\n",
      "40/281, train_loss: 0.2284, step time: 0.2495\n",
      "41/281, train_loss: 0.2399, step time: 0.2462\n",
      "42/281, train_loss: 0.2654, step time: 0.2416\n",
      "43/281, train_loss: 0.0831, step time: 0.2454\n",
      "44/281, train_loss: 0.1260, step time: 0.2506\n",
      "45/281, train_loss: 0.0965, step time: 0.2415\n",
      "46/281, train_loss: 0.0778, step time: 0.2497\n",
      "47/281, train_loss: 0.0937, step time: 0.2454\n",
      "48/281, train_loss: 0.0984, step time: 0.2487\n",
      "49/281, train_loss: 0.0726, step time: 0.2559\n",
      "50/281, train_loss: 0.0699, step time: 0.2503\n",
      "51/281, train_loss: 0.0782, step time: 0.2813\n",
      "52/281, train_loss: 0.0683, step time: 0.2543\n",
      "53/281, train_loss: 0.0573, step time: 0.2507\n",
      "54/281, train_loss: 0.0728, step time: 0.2519\n",
      "55/281, train_loss: 0.1040, step time: 0.2457\n",
      "56/281, train_loss: 0.1003, step time: 0.2477\n",
      "57/281, train_loss: 0.2344, step time: 0.2518\n",
      "58/281, train_loss: 0.1001, step time: 0.2507\n",
      "59/281, train_loss: 0.0721, step time: 0.2491\n",
      "60/281, train_loss: 0.0827, step time: 0.2542\n",
      "61/281, train_loss: 0.0827, step time: 0.2479\n",
      "62/281, train_loss: 0.2409, step time: 0.2458\n",
      "63/281, train_loss: 0.1408, step time: 0.2517\n",
      "64/281, train_loss: 0.1196, step time: 0.2511\n",
      "65/281, train_loss: 0.1024, step time: 0.2524\n",
      "66/281, train_loss: 0.0942, step time: 0.2489\n",
      "67/281, train_loss: 0.1365, step time: 0.2544\n",
      "68/281, train_loss: 0.2284, step time: 0.2470\n",
      "69/281, train_loss: 0.2341, step time: 0.2476\n",
      "70/281, train_loss: 0.0882, step time: 0.2445\n",
      "71/281, train_loss: 0.1163, step time: 0.2438\n",
      "72/281, train_loss: 0.0910, step time: 0.2482\n",
      "73/281, train_loss: 0.2417, step time: 0.2425\n",
      "74/281, train_loss: 0.1163, step time: 0.2501\n",
      "75/281, train_loss: 0.0928, step time: 0.2422\n",
      "76/281, train_loss: 0.2597, step time: 0.2495\n",
      "77/281, train_loss: 0.1246, step time: 0.2469\n",
      "78/281, train_loss: 0.2290, step time: 0.2486\n",
      "79/281, train_loss: 0.0806, step time: 0.2577\n",
      "80/281, train_loss: 0.2412, step time: 0.2478\n",
      "81/281, train_loss: 0.0762, step time: 0.2446\n",
      "82/281, train_loss: 0.1042, step time: 0.2443\n",
      "83/281, train_loss: 0.0813, step time: 0.2451\n",
      "84/281, train_loss: 0.0541, step time: 0.2411\n",
      "85/281, train_loss: 0.0556, step time: 0.2438\n",
      "86/281, train_loss: 0.0612, step time: 0.2420\n",
      "87/281, train_loss: 0.0714, step time: 0.2451\n",
      "88/281, train_loss: 0.1222, step time: 0.2546\n",
      "89/281, train_loss: 0.0641, step time: 0.2518\n",
      "90/281, train_loss: 0.1100, step time: 0.2436\n",
      "91/281, train_loss: 0.0713, step time: 0.2448\n",
      "92/281, train_loss: 0.2293, step time: 0.2463\n",
      "93/281, train_loss: 0.1176, step time: 0.2431\n",
      "94/281, train_loss: 0.2674, step time: 0.2464\n",
      "95/281, train_loss: 0.1135, step time: 0.2477\n",
      "96/281, train_loss: 0.0546, step time: 0.2442\n",
      "97/281, train_loss: 0.0695, step time: 0.2419\n",
      "98/281, train_loss: 0.1049, step time: 0.2441\n",
      "99/281, train_loss: 0.1299, step time: 0.2478\n",
      "100/281, train_loss: 0.0931, step time: 0.2500\n",
      "101/281, train_loss: 0.0959, step time: 0.2697\n",
      "102/281, train_loss: 0.2520, step time: 0.2583\n",
      "103/281, train_loss: 0.1035, step time: 0.2459\n",
      "104/281, train_loss: 0.0829, step time: 0.2439\n",
      "105/281, train_loss: 0.0671, step time: 0.2423\n",
      "106/281, train_loss: 0.0539, step time: 0.2432\n",
      "107/281, train_loss: 0.0742, step time: 0.2483\n",
      "108/281, train_loss: 0.0989, step time: 0.2449\n",
      "109/281, train_loss: 0.1111, step time: 0.2495\n",
      "110/281, train_loss: 0.1220, step time: 0.2438\n",
      "111/281, train_loss: 0.0719, step time: 0.2421\n",
      "112/281, train_loss: 0.0993, step time: 0.2446\n",
      "113/281, train_loss: 0.2251, step time: 0.2491\n",
      "114/281, train_loss: 0.0822, step time: 0.2447\n",
      "115/281, train_loss: 0.0663, step time: 0.2410\n",
      "116/281, train_loss: 0.1070, step time: 0.2613\n",
      "117/281, train_loss: 0.0708, step time: 0.2625\n",
      "118/281, train_loss: 0.0853, step time: 0.2406\n",
      "119/281, train_loss: 0.0896, step time: 0.2429\n",
      "120/281, train_loss: 0.0688, step time: 0.2489\n",
      "121/281, train_loss: 0.1293, step time: 0.2415\n",
      "122/281, train_loss: 0.1096, step time: 0.2456\n",
      "123/281, train_loss: 0.1011, step time: 0.2428\n",
      "124/281, train_loss: 0.0551, step time: 0.2450\n",
      "125/281, train_loss: 0.1257, step time: 0.2491\n",
      "126/281, train_loss: 0.0727, step time: 0.2437\n",
      "127/281, train_loss: 0.0610, step time: 0.2436\n",
      "128/281, train_loss: 0.0689, step time: 0.2536\n",
      "129/281, train_loss: 0.1132, step time: 0.2548\n",
      "130/281, train_loss: 0.0754, step time: 0.2457\n",
      "131/281, train_loss: 0.0836, step time: 0.2481\n",
      "132/281, train_loss: 0.0539, step time: 0.2513\n",
      "133/281, train_loss: 0.0734, step time: 0.2400\n",
      "134/281, train_loss: 0.0581, step time: 0.2652\n",
      "135/281, train_loss: 0.0835, step time: 0.2447\n",
      "136/281, train_loss: 0.1175, step time: 0.2422\n",
      "137/281, train_loss: 0.0895, step time: 0.2384\n",
      "138/281, train_loss: 0.1097, step time: 0.2426\n",
      "139/281, train_loss: 0.0720, step time: 0.2417\n",
      "140/281, train_loss: 0.0779, step time: 0.2474\n",
      "141/281, train_loss: 0.1069, step time: 0.2463\n",
      "142/281, train_loss: 0.0685, step time: 0.2428\n",
      "143/281, train_loss: 0.0794, step time: 0.2414\n",
      "144/281, train_loss: 0.0565, step time: 0.2486\n",
      "145/281, train_loss: 0.1208, step time: 0.2452\n",
      "146/281, train_loss: 0.0805, step time: 0.2410\n",
      "147/281, train_loss: 0.0872, step time: 0.2443\n",
      "148/281, train_loss: 0.1004, step time: 0.2560\n",
      "149/281, train_loss: 0.0847, step time: 0.2451\n",
      "150/281, train_loss: 0.2236, step time: 0.2481\n",
      "151/281, train_loss: 0.0805, step time: 0.2457\n",
      "152/281, train_loss: 0.1117, step time: 0.2481\n",
      "153/281, train_loss: 0.0720, step time: 0.2439\n",
      "154/281, train_loss: 0.0613, step time: 0.2425\n",
      "155/281, train_loss: 0.0557, step time: 0.2443\n",
      "156/281, train_loss: 0.0653, step time: 0.2498\n",
      "157/281, train_loss: 0.2237, step time: 0.2431\n",
      "158/281, train_loss: 0.1159, step time: 0.2488\n",
      "159/281, train_loss: 0.0751, step time: 0.2454\n",
      "160/281, train_loss: 0.0746, step time: 0.2433\n",
      "161/281, train_loss: 0.1063, step time: 0.2452\n",
      "162/281, train_loss: 0.0748, step time: 0.2501\n",
      "163/281, train_loss: 0.0830, step time: 0.2465\n",
      "164/281, train_loss: 0.2387, step time: 0.2477\n",
      "165/281, train_loss: 0.0788, step time: 0.2428\n",
      "166/281, train_loss: 0.0963, step time: 0.2435\n",
      "167/281, train_loss: 0.0482, step time: 0.2417\n",
      "168/281, train_loss: 0.0866, step time: 0.2454\n",
      "169/281, train_loss: 0.2626, step time: 0.2426\n",
      "170/281, train_loss: 0.0802, step time: 0.2460\n",
      "171/281, train_loss: 0.0972, step time: 0.2500\n",
      "172/281, train_loss: 0.0913, step time: 0.2514\n",
      "173/281, train_loss: 0.2597, step time: 0.2484\n",
      "174/281, train_loss: 0.0638, step time: 0.2525\n",
      "175/281, train_loss: 0.0955, step time: 0.2532\n",
      "176/281, train_loss: 0.0754, step time: 0.2524\n",
      "177/281, train_loss: 0.0573, step time: 0.2519\n",
      "178/281, train_loss: 0.1059, step time: 0.2453\n",
      "179/281, train_loss: 0.1525, step time: 0.2497\n",
      "180/281, train_loss: 0.0684, step time: 0.2483\n",
      "181/281, train_loss: 0.0775, step time: 0.2475\n",
      "182/281, train_loss: 0.0989, step time: 0.2459\n",
      "183/281, train_loss: 0.0723, step time: 0.2488\n",
      "184/281, train_loss: 0.0516, step time: 0.2459\n",
      "185/281, train_loss: 0.0818, step time: 0.2460\n",
      "186/281, train_loss: 0.1228, step time: 0.2444\n",
      "187/281, train_loss: 0.0805, step time: 0.2530\n",
      "188/281, train_loss: 0.0709, step time: 0.2486\n",
      "189/281, train_loss: 0.0630, step time: 0.2458\n",
      "190/281, train_loss: 0.0684, step time: 0.2453\n",
      "191/281, train_loss: 0.2544, step time: 0.2450\n",
      "192/281, train_loss: 0.1145, step time: 0.2455\n",
      "193/281, train_loss: 0.2267, step time: 0.2450\n",
      "194/281, train_loss: 0.0534, step time: 0.2487\n",
      "195/281, train_loss: 0.0729, step time: 0.2467\n",
      "196/281, train_loss: 0.0787, step time: 0.2454\n",
      "197/281, train_loss: 0.0862, step time: 0.2484\n",
      "198/281, train_loss: 0.0879, step time: 0.2441\n",
      "199/281, train_loss: 0.1021, step time: 0.2431\n",
      "200/281, train_loss: 0.1098, step time: 0.2412\n",
      "201/281, train_loss: 0.0620, step time: 0.2488\n",
      "202/281, train_loss: 0.0921, step time: 0.2472\n",
      "203/281, train_loss: 0.0593, step time: 0.2451\n",
      "204/281, train_loss: 0.2365, step time: 0.2450\n",
      "205/281, train_loss: 0.0903, step time: 0.2515\n",
      "206/281, train_loss: 0.1011, step time: 0.2478\n",
      "207/281, train_loss: 0.0766, step time: 0.2568\n",
      "208/281, train_loss: 0.0702, step time: 0.2524\n",
      "209/281, train_loss: 0.2558, step time: 0.2470\n",
      "210/281, train_loss: 0.0502, step time: 0.2434\n",
      "211/281, train_loss: 0.1024, step time: 0.2456\n",
      "212/281, train_loss: 0.0783, step time: 0.2386\n",
      "213/281, train_loss: 0.1007, step time: 0.2492\n",
      "214/281, train_loss: 0.0813, step time: 0.2719\n",
      "215/281, train_loss: 0.0953, step time: 0.2437\n",
      "216/281, train_loss: 0.0657, step time: 0.2507\n",
      "217/281, train_loss: 0.0688, step time: 0.2485\n",
      "218/281, train_loss: 0.0738, step time: 0.2518\n",
      "219/281, train_loss: 0.1053, step time: 0.2492\n",
      "220/281, train_loss: 0.0660, step time: 0.2543\n",
      "221/281, train_loss: 0.0776, step time: 0.2476\n",
      "222/281, train_loss: 0.0904, step time: 0.2452\n",
      "223/281, train_loss: 0.0813, step time: 0.2482\n",
      "224/281, train_loss: 0.0933, step time: 0.2478\n",
      "225/281, train_loss: 0.1226, step time: 0.2518\n",
      "226/281, train_loss: 0.0849, step time: 0.2483\n",
      "227/281, train_loss: 0.0823, step time: 0.2478\n",
      "228/281, train_loss: 0.0634, step time: 0.2490\n",
      "229/281, train_loss: 0.0717, step time: 0.2515\n",
      "230/281, train_loss: 0.1260, step time: 0.2452\n",
      "231/281, train_loss: 0.2587, step time: 0.2526\n",
      "232/281, train_loss: 0.0790, step time: 0.2447\n",
      "233/281, train_loss: 0.1186, step time: 0.2463\n",
      "234/281, train_loss: 0.0868, step time: 0.2472\n",
      "235/281, train_loss: 0.0564, step time: 0.2503\n",
      "236/281, train_loss: 0.0923, step time: 0.2465\n",
      "237/281, train_loss: 0.0895, step time: 0.2508\n",
      "238/281, train_loss: 0.2227, step time: 0.2495\n",
      "239/281, train_loss: 0.0821, step time: 0.2533\n",
      "240/281, train_loss: 0.0870, step time: 0.2527\n",
      "241/281, train_loss: 0.0590, step time: 0.2491\n",
      "242/281, train_loss: 0.0845, step time: 0.2507\n",
      "243/281, train_loss: 0.0507, step time: 0.2542\n",
      "244/281, train_loss: 0.0987, step time: 0.2511\n",
      "245/281, train_loss: 0.1034, step time: 0.2456\n",
      "246/281, train_loss: 0.0787, step time: 0.2521\n",
      "247/281, train_loss: 0.0988, step time: 0.2422\n",
      "248/281, train_loss: 0.1620, step time: 0.2537\n",
      "249/281, train_loss: 0.1093, step time: 0.2471\n",
      "250/281, train_loss: 0.0778, step time: 0.2491\n",
      "251/281, train_loss: 0.0891, step time: 0.2519\n",
      "252/281, train_loss: 0.0927, step time: 0.2495\n",
      "253/281, train_loss: 0.0697, step time: 0.2508\n",
      "254/281, train_loss: 0.0997, step time: 0.2444\n",
      "255/281, train_loss: 0.0595, step time: 0.2470\n",
      "256/281, train_loss: 0.0653, step time: 0.2498\n",
      "257/281, train_loss: 0.0894, step time: 0.2494\n",
      "258/281, train_loss: 0.2415, step time: 0.2512\n",
      "259/281, train_loss: 0.0867, step time: 0.2525\n",
      "260/281, train_loss: 0.0846, step time: 0.2550\n",
      "261/281, train_loss: 0.0855, step time: 0.2508\n",
      "262/281, train_loss: 0.2282, step time: 0.2486\n",
      "263/281, train_loss: 0.2109, step time: 0.2482\n",
      "264/281, train_loss: 0.1040, step time: 0.2501\n",
      "265/281, train_loss: 0.2636, step time: 0.2546\n",
      "266/281, train_loss: 0.1095, step time: 0.2444\n",
      "267/281, train_loss: 0.0676, step time: 0.2475\n",
      "268/281, train_loss: 0.1053, step time: 0.2521\n",
      "269/281, train_loss: 0.1013, step time: 0.2527\n",
      "270/281, train_loss: 0.0707, step time: 0.2522\n",
      "271/281, train_loss: 0.0708, step time: 0.2520\n",
      "272/281, train_loss: 0.0625, step time: 0.2518\n",
      "273/281, train_loss: 0.0972, step time: 0.2540\n",
      "274/281, train_loss: 0.0891, step time: 0.2499\n",
      "275/281, train_loss: 0.2706, step time: 0.2520\n",
      "276/281, train_loss: 0.0543, step time: 0.2543\n",
      "277/281, train_loss: 0.2527, step time: 0.2493\n",
      "278/281, train_loss: 0.2407, step time: 0.2514\n",
      "279/281, train_loss: 0.0823, step time: 0.2539\n",
      "280/281, train_loss: 0.0902, step time: 0.2512\n",
      "281/281, train_loss: 0.1022, step time: 0.2481\n",
      "282/281, train_loss: 0.0837, step time: 0.1487\n",
      "epoch 137 average loss: 0.1102\n",
      "current epoch: 137 current mean dice: 0.8747 tc: 0.8742 wt: 0.8937 et: 0.8684\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 137 is: 396.4958\n",
      "----------\n",
      "epoch 138/200\n",
      "1/281, train_loss: 0.0762, step time: 0.2526\n",
      "2/281, train_loss: 0.0644, step time: 0.2506\n",
      "3/281, train_loss: 0.0988, step time: 0.2775\n",
      "4/281, train_loss: 0.0777, step time: 0.2574\n",
      "5/281, train_loss: 0.0571, step time: 0.2593\n",
      "6/281, train_loss: 0.0524, step time: 0.2686\n",
      "7/281, train_loss: 0.2338, step time: 0.2558\n",
      "8/281, train_loss: 0.1137, step time: 0.2605\n",
      "9/281, train_loss: 0.0823, step time: 0.2513\n",
      "10/281, train_loss: 0.1248, step time: 0.2541\n",
      "11/281, train_loss: 0.1051, step time: 0.2532\n",
      "12/281, train_loss: 0.1145, step time: 0.2477\n",
      "13/281, train_loss: 0.0864, step time: 0.2672\n",
      "14/281, train_loss: 0.2265, step time: 0.2490\n",
      "15/281, train_loss: 0.0887, step time: 0.2528\n",
      "16/281, train_loss: 0.0838, step time: 0.2491\n",
      "17/281, train_loss: 0.0746, step time: 0.2581\n",
      "18/281, train_loss: 0.0879, step time: 0.2564\n",
      "19/281, train_loss: 0.0687, step time: 0.2592\n",
      "20/281, train_loss: 0.0838, step time: 0.2972\n",
      "21/281, train_loss: 0.2693, step time: 0.2490\n",
      "22/281, train_loss: 0.0885, step time: 0.2521\n",
      "23/281, train_loss: 0.0589, step time: 0.2542\n",
      "24/281, train_loss: 0.0652, step time: 0.2576\n",
      "25/281, train_loss: 0.0905, step time: 0.2539\n",
      "26/281, train_loss: 0.2361, step time: 0.2569\n",
      "27/281, train_loss: 0.0575, step time: 0.2515\n",
      "28/281, train_loss: 0.1046, step time: 0.2544\n",
      "29/281, train_loss: 0.1071, step time: 0.2620\n",
      "30/281, train_loss: 0.1058, step time: 0.2589\n",
      "31/281, train_loss: 0.0853, step time: 0.2541\n",
      "32/281, train_loss: 0.0504, step time: 0.2577\n",
      "33/281, train_loss: 0.0914, step time: 0.2725\n",
      "34/281, train_loss: 0.0849, step time: 0.2562\n",
      "35/281, train_loss: 0.0643, step time: 0.2572\n",
      "36/281, train_loss: 0.1090, step time: 0.2609\n",
      "37/281, train_loss: 0.0935, step time: 0.2556\n",
      "38/281, train_loss: 0.0872, step time: 0.2532\n",
      "39/281, train_loss: 0.2457, step time: 0.2522\n",
      "40/281, train_loss: 0.0671, step time: 0.2643\n",
      "41/281, train_loss: 0.0755, step time: 0.2547\n",
      "42/281, train_loss: 0.3775, step time: 0.2548\n",
      "43/281, train_loss: 0.1336, step time: 0.2572\n",
      "44/281, train_loss: 0.0931, step time: 0.2558\n",
      "45/281, train_loss: 0.0930, step time: 0.2544\n",
      "46/281, train_loss: 0.0681, step time: 0.2539\n",
      "47/281, train_loss: 0.0982, step time: 0.2561\n",
      "48/281, train_loss: 0.1394, step time: 0.2533\n",
      "49/281, train_loss: 0.2706, step time: 0.2564\n",
      "50/281, train_loss: 0.2592, step time: 0.2556\n",
      "51/281, train_loss: 0.0617, step time: 0.2548\n",
      "52/281, train_loss: 0.2363, step time: 0.2581\n",
      "53/281, train_loss: 0.0656, step time: 0.2549\n",
      "54/281, train_loss: 0.0630, step time: 0.2567\n",
      "55/281, train_loss: 0.0981, step time: 0.2536\n",
      "56/281, train_loss: 0.0749, step time: 0.2580\n",
      "57/281, train_loss: 0.1143, step time: 0.2764\n",
      "58/281, train_loss: 0.0941, step time: 0.2578\n",
      "59/281, train_loss: 0.0666, step time: 0.2579\n",
      "60/281, train_loss: 0.4127, step time: 0.2542\n",
      "61/281, train_loss: 0.1124, step time: 0.2485\n",
      "62/281, train_loss: 0.0876, step time: 0.2488\n",
      "63/281, train_loss: 0.2199, step time: 0.2477\n",
      "64/281, train_loss: 0.1059, step time: 0.2543\n",
      "65/281, train_loss: 0.0954, step time: 0.2507\n",
      "66/281, train_loss: 0.0808, step time: 0.2485\n",
      "67/281, train_loss: 0.0651, step time: 0.2508\n",
      "68/281, train_loss: 0.0538, step time: 0.2498\n",
      "69/281, train_loss: 0.0708, step time: 0.2596\n",
      "70/281, train_loss: 0.0644, step time: 0.2497\n",
      "71/281, train_loss: 0.0821, step time: 0.2479\n",
      "72/281, train_loss: 0.1157, step time: 0.2499\n",
      "73/281, train_loss: 0.2392, step time: 0.2473\n",
      "74/281, train_loss: 0.0923, step time: 0.2532\n",
      "75/281, train_loss: 0.0976, step time: 0.2529\n",
      "76/281, train_loss: 0.0913, step time: 0.2491\n",
      "77/281, train_loss: 0.0810, step time: 0.2535\n",
      "78/281, train_loss: 0.1097, step time: 0.2546\n",
      "79/281, train_loss: 0.0880, step time: 0.2561\n",
      "80/281, train_loss: 0.0857, step time: 0.2537\n",
      "81/281, train_loss: 0.2130, step time: 0.2559\n",
      "82/281, train_loss: 0.0808, step time: 0.2629\n",
      "83/281, train_loss: 0.0605, step time: 0.2567\n",
      "84/281, train_loss: 0.0645, step time: 0.2549\n",
      "85/281, train_loss: 0.1105, step time: 0.2515\n",
      "86/281, train_loss: 0.0801, step time: 0.2570\n",
      "87/281, train_loss: 0.0953, step time: 0.2535\n",
      "88/281, train_loss: 0.2348, step time: 0.2475\n",
      "89/281, train_loss: 0.0705, step time: 0.2498\n",
      "90/281, train_loss: 0.0924, step time: 0.2523\n",
      "91/281, train_loss: 0.0914, step time: 0.2540\n",
      "92/281, train_loss: 0.0912, step time: 0.2567\n",
      "93/281, train_loss: 0.0587, step time: 0.2520\n",
      "94/281, train_loss: 0.2336, step time: 0.2555\n",
      "95/281, train_loss: 0.0564, step time: 0.2565\n",
      "96/281, train_loss: 0.0740, step time: 0.2485\n",
      "97/281, train_loss: 0.0975, step time: 0.2531\n",
      "98/281, train_loss: 0.0795, step time: 0.2526\n",
      "99/281, train_loss: 0.0571, step time: 0.2648\n",
      "100/281, train_loss: 0.0729, step time: 0.2591\n",
      "101/281, train_loss: 0.0725, step time: 0.2597\n",
      "102/281, train_loss: 0.0852, step time: 0.2565\n",
      "103/281, train_loss: 0.0969, step time: 0.2493\n",
      "104/281, train_loss: 0.0909, step time: 0.2480\n",
      "105/281, train_loss: 0.1083, step time: 0.2494\n",
      "106/281, train_loss: 0.1342, step time: 0.2606\n",
      "107/281, train_loss: 0.1112, step time: 0.2933\n",
      "108/281, train_loss: 0.0899, step time: 0.2613\n",
      "109/281, train_loss: 0.0848, step time: 0.2603\n",
      "110/281, train_loss: 0.1099, step time: 0.2557\n",
      "111/281, train_loss: 0.0625, step time: 0.2583\n",
      "112/281, train_loss: 0.0735, step time: 0.2547\n",
      "113/281, train_loss: 0.2676, step time: 0.2505\n",
      "114/281, train_loss: 0.0702, step time: 0.2546\n",
      "115/281, train_loss: 0.0588, step time: 0.2587\n",
      "116/281, train_loss: 0.0886, step time: 0.2529\n",
      "117/281, train_loss: 0.0877, step time: 0.2509\n",
      "118/281, train_loss: 0.0751, step time: 0.2522\n",
      "119/281, train_loss: 0.0784, step time: 0.2534\n",
      "120/281, train_loss: 0.0754, step time: 0.2521\n",
      "121/281, train_loss: 0.0759, step time: 0.2521\n",
      "122/281, train_loss: 0.2449, step time: 0.2473\n",
      "123/281, train_loss: 0.1512, step time: 0.2484\n",
      "124/281, train_loss: 0.1021, step time: 0.2516\n",
      "125/281, train_loss: 0.1091, step time: 0.2572\n",
      "126/281, train_loss: 0.1084, step time: 0.2521\n",
      "127/281, train_loss: 0.0459, step time: 0.2561\n",
      "128/281, train_loss: 0.2512, step time: 0.2501\n",
      "129/281, train_loss: 0.1075, step time: 0.2497\n",
      "130/281, train_loss: 0.0985, step time: 0.2716\n",
      "131/281, train_loss: 0.0634, step time: 0.2473\n",
      "132/281, train_loss: 0.2667, step time: 0.2490\n",
      "133/281, train_loss: 0.0717, step time: 0.2502\n",
      "134/281, train_loss: 0.0627, step time: 0.2533\n",
      "135/281, train_loss: 0.0841, step time: 0.2560\n",
      "136/281, train_loss: 0.0670, step time: 0.2521\n",
      "137/281, train_loss: 0.0697, step time: 0.2485\n",
      "138/281, train_loss: 0.0665, step time: 0.2519\n",
      "139/281, train_loss: 0.0833, step time: 0.2502\n",
      "140/281, train_loss: 0.1803, step time: 0.2500\n",
      "141/281, train_loss: 0.0497, step time: 0.2498\n",
      "142/281, train_loss: 0.2221, step time: 0.2483\n",
      "143/281, train_loss: 0.0834, step time: 0.2521\n",
      "144/281, train_loss: 0.1145, step time: 0.2534\n",
      "145/281, train_loss: 0.0769, step time: 0.2530\n",
      "146/281, train_loss: 0.3831, step time: 0.2542\n",
      "147/281, train_loss: 0.0666, step time: 0.2560\n",
      "148/281, train_loss: 0.2324, step time: 0.2627\n",
      "149/281, train_loss: 0.0787, step time: 0.2580\n",
      "150/281, train_loss: 0.2507, step time: 0.2628\n",
      "151/281, train_loss: 0.2188, step time: 0.2618\n",
      "152/281, train_loss: 0.0798, step time: 0.2611\n",
      "153/281, train_loss: 0.0748, step time: 0.2539\n",
      "154/281, train_loss: 0.0994, step time: 0.2519\n",
      "155/281, train_loss: 0.2629, step time: 0.2540\n",
      "156/281, train_loss: 0.0920, step time: 0.2507\n",
      "157/281, train_loss: 0.0800, step time: 0.2539\n",
      "158/281, train_loss: 0.0822, step time: 0.2496\n",
      "159/281, train_loss: 0.2723, step time: 0.2526\n",
      "160/281, train_loss: 0.1054, step time: 0.2468\n",
      "161/281, train_loss: 0.0801, step time: 0.2487\n",
      "162/281, train_loss: 0.2384, step time: 0.2486\n",
      "163/281, train_loss: 0.1045, step time: 0.2554\n",
      "164/281, train_loss: 0.0678, step time: 0.2557\n",
      "165/281, train_loss: 0.0727, step time: 0.2494\n",
      "166/281, train_loss: 0.0824, step time: 0.2565\n",
      "167/281, train_loss: 0.0886, step time: 0.2520\n",
      "168/281, train_loss: 0.0754, step time: 0.2526\n",
      "169/281, train_loss: 0.1017, step time: 0.2515\n",
      "170/281, train_loss: 0.0835, step time: 0.2522\n",
      "171/281, train_loss: 0.0859, step time: 0.2499\n",
      "172/281, train_loss: 0.0909, step time: 0.2515\n",
      "173/281, train_loss: 0.0622, step time: 0.2527\n",
      "174/281, train_loss: 0.0995, step time: 0.2580\n",
      "175/281, train_loss: 0.0694, step time: 0.2545\n",
      "176/281, train_loss: 0.0489, step time: 0.2491\n",
      "177/281, train_loss: 0.0780, step time: 0.2477\n",
      "178/281, train_loss: 0.0769, step time: 0.2490\n",
      "179/281, train_loss: 0.0799, step time: 0.2522\n",
      "180/281, train_loss: 0.0757, step time: 0.2503\n",
      "181/281, train_loss: 0.0762, step time: 0.2525\n",
      "182/281, train_loss: 0.0974, step time: 0.2585\n",
      "183/281, train_loss: 0.0725, step time: 0.2517\n",
      "184/281, train_loss: 0.2186, step time: 0.2579\n",
      "185/281, train_loss: 0.0679, step time: 0.2612\n",
      "186/281, train_loss: 0.1134, step time: 0.2536\n",
      "187/281, train_loss: 0.0744, step time: 0.2507\n",
      "188/281, train_loss: 0.1011, step time: 0.2536\n",
      "189/281, train_loss: 0.1314, step time: 0.2529\n",
      "190/281, train_loss: 0.0850, step time: 0.2580\n",
      "191/281, train_loss: 0.2299, step time: 0.2544\n",
      "192/281, train_loss: 0.0757, step time: 0.2540\n",
      "193/281, train_loss: 0.0807, step time: 0.2537\n",
      "194/281, train_loss: 0.0810, step time: 0.2495\n",
      "195/281, train_loss: 0.0719, step time: 0.2460\n",
      "196/281, train_loss: 0.0742, step time: 0.2481\n",
      "197/281, train_loss: 0.0748, step time: 0.2479\n",
      "198/281, train_loss: 0.0964, step time: 0.2481\n",
      "199/281, train_loss: 0.0665, step time: 0.2469\n",
      "200/281, train_loss: 0.2302, step time: 0.2506\n",
      "201/281, train_loss: 0.2699, step time: 0.2500\n",
      "202/281, train_loss: 0.0483, step time: 0.2490\n",
      "203/281, train_loss: 0.0907, step time: 0.2466\n",
      "204/281, train_loss: 0.0742, step time: 0.2554\n",
      "205/281, train_loss: 0.0756, step time: 0.2520\n",
      "206/281, train_loss: 0.2175, step time: 0.2481\n",
      "207/281, train_loss: 0.0646, step time: 0.2597\n",
      "208/281, train_loss: 0.0576, step time: 0.2509\n",
      "209/281, train_loss: 0.0482, step time: 0.2486\n",
      "210/281, train_loss: 0.2701, step time: 0.2511\n",
      "211/281, train_loss: 0.2773, step time: 0.2580\n",
      "212/281, train_loss: 0.2336, step time: 0.2506\n",
      "213/281, train_loss: 0.1036, step time: 0.2507\n",
      "214/281, train_loss: 0.1000, step time: 0.2507\n",
      "215/281, train_loss: 0.2788, step time: 0.2512\n",
      "216/281, train_loss: 0.0793, step time: 0.2686\n",
      "217/281, train_loss: 0.0940, step time: 0.2549\n",
      "218/281, train_loss: 0.0662, step time: 0.2530\n",
      "219/281, train_loss: 0.0731, step time: 0.2525\n",
      "220/281, train_loss: 0.2630, step time: 0.2537\n",
      "221/281, train_loss: 0.0942, step time: 0.2506\n",
      "222/281, train_loss: 0.2214, step time: 0.2478\n",
      "223/281, train_loss: 0.0914, step time: 0.2569\n",
      "224/281, train_loss: 0.0858, step time: 0.2465\n",
      "225/281, train_loss: 0.0755, step time: 0.2489\n",
      "226/281, train_loss: 0.0544, step time: 0.2416\n",
      "227/281, train_loss: 0.1267, step time: 0.2495\n",
      "228/281, train_loss: 0.0721, step time: 0.2502\n",
      "229/281, train_loss: 0.0960, step time: 0.2501\n",
      "230/281, train_loss: 0.1537, step time: 0.2456\n",
      "231/281, train_loss: 0.1555, step time: 0.2503\n",
      "232/281, train_loss: 0.0663, step time: 0.2546\n",
      "233/281, train_loss: 0.0962, step time: 0.2529\n",
      "234/281, train_loss: 0.0978, step time: 0.2493\n",
      "235/281, train_loss: 0.0853, step time: 0.2455\n",
      "236/281, train_loss: 0.0871, step time: 0.2492\n",
      "237/281, train_loss: 0.0942, step time: 0.2493\n",
      "238/281, train_loss: 0.1004, step time: 0.2494\n",
      "239/281, train_loss: 0.0810, step time: 0.2448\n",
      "240/281, train_loss: 0.1179, step time: 0.2488\n",
      "241/281, train_loss: 0.0563, step time: 0.2454\n",
      "242/281, train_loss: 0.0820, step time: 0.2509\n",
      "243/281, train_loss: 0.0845, step time: 0.2484\n",
      "244/281, train_loss: 0.2593, step time: 0.2510\n",
      "245/281, train_loss: 0.1037, step time: 0.2514\n",
      "246/281, train_loss: 0.1046, step time: 0.2452\n",
      "247/281, train_loss: 0.1295, step time: 0.2475\n",
      "248/281, train_loss: 0.1128, step time: 0.2444\n",
      "249/281, train_loss: 0.1204, step time: 0.2504\n",
      "250/281, train_loss: 0.0992, step time: 0.2438\n",
      "251/281, train_loss: 0.1072, step time: 0.2481\n",
      "252/281, train_loss: 0.0744, step time: 0.2515\n",
      "253/281, train_loss: 0.0697, step time: 0.2512\n",
      "254/281, train_loss: 0.2279, step time: 0.2467\n",
      "255/281, train_loss: 0.0604, step time: 0.2497\n",
      "256/281, train_loss: 0.2375, step time: 0.2452\n",
      "257/281, train_loss: 0.0797, step time: 0.2536\n",
      "258/281, train_loss: 0.0856, step time: 0.2495\n",
      "259/281, train_loss: 0.0821, step time: 0.2482\n",
      "260/281, train_loss: 0.0750, step time: 0.2433\n",
      "261/281, train_loss: 0.0881, step time: 0.2459\n",
      "262/281, train_loss: 0.2780, step time: 0.2428\n",
      "263/281, train_loss: 0.0697, step time: 0.2445\n",
      "264/281, train_loss: 0.0995, step time: 0.2430\n",
      "265/281, train_loss: 0.0866, step time: 0.2456\n",
      "266/281, train_loss: 0.0921, step time: 0.2480\n",
      "267/281, train_loss: 0.1113, step time: 0.2477\n",
      "268/281, train_loss: 0.0702, step time: 0.2463\n",
      "269/281, train_loss: 0.0696, step time: 0.2502\n",
      "270/281, train_loss: 0.0746, step time: 0.2491\n",
      "271/281, train_loss: 0.0818, step time: 0.2474\n",
      "272/281, train_loss: 0.1029, step time: 0.2453\n",
      "273/281, train_loss: 0.1311, step time: 0.2512\n",
      "274/281, train_loss: 0.0861, step time: 0.2433\n",
      "275/281, train_loss: 0.0506, step time: 0.2454\n",
      "276/281, train_loss: 0.0903, step time: 0.2452\n",
      "277/281, train_loss: 0.0595, step time: 0.2496\n",
      "278/281, train_loss: 0.0800, step time: 0.2428\n",
      "279/281, train_loss: 0.0590, step time: 0.2424\n",
      "280/281, train_loss: 0.0532, step time: 0.2456\n",
      "281/281, train_loss: 0.0923, step time: 0.2445\n",
      "282/281, train_loss: 0.1067, step time: 0.1493\n",
      "epoch 138 average loss: 0.1109\n",
      "current epoch: 138 current mean dice: 0.8664 tc: 0.8714 wt: 0.8829 et: 0.8588\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 138 is: 353.2972\n",
      "----------\n",
      "epoch 139/200\n",
      "1/281, train_loss: 0.1487, step time: 0.2521\n",
      "2/281, train_loss: 0.0973, step time: 0.2496\n",
      "3/281, train_loss: 0.0751, step time: 0.2434\n",
      "4/281, train_loss: 0.0767, step time: 0.2452\n",
      "5/281, train_loss: 0.0786, step time: 0.2442\n",
      "6/281, train_loss: 0.0919, step time: 0.2511\n",
      "7/281, train_loss: 0.2573, step time: 0.2512\n",
      "8/281, train_loss: 0.0792, step time: 0.2474\n",
      "9/281, train_loss: 0.0858, step time: 0.2502\n",
      "10/281, train_loss: 0.0964, step time: 0.2539\n",
      "11/281, train_loss: 0.0912, step time: 0.2569\n",
      "12/281, train_loss: 0.2197, step time: 0.2538\n",
      "13/281, train_loss: 0.2240, step time: 0.2503\n",
      "14/281, train_loss: 0.0691, step time: 0.2501\n",
      "15/281, train_loss: 0.0527, step time: 0.2474\n",
      "16/281, train_loss: 0.0692, step time: 0.2482\n",
      "17/281, train_loss: 0.0835, step time: 0.2503\n",
      "18/281, train_loss: 0.0659, step time: 0.2472\n",
      "19/281, train_loss: 0.0704, step time: 0.2486\n",
      "20/281, train_loss: 0.1354, step time: 0.2731\n",
      "21/281, train_loss: 0.0457, step time: 0.2965\n",
      "22/281, train_loss: 0.0651, step time: 0.2469\n",
      "23/281, train_loss: 0.1164, step time: 0.2482\n",
      "24/281, train_loss: 0.1118, step time: 0.2508\n",
      "25/281, train_loss: 0.0714, step time: 0.2493\n",
      "26/281, train_loss: 0.2450, step time: 0.2469\n",
      "27/281, train_loss: 0.0678, step time: 0.2490\n",
      "28/281, train_loss: 0.0842, step time: 0.2552\n",
      "29/281, train_loss: 0.0518, step time: 0.2526\n",
      "30/281, train_loss: 0.0997, step time: 0.2512\n",
      "31/281, train_loss: 0.0701, step time: 0.2480\n",
      "32/281, train_loss: 0.1341, step time: 0.2564\n",
      "33/281, train_loss: 0.0866, step time: 0.2597\n",
      "34/281, train_loss: 0.0830, step time: 0.2592\n",
      "35/281, train_loss: 0.0753, step time: 0.2530\n",
      "36/281, train_loss: 0.0630, step time: 0.2516\n",
      "37/281, train_loss: 0.0624, step time: 0.2465\n",
      "38/281, train_loss: 0.2544, step time: 0.2542\n",
      "39/281, train_loss: 0.1202, step time: 0.2453\n",
      "40/281, train_loss: 0.2287, step time: 0.2448\n",
      "41/281, train_loss: 0.0827, step time: 0.2479\n",
      "42/281, train_loss: 0.0684, step time: 0.2462\n",
      "43/281, train_loss: 0.2346, step time: 0.2549\n",
      "44/281, train_loss: 0.2196, step time: 0.2523\n",
      "45/281, train_loss: 0.2304, step time: 0.2475\n",
      "46/281, train_loss: 0.0705, step time: 0.2461\n",
      "47/281, train_loss: 0.0910, step time: 0.2481\n",
      "48/281, train_loss: 0.0743, step time: 0.2536\n",
      "49/281, train_loss: 0.0787, step time: 0.2548\n",
      "50/281, train_loss: 0.2632, step time: 0.2575\n",
      "51/281, train_loss: 0.0796, step time: 0.2523\n",
      "52/281, train_loss: 0.2325, step time: 0.2516\n",
      "53/281, train_loss: 0.0594, step time: 0.2483\n",
      "54/281, train_loss: 0.0720, step time: 0.2475\n",
      "55/281, train_loss: 0.0954, step time: 0.2459\n",
      "56/281, train_loss: 0.0675, step time: 0.2485\n",
      "57/281, train_loss: 0.1144, step time: 0.2425\n",
      "58/281, train_loss: 0.0866, step time: 0.2420\n",
      "59/281, train_loss: 0.2284, step time: 0.2517\n",
      "60/281, train_loss: 0.0851, step time: 0.2495\n",
      "61/281, train_loss: 0.0894, step time: 0.2488\n",
      "62/281, train_loss: 0.0754, step time: 0.2553\n",
      "63/281, train_loss: 0.0543, step time: 0.2664\n",
      "64/281, train_loss: 0.1135, step time: 0.2613\n",
      "65/281, train_loss: 0.0566, step time: 0.2492\n",
      "66/281, train_loss: 0.2457, step time: 0.2492\n",
      "67/281, train_loss: 0.0690, step time: 0.2489\n",
      "68/281, train_loss: 0.0772, step time: 0.2463\n",
      "69/281, train_loss: 0.0998, step time: 0.2535\n",
      "70/281, train_loss: 0.0779, step time: 0.2485\n",
      "71/281, train_loss: 0.0620, step time: 0.2508\n",
      "72/281, train_loss: 0.1003, step time: 0.2514\n",
      "73/281, train_loss: 0.0587, step time: 0.2450\n",
      "74/281, train_loss: 0.0776, step time: 0.2441\n",
      "75/281, train_loss: 0.0866, step time: 0.2486\n",
      "76/281, train_loss: 0.0856, step time: 0.2516\n",
      "77/281, train_loss: 0.0978, step time: 0.2457\n",
      "78/281, train_loss: 0.2271, step time: 0.2467\n",
      "79/281, train_loss: 0.0627, step time: 0.2451\n",
      "80/281, train_loss: 0.2169, step time: 0.2429\n",
      "81/281, train_loss: 0.0622, step time: 0.2400\n",
      "82/281, train_loss: 0.1036, step time: 0.2435\n",
      "83/281, train_loss: 0.0874, step time: 0.2575\n",
      "84/281, train_loss: 0.1327, step time: 0.2472\n",
      "85/281, train_loss: 0.0810, step time: 0.2431\n",
      "86/281, train_loss: 0.0906, step time: 0.2442\n",
      "87/281, train_loss: 0.0542, step time: 0.2424\n",
      "88/281, train_loss: 0.0740, step time: 0.2491\n",
      "89/281, train_loss: 0.1405, step time: 0.2442\n",
      "90/281, train_loss: 0.0851, step time: 0.2423\n",
      "91/281, train_loss: 0.0681, step time: 0.2542\n",
      "92/281, train_loss: 0.1284, step time: 0.2439\n",
      "93/281, train_loss: 0.1018, step time: 0.2423\n",
      "94/281, train_loss: 0.0767, step time: 0.2502\n",
      "95/281, train_loss: 0.0836, step time: 0.2444\n",
      "96/281, train_loss: 0.0465, step time: 0.2425\n",
      "97/281, train_loss: 0.1697, step time: 0.2396\n",
      "98/281, train_loss: 0.0636, step time: 0.2485\n",
      "99/281, train_loss: 0.0986, step time: 0.2429\n",
      "100/281, train_loss: 0.0646, step time: 0.2482\n",
      "101/281, train_loss: 0.0790, step time: 0.2472\n",
      "102/281, train_loss: 0.0918, step time: 0.2446\n",
      "103/281, train_loss: 0.0691, step time: 0.2448\n",
      "104/281, train_loss: 0.0698, step time: 0.2446\n",
      "105/281, train_loss: 0.0677, step time: 0.2424\n",
      "106/281, train_loss: 0.0857, step time: 0.2481\n",
      "107/281, train_loss: 0.0580, step time: 0.2438\n",
      "108/281, train_loss: 0.2388, step time: 0.2398\n",
      "109/281, train_loss: 0.2561, step time: 0.2427\n",
      "110/281, train_loss: 0.0920, step time: 0.2474\n",
      "111/281, train_loss: 0.0501, step time: 0.2432\n",
      "112/281, train_loss: 0.1008, step time: 0.2434\n",
      "113/281, train_loss: 0.1065, step time: 0.2441\n",
      "114/281, train_loss: 0.0709, step time: 0.2425\n",
      "115/281, train_loss: 0.0776, step time: 0.2431\n",
      "116/281, train_loss: 0.1081, step time: 0.2489\n",
      "117/281, train_loss: 0.0750, step time: 0.2474\n",
      "118/281, train_loss: 0.1135, step time: 0.2467\n",
      "119/281, train_loss: 0.2454, step time: 0.2482\n",
      "120/281, train_loss: 0.0897, step time: 0.2476\n",
      "121/281, train_loss: 0.0991, step time: 0.2449\n",
      "122/281, train_loss: 0.0784, step time: 0.2511\n",
      "123/281, train_loss: 0.0727, step time: 0.2475\n",
      "124/281, train_loss: 0.2113, step time: 0.2397\n",
      "125/281, train_loss: 0.1152, step time: 0.2465\n",
      "126/281, train_loss: 0.1299, step time: 0.2413\n",
      "127/281, train_loss: 0.0775, step time: 0.2454\n",
      "128/281, train_loss: 0.1194, step time: 0.2468\n",
      "129/281, train_loss: 0.0583, step time: 0.2469\n",
      "130/281, train_loss: 0.0888, step time: 0.2497\n",
      "131/281, train_loss: 0.0846, step time: 0.2459\n",
      "132/281, train_loss: 0.1085, step time: 0.2452\n",
      "133/281, train_loss: 0.0809, step time: 0.2518\n",
      "134/281, train_loss: 0.0484, step time: 0.2471\n",
      "135/281, train_loss: 0.0523, step time: 0.2394\n",
      "136/281, train_loss: 0.0690, step time: 0.2419\n",
      "137/281, train_loss: 0.0993, step time: 0.2408\n",
      "138/281, train_loss: 0.0725, step time: 0.2466\n",
      "139/281, train_loss: 0.0643, step time: 0.2483\n",
      "140/281, train_loss: 0.2507, step time: 0.2476\n",
      "141/281, train_loss: 0.0802, step time: 0.2536\n",
      "142/281, train_loss: 0.0584, step time: 0.2620\n",
      "143/281, train_loss: 0.0544, step time: 0.2454\n",
      "144/281, train_loss: 0.0686, step time: 0.2491\n",
      "145/281, train_loss: 0.0952, step time: 0.2513\n",
      "146/281, train_loss: 0.0891, step time: 0.2503\n",
      "147/281, train_loss: 0.1158, step time: 0.2533\n",
      "148/281, train_loss: 0.0827, step time: 0.2559\n",
      "149/281, train_loss: 0.2239, step time: 0.2480\n",
      "150/281, train_loss: 0.1655, step time: 0.2498\n",
      "151/281, train_loss: 0.0882, step time: 0.2459\n",
      "152/281, train_loss: 0.0576, step time: 0.2498\n",
      "153/281, train_loss: 0.0895, step time: 0.2460\n",
      "154/281, train_loss: 0.2820, step time: 0.2436\n",
      "155/281, train_loss: 0.0935, step time: 0.2496\n",
      "156/281, train_loss: 0.1297, step time: 0.2466\n",
      "157/281, train_loss: 0.0661, step time: 0.2507\n",
      "158/281, train_loss: 0.0844, step time: 0.2426\n",
      "159/281, train_loss: 0.1043, step time: 0.2528\n",
      "160/281, train_loss: 0.0727, step time: 0.2433\n",
      "161/281, train_loss: 0.0815, step time: 0.2474\n",
      "162/281, train_loss: 0.2681, step time: 0.2426\n",
      "163/281, train_loss: 0.0705, step time: 0.2508\n",
      "164/281, train_loss: 0.1084, step time: 0.2415\n",
      "165/281, train_loss: 0.0691, step time: 0.2461\n",
      "166/281, train_loss: 0.0364, step time: 0.2464\n",
      "167/281, train_loss: 0.1129, step time: 0.2511\n",
      "168/281, train_loss: 0.2456, step time: 0.2434\n",
      "169/281, train_loss: 0.2602, step time: 0.2425\n",
      "170/281, train_loss: 0.0580, step time: 0.2396\n",
      "171/281, train_loss: 0.0646, step time: 0.2492\n",
      "172/281, train_loss: 0.2953, step time: 0.2700\n",
      "173/281, train_loss: 0.0816, step time: 0.2502\n",
      "174/281, train_loss: 0.2379, step time: 0.2519\n",
      "175/281, train_loss: 0.0540, step time: 0.2484\n",
      "176/281, train_loss: 0.0529, step time: 0.2513\n",
      "177/281, train_loss: 0.1057, step time: 0.2493\n",
      "178/281, train_loss: 0.1371, step time: 0.2554\n",
      "179/281, train_loss: 0.1112, step time: 0.2452\n",
      "180/281, train_loss: 0.1043, step time: 0.2448\n",
      "181/281, train_loss: 0.1248, step time: 0.2505\n",
      "182/281, train_loss: 0.1319, step time: 0.2438\n",
      "183/281, train_loss: 0.1081, step time: 0.2467\n",
      "184/281, train_loss: 0.0639, step time: 0.2448\n",
      "185/281, train_loss: 0.2532, step time: 0.2446\n",
      "186/281, train_loss: 0.0487, step time: 0.2457\n",
      "187/281, train_loss: 0.0965, step time: 0.2431\n",
      "188/281, train_loss: 0.0853, step time: 0.2477\n",
      "189/281, train_loss: 0.0777, step time: 0.2471\n",
      "190/281, train_loss: 0.0445, step time: 0.2500\n",
      "191/281, train_loss: 0.1008, step time: 0.2473\n",
      "192/281, train_loss: 0.0805, step time: 0.2422\n",
      "193/281, train_loss: 0.0773, step time: 0.2515\n",
      "194/281, train_loss: 0.0901, step time: 0.2477\n",
      "195/281, train_loss: 0.0628, step time: 0.2478\n",
      "196/281, train_loss: 0.0530, step time: 0.2441\n",
      "197/281, train_loss: 0.0802, step time: 0.2411\n",
      "198/281, train_loss: 0.0724, step time: 0.2463\n",
      "199/281, train_loss: 0.0640, step time: 0.2422\n",
      "200/281, train_loss: 0.0940, step time: 0.2427\n",
      "201/281, train_loss: 0.0742, step time: 0.2414\n",
      "202/281, train_loss: 0.0688, step time: 0.2514\n",
      "203/281, train_loss: 0.0593, step time: 0.2515\n",
      "204/281, train_loss: 0.1014, step time: 0.2513\n",
      "205/281, train_loss: 0.2196, step time: 0.2501\n",
      "206/281, train_loss: 0.0674, step time: 0.2489\n",
      "207/281, train_loss: 0.0670, step time: 0.2424\n",
      "208/281, train_loss: 0.1182, step time: 0.2445\n",
      "209/281, train_loss: 0.0603, step time: 0.2453\n",
      "210/281, train_loss: 0.1026, step time: 0.2509\n",
      "211/281, train_loss: 0.1049, step time: 0.2489\n",
      "212/281, train_loss: 0.0763, step time: 0.2451\n",
      "213/281, train_loss: 0.0767, step time: 0.2417\n",
      "214/281, train_loss: 0.0674, step time: 0.2487\n",
      "215/281, train_loss: 0.0880, step time: 0.2472\n",
      "216/281, train_loss: 0.2495, step time: 0.2495\n",
      "217/281, train_loss: 0.0838, step time: 0.2465\n",
      "218/281, train_loss: 0.0878, step time: 0.2509\n",
      "219/281, train_loss: 0.2332, step time: 0.2565\n",
      "220/281, train_loss: 0.0668, step time: 0.2539\n",
      "221/281, train_loss: 0.1207, step time: 0.2521\n",
      "222/281, train_loss: 0.0579, step time: 0.2463\n",
      "223/281, train_loss: 0.0694, step time: 0.2537\n",
      "224/281, train_loss: 0.2186, step time: 0.2425\n",
      "225/281, train_loss: 0.2242, step time: 0.2424\n",
      "226/281, train_loss: 0.0734, step time: 0.2484\n",
      "227/281, train_loss: 0.0963, step time: 0.2456\n",
      "228/281, train_loss: 0.2355, step time: 0.2489\n",
      "229/281, train_loss: 0.0443, step time: 0.2435\n",
      "230/281, train_loss: 0.0681, step time: 0.2472\n",
      "231/281, train_loss: 0.1253, step time: 0.2496\n",
      "232/281, train_loss: 0.0611, step time: 0.2415\n",
      "233/281, train_loss: 0.2285, step time: 0.2427\n",
      "234/281, train_loss: 0.2392, step time: 0.2497\n",
      "235/281, train_loss: 0.1123, step time: 0.2420\n",
      "236/281, train_loss: 0.2592, step time: 0.2404\n",
      "237/281, train_loss: 0.2104, step time: 0.2405\n",
      "238/281, train_loss: 0.0937, step time: 0.2457\n",
      "239/281, train_loss: 0.1188, step time: 0.2461\n",
      "240/281, train_loss: 0.0916, step time: 0.2457\n",
      "241/281, train_loss: 0.0776, step time: 0.2450\n",
      "242/281, train_loss: 0.2471, step time: 0.2476\n",
      "243/281, train_loss: 0.0792, step time: 0.2451\n",
      "244/281, train_loss: 0.1264, step time: 0.2411\n",
      "245/281, train_loss: 0.0868, step time: 0.2439\n",
      "246/281, train_loss: 0.0783, step time: 0.2451\n",
      "247/281, train_loss: 0.0811, step time: 0.2477\n",
      "248/281, train_loss: 0.1364, step time: 0.2465\n",
      "249/281, train_loss: 0.1261, step time: 0.2388\n",
      "250/281, train_loss: 0.0618, step time: 0.2463\n",
      "251/281, train_loss: 0.0816, step time: 0.2448\n",
      "252/281, train_loss: 0.0695, step time: 0.2419\n",
      "253/281, train_loss: 0.0824, step time: 0.2455\n",
      "254/281, train_loss: 0.1314, step time: 0.2490\n",
      "255/281, train_loss: 0.2372, step time: 0.2453\n",
      "256/281, train_loss: 0.0691, step time: 0.2383\n",
      "257/281, train_loss: 0.0814, step time: 0.2478\n",
      "258/281, train_loss: 0.1195, step time: 0.2420\n",
      "259/281, train_loss: 0.1216, step time: 0.2432\n",
      "260/281, train_loss: 0.0741, step time: 0.2407\n",
      "261/281, train_loss: 0.0772, step time: 0.2466\n",
      "262/281, train_loss: 0.2041, step time: 0.2452\n",
      "263/281, train_loss: 0.0802, step time: 0.2446\n",
      "264/281, train_loss: 0.1039, step time: 0.2473\n",
      "265/281, train_loss: 0.0825, step time: 0.2473\n",
      "266/281, train_loss: 0.1097, step time: 0.2485\n",
      "267/281, train_loss: 0.0931, step time: 0.2460\n",
      "268/281, train_loss: 0.1120, step time: 0.2460\n",
      "269/281, train_loss: 0.0679, step time: 0.2522\n",
      "270/281, train_loss: 0.0762, step time: 0.2441\n",
      "271/281, train_loss: 0.1028, step time: 0.2427\n",
      "272/281, train_loss: 0.0820, step time: 0.2485\n",
      "273/281, train_loss: 0.3070, step time: 0.2453\n",
      "274/281, train_loss: 0.0715, step time: 0.2487\n",
      "275/281, train_loss: 0.2646, step time: 0.2499\n",
      "276/281, train_loss: 0.0714, step time: 0.2552\n",
      "277/281, train_loss: 0.0796, step time: 0.2458\n",
      "278/281, train_loss: 0.2752, step time: 0.2440\n",
      "279/281, train_loss: 0.1015, step time: 0.2466\n",
      "280/281, train_loss: 0.1038, step time: 0.2484\n",
      "281/281, train_loss: 0.1558, step time: 0.2461\n",
      "282/281, train_loss: 0.0530, step time: 0.1461\n",
      "epoch 139 average loss: 0.1097\n",
      "current epoch: 139 current mean dice: 0.8647 tc: 0.8743 wt: 0.8815 et: 0.8518\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 139 is: 387.6763\n",
      "----------\n",
      "epoch 140/200\n",
      "1/281, train_loss: 0.0738, step time: 0.2536\n",
      "2/281, train_loss: 0.0472, step time: 0.2425\n",
      "3/281, train_loss: 0.0972, step time: 0.2440\n",
      "4/281, train_loss: 0.0897, step time: 0.2430\n",
      "5/281, train_loss: 0.1156, step time: 0.2542\n",
      "6/281, train_loss: 0.0715, step time: 0.2622\n",
      "7/281, train_loss: 0.0510, step time: 0.2504\n",
      "8/281, train_loss: 0.2231, step time: 0.2470\n",
      "9/281, train_loss: 0.1193, step time: 0.2486\n",
      "10/281, train_loss: 0.0623, step time: 0.2458\n",
      "11/281, train_loss: 0.1222, step time: 0.2473\n",
      "12/281, train_loss: 0.0822, step time: 0.2486\n",
      "13/281, train_loss: 0.0666, step time: 0.2501\n",
      "14/281, train_loss: 0.0574, step time: 0.2492\n",
      "15/281, train_loss: 0.0845, step time: 0.2478\n",
      "16/281, train_loss: 0.1075, step time: 0.2489\n",
      "17/281, train_loss: 0.1327, step time: 0.2483\n",
      "18/281, train_loss: 0.0826, step time: 0.2464\n",
      "19/281, train_loss: 0.0549, step time: 0.2494\n",
      "20/281, train_loss: 0.0539, step time: 0.2484\n",
      "21/281, train_loss: 0.0667, step time: 0.2658\n",
      "22/281, train_loss: 0.0965, step time: 0.2646\n",
      "23/281, train_loss: 0.0792, step time: 0.2558\n",
      "24/281, train_loss: 0.0956, step time: 0.2536\n",
      "25/281, train_loss: 0.2541, step time: 0.2555\n",
      "26/281, train_loss: 0.0435, step time: 0.2502\n",
      "27/281, train_loss: 0.2476, step time: 0.2520\n",
      "28/281, train_loss: 0.0844, step time: 0.2534\n",
      "29/281, train_loss: 0.0748, step time: 0.2469\n",
      "30/281, train_loss: 0.1079, step time: 0.2465\n",
      "31/281, train_loss: 0.0462, step time: 0.2461\n",
      "32/281, train_loss: 0.0790, step time: 0.2461\n",
      "33/281, train_loss: 0.0610, step time: 0.2542\n",
      "34/281, train_loss: 0.0661, step time: 0.2557\n",
      "35/281, train_loss: 0.2154, step time: 0.2486\n",
      "36/281, train_loss: 0.0709, step time: 0.2523\n",
      "37/281, train_loss: 0.0669, step time: 0.2546\n",
      "38/281, train_loss: 0.0535, step time: 0.2508\n",
      "39/281, train_loss: 0.0652, step time: 0.2518\n",
      "40/281, train_loss: 0.0647, step time: 0.2524\n",
      "41/281, train_loss: 0.0744, step time: 0.2510\n",
      "42/281, train_loss: 0.2522, step time: 0.2565\n",
      "43/281, train_loss: 0.0991, step time: 0.2503\n",
      "44/281, train_loss: 0.1064, step time: 0.2503\n",
      "45/281, train_loss: 0.1251, step time: 0.2550\n",
      "46/281, train_loss: 0.0718, step time: 0.2450\n",
      "47/281, train_loss: 0.0609, step time: 0.2483\n",
      "48/281, train_loss: 0.0525, step time: 0.2444\n",
      "49/281, train_loss: 0.0912, step time: 0.2504\n",
      "50/281, train_loss: 0.0949, step time: 0.2450\n",
      "51/281, train_loss: 0.0923, step time: 0.2509\n",
      "52/281, train_loss: 0.0818, step time: 0.2569\n",
      "53/281, train_loss: 0.2551, step time: 0.2577\n",
      "54/281, train_loss: 0.0992, step time: 0.2545\n",
      "55/281, train_loss: 0.0872, step time: 0.2541\n",
      "56/281, train_loss: 0.0699, step time: 0.2536\n",
      "57/281, train_loss: 0.1166, step time: 0.2453\n",
      "58/281, train_loss: 0.0820, step time: 0.2560\n",
      "59/281, train_loss: 0.2337, step time: 0.2484\n",
      "60/281, train_loss: 0.0565, step time: 0.2442\n",
      "61/281, train_loss: 0.0924, step time: 0.2471\n",
      "62/281, train_loss: 0.0970, step time: 0.2555\n",
      "63/281, train_loss: 0.0545, step time: 0.2527\n",
      "64/281, train_loss: 0.0594, step time: 0.2522\n",
      "65/281, train_loss: 0.1205, step time: 0.2474\n",
      "66/281, train_loss: 0.0960, step time: 0.2498\n",
      "67/281, train_loss: 0.2907, step time: 0.2506\n",
      "68/281, train_loss: 0.0957, step time: 0.2490\n",
      "69/281, train_loss: 0.0875, step time: 0.2468\n",
      "70/281, train_loss: 0.1199, step time: 0.2513\n",
      "71/281, train_loss: 0.2563, step time: 0.2507\n",
      "72/281, train_loss: 0.0514, step time: 0.2520\n",
      "73/281, train_loss: 0.1505, step time: 0.2482\n",
      "74/281, train_loss: 0.1307, step time: 0.2659\n",
      "75/281, train_loss: 0.0655, step time: 0.2671\n",
      "76/281, train_loss: 0.1299, step time: 0.2570\n",
      "77/281, train_loss: 0.0673, step time: 0.2468\n",
      "78/281, train_loss: 0.0827, step time: 0.2569\n",
      "79/281, train_loss: 0.0927, step time: 0.2534\n",
      "80/281, train_loss: 0.1565, step time: 0.2461\n",
      "81/281, train_loss: 0.1282, step time: 0.2512\n",
      "82/281, train_loss: 0.1876, step time: 0.2540\n",
      "83/281, train_loss: 0.2658, step time: 0.2545\n",
      "84/281, train_loss: 0.0728, step time: 0.2529\n",
      "85/281, train_loss: 0.0896, step time: 0.2596\n",
      "86/281, train_loss: 0.0811, step time: 0.2525\n",
      "87/281, train_loss: 0.0563, step time: 0.2504\n",
      "88/281, train_loss: 0.0988, step time: 0.2463\n",
      "89/281, train_loss: 0.0938, step time: 0.2579\n",
      "90/281, train_loss: 0.0985, step time: 0.2501\n",
      "91/281, train_loss: 0.0738, step time: 0.2580\n",
      "92/281, train_loss: 0.1194, step time: 0.2629\n",
      "93/281, train_loss: 0.0917, step time: 0.2576\n",
      "94/281, train_loss: 0.0706, step time: 0.2522\n",
      "95/281, train_loss: 0.0664, step time: 0.2480\n",
      "96/281, train_loss: 0.0649, step time: 0.2519\n",
      "97/281, train_loss: 0.0833, step time: 0.2510\n",
      "98/281, train_loss: 0.0842, step time: 0.2581\n",
      "99/281, train_loss: 0.0894, step time: 0.2550\n",
      "100/281, train_loss: 0.2130, step time: 0.2542\n",
      "101/281, train_loss: 0.0934, step time: 0.2564\n",
      "102/281, train_loss: 0.2207, step time: 0.2512\n",
      "103/281, train_loss: 0.1194, step time: 0.2527\n",
      "104/281, train_loss: 0.1125, step time: 0.2522\n",
      "105/281, train_loss: 0.0646, step time: 0.2482\n",
      "106/281, train_loss: 0.0670, step time: 0.2559\n",
      "107/281, train_loss: 0.0770, step time: 0.2564\n",
      "108/281, train_loss: 0.2460, step time: 0.2555\n",
      "109/281, train_loss: 0.2135, step time: 0.2505\n",
      "110/281, train_loss: 0.2468, step time: 0.2531\n",
      "111/281, train_loss: 0.0980, step time: 0.2526\n",
      "112/281, train_loss: 0.1293, step time: 0.2528\n",
      "113/281, train_loss: 0.1126, step time: 0.2533\n",
      "114/281, train_loss: 0.0811, step time: 0.2550\n",
      "115/281, train_loss: 0.0754, step time: 0.2549\n",
      "116/281, train_loss: 0.1039, step time: 0.2477\n",
      "117/281, train_loss: 0.0821, step time: 0.2503\n",
      "118/281, train_loss: 0.1145, step time: 0.2504\n",
      "119/281, train_loss: 0.1105, step time: 0.2515\n",
      "120/281, train_loss: 0.2187, step time: 0.2493\n",
      "121/281, train_loss: 0.1133, step time: 0.2497\n",
      "122/281, train_loss: 0.2259, step time: 0.2491\n",
      "123/281, train_loss: 0.0650, step time: 0.2488\n",
      "124/281, train_loss: 0.0704, step time: 0.2497\n",
      "125/281, train_loss: 0.0827, step time: 0.2484\n",
      "126/281, train_loss: 0.0900, step time: 0.2518\n",
      "127/281, train_loss: 0.0651, step time: 0.2569\n",
      "128/281, train_loss: 0.1081, step time: 0.2524\n",
      "129/281, train_loss: 0.0815, step time: 0.2526\n",
      "130/281, train_loss: 0.0923, step time: 0.2504\n",
      "131/281, train_loss: 0.1005, step time: 0.2535\n",
      "132/281, train_loss: 0.0747, step time: 0.2535\n",
      "133/281, train_loss: 0.0887, step time: 0.2500\n",
      "134/281, train_loss: 0.0589, step time: 0.2507\n",
      "135/281, train_loss: 0.0580, step time: 0.2515\n",
      "136/281, train_loss: 0.0840, step time: 0.2546\n",
      "137/281, train_loss: 0.0798, step time: 0.2533\n",
      "138/281, train_loss: 0.2503, step time: 0.2509\n",
      "139/281, train_loss: 0.2147, step time: 0.2545\n",
      "140/281, train_loss: 0.1396, step time: 0.2556\n",
      "141/281, train_loss: 0.0817, step time: 0.2602\n",
      "142/281, train_loss: 0.0893, step time: 0.2557\n",
      "143/281, train_loss: 0.0737, step time: 0.2505\n",
      "144/281, train_loss: 0.0999, step time: 0.2539\n",
      "145/281, train_loss: 0.0922, step time: 0.2513\n",
      "146/281, train_loss: 0.0727, step time: 0.2505\n",
      "147/281, train_loss: 0.0726, step time: 0.2498\n",
      "148/281, train_loss: 0.0494, step time: 0.2532\n",
      "149/281, train_loss: 0.1040, step time: 0.2571\n",
      "150/281, train_loss: 0.0883, step time: 0.2546\n",
      "151/281, train_loss: 0.1014, step time: 0.2502\n",
      "152/281, train_loss: 0.0479, step time: 0.2466\n",
      "153/281, train_loss: 0.1043, step time: 0.2513\n",
      "154/281, train_loss: 0.1167, step time: 0.2502\n",
      "155/281, train_loss: 0.0717, step time: 0.2539\n",
      "156/281, train_loss: 0.0814, step time: 0.2495\n",
      "157/281, train_loss: 0.0877, step time: 0.2452\n",
      "158/281, train_loss: 0.1002, step time: 0.2555\n",
      "159/281, train_loss: 0.0683, step time: 0.2522\n",
      "160/281, train_loss: 0.1218, step time: 0.2490\n",
      "161/281, train_loss: 0.1155, step time: 0.2512\n",
      "162/281, train_loss: 0.0814, step time: 0.2515\n",
      "163/281, train_loss: 0.1121, step time: 0.2532\n",
      "164/281, train_loss: 0.0809, step time: 0.2487\n",
      "165/281, train_loss: 0.0545, step time: 0.2573\n",
      "166/281, train_loss: 0.0726, step time: 0.2563\n",
      "167/281, train_loss: 0.0696, step time: 0.2497\n",
      "168/281, train_loss: 0.2428, step time: 0.2498\n",
      "169/281, train_loss: 0.0790, step time: 0.2460\n",
      "170/281, train_loss: 0.2133, step time: 0.2468\n",
      "171/281, train_loss: 0.2265, step time: 0.2508\n",
      "172/281, train_loss: 0.0487, step time: 0.2556\n",
      "173/281, train_loss: 0.2473, step time: 0.2546\n",
      "174/281, train_loss: 0.0420, step time: 0.2507\n",
      "175/281, train_loss: 0.2503, step time: 0.2516\n",
      "176/281, train_loss: 0.0786, step time: 0.2465\n",
      "177/281, train_loss: 0.2189, step time: 0.2492\n",
      "178/281, train_loss: 0.0950, step time: 0.2508\n",
      "179/281, train_loss: 0.2151, step time: 0.2544\n",
      "180/281, train_loss: 0.0731, step time: 0.2545\n",
      "181/281, train_loss: 0.0598, step time: 0.2537\n",
      "182/281, train_loss: 0.2500, step time: 0.2514\n",
      "183/281, train_loss: 0.0697, step time: 0.2522\n",
      "184/281, train_loss: 0.0663, step time: 0.2518\n",
      "185/281, train_loss: 0.2586, step time: 0.2842\n",
      "186/281, train_loss: 0.0479, step time: 0.2488\n",
      "187/281, train_loss: 0.0706, step time: 0.2524\n",
      "188/281, train_loss: 0.0962, step time: 0.2544\n",
      "189/281, train_loss: 0.0862, step time: 0.2589\n",
      "190/281, train_loss: 0.0712, step time: 0.2595\n",
      "191/281, train_loss: 0.0639, step time: 0.2482\n",
      "192/281, train_loss: 0.1131, step time: 0.2551\n",
      "193/281, train_loss: 0.0854, step time: 0.2494\n",
      "194/281, train_loss: 0.0933, step time: 0.2502\n",
      "195/281, train_loss: 0.0611, step time: 0.2532\n",
      "196/281, train_loss: 0.1167, step time: 0.2552\n",
      "197/281, train_loss: 0.1190, step time: 0.2512\n",
      "198/281, train_loss: 0.0910, step time: 0.2509\n",
      "199/281, train_loss: 0.2194, step time: 0.2516\n",
      "200/281, train_loss: 0.0621, step time: 0.2516\n",
      "201/281, train_loss: 0.0750, step time: 0.2516\n",
      "202/281, train_loss: 0.0778, step time: 0.2497\n",
      "203/281, train_loss: 0.0724, step time: 0.2507\n",
      "204/281, train_loss: 0.0761, step time: 0.2551\n",
      "205/281, train_loss: 0.0849, step time: 0.2512\n",
      "206/281, train_loss: 0.1280, step time: 0.2528\n",
      "207/281, train_loss: 0.0701, step time: 0.2505\n",
      "208/281, train_loss: 0.1112, step time: 0.2501\n",
      "209/281, train_loss: 0.0657, step time: 0.2512\n",
      "210/281, train_loss: 0.0788, step time: 0.2549\n",
      "211/281, train_loss: 0.0686, step time: 0.2514\n",
      "212/281, train_loss: 0.0640, step time: 0.2544\n",
      "213/281, train_loss: 0.2494, step time: 0.2482\n",
      "214/281, train_loss: 0.0793, step time: 0.2525\n",
      "215/281, train_loss: 0.2453, step time: 0.2521\n",
      "216/281, train_loss: 0.0806, step time: 0.2510\n",
      "217/281, train_loss: 0.3788, step time: 0.2511\n",
      "218/281, train_loss: 0.1543, step time: 0.2462\n",
      "219/281, train_loss: 0.0667, step time: 0.2493\n",
      "220/281, train_loss: 0.1524, step time: 0.2603\n",
      "221/281, train_loss: 0.2259, step time: 0.2505\n",
      "222/281, train_loss: 0.0806, step time: 0.2521\n",
      "223/281, train_loss: 0.0828, step time: 0.2542\n",
      "224/281, train_loss: 0.0731, step time: 0.2541\n",
      "225/281, train_loss: 0.2568, step time: 0.2574\n",
      "226/281, train_loss: 0.2158, step time: 0.2565\n",
      "227/281, train_loss: 0.1541, step time: 0.2515\n",
      "228/281, train_loss: 0.2288, step time: 0.2544\n",
      "229/281, train_loss: 0.2296, step time: 0.2494\n",
      "230/281, train_loss: 0.0751, step time: 0.2503\n",
      "231/281, train_loss: 0.0904, step time: 0.2485\n",
      "232/281, train_loss: 0.0827, step time: 0.2504\n",
      "233/281, train_loss: 0.0855, step time: 0.2512\n",
      "234/281, train_loss: 0.0647, step time: 0.2518\n",
      "235/281, train_loss: 0.0689, step time: 0.2484\n",
      "236/281, train_loss: 0.1718, step time: 0.2543\n",
      "237/281, train_loss: 0.2400, step time: 0.2497\n",
      "238/281, train_loss: 0.0785, step time: 0.2504\n",
      "239/281, train_loss: 0.0735, step time: 0.2522\n",
      "240/281, train_loss: 0.2454, step time: 0.2547\n",
      "241/281, train_loss: 0.0672, step time: 0.2495\n",
      "242/281, train_loss: 0.0981, step time: 0.2575\n",
      "243/281, train_loss: 0.0712, step time: 0.2510\n",
      "244/281, train_loss: 0.1263, step time: 0.2604\n",
      "245/281, train_loss: 0.0547, step time: 0.2550\n",
      "246/281, train_loss: 0.0688, step time: 0.2510\n",
      "247/281, train_loss: 0.0708, step time: 0.2465\n",
      "248/281, train_loss: 0.1072, step time: 0.2514\n",
      "249/281, train_loss: 0.0862, step time: 0.2509\n",
      "250/281, train_loss: 0.0615, step time: 0.2519\n",
      "251/281, train_loss: 0.1066, step time: 0.2471\n",
      "252/281, train_loss: 0.0937, step time: 0.2529\n",
      "253/281, train_loss: 0.0545, step time: 0.2533\n",
      "254/281, train_loss: 0.0476, step time: 0.2524\n",
      "255/281, train_loss: 0.0891, step time: 0.2515\n",
      "256/281, train_loss: 0.2358, step time: 0.2514\n",
      "257/281, train_loss: 0.2569, step time: 0.2506\n",
      "258/281, train_loss: 0.0585, step time: 0.2522\n",
      "259/281, train_loss: 0.0633, step time: 0.2525\n",
      "260/281, train_loss: 0.0726, step time: 0.2551\n",
      "261/281, train_loss: 0.0831, step time: 0.2567\n",
      "262/281, train_loss: 0.2376, step time: 0.2528\n",
      "263/281, train_loss: 0.0790, step time: 0.2497\n",
      "264/281, train_loss: 0.0696, step time: 0.2562\n",
      "265/281, train_loss: 0.0760, step time: 0.2453\n",
      "266/281, train_loss: 0.1058, step time: 0.2510\n",
      "267/281, train_loss: 0.0622, step time: 0.2502\n",
      "268/281, train_loss: 0.0692, step time: 0.2565\n",
      "269/281, train_loss: 0.0622, step time: 0.2484\n",
      "270/281, train_loss: 0.1383, step time: 0.2503\n",
      "271/281, train_loss: 0.1265, step time: 0.2505\n",
      "272/281, train_loss: 0.0819, step time: 0.2499\n",
      "273/281, train_loss: 0.0620, step time: 0.2500\n",
      "274/281, train_loss: 0.0457, step time: 0.2520\n",
      "275/281, train_loss: 0.0998, step time: 0.2500\n",
      "276/281, train_loss: 0.2227, step time: 0.2472\n",
      "277/281, train_loss: 0.1226, step time: 0.2471\n",
      "278/281, train_loss: 0.0914, step time: 0.2521\n",
      "279/281, train_loss: 0.0671, step time: 0.2530\n",
      "280/281, train_loss: 0.2195, step time: 0.2481\n",
      "281/281, train_loss: 0.0916, step time: 0.2479\n",
      "282/281, train_loss: 0.0425, step time: 0.1497\n",
      "epoch 140 average loss: 0.1095\n",
      "current epoch: 140 current mean dice: 0.8781 tc: 0.8830 wt: 0.8992 et: 0.8655\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 140 is: 392.6499\n",
      "----------\n",
      "epoch 141/200\n",
      "1/281, train_loss: 0.0613, step time: 0.2516\n",
      "2/281, train_loss: 0.1394, step time: 0.2527\n",
      "3/281, train_loss: 0.0581, step time: 0.2451\n",
      "4/281, train_loss: 0.2220, step time: 0.2465\n",
      "5/281, train_loss: 0.0976, step time: 0.2542\n",
      "6/281, train_loss: 0.2232, step time: 0.2494\n",
      "7/281, train_loss: 0.2344, step time: 0.2467\n",
      "8/281, train_loss: 0.0768, step time: 0.2549\n",
      "9/281, train_loss: 0.1077, step time: 0.2503\n",
      "10/281, train_loss: 0.1186, step time: 0.2557\n",
      "11/281, train_loss: 0.0764, step time: 0.2534\n",
      "12/281, train_loss: 0.0565, step time: 0.2597\n",
      "13/281, train_loss: 0.0905, step time: 0.2547\n",
      "14/281, train_loss: 0.2096, step time: 0.2534\n",
      "15/281, train_loss: 0.0665, step time: 0.2573\n",
      "16/281, train_loss: 0.0937, step time: 0.2524\n",
      "17/281, train_loss: 0.2656, step time: 0.2566\n",
      "18/281, train_loss: 0.0754, step time: 0.2575\n",
      "19/281, train_loss: 0.2161, step time: 0.2546\n",
      "20/281, train_loss: 0.0578, step time: 0.2581\n",
      "21/281, train_loss: 0.0962, step time: 0.2602\n",
      "22/281, train_loss: 0.0648, step time: 0.2490\n",
      "23/281, train_loss: 0.0837, step time: 0.2487\n",
      "24/281, train_loss: 0.1024, step time: 0.2499\n",
      "25/281, train_loss: 0.0996, step time: 0.2587\n",
      "26/281, train_loss: 0.1052, step time: 0.2535\n",
      "27/281, train_loss: 0.0635, step time: 0.2538\n",
      "28/281, train_loss: 0.2807, step time: 0.2496\n",
      "29/281, train_loss: 0.0493, step time: 0.2477\n",
      "30/281, train_loss: 0.0699, step time: 0.2494\n",
      "31/281, train_loss: 0.1047, step time: 0.2506\n",
      "32/281, train_loss: 0.0922, step time: 0.2504\n",
      "33/281, train_loss: 0.0502, step time: 0.2512\n",
      "34/281, train_loss: 0.0672, step time: 0.2563\n",
      "35/281, train_loss: 0.1007, step time: 0.2496\n",
      "36/281, train_loss: 0.1066, step time: 0.2489\n",
      "37/281, train_loss: 0.1064, step time: 0.2572\n",
      "38/281, train_loss: 0.0846, step time: 0.2504\n",
      "39/281, train_loss: 0.0885, step time: 0.2478\n",
      "40/281, train_loss: 0.0631, step time: 0.2443\n",
      "41/281, train_loss: 0.0751, step time: 0.2554\n",
      "42/281, train_loss: 0.0961, step time: 0.2478\n",
      "43/281, train_loss: 0.0802, step time: 0.2471\n",
      "44/281, train_loss: 0.0596, step time: 0.2459\n",
      "45/281, train_loss: 0.0996, step time: 0.2492\n",
      "46/281, train_loss: 0.2156, step time: 0.2567\n",
      "47/281, train_loss: 0.2191, step time: 0.2556\n",
      "48/281, train_loss: 0.0851, step time: 0.2558\n",
      "49/281, train_loss: 0.0907, step time: 0.2509\n",
      "50/281, train_loss: 0.0651, step time: 0.2476\n",
      "51/281, train_loss: 0.0544, step time: 0.2497\n",
      "52/281, train_loss: 0.0555, step time: 0.2474\n",
      "53/281, train_loss: 0.0871, step time: 0.2548\n",
      "54/281, train_loss: 0.0696, step time: 0.2501\n",
      "55/281, train_loss: 0.1043, step time: 0.2516\n",
      "56/281, train_loss: 0.0642, step time: 0.2495\n",
      "57/281, train_loss: 0.1442, step time: 0.2527\n",
      "58/281, train_loss: 0.1243, step time: 0.2493\n",
      "59/281, train_loss: 0.1112, step time: 0.2480\n",
      "60/281, train_loss: 0.0949, step time: 0.2519\n",
      "61/281, train_loss: 0.0645, step time: 0.2648\n",
      "62/281, train_loss: 0.0554, step time: 0.2483\n",
      "63/281, train_loss: 0.0557, step time: 0.2528\n",
      "64/281, train_loss: 0.0911, step time: 0.2466\n",
      "65/281, train_loss: 0.0895, step time: 0.2473\n",
      "66/281, train_loss: 0.0737, step time: 0.2544\n",
      "67/281, train_loss: 0.0864, step time: 0.2543\n",
      "68/281, train_loss: 0.0657, step time: 0.2505\n",
      "69/281, train_loss: 0.0742, step time: 0.2487\n",
      "70/281, train_loss: 0.1347, step time: 0.2474\n",
      "71/281, train_loss: 0.0673, step time: 0.2529\n",
      "72/281, train_loss: 0.2392, step time: 0.2553\n",
      "73/281, train_loss: 0.0675, step time: 0.2479\n",
      "74/281, train_loss: 0.0804, step time: 0.2472\n",
      "75/281, train_loss: 0.2296, step time: 0.2526\n",
      "76/281, train_loss: 0.0766, step time: 0.2506\n",
      "77/281, train_loss: 0.0904, step time: 0.2475\n",
      "78/281, train_loss: 0.0423, step time: 0.2501\n",
      "79/281, train_loss: 0.1071, step time: 0.2501\n",
      "80/281, train_loss: 0.0560, step time: 0.2498\n",
      "81/281, train_loss: 0.0675, step time: 0.2509\n",
      "82/281, train_loss: 0.0615, step time: 0.2488\n",
      "83/281, train_loss: 0.1116, step time: 0.2495\n",
      "84/281, train_loss: 0.0874, step time: 0.2479\n",
      "85/281, train_loss: 0.2353, step time: 0.2488\n",
      "86/281, train_loss: 0.1244, step time: 0.2505\n",
      "87/281, train_loss: 0.0634, step time: 0.2650\n",
      "88/281, train_loss: 0.0535, step time: 0.2558\n",
      "89/281, train_loss: 0.2502, step time: 0.2490\n",
      "90/281, train_loss: 0.0552, step time: 0.2489\n",
      "91/281, train_loss: 0.0728, step time: 0.2479\n",
      "92/281, train_loss: 0.0627, step time: 0.2463\n",
      "93/281, train_loss: 0.0593, step time: 0.2423\n",
      "94/281, train_loss: 0.0663, step time: 0.2527\n",
      "95/281, train_loss: 0.0564, step time: 0.2545\n",
      "96/281, train_loss: 0.1141, step time: 0.2498\n",
      "97/281, train_loss: 0.0584, step time: 0.2477\n",
      "98/281, train_loss: 0.0961, step time: 0.2540\n",
      "99/281, train_loss: 0.0723, step time: 0.2474\n",
      "100/281, train_loss: 0.1809, step time: 0.2434\n",
      "101/281, train_loss: 0.0794, step time: 0.2445\n",
      "102/281, train_loss: 0.2265, step time: 0.2498\n",
      "103/281, train_loss: 0.2341, step time: 0.2498\n",
      "104/281, train_loss: 0.0630, step time: 0.2526\n",
      "105/281, train_loss: 0.0762, step time: 0.2499\n",
      "106/281, train_loss: 0.0624, step time: 0.2511\n",
      "107/281, train_loss: 0.1300, step time: 0.2438\n",
      "108/281, train_loss: 0.0625, step time: 0.2496\n",
      "109/281, train_loss: 0.0580, step time: 0.2501\n",
      "110/281, train_loss: 0.1025, step time: 0.2522\n",
      "111/281, train_loss: 0.1341, step time: 0.2502\n",
      "112/281, train_loss: 0.1052, step time: 0.2492\n",
      "113/281, train_loss: 0.0899, step time: 0.2495\n",
      "114/281, train_loss: 0.0643, step time: 0.2464\n",
      "115/281, train_loss: 0.0701, step time: 0.2483\n",
      "116/281, train_loss: 0.0778, step time: 0.2500\n",
      "117/281, train_loss: 0.0842, step time: 0.2572\n",
      "118/281, train_loss: 0.2227, step time: 0.2515\n",
      "119/281, train_loss: 0.2720, step time: 0.2522\n",
      "120/281, train_loss: 0.0511, step time: 0.2507\n",
      "121/281, train_loss: 0.0944, step time: 0.2459\n",
      "122/281, train_loss: 0.1008, step time: 0.2439\n",
      "123/281, train_loss: 0.0734, step time: 0.2485\n",
      "124/281, train_loss: 0.0714, step time: 0.2577\n",
      "125/281, train_loss: 0.0533, step time: 0.2709\n",
      "126/281, train_loss: 0.0696, step time: 0.2508\n",
      "127/281, train_loss: 0.2397, step time: 0.2508\n",
      "128/281, train_loss: 0.4003, step time: 0.2507\n",
      "129/281, train_loss: 0.1204, step time: 0.2513\n",
      "130/281, train_loss: 0.0845, step time: 0.2535\n",
      "131/281, train_loss: 0.0676, step time: 0.2587\n",
      "132/281, train_loss: 0.0896, step time: 0.2512\n",
      "133/281, train_loss: 0.2332, step time: 0.2522\n",
      "134/281, train_loss: 0.0799, step time: 0.2488\n",
      "135/281, train_loss: 0.0706, step time: 0.2517\n",
      "136/281, train_loss: 0.0922, step time: 0.2445\n",
      "137/281, train_loss: 0.1167, step time: 0.2441\n",
      "138/281, train_loss: 0.0823, step time: 0.2511\n",
      "139/281, train_loss: 0.0852, step time: 0.2494\n",
      "140/281, train_loss: 0.0696, step time: 0.2470\n",
      "141/281, train_loss: 0.0834, step time: 0.2452\n",
      "142/281, train_loss: 0.1061, step time: 0.2444\n",
      "143/281, train_loss: 0.0478, step time: 0.2469\n",
      "144/281, train_loss: 0.0730, step time: 0.2466\n",
      "145/281, train_loss: 0.0682, step time: 0.2493\n",
      "146/281, train_loss: 0.0831, step time: 0.2500\n",
      "147/281, train_loss: 0.2588, step time: 0.2468\n",
      "148/281, train_loss: 0.0731, step time: 0.2459\n",
      "149/281, train_loss: 0.0725, step time: 0.2533\n",
      "150/281, train_loss: 0.0656, step time: 0.2469\n",
      "151/281, train_loss: 0.0723, step time: 0.2480\n",
      "152/281, train_loss: 0.1136, step time: 0.2478\n",
      "153/281, train_loss: 0.0638, step time: 0.2463\n",
      "154/281, train_loss: 0.1355, step time: 0.2467\n",
      "155/281, train_loss: 0.0845, step time: 0.2503\n",
      "156/281, train_loss: 0.2423, step time: 0.2580\n",
      "157/281, train_loss: 0.0799, step time: 0.2547\n",
      "158/281, train_loss: 0.1204, step time: 0.2505\n",
      "159/281, train_loss: 0.2437, step time: 0.2511\n",
      "160/281, train_loss: 0.0761, step time: 0.2477\n",
      "161/281, train_loss: 0.0755, step time: 0.2479\n",
      "162/281, train_loss: 0.0573, step time: 0.2479\n",
      "163/281, train_loss: 0.2230, step time: 0.2470\n",
      "164/281, train_loss: 0.1048, step time: 0.2497\n",
      "165/281, train_loss: 0.0830, step time: 0.2525\n",
      "166/281, train_loss: 0.2230, step time: 0.2494\n",
      "167/281, train_loss: 0.0943, step time: 0.2455\n",
      "168/281, train_loss: 0.0604, step time: 0.2461\n",
      "169/281, train_loss: 0.0792, step time: 0.2489\n",
      "170/281, train_loss: 0.0660, step time: 0.2484\n",
      "171/281, train_loss: 0.0620, step time: 0.2434\n",
      "172/281, train_loss: 0.2336, step time: 0.2474\n",
      "173/281, train_loss: 0.0827, step time: 0.2494\n",
      "174/281, train_loss: 0.0494, step time: 0.2478\n",
      "175/281, train_loss: 0.0976, step time: 0.2477\n",
      "176/281, train_loss: 0.2161, step time: 0.2506\n",
      "177/281, train_loss: 0.0695, step time: 0.2457\n",
      "178/281, train_loss: 0.0586, step time: 0.2473\n",
      "179/281, train_loss: 0.0964, step time: 0.2484\n",
      "180/281, train_loss: 0.0897, step time: 0.2476\n",
      "181/281, train_loss: 0.0639, step time: 0.2516\n",
      "182/281, train_loss: 0.0612, step time: 0.2521\n",
      "183/281, train_loss: 0.0978, step time: 0.2479\n",
      "184/281, train_loss: 0.0903, step time: 0.2426\n",
      "185/281, train_loss: 0.2656, step time: 0.2486\n",
      "186/281, train_loss: 0.0785, step time: 0.2498\n",
      "187/281, train_loss: 0.2433, step time: 0.2728\n",
      "188/281, train_loss: 0.1080, step time: 0.2469\n",
      "189/281, train_loss: 0.0580, step time: 0.2484\n",
      "190/281, train_loss: 0.0587, step time: 0.2499\n",
      "191/281, train_loss: 0.1240, step time: 0.2530\n",
      "192/281, train_loss: 0.0660, step time: 0.2541\n",
      "193/281, train_loss: 0.2482, step time: 0.2517\n",
      "194/281, train_loss: 0.1001, step time: 0.2507\n",
      "195/281, train_loss: 0.0979, step time: 0.2441\n",
      "196/281, train_loss: 0.0774, step time: 0.2525\n",
      "197/281, train_loss: 0.0895, step time: 0.2506\n",
      "198/281, train_loss: 0.2720, step time: 0.2513\n",
      "199/281, train_loss: 0.0931, step time: 0.2505\n",
      "200/281, train_loss: 0.2372, step time: 0.2484\n",
      "201/281, train_loss: 0.0371, step time: 0.2548\n",
      "202/281, train_loss: 0.0729, step time: 0.2486\n",
      "203/281, train_loss: 0.0867, step time: 0.2489\n",
      "204/281, train_loss: 0.0934, step time: 0.2542\n",
      "205/281, train_loss: 0.0605, step time: 0.2501\n",
      "206/281, train_loss: 0.2213, step time: 0.2525\n",
      "207/281, train_loss: 0.0755, step time: 0.2543\n",
      "208/281, train_loss: 0.0858, step time: 0.2468\n",
      "209/281, train_loss: 0.0718, step time: 0.2468\n",
      "210/281, train_loss: 0.1032, step time: 0.2524\n",
      "211/281, train_loss: 0.0739, step time: 0.2455\n",
      "212/281, train_loss: 0.0842, step time: 0.2476\n",
      "213/281, train_loss: 0.0850, step time: 0.2451\n",
      "214/281, train_loss: 0.1155, step time: 0.2513\n",
      "215/281, train_loss: 0.2562, step time: 0.2512\n",
      "216/281, train_loss: 0.0933, step time: 0.2423\n",
      "217/281, train_loss: 0.0613, step time: 0.2395\n",
      "218/281, train_loss: 0.2352, step time: 0.2454\n",
      "219/281, train_loss: 0.0911, step time: 0.2479\n",
      "220/281, train_loss: 0.1166, step time: 0.2491\n",
      "221/281, train_loss: 0.0297, step time: 0.2506\n",
      "222/281, train_loss: 0.1194, step time: 0.2446\n",
      "223/281, train_loss: 0.0937, step time: 0.2443\n",
      "224/281, train_loss: 0.0731, step time: 0.2445\n",
      "225/281, train_loss: 0.1145, step time: 0.2475\n",
      "226/281, train_loss: 0.0760, step time: 0.2436\n",
      "227/281, train_loss: 0.0885, step time: 0.2464\n",
      "228/281, train_loss: 0.0579, step time: 0.2450\n",
      "229/281, train_loss: 0.0814, step time: 0.2449\n",
      "230/281, train_loss: 0.0531, step time: 0.2451\n",
      "231/281, train_loss: 0.0628, step time: 0.2480\n",
      "232/281, train_loss: 0.0573, step time: 0.2460\n",
      "233/281, train_loss: 0.0992, step time: 0.2472\n",
      "234/281, train_loss: 0.2519, step time: 0.2474\n",
      "235/281, train_loss: 0.0521, step time: 0.2443\n",
      "236/281, train_loss: 0.0748, step time: 0.2461\n",
      "237/281, train_loss: 0.2625, step time: 0.2460\n",
      "238/281, train_loss: 0.0524, step time: 0.2846\n",
      "239/281, train_loss: 0.0412, step time: 0.2664\n",
      "240/281, train_loss: 0.0596, step time: 0.2515\n",
      "241/281, train_loss: 0.0647, step time: 0.2484\n",
      "242/281, train_loss: 0.0629, step time: 0.2428\n",
      "243/281, train_loss: 0.0931, step time: 0.2479\n",
      "244/281, train_loss: 0.0906, step time: 0.2518\n",
      "245/281, train_loss: 0.0712, step time: 0.2528\n",
      "246/281, train_loss: 0.1288, step time: 0.2520\n",
      "247/281, train_loss: 0.2190, step time: 0.2458\n",
      "248/281, train_loss: 0.0695, step time: 0.2522\n",
      "249/281, train_loss: 0.0820, step time: 0.2464\n",
      "250/281, train_loss: 0.0789, step time: 0.2471\n",
      "251/281, train_loss: 0.2463, step time: 0.2445\n",
      "252/281, train_loss: 0.0743, step time: 0.2431\n",
      "253/281, train_loss: 0.1393, step time: 0.2447\n",
      "254/281, train_loss: 0.0986, step time: 0.2487\n",
      "255/281, train_loss: 0.0989, step time: 0.2433\n",
      "256/281, train_loss: 0.0710, step time: 0.2429\n",
      "257/281, train_loss: 0.0643, step time: 0.2494\n",
      "258/281, train_loss: 0.0891, step time: 0.2512\n",
      "259/281, train_loss: 0.0746, step time: 0.2454\n",
      "260/281, train_loss: 0.0659, step time: 0.2505\n",
      "261/281, train_loss: 0.0473, step time: 0.2469\n",
      "262/281, train_loss: 0.2098, step time: 0.2473\n",
      "263/281, train_loss: 0.0954, step time: 0.2452\n",
      "264/281, train_loss: 0.0851, step time: 0.2492\n",
      "265/281, train_loss: 0.0529, step time: 0.2461\n",
      "266/281, train_loss: 0.0980, step time: 0.2521\n",
      "267/281, train_loss: 0.2215, step time: 0.2402\n",
      "268/281, train_loss: 0.0663, step time: 0.2581\n",
      "269/281, train_loss: 0.0819, step time: 0.2375\n",
      "270/281, train_loss: 0.0804, step time: 0.2465\n",
      "271/281, train_loss: 0.0812, step time: 0.2478\n",
      "272/281, train_loss: 0.0555, step time: 0.2445\n",
      "273/281, train_loss: 0.0757, step time: 0.2446\n",
      "274/281, train_loss: 0.0661, step time: 0.2501\n",
      "275/281, train_loss: 0.0678, step time: 0.2465\n",
      "276/281, train_loss: 0.2391, step time: 0.2434\n",
      "277/281, train_loss: 0.0828, step time: 0.2444\n",
      "278/281, train_loss: 0.1024, step time: 0.2385\n",
      "279/281, train_loss: 0.0848, step time: 0.2408\n",
      "280/281, train_loss: 0.0854, step time: 0.2380\n",
      "281/281, train_loss: 0.0881, step time: 0.2413\n",
      "282/281, train_loss: 0.0615, step time: 0.1429\n",
      "epoch 141 average loss: 0.1051\n",
      "current epoch: 141 current mean dice: 0.8702 tc: 0.8719 wt: 0.8892 et: 0.8629\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 141 is: 386.6673\n",
      "----------\n",
      "epoch 142/200\n",
      "1/281, train_loss: 0.0782, step time: 0.2584\n",
      "2/281, train_loss: 0.0884, step time: 0.2448\n",
      "3/281, train_loss: 0.0876, step time: 0.2423\n",
      "4/281, train_loss: 0.1177, step time: 0.2447\n",
      "5/281, train_loss: 0.0758, step time: 0.2484\n",
      "6/281, train_loss: 0.0744, step time: 0.2461\n",
      "7/281, train_loss: 0.0709, step time: 0.2453\n",
      "8/281, train_loss: 0.1146, step time: 0.2573\n",
      "9/281, train_loss: 0.2117, step time: 0.2442\n",
      "10/281, train_loss: 0.0733, step time: 0.2428\n",
      "11/281, train_loss: 0.0808, step time: 0.2469\n",
      "12/281, train_loss: 0.0685, step time: 0.2519\n",
      "13/281, train_loss: 0.0722, step time: 0.2531\n",
      "14/281, train_loss: 0.0822, step time: 0.2536\n",
      "15/281, train_loss: 0.0732, step time: 0.2491\n",
      "16/281, train_loss: 0.0605, step time: 0.2517\n",
      "17/281, train_loss: 0.0594, step time: 0.2472\n",
      "18/281, train_loss: 0.0664, step time: 0.2471\n",
      "19/281, train_loss: 0.2156, step time: 0.2508\n",
      "20/281, train_loss: 0.0896, step time: 0.2515\n",
      "21/281, train_loss: 0.0826, step time: 0.2500\n",
      "22/281, train_loss: 0.2504, step time: 0.2497\n",
      "23/281, train_loss: 0.0922, step time: 0.2663\n",
      "24/281, train_loss: 0.0509, step time: 0.2702\n",
      "25/281, train_loss: 0.0591, step time: 0.2570\n",
      "26/281, train_loss: 0.0878, step time: 0.2584\n",
      "27/281, train_loss: 0.2216, step time: 0.2524\n",
      "28/281, train_loss: 0.0966, step time: 0.2532\n",
      "29/281, train_loss: 0.1028, step time: 0.2517\n",
      "30/281, train_loss: 0.0528, step time: 0.2557\n",
      "31/281, train_loss: 0.1165, step time: 0.2485\n",
      "32/281, train_loss: 0.0893, step time: 0.2502\n",
      "33/281, train_loss: 0.0630, step time: 0.2472\n",
      "34/281, train_loss: 0.0799, step time: 0.2530\n",
      "35/281, train_loss: 0.0676, step time: 0.2444\n",
      "36/281, train_loss: 0.0785, step time: 0.2435\n",
      "37/281, train_loss: 0.0966, step time: 0.2447\n",
      "38/281, train_loss: 0.0557, step time: 0.2486\n",
      "39/281, train_loss: 0.2491, step time: 0.2450\n",
      "40/281, train_loss: 0.0911, step time: 0.2498\n",
      "41/281, train_loss: 0.1014, step time: 0.2476\n",
      "42/281, train_loss: 0.0609, step time: 0.2508\n",
      "43/281, train_loss: 0.0788, step time: 0.2528\n",
      "44/281, train_loss: 0.0767, step time: 0.2474\n",
      "45/281, train_loss: 0.0693, step time: 0.2435\n",
      "46/281, train_loss: 0.0592, step time: 0.2499\n",
      "47/281, train_loss: 0.0944, step time: 0.2542\n",
      "48/281, train_loss: 0.0608, step time: 0.2485\n",
      "49/281, train_loss: 0.0663, step time: 0.2500\n",
      "50/281, train_loss: 0.0667, step time: 0.2565\n",
      "51/281, train_loss: 0.0741, step time: 0.2585\n",
      "52/281, train_loss: 0.0799, step time: 0.2535\n",
      "53/281, train_loss: 0.2173, step time: 0.2518\n",
      "54/281, train_loss: 0.0898, step time: 0.2477\n",
      "55/281, train_loss: 0.1069, step time: 0.2540\n",
      "56/281, train_loss: 0.0929, step time: 0.2574\n",
      "57/281, train_loss: 0.0509, step time: 0.2548\n",
      "58/281, train_loss: 0.0801, step time: 0.2487\n",
      "59/281, train_loss: 0.0720, step time: 0.2510\n",
      "60/281, train_loss: 0.0883, step time: 0.2534\n",
      "61/281, train_loss: 0.2181, step time: 0.2490\n",
      "62/281, train_loss: 0.0721, step time: 0.2462\n",
      "63/281, train_loss: 0.0520, step time: 0.2518\n",
      "64/281, train_loss: 0.1206, step time: 0.2500\n",
      "65/281, train_loss: 0.2193, step time: 0.2544\n",
      "66/281, train_loss: 0.0777, step time: 0.2545\n",
      "67/281, train_loss: 0.1200, step time: 0.2453\n",
      "68/281, train_loss: 0.0829, step time: 0.2478\n",
      "69/281, train_loss: 0.0585, step time: 0.2484\n",
      "70/281, train_loss: 0.0826, step time: 0.2521\n",
      "71/281, train_loss: 0.1181, step time: 0.2457\n",
      "72/281, train_loss: 0.0653, step time: 0.2483\n",
      "73/281, train_loss: 0.0468, step time: 0.2461\n",
      "74/281, train_loss: 0.0371, step time: 0.2531\n",
      "75/281, train_loss: 0.0553, step time: 0.2480\n",
      "76/281, train_loss: 0.2688, step time: 0.2463\n",
      "77/281, train_loss: 0.0775, step time: 0.2736\n",
      "78/281, train_loss: 0.1269, step time: 0.2484\n",
      "79/281, train_loss: 0.0966, step time: 0.2468\n",
      "80/281, train_loss: 0.0799, step time: 0.2496\n",
      "81/281, train_loss: 0.1080, step time: 0.2479\n",
      "82/281, train_loss: 0.0706, step time: 0.2436\n",
      "83/281, train_loss: 0.0688, step time: 0.2463\n",
      "84/281, train_loss: 0.2366, step time: 0.2510\n",
      "85/281, train_loss: 0.2454, step time: 0.2479\n",
      "86/281, train_loss: 0.1147, step time: 0.2483\n",
      "87/281, train_loss: 0.0683, step time: 0.2513\n",
      "88/281, train_loss: 0.2470, step time: 0.2502\n",
      "89/281, train_loss: 0.0894, step time: 0.2527\n",
      "90/281, train_loss: 0.0787, step time: 0.2579\n",
      "91/281, train_loss: 0.0599, step time: 0.2601\n",
      "92/281, train_loss: 0.0685, step time: 0.2450\n",
      "93/281, train_loss: 0.2284, step time: 0.2477\n",
      "94/281, train_loss: 0.0694, step time: 0.2502\n",
      "95/281, train_loss: 0.0665, step time: 0.2512\n",
      "96/281, train_loss: 0.0667, step time: 0.2472\n",
      "97/281, train_loss: 0.2496, step time: 0.2436\n",
      "98/281, train_loss: 0.0803, step time: 0.2493\n",
      "99/281, train_loss: 0.0790, step time: 0.2543\n",
      "100/281, train_loss: 0.0922, step time: 0.2431\n",
      "101/281, train_loss: 0.0670, step time: 0.2463\n",
      "102/281, train_loss: 0.2327, step time: 0.2464\n",
      "103/281, train_loss: 0.0985, step time: 0.2472\n",
      "104/281, train_loss: 0.0750, step time: 0.2449\n",
      "105/281, train_loss: 0.0907, step time: 0.2503\n",
      "106/281, train_loss: 0.0608, step time: 0.2481\n",
      "107/281, train_loss: 0.0833, step time: 0.2523\n",
      "108/281, train_loss: 0.0678, step time: 0.2526\n",
      "109/281, train_loss: 0.0471, step time: 0.2500\n",
      "110/281, train_loss: 0.0555, step time: 0.2514\n",
      "111/281, train_loss: 0.0784, step time: 0.2476\n",
      "112/281, train_loss: 0.0733, step time: 0.2515\n",
      "113/281, train_loss: 0.0680, step time: 0.2521\n",
      "114/281, train_loss: 0.0490, step time: 0.2544\n",
      "115/281, train_loss: 0.0581, step time: 0.2531\n",
      "116/281, train_loss: 0.2512, step time: 0.2535\n",
      "117/281, train_loss: 0.1144, step time: 0.2489\n",
      "118/281, train_loss: 0.0652, step time: 0.2648\n",
      "119/281, train_loss: 0.0474, step time: 0.2504\n",
      "120/281, train_loss: 0.2559, step time: 0.2435\n",
      "121/281, train_loss: 0.0540, step time: 0.2529\n",
      "122/281, train_loss: 0.0598, step time: 0.2531\n",
      "123/281, train_loss: 0.0664, step time: 0.2488\n",
      "124/281, train_loss: 0.0777, step time: 0.2534\n",
      "125/281, train_loss: 0.2205, step time: 0.2547\n",
      "126/281, train_loss: 0.0759, step time: 0.2506\n",
      "127/281, train_loss: 0.0991, step time: 0.2486\n",
      "128/281, train_loss: 0.0743, step time: 0.2535\n",
      "129/281, train_loss: 0.0729, step time: 0.2569\n",
      "130/281, train_loss: 0.0506, step time: 0.2518\n",
      "131/281, train_loss: 0.0877, step time: 0.2490\n",
      "132/281, train_loss: 0.2191, step time: 0.2520\n",
      "133/281, train_loss: 0.1092, step time: 0.2498\n",
      "134/281, train_loss: 0.1345, step time: 0.2527\n",
      "135/281, train_loss: 0.0761, step time: 0.2531\n",
      "136/281, train_loss: 0.0388, step time: 0.2476\n",
      "137/281, train_loss: 0.0802, step time: 0.2532\n",
      "138/281, train_loss: 0.0579, step time: 0.2589\n",
      "139/281, train_loss: 0.0586, step time: 0.2547\n",
      "140/281, train_loss: 0.0463, step time: 0.2560\n",
      "141/281, train_loss: 0.0633, step time: 0.2546\n",
      "142/281, train_loss: 0.0484, step time: 0.2495\n",
      "143/281, train_loss: 0.0668, step time: 0.2572\n",
      "144/281, train_loss: 0.2064, step time: 0.2508\n",
      "145/281, train_loss: 0.1090, step time: 0.2518\n",
      "146/281, train_loss: 0.2453, step time: 0.2498\n",
      "147/281, train_loss: 0.0506, step time: 0.2573\n",
      "148/281, train_loss: 0.0720, step time: 0.2498\n",
      "149/281, train_loss: 0.0823, step time: 0.2503\n",
      "150/281, train_loss: 0.2396, step time: 0.2483\n",
      "151/281, train_loss: 0.0858, step time: 0.2578\n",
      "152/281, train_loss: 0.2312, step time: 0.2551\n",
      "153/281, train_loss: 0.1164, step time: 0.2566\n",
      "154/281, train_loss: 0.0709, step time: 0.2543\n",
      "155/281, train_loss: 0.0630, step time: 0.2509\n",
      "156/281, train_loss: 0.0554, step time: 0.2554\n",
      "157/281, train_loss: 0.0717, step time: 0.2520\n",
      "158/281, train_loss: 0.0624, step time: 0.2510\n",
      "159/281, train_loss: 0.0849, step time: 0.2521\n",
      "160/281, train_loss: 0.0642, step time: 0.2559\n",
      "161/281, train_loss: 0.0733, step time: 0.2506\n",
      "162/281, train_loss: 0.0521, step time: 0.2543\n",
      "163/281, train_loss: 0.1092, step time: 0.2545\n",
      "164/281, train_loss: 0.0828, step time: 0.2517\n",
      "165/281, train_loss: 0.0619, step time: 0.2591\n",
      "166/281, train_loss: 0.0620, step time: 0.2770\n",
      "167/281, train_loss: 0.0797, step time: 0.2466\n",
      "168/281, train_loss: 0.0861, step time: 0.2477\n",
      "169/281, train_loss: 0.0713, step time: 0.2568\n",
      "170/281, train_loss: 0.0624, step time: 0.2559\n",
      "171/281, train_loss: 0.1019, step time: 0.2569\n",
      "172/281, train_loss: 0.0605, step time: 0.2571\n",
      "173/281, train_loss: 0.0856, step time: 0.2598\n",
      "174/281, train_loss: 0.0722, step time: 0.2496\n",
      "175/281, train_loss: 0.0814, step time: 0.2503\n",
      "176/281, train_loss: 0.0888, step time: 0.2527\n",
      "177/281, train_loss: 0.1148, step time: 0.2509\n",
      "178/281, train_loss: 0.0970, step time: 0.2509\n",
      "179/281, train_loss: 0.0890, step time: 0.2550\n",
      "180/281, train_loss: 0.0936, step time: 0.2470\n",
      "181/281, train_loss: 0.0640, step time: 0.2493\n",
      "182/281, train_loss: 0.0783, step time: 0.2470\n",
      "183/281, train_loss: 0.0498, step time: 0.2570\n",
      "184/281, train_loss: 0.0861, step time: 0.2484\n",
      "185/281, train_loss: 0.0738, step time: 0.2509\n",
      "186/281, train_loss: 0.1038, step time: 0.2523\n",
      "187/281, train_loss: 0.1124, step time: 0.2525\n",
      "188/281, train_loss: 0.0687, step time: 0.2540\n",
      "189/281, train_loss: 0.2132, step time: 0.2524\n",
      "190/281, train_loss: 0.0941, step time: 0.2516\n",
      "191/281, train_loss: 0.0804, step time: 0.2521\n",
      "192/281, train_loss: 0.0813, step time: 0.2486\n",
      "193/281, train_loss: 0.0884, step time: 0.2465\n",
      "194/281, train_loss: 0.0760, step time: 0.2507\n",
      "195/281, train_loss: 0.0874, step time: 0.2528\n",
      "196/281, train_loss: 0.0784, step time: 0.2507\n",
      "197/281, train_loss: 0.0744, step time: 0.2532\n",
      "198/281, train_loss: 0.0847, step time: 0.2502\n",
      "199/281, train_loss: 0.0443, step time: 0.2479\n",
      "200/281, train_loss: 0.0624, step time: 0.2533\n",
      "201/281, train_loss: 0.0591, step time: 0.2521\n",
      "202/281, train_loss: 0.0713, step time: 0.2529\n",
      "203/281, train_loss: 0.0911, step time: 0.2462\n",
      "204/281, train_loss: 0.0444, step time: 0.2529\n",
      "205/281, train_loss: 0.0801, step time: 0.2449\n",
      "206/281, train_loss: 0.0646, step time: 0.2544\n",
      "207/281, train_loss: 0.3825, step time: 0.2504\n",
      "208/281, train_loss: 0.1185, step time: 0.2501\n",
      "209/281, train_loss: 0.2387, step time: 0.2528\n",
      "210/281, train_loss: 0.2111, step time: 0.2574\n",
      "211/281, train_loss: 0.0520, step time: 0.2502\n",
      "212/281, train_loss: 0.2507, step time: 0.2527\n",
      "213/281, train_loss: 0.0650, step time: 0.2494\n",
      "214/281, train_loss: 0.0566, step time: 0.2504\n",
      "215/281, train_loss: 0.0829, step time: 0.2503\n",
      "216/281, train_loss: 0.0622, step time: 0.2455\n",
      "217/281, train_loss: 0.0779, step time: 0.2447\n",
      "218/281, train_loss: 0.0603, step time: 0.2545\n",
      "219/281, train_loss: 0.0738, step time: 0.2514\n",
      "220/281, train_loss: 0.2430, step time: 0.2530\n",
      "221/281, train_loss: 0.1073, step time: 0.2496\n",
      "222/281, train_loss: 0.0325, step time: 0.2507\n",
      "223/281, train_loss: 0.2136, step time: 0.2518\n",
      "224/281, train_loss: 0.2465, step time: 0.2535\n",
      "225/281, train_loss: 0.2295, step time: 0.2476\n",
      "226/281, train_loss: 0.0606, step time: 0.2481\n",
      "227/281, train_loss: 0.0682, step time: 0.2519\n",
      "228/281, train_loss: 0.0681, step time: 0.2500\n",
      "229/281, train_loss: 0.0570, step time: 0.2499\n",
      "230/281, train_loss: 0.0659, step time: 0.2494\n",
      "231/281, train_loss: 0.2455, step time: 0.2508\n",
      "232/281, train_loss: 0.0516, step time: 0.2482\n",
      "233/281, train_loss: 0.1239, step time: 0.2527\n",
      "234/281, train_loss: 0.0598, step time: 0.2542\n",
      "235/281, train_loss: 0.0604, step time: 0.2447\n",
      "236/281, train_loss: 0.0791, step time: 0.2488\n",
      "237/281, train_loss: 0.0880, step time: 0.2476\n",
      "238/281, train_loss: 0.1220, step time: 0.2492\n",
      "239/281, train_loss: 0.1715, step time: 0.2619\n",
      "240/281, train_loss: 0.1349, step time: 0.2566\n",
      "241/281, train_loss: 0.0514, step time: 0.2570\n",
      "242/281, train_loss: 0.2419, step time: 0.2487\n",
      "243/281, train_loss: 0.2682, step time: 0.2540\n",
      "244/281, train_loss: 0.0635, step time: 0.2538\n",
      "245/281, train_loss: 0.0811, step time: 0.2530\n",
      "246/281, train_loss: 0.0559, step time: 0.2494\n",
      "247/281, train_loss: 0.2397, step time: 0.2524\n",
      "248/281, train_loss: 0.0658, step time: 0.2523\n",
      "249/281, train_loss: 0.0711, step time: 0.2546\n",
      "250/281, train_loss: 0.0602, step time: 0.2501\n",
      "251/281, train_loss: 0.3908, step time: 0.2533\n",
      "252/281, train_loss: 0.0512, step time: 0.2537\n",
      "253/281, train_loss: 0.2360, step time: 0.2554\n",
      "254/281, train_loss: 0.0791, step time: 0.2538\n",
      "255/281, train_loss: 0.0593, step time: 0.2505\n",
      "256/281, train_loss: 0.2102, step time: 0.2476\n",
      "257/281, train_loss: 0.2619, step time: 0.2481\n",
      "258/281, train_loss: 0.0880, step time: 0.2479\n",
      "259/281, train_loss: 0.0751, step time: 0.2471\n",
      "260/281, train_loss: 0.1241, step time: 0.2519\n",
      "261/281, train_loss: 0.0657, step time: 0.2512\n",
      "262/281, train_loss: 0.0711, step time: 0.2529\n",
      "263/281, train_loss: 0.0922, step time: 0.2606\n",
      "264/281, train_loss: 0.0541, step time: 0.2505\n",
      "265/281, train_loss: 0.0824, step time: 0.2594\n",
      "266/281, train_loss: 0.0471, step time: 0.2484\n",
      "267/281, train_loss: 0.0815, step time: 0.2478\n",
      "268/281, train_loss: 0.0460, step time: 0.2489\n",
      "269/281, train_loss: 0.0728, step time: 0.2487\n",
      "270/281, train_loss: 0.0461, step time: 0.2527\n",
      "271/281, train_loss: 0.2357, step time: 0.2537\n",
      "272/281, train_loss: 0.0801, step time: 0.2531\n",
      "273/281, train_loss: 0.1308, step time: 0.2515\n",
      "274/281, train_loss: 0.0783, step time: 0.2503\n",
      "275/281, train_loss: 0.1084, step time: 0.2500\n",
      "276/281, train_loss: 0.0834, step time: 0.2527\n",
      "277/281, train_loss: 0.1069, step time: 0.2545\n",
      "278/281, train_loss: 0.0584, step time: 0.2493\n",
      "279/281, train_loss: 0.0752, step time: 0.2503\n",
      "280/281, train_loss: 0.0861, step time: 0.2512\n",
      "281/281, train_loss: 0.0616, step time: 0.2501\n",
      "282/281, train_loss: 0.1064, step time: 0.1503\n",
      "epoch 142 average loss: 0.1012\n",
      "current epoch: 142 current mean dice: 0.8798 tc: 0.8837 wt: 0.9038 et: 0.8643\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 142 is: 399.2808\n",
      "----------\n",
      "epoch 143/200\n",
      "1/281, train_loss: 0.0433, step time: 0.2519\n",
      "2/281, train_loss: 0.2270, step time: 0.2535\n",
      "3/281, train_loss: 0.2291, step time: 0.2506\n",
      "4/281, train_loss: 0.0870, step time: 0.2508\n",
      "5/281, train_loss: 0.2385, step time: 0.2519\n",
      "6/281, train_loss: 0.0585, step time: 0.2701\n",
      "7/281, train_loss: 0.0917, step time: 0.2507\n",
      "8/281, train_loss: 0.1040, step time: 0.2520\n",
      "9/281, train_loss: 0.0416, step time: 0.2520\n",
      "10/281, train_loss: 0.0667, step time: 0.2550\n",
      "11/281, train_loss: 0.0598, step time: 0.2583\n",
      "12/281, train_loss: 0.1018, step time: 0.2526\n",
      "13/281, train_loss: 0.0602, step time: 0.2538\n",
      "14/281, train_loss: 0.2200, step time: 0.2541\n",
      "15/281, train_loss: 0.1035, step time: 0.2550\n",
      "16/281, train_loss: 0.0846, step time: 0.2616\n",
      "17/281, train_loss: 0.2454, step time: 0.2529\n",
      "18/281, train_loss: 0.0615, step time: 0.2538\n",
      "19/281, train_loss: 0.2392, step time: 0.2570\n",
      "20/281, train_loss: 0.0755, step time: 0.2542\n",
      "21/281, train_loss: 0.0769, step time: 0.2439\n",
      "22/281, train_loss: 0.0544, step time: 0.2464\n",
      "23/281, train_loss: 0.1049, step time: 0.2498\n",
      "24/281, train_loss: 0.0449, step time: 0.2555\n",
      "25/281, train_loss: 0.0814, step time: 0.2478\n",
      "26/281, train_loss: 0.0567, step time: 0.2496\n",
      "27/281, train_loss: 0.0708, step time: 0.2537\n",
      "28/281, train_loss: 0.0658, step time: 0.2506\n",
      "29/281, train_loss: 0.0595, step time: 0.2536\n",
      "30/281, train_loss: 0.0918, step time: 0.2517\n",
      "31/281, train_loss: 0.2225, step time: 0.2555\n",
      "32/281, train_loss: 0.0758, step time: 0.2457\n",
      "33/281, train_loss: 0.0637, step time: 0.2502\n",
      "34/281, train_loss: 0.0654, step time: 0.2485\n",
      "35/281, train_loss: 0.0891, step time: 0.2498\n",
      "36/281, train_loss: 0.0558, step time: 0.2587\n",
      "37/281, train_loss: 0.0887, step time: 0.2474\n",
      "38/281, train_loss: 0.2390, step time: 0.2506\n",
      "39/281, train_loss: 0.0594, step time: 0.2442\n",
      "40/281, train_loss: 0.0749, step time: 0.2490\n",
      "41/281, train_loss: 0.0722, step time: 0.2522\n",
      "42/281, train_loss: 0.0810, step time: 0.2527\n",
      "43/281, train_loss: 0.0701, step time: 0.2524\n",
      "44/281, train_loss: 0.0704, step time: 0.2534\n",
      "45/281, train_loss: 0.2519, step time: 0.2561\n",
      "46/281, train_loss: 0.2293, step time: 0.2565\n",
      "47/281, train_loss: 0.1255, step time: 0.2492\n",
      "48/281, train_loss: 0.0380, step time: 0.2511\n",
      "49/281, train_loss: 0.0720, step time: 0.2592\n",
      "50/281, train_loss: 0.0614, step time: 0.2553\n",
      "51/281, train_loss: 0.0511, step time: 0.2521\n",
      "52/281, train_loss: 0.1232, step time: 0.2528\n",
      "53/281, train_loss: 0.3914, step time: 0.2578\n",
      "54/281, train_loss: 0.0554, step time: 0.2570\n",
      "55/281, train_loss: 0.0369, step time: 0.2482\n",
      "56/281, train_loss: 0.1035, step time: 0.2609\n",
      "57/281, train_loss: 0.0676, step time: 0.2558\n",
      "58/281, train_loss: 0.0989, step time: 0.2564\n",
      "59/281, train_loss: 0.0900, step time: 0.2516\n",
      "60/281, train_loss: 0.0697, step time: 0.2521\n",
      "61/281, train_loss: 0.1238, step time: 0.2586\n",
      "62/281, train_loss: 0.0729, step time: 0.2522\n",
      "63/281, train_loss: 0.2484, step time: 0.2554\n",
      "64/281, train_loss: 0.0475, step time: 0.2497\n",
      "65/281, train_loss: 0.0908, step time: 0.2528\n",
      "66/281, train_loss: 0.0571, step time: 0.2530\n",
      "67/281, train_loss: 0.1006, step time: 0.2547\n",
      "68/281, train_loss: 0.0661, step time: 0.2604\n",
      "69/281, train_loss: 0.0806, step time: 0.2560\n",
      "70/281, train_loss: 0.0483, step time: 0.2514\n",
      "71/281, train_loss: 0.0905, step time: 0.2520\n",
      "72/281, train_loss: 0.0831, step time: 0.2533\n",
      "73/281, train_loss: 0.0686, step time: 0.2569\n",
      "74/281, train_loss: 0.0697, step time: 0.2534\n",
      "75/281, train_loss: 0.0809, step time: 0.2528\n",
      "76/281, train_loss: 0.1283, step time: 0.2547\n",
      "77/281, train_loss: 0.1174, step time: 0.2498\n",
      "78/281, train_loss: 0.0705, step time: 0.2498\n",
      "79/281, train_loss: 0.0681, step time: 0.2517\n",
      "80/281, train_loss: 0.0619, step time: 0.2509\n",
      "81/281, train_loss: 0.0677, step time: 0.2481\n",
      "82/281, train_loss: 0.0632, step time: 0.2545\n",
      "83/281, train_loss: 0.0659, step time: 0.2533\n",
      "84/281, train_loss: 0.2134, step time: 0.2511\n",
      "85/281, train_loss: 0.0684, step time: 0.2449\n",
      "86/281, train_loss: 0.0420, step time: 0.2588\n",
      "87/281, train_loss: 0.1210, step time: 0.2591\n",
      "88/281, train_loss: 0.0777, step time: 0.2579\n",
      "89/281, train_loss: 0.0734, step time: 0.2587\n",
      "90/281, train_loss: 0.1301, step time: 0.2508\n",
      "91/281, train_loss: 0.0630, step time: 0.2588\n",
      "92/281, train_loss: 0.0528, step time: 0.2509\n",
      "93/281, train_loss: 0.0768, step time: 0.2684\n",
      "94/281, train_loss: 0.2600, step time: 0.2503\n",
      "95/281, train_loss: 0.0614, step time: 0.2497\n",
      "96/281, train_loss: 0.0779, step time: 0.2499\n",
      "97/281, train_loss: 0.0916, step time: 0.2534\n",
      "98/281, train_loss: 0.0921, step time: 0.2468\n",
      "99/281, train_loss: 0.1040, step time: 0.2490\n",
      "100/281, train_loss: 0.2165, step time: 0.2486\n",
      "101/281, train_loss: 0.0580, step time: 0.2497\n",
      "102/281, train_loss: 0.0702, step time: 0.2556\n",
      "103/281, train_loss: 0.0661, step time: 0.2512\n",
      "104/281, train_loss: 0.0747, step time: 0.2503\n",
      "105/281, train_loss: 0.0549, step time: 0.2502\n",
      "106/281, train_loss: 0.0942, step time: 0.2556\n",
      "107/281, train_loss: 0.0822, step time: 0.2530\n",
      "108/281, train_loss: 0.1351, step time: 0.2517\n",
      "109/281, train_loss: 0.1200, step time: 0.2783\n",
      "110/281, train_loss: 0.2136, step time: 0.2529\n",
      "111/281, train_loss: 0.0824, step time: 0.2576\n",
      "112/281, train_loss: 0.0613, step time: 0.2488\n",
      "113/281, train_loss: 0.0913, step time: 0.2488\n",
      "114/281, train_loss: 0.0980, step time: 0.2496\n",
      "115/281, train_loss: 0.0717, step time: 0.2496\n",
      "116/281, train_loss: 0.0786, step time: 0.2502\n",
      "117/281, train_loss: 0.1025, step time: 0.2521\n",
      "118/281, train_loss: 0.0800, step time: 0.2495\n",
      "119/281, train_loss: 0.0713, step time: 0.2438\n",
      "120/281, train_loss: 0.0983, step time: 0.2444\n",
      "121/281, train_loss: 0.0952, step time: 0.2482\n",
      "122/281, train_loss: 0.0467, step time: 0.2524\n",
      "123/281, train_loss: 0.0396, step time: 0.2598\n",
      "124/281, train_loss: 0.0800, step time: 0.2551\n",
      "125/281, train_loss: 0.1220, step time: 0.2557\n",
      "126/281, train_loss: 0.0592, step time: 0.2485\n",
      "127/281, train_loss: 0.0728, step time: 0.2447\n",
      "128/281, train_loss: 0.0633, step time: 0.2422\n",
      "129/281, train_loss: 0.0530, step time: 0.2462\n",
      "130/281, train_loss: 0.1233, step time: 0.2490\n",
      "131/281, train_loss: 0.0514, step time: 0.2488\n",
      "132/281, train_loss: 0.0634, step time: 0.2480\n",
      "133/281, train_loss: 0.1179, step time: 0.2550\n",
      "134/281, train_loss: 0.2280, step time: 0.2472\n",
      "135/281, train_loss: 0.0739, step time: 0.2481\n",
      "136/281, train_loss: 0.1135, step time: 0.2468\n",
      "137/281, train_loss: 0.2267, step time: 0.2654\n",
      "138/281, train_loss: 0.2389, step time: 0.2541\n",
      "139/281, train_loss: 0.0936, step time: 0.2571\n",
      "140/281, train_loss: 0.0679, step time: 0.2604\n",
      "141/281, train_loss: 0.1171, step time: 0.2551\n",
      "142/281, train_loss: 0.0656, step time: 0.2622\n",
      "143/281, train_loss: 0.0530, step time: 0.2452\n",
      "144/281, train_loss: 0.0937, step time: 0.2725\n",
      "145/281, train_loss: 0.0614, step time: 0.2489\n",
      "146/281, train_loss: 0.2525, step time: 0.2500\n",
      "147/281, train_loss: 0.0716, step time: 0.2498\n",
      "148/281, train_loss: 0.0812, step time: 0.2475\n",
      "149/281, train_loss: 0.2236, step time: 0.2443\n",
      "150/281, train_loss: 0.0936, step time: 0.2489\n",
      "151/281, train_loss: 0.0617, step time: 0.2466\n",
      "152/281, train_loss: 0.2214, step time: 0.2451\n",
      "153/281, train_loss: 0.0585, step time: 0.2478\n",
      "154/281, train_loss: 0.0780, step time: 0.2461\n",
      "155/281, train_loss: 0.2280, step time: 0.2456\n",
      "156/281, train_loss: 0.0638, step time: 0.2513\n",
      "157/281, train_loss: 0.2315, step time: 0.2482\n",
      "158/281, train_loss: 0.2185, step time: 0.2512\n",
      "159/281, train_loss: 0.0672, step time: 0.2501\n",
      "160/281, train_loss: 0.0709, step time: 0.2530\n",
      "161/281, train_loss: 0.1038, step time: 0.2460\n",
      "162/281, train_loss: 0.1173, step time: 0.2517\n",
      "163/281, train_loss: 0.0666, step time: 0.2492\n",
      "164/281, train_loss: 0.0728, step time: 0.2458\n",
      "165/281, train_loss: 0.0762, step time: 0.2506\n",
      "166/281, train_loss: 0.3732, step time: 0.2487\n",
      "167/281, train_loss: 0.2132, step time: 0.2465\n",
      "168/281, train_loss: 0.0695, step time: 0.2483\n",
      "169/281, train_loss: 0.0665, step time: 0.2466\n",
      "170/281, train_loss: 0.0662, step time: 0.2483\n",
      "171/281, train_loss: 0.0618, step time: 0.2486\n",
      "172/281, train_loss: 0.0617, step time: 0.2510\n",
      "173/281, train_loss: 0.0697, step time: 0.2666\n",
      "174/281, train_loss: 0.0780, step time: 0.2555\n",
      "175/281, train_loss: 0.0671, step time: 0.2573\n",
      "176/281, train_loss: 0.0801, step time: 0.2486\n",
      "177/281, train_loss: 0.1441, step time: 0.2489\n",
      "178/281, train_loss: 0.0770, step time: 0.2482\n",
      "179/281, train_loss: 0.0596, step time: 0.2476\n",
      "180/281, train_loss: 0.0496, step time: 0.2485\n",
      "181/281, train_loss: 0.0658, step time: 0.2526\n",
      "182/281, train_loss: 0.1316, step time: 0.2506\n",
      "183/281, train_loss: 0.1249, step time: 0.2511\n",
      "184/281, train_loss: 0.0730, step time: 0.2460\n",
      "185/281, train_loss: 0.1003, step time: 0.2430\n",
      "186/281, train_loss: 0.0848, step time: 0.2535\n",
      "187/281, train_loss: 0.1785, step time: 0.2480\n",
      "188/281, train_loss: 0.0821, step time: 0.2501\n",
      "189/281, train_loss: 0.0877, step time: 0.2477\n",
      "190/281, train_loss: 0.0822, step time: 0.2535\n",
      "191/281, train_loss: 0.0870, step time: 0.2440\n",
      "192/281, train_loss: 0.2025, step time: 0.2446\n",
      "193/281, train_loss: 0.0777, step time: 0.2495\n",
      "194/281, train_loss: 0.1121, step time: 0.2470\n",
      "195/281, train_loss: 0.0936, step time: 0.2488\n",
      "196/281, train_loss: 0.1300, step time: 0.2485\n",
      "197/281, train_loss: 0.0579, step time: 0.2458\n",
      "198/281, train_loss: 0.0711, step time: 0.2513\n",
      "199/281, train_loss: 0.2122, step time: 0.2477\n",
      "200/281, train_loss: 0.0683, step time: 0.2445\n",
      "201/281, train_loss: 0.0511, step time: 0.2431\n",
      "202/281, train_loss: 0.0763, step time: 0.2512\n",
      "203/281, train_loss: 0.2705, step time: 0.2537\n",
      "204/281, train_loss: 0.0858, step time: 0.2428\n",
      "205/281, train_loss: 0.1129, step time: 0.2524\n",
      "206/281, train_loss: 0.0678, step time: 0.2510\n",
      "207/281, train_loss: 0.1015, step time: 0.2500\n",
      "208/281, train_loss: 0.0762, step time: 0.2487\n",
      "209/281, train_loss: 0.2584, step time: 0.2456\n",
      "210/281, train_loss: 0.2140, step time: 0.2497\n",
      "211/281, train_loss: 0.0710, step time: 0.2518\n",
      "212/281, train_loss: 0.1016, step time: 0.2453\n",
      "213/281, train_loss: 0.0594, step time: 0.2488\n",
      "214/281, train_loss: 0.0541, step time: 0.2501\n",
      "215/281, train_loss: 0.0734, step time: 0.2459\n",
      "216/281, train_loss: 0.0645, step time: 0.2501\n",
      "217/281, train_loss: 0.0598, step time: 0.2495\n",
      "218/281, train_loss: 0.0463, step time: 0.2600\n",
      "219/281, train_loss: 0.2360, step time: 0.2508\n",
      "220/281, train_loss: 0.1102, step time: 0.2490\n",
      "221/281, train_loss: 0.0812, step time: 0.2437\n",
      "222/281, train_loss: 0.0589, step time: 0.2501\n",
      "223/281, train_loss: 0.0806, step time: 0.2496\n",
      "224/281, train_loss: 0.0889, step time: 0.2410\n",
      "225/281, train_loss: 0.1053, step time: 0.2443\n",
      "226/281, train_loss: 0.0560, step time: 0.2516\n",
      "227/281, train_loss: 0.2192, step time: 0.2503\n",
      "228/281, train_loss: 0.0494, step time: 0.2523\n",
      "229/281, train_loss: 0.0362, step time: 0.2533\n",
      "230/281, train_loss: 0.0922, step time: 0.2446\n",
      "231/281, train_loss: 0.2216, step time: 0.2486\n",
      "232/281, train_loss: 0.2121, step time: 0.2424\n",
      "233/281, train_loss: 0.0739, step time: 0.2497\n",
      "234/281, train_loss: 0.0808, step time: 0.2478\n",
      "235/281, train_loss: 0.0555, step time: 0.2482\n",
      "236/281, train_loss: 0.0653, step time: 0.2484\n",
      "237/281, train_loss: 0.0524, step time: 0.2436\n",
      "238/281, train_loss: 0.0401, step time: 0.2519\n",
      "239/281, train_loss: 0.1044, step time: 0.2475\n",
      "240/281, train_loss: 0.0743, step time: 0.2533\n",
      "241/281, train_loss: 0.0803, step time: 0.2505\n",
      "242/281, train_loss: 0.1019, step time: 0.2481\n",
      "243/281, train_loss: 0.1048, step time: 0.2480\n",
      "244/281, train_loss: 0.0744, step time: 0.2425\n",
      "245/281, train_loss: 0.0623, step time: 0.2427\n",
      "246/281, train_loss: 0.0810, step time: 0.2485\n",
      "247/281, train_loss: 0.0578, step time: 0.2443\n",
      "248/281, train_loss: 0.0803, step time: 0.2467\n",
      "249/281, train_loss: 0.0717, step time: 0.2517\n",
      "250/281, train_loss: 0.0749, step time: 0.2456\n",
      "251/281, train_loss: 0.1004, step time: 0.2441\n",
      "252/281, train_loss: 0.0924, step time: 0.2439\n",
      "253/281, train_loss: 0.0492, step time: 0.2412\n",
      "254/281, train_loss: 0.0626, step time: 0.2443\n",
      "255/281, train_loss: 0.0606, step time: 0.2412\n",
      "256/281, train_loss: 0.0695, step time: 0.2482\n",
      "257/281, train_loss: 0.0593, step time: 0.2463\n",
      "258/281, train_loss: 0.0842, step time: 0.2418\n",
      "259/281, train_loss: 0.0647, step time: 0.2417\n",
      "260/281, train_loss: 0.2102, step time: 0.2489\n",
      "261/281, train_loss: 0.0575, step time: 0.2457\n",
      "262/281, train_loss: 0.0700, step time: 0.2399\n",
      "263/281, train_loss: 0.1216, step time: 0.2443\n",
      "264/281, train_loss: 0.0552, step time: 0.2398\n",
      "265/281, train_loss: 0.0900, step time: 0.2389\n",
      "266/281, train_loss: 0.0536, step time: 0.2426\n",
      "267/281, train_loss: 0.0637, step time: 0.2484\n",
      "268/281, train_loss: 0.0720, step time: 0.2500\n",
      "269/281, train_loss: 0.2344, step time: 0.2504\n",
      "270/281, train_loss: 0.0768, step time: 0.2513\n",
      "271/281, train_loss: 0.0655, step time: 0.2482\n",
      "272/281, train_loss: 0.0597, step time: 0.2457\n",
      "273/281, train_loss: 0.0858, step time: 0.2531\n",
      "274/281, train_loss: 0.0430, step time: 0.2472\n",
      "275/281, train_loss: 0.2429, step time: 0.2464\n",
      "276/281, train_loss: 0.2432, step time: 0.2501\n",
      "277/281, train_loss: 0.0592, step time: 0.2466\n",
      "278/281, train_loss: 0.0569, step time: 0.2465\n",
      "279/281, train_loss: 0.2481, step time: 0.2491\n",
      "280/281, train_loss: 0.0664, step time: 0.2465\n",
      "281/281, train_loss: 0.0787, step time: 0.2440\n",
      "282/281, train_loss: 0.0716, step time: 0.1507\n",
      "epoch 143 average loss: 0.1006\n",
      "current epoch: 143 current mean dice: 0.8816 tc: 0.8819 wt: 0.9027 et: 0.8736\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 143 is: 373.9738\n",
      "----------\n",
      "epoch 144/200\n",
      "1/281, train_loss: 0.2249, step time: 0.2496\n",
      "2/281, train_loss: 0.0594, step time: 0.2479\n",
      "3/281, train_loss: 0.0610, step time: 0.2455\n",
      "4/281, train_loss: 0.0718, step time: 0.2465\n",
      "5/281, train_loss: 0.0595, step time: 0.2446\n",
      "6/281, train_loss: 0.0581, step time: 0.2438\n",
      "7/281, train_loss: 0.2252, step time: 0.2430\n",
      "8/281, train_loss: 0.2324, step time: 0.2483\n",
      "9/281, train_loss: 0.0767, step time: 0.2460\n",
      "10/281, train_loss: 0.0876, step time: 0.2428\n",
      "11/281, train_loss: 0.0761, step time: 0.2413\n",
      "12/281, train_loss: 0.0544, step time: 0.2442\n",
      "13/281, train_loss: 0.0557, step time: 0.2484\n",
      "14/281, train_loss: 0.0664, step time: 0.2470\n",
      "15/281, train_loss: 0.0845, step time: 0.2435\n",
      "16/281, train_loss: 0.2218, step time: 0.2471\n",
      "17/281, train_loss: 0.0436, step time: 0.2425\n",
      "18/281, train_loss: 0.0785, step time: 0.2405\n",
      "19/281, train_loss: 0.0756, step time: 0.2379\n",
      "20/281, train_loss: 0.0761, step time: 0.2448\n",
      "21/281, train_loss: 0.0977, step time: 0.2417\n",
      "22/281, train_loss: 0.1002, step time: 0.2471\n",
      "23/281, train_loss: 0.0450, step time: 0.2514\n",
      "24/281, train_loss: 0.0468, step time: 0.2519\n",
      "25/281, train_loss: 0.0692, step time: 0.2515\n",
      "26/281, train_loss: 0.0585, step time: 0.2384\n",
      "27/281, train_loss: 0.0659, step time: 0.2421\n",
      "28/281, train_loss: 0.0643, step time: 0.2453\n",
      "29/281, train_loss: 0.2118, step time: 0.2477\n",
      "30/281, train_loss: 0.0766, step time: 0.2429\n",
      "31/281, train_loss: 0.0750, step time: 0.2404\n",
      "32/281, train_loss: 0.0739, step time: 0.2489\n",
      "33/281, train_loss: 0.0684, step time: 0.2459\n",
      "34/281, train_loss: 0.0917, step time: 0.2448\n",
      "35/281, train_loss: 0.0715, step time: 0.2609\n",
      "36/281, train_loss: 0.0854, step time: 0.2518\n",
      "37/281, train_loss: 0.1044, step time: 0.2546\n",
      "38/281, train_loss: 0.0648, step time: 0.2377\n",
      "39/281, train_loss: 0.0411, step time: 0.2405\n",
      "40/281, train_loss: 0.2240, step time: 0.2441\n",
      "41/281, train_loss: 0.2404, step time: 0.2500\n",
      "42/281, train_loss: 0.0922, step time: 0.2489\n",
      "43/281, train_loss: 0.0829, step time: 0.2454\n",
      "44/281, train_loss: 0.0850, step time: 0.2402\n",
      "45/281, train_loss: 0.0791, step time: 0.2416\n",
      "46/281, train_loss: 0.0553, step time: 0.2428\n",
      "47/281, train_loss: 0.1121, step time: 0.2429\n",
      "48/281, train_loss: 0.0665, step time: 0.2435\n",
      "49/281, train_loss: 0.0891, step time: 0.2463\n",
      "50/281, train_loss: 0.0552, step time: 0.2438\n",
      "51/281, train_loss: 0.0861, step time: 0.2427\n",
      "52/281, train_loss: 0.0955, step time: 0.2461\n",
      "53/281, train_loss: 0.1010, step time: 0.2446\n",
      "54/281, train_loss: 0.2160, step time: 0.2496\n",
      "55/281, train_loss: 0.0853, step time: 0.2525\n",
      "56/281, train_loss: 0.0591, step time: 0.2434\n",
      "57/281, train_loss: 0.0836, step time: 0.2437\n",
      "58/281, train_loss: 0.0435, step time: 0.2516\n",
      "59/281, train_loss: 0.0870, step time: 0.2504\n",
      "60/281, train_loss: 0.0732, step time: 0.2454\n",
      "61/281, train_loss: 0.0711, step time: 0.2451\n",
      "62/281, train_loss: 0.0599, step time: 0.2535\n",
      "63/281, train_loss: 0.0513, step time: 0.2608\n",
      "64/281, train_loss: 0.0740, step time: 0.2495\n",
      "65/281, train_loss: 0.0805, step time: 0.2493\n",
      "66/281, train_loss: 0.0791, step time: 0.2441\n",
      "67/281, train_loss: 0.0646, step time: 0.2451\n",
      "68/281, train_loss: 0.0577, step time: 0.2506\n",
      "69/281, train_loss: 0.0919, step time: 0.2531\n",
      "70/281, train_loss: 0.0511, step time: 0.2506\n",
      "71/281, train_loss: 0.2380, step time: 0.2444\n",
      "72/281, train_loss: 0.1232, step time: 0.2463\n",
      "73/281, train_loss: 0.0552, step time: 0.2455\n",
      "74/281, train_loss: 0.0754, step time: 0.2484\n",
      "75/281, train_loss: 0.0766, step time: 0.2520\n",
      "76/281, train_loss: 0.0568, step time: 0.2428\n",
      "77/281, train_loss: 0.2097, step time: 0.2477\n",
      "78/281, train_loss: 0.2392, step time: 0.2447\n",
      "79/281, train_loss: 0.2097, step time: 0.2469\n",
      "80/281, train_loss: 0.0602, step time: 0.2492\n",
      "81/281, train_loss: 0.0720, step time: 0.2442\n",
      "82/281, train_loss: 0.0802, step time: 0.2417\n",
      "83/281, train_loss: 0.2539, step time: 0.2461\n",
      "84/281, train_loss: 0.0831, step time: 0.2447\n",
      "85/281, train_loss: 0.1196, step time: 0.2492\n",
      "86/281, train_loss: 0.2299, step time: 0.2471\n",
      "87/281, train_loss: 0.1021, step time: 0.2538\n",
      "88/281, train_loss: 0.0470, step time: 0.2511\n",
      "89/281, train_loss: 0.0560, step time: 0.2437\n",
      "90/281, train_loss: 0.0761, step time: 0.2477\n",
      "91/281, train_loss: 0.0841, step time: 0.2456\n",
      "92/281, train_loss: 0.2181, step time: 0.2397\n",
      "93/281, train_loss: 0.0721, step time: 0.2411\n",
      "94/281, train_loss: 0.0627, step time: 0.2423\n",
      "95/281, train_loss: 0.2504, step time: 0.2531\n",
      "96/281, train_loss: 0.2650, step time: 0.2427\n",
      "97/281, train_loss: 0.0540, step time: 0.2417\n",
      "98/281, train_loss: 0.2219, step time: 0.2499\n",
      "99/281, train_loss: 0.2759, step time: 0.2465\n",
      "100/281, train_loss: 0.0877, step time: 0.2436\n",
      "101/281, train_loss: 0.0503, step time: 0.2445\n",
      "102/281, train_loss: 0.0914, step time: 0.2411\n",
      "103/281, train_loss: 0.0750, step time: 0.2436\n",
      "104/281, train_loss: 0.0663, step time: 0.2411\n",
      "105/281, train_loss: 0.0931, step time: 0.2489\n",
      "106/281, train_loss: 0.0894, step time: 0.2470\n",
      "107/281, train_loss: 0.2337, step time: 0.2418\n",
      "108/281, train_loss: 0.0797, step time: 0.2469\n",
      "109/281, train_loss: 0.2374, step time: 0.2461\n",
      "110/281, train_loss: 0.0709, step time: 0.2475\n",
      "111/281, train_loss: 0.0817, step time: 0.2460\n",
      "112/281, train_loss: 0.0642, step time: 0.2505\n",
      "113/281, train_loss: 0.1013, step time: 0.2471\n",
      "114/281, train_loss: 0.0628, step time: 0.2467\n",
      "115/281, train_loss: 0.1070, step time: 0.2440\n",
      "116/281, train_loss: 0.0730, step time: 0.2500\n",
      "117/281, train_loss: 0.2166, step time: 0.2483\n",
      "118/281, train_loss: 0.1033, step time: 0.2444\n",
      "119/281, train_loss: 0.0720, step time: 0.2426\n",
      "120/281, train_loss: 0.0710, step time: 0.2484\n",
      "121/281, train_loss: 0.0936, step time: 0.2529\n",
      "122/281, train_loss: 0.1215, step time: 0.2488\n",
      "123/281, train_loss: 0.0632, step time: 0.2473\n",
      "124/281, train_loss: 0.2265, step time: 0.2489\n",
      "125/281, train_loss: 0.0710, step time: 0.2477\n",
      "126/281, train_loss: 0.0997, step time: 0.2487\n",
      "127/281, train_loss: 0.1044, step time: 0.2502\n",
      "128/281, train_loss: 0.0955, step time: 0.2488\n",
      "129/281, train_loss: 0.0666, step time: 0.2462\n",
      "130/281, train_loss: 0.0831, step time: 0.2498\n",
      "131/281, train_loss: 0.0678, step time: 0.2498\n",
      "132/281, train_loss: 0.0905, step time: 0.2552\n",
      "133/281, train_loss: 0.0786, step time: 0.2481\n",
      "134/281, train_loss: 0.0892, step time: 0.2464\n",
      "135/281, train_loss: 0.0762, step time: 0.2471\n",
      "136/281, train_loss: 0.0986, step time: 0.2474\n",
      "137/281, train_loss: 0.0859, step time: 0.2492\n",
      "138/281, train_loss: 0.0551, step time: 0.2461\n",
      "139/281, train_loss: 0.0540, step time: 0.2487\n",
      "140/281, train_loss: 0.1184, step time: 0.2444\n",
      "141/281, train_loss: 0.2428, step time: 0.2523\n",
      "142/281, train_loss: 0.0525, step time: 0.2461\n",
      "143/281, train_loss: 0.0643, step time: 0.2433\n",
      "144/281, train_loss: 0.2270, step time: 0.2470\n",
      "145/281, train_loss: 0.0930, step time: 0.2463\n",
      "146/281, train_loss: 0.0897, step time: 0.2425\n",
      "147/281, train_loss: 0.0928, step time: 0.2428\n",
      "148/281, train_loss: 0.0831, step time: 0.2407\n",
      "149/281, train_loss: 0.0728, step time: 0.2464\n",
      "150/281, train_loss: 0.2341, step time: 0.2513\n",
      "151/281, train_loss: 0.0688, step time: 0.2486\n",
      "152/281, train_loss: 0.2154, step time: 0.2499\n",
      "153/281, train_loss: 0.0517, step time: 0.2438\n",
      "154/281, train_loss: 0.0667, step time: 0.2497\n",
      "155/281, train_loss: 0.1100, step time: 0.2462\n",
      "156/281, train_loss: 0.0448, step time: 0.2468\n",
      "157/281, train_loss: 0.2171, step time: 0.2482\n",
      "158/281, train_loss: 0.2411, step time: 0.2512\n",
      "159/281, train_loss: 0.0611, step time: 0.2474\n",
      "160/281, train_loss: 0.0519, step time: 0.2480\n",
      "161/281, train_loss: 0.2596, step time: 0.2526\n",
      "162/281, train_loss: 0.0499, step time: 0.2476\n",
      "163/281, train_loss: 0.0718, step time: 0.2455\n",
      "164/281, train_loss: 0.0886, step time: 0.2529\n",
      "165/281, train_loss: 0.0856, step time: 0.2480\n",
      "166/281, train_loss: 0.2236, step time: 0.2506\n",
      "167/281, train_loss: 0.1003, step time: 0.2460\n",
      "168/281, train_loss: 0.0470, step time: 0.2443\n",
      "169/281, train_loss: 0.2466, step time: 0.2493\n",
      "170/281, train_loss: 0.0707, step time: 0.2526\n",
      "171/281, train_loss: 0.0763, step time: 0.2519\n",
      "172/281, train_loss: 0.0675, step time: 0.2500\n",
      "173/281, train_loss: 0.0685, step time: 0.2552\n",
      "174/281, train_loss: 0.0849, step time: 0.2592\n",
      "175/281, train_loss: 0.0815, step time: 0.2501\n",
      "176/281, train_loss: 0.1011, step time: 0.2515\n",
      "177/281, train_loss: 0.0690, step time: 0.2552\n",
      "178/281, train_loss: 0.2190, step time: 0.2528\n",
      "179/281, train_loss: 0.0559, step time: 0.2483\n",
      "180/281, train_loss: 0.0967, step time: 0.2533\n",
      "181/281, train_loss: 0.0632, step time: 0.2539\n",
      "182/281, train_loss: 0.1576, step time: 0.2536\n",
      "183/281, train_loss: 0.0518, step time: 0.2503\n",
      "184/281, train_loss: 0.0941, step time: 0.2473\n",
      "185/281, train_loss: 0.0766, step time: 0.2491\n",
      "186/281, train_loss: 0.0590, step time: 0.2480\n",
      "187/281, train_loss: 0.0720, step time: 0.2458\n",
      "188/281, train_loss: 0.2205, step time: 0.2489\n",
      "189/281, train_loss: 0.1498, step time: 0.2563\n",
      "190/281, train_loss: 0.2495, step time: 0.2512\n",
      "191/281, train_loss: 0.0671, step time: 0.2495\n",
      "192/281, train_loss: 0.0740, step time: 0.2560\n",
      "193/281, train_loss: 0.0554, step time: 0.2500\n",
      "194/281, train_loss: 0.0847, step time: 0.2491\n",
      "195/281, train_loss: 0.2162, step time: 0.2526\n",
      "196/281, train_loss: 0.0685, step time: 0.2548\n",
      "197/281, train_loss: 0.0661, step time: 0.2506\n",
      "198/281, train_loss: 0.0579, step time: 0.2515\n",
      "199/281, train_loss: 0.1107, step time: 0.2536\n",
      "200/281, train_loss: 0.1569, step time: 0.2578\n",
      "201/281, train_loss: 0.1070, step time: 0.2532\n",
      "202/281, train_loss: 0.0567, step time: 0.2520\n",
      "203/281, train_loss: 0.0841, step time: 0.2466\n",
      "204/281, train_loss: 0.0976, step time: 0.2507\n",
      "205/281, train_loss: 0.0596, step time: 0.2540\n",
      "206/281, train_loss: 0.0393, step time: 0.2523\n",
      "207/281, train_loss: 0.0618, step time: 0.2591\n",
      "208/281, train_loss: 0.0462, step time: 0.2565\n",
      "209/281, train_loss: 0.0993, step time: 0.2528\n",
      "210/281, train_loss: 0.0784, step time: 0.2496\n",
      "211/281, train_loss: 0.0613, step time: 0.2512\n",
      "212/281, train_loss: 0.0915, step time: 0.2475\n",
      "213/281, train_loss: 0.0732, step time: 0.2474\n",
      "214/281, train_loss: 0.0604, step time: 0.2465\n",
      "215/281, train_loss: 0.0975, step time: 0.2506\n",
      "216/281, train_loss: 0.0754, step time: 0.2496\n",
      "217/281, train_loss: 0.1372, step time: 0.2491\n",
      "218/281, train_loss: 0.0623, step time: 0.2492\n",
      "219/281, train_loss: 0.1152, step time: 0.2464\n",
      "220/281, train_loss: 0.0549, step time: 0.2482\n",
      "221/281, train_loss: 0.0611, step time: 0.2538\n",
      "222/281, train_loss: 0.1119, step time: 0.2493\n",
      "223/281, train_loss: 0.0941, step time: 0.2463\n",
      "224/281, train_loss: 0.0533, step time: 0.2494\n",
      "225/281, train_loss: 0.0738, step time: 0.2524\n",
      "226/281, train_loss: 0.0558, step time: 0.2655\n",
      "227/281, train_loss: 0.0703, step time: 0.2604\n",
      "228/281, train_loss: 0.2613, step time: 0.2579\n",
      "229/281, train_loss: 0.1056, step time: 0.2576\n",
      "230/281, train_loss: 0.2041, step time: 0.2491\n",
      "231/281, train_loss: 0.1323, step time: 0.2524\n",
      "232/281, train_loss: 0.0607, step time: 0.2467\n",
      "233/281, train_loss: 0.2660, step time: 0.2543\n",
      "234/281, train_loss: 0.0721, step time: 0.2516\n",
      "235/281, train_loss: 0.0617, step time: 0.2494\n",
      "236/281, train_loss: 0.1025, step time: 0.2504\n",
      "237/281, train_loss: 0.0722, step time: 0.2501\n",
      "238/281, train_loss: 0.0924, step time: 0.2523\n",
      "239/281, train_loss: 0.0872, step time: 0.2547\n",
      "240/281, train_loss: 0.0725, step time: 0.2494\n",
      "241/281, train_loss: 0.2803, step time: 0.2707\n",
      "242/281, train_loss: 0.0791, step time: 0.2701\n",
      "243/281, train_loss: 0.0788, step time: 0.2526\n",
      "244/281, train_loss: 0.0617, step time: 0.2532\n",
      "245/281, train_loss: 0.1219, step time: 0.2504\n",
      "246/281, train_loss: 0.0849, step time: 0.2536\n",
      "247/281, train_loss: 0.1263, step time: 0.2525\n",
      "248/281, train_loss: 0.0823, step time: 0.2535\n",
      "249/281, train_loss: 0.0698, step time: 0.2486\n",
      "250/281, train_loss: 0.0600, step time: 0.2534\n",
      "251/281, train_loss: 0.2981, step time: 0.2562\n",
      "252/281, train_loss: 0.0670, step time: 0.2586\n",
      "253/281, train_loss: 0.0680, step time: 0.2498\n",
      "254/281, train_loss: 0.0565, step time: 0.2483\n",
      "255/281, train_loss: 0.1133, step time: 0.2491\n",
      "256/281, train_loss: 0.0607, step time: 0.2517\n",
      "257/281, train_loss: 0.0679, step time: 0.2540\n",
      "258/281, train_loss: 0.2302, step time: 0.2544\n",
      "259/281, train_loss: 0.0668, step time: 0.2480\n",
      "260/281, train_loss: 0.0628, step time: 0.2497\n",
      "261/281, train_loss: 0.0575, step time: 0.2495\n",
      "262/281, train_loss: 0.0696, step time: 0.2487\n",
      "263/281, train_loss: 0.0548, step time: 0.2605\n",
      "264/281, train_loss: 0.0784, step time: 0.2465\n",
      "265/281, train_loss: 0.0638, step time: 0.2527\n",
      "266/281, train_loss: 0.1212, step time: 0.2480\n",
      "267/281, train_loss: 0.0774, step time: 0.2523\n",
      "268/281, train_loss: 0.0446, step time: 0.2481\n",
      "269/281, train_loss: 0.0665, step time: 0.2594\n",
      "270/281, train_loss: 0.1211, step time: 0.2490\n",
      "271/281, train_loss: 0.0567, step time: 0.2513\n",
      "272/281, train_loss: 0.0901, step time: 0.2489\n",
      "273/281, train_loss: 0.0479, step time: 0.2533\n",
      "274/281, train_loss: 0.2434, step time: 0.2554\n",
      "275/281, train_loss: 0.0750, step time: 0.2542\n",
      "276/281, train_loss: 0.0710, step time: 0.2488\n",
      "277/281, train_loss: 0.0843, step time: 0.2537\n",
      "278/281, train_loss: 0.0742, step time: 0.2546\n",
      "279/281, train_loss: 0.0674, step time: 0.2514\n",
      "280/281, train_loss: 0.0877, step time: 0.2539\n",
      "281/281, train_loss: 0.0818, step time: 0.2532\n",
      "282/281, train_loss: 0.0578, step time: 0.1510\n",
      "epoch 144 average loss: 0.1012\n",
      "current epoch: 144 current mean dice: 0.8763 tc: 0.8819 wt: 0.8933 et: 0.8675\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 144 is: 410.4425\n",
      "----------\n",
      "epoch 145/200\n",
      "1/281, train_loss: 0.0313, step time: 0.2611\n",
      "2/281, train_loss: 0.0823, step time: 0.2530\n",
      "3/281, train_loss: 0.2213, step time: 0.2500\n",
      "4/281, train_loss: 0.1048, step time: 0.2508\n",
      "5/281, train_loss: 0.0420, step time: 0.2554\n",
      "6/281, train_loss: 0.0447, step time: 0.2508\n",
      "7/281, train_loss: 0.1316, step time: 0.2482\n",
      "8/281, train_loss: 0.0546, step time: 0.2547\n",
      "9/281, train_loss: 0.0719, step time: 0.2536\n",
      "10/281, train_loss: 0.2383, step time: 0.2513\n",
      "11/281, train_loss: 0.0796, step time: 0.2574\n",
      "12/281, train_loss: 0.0986, step time: 0.2501\n",
      "13/281, train_loss: 0.0754, step time: 0.2532\n",
      "14/281, train_loss: 0.1271, step time: 0.2490\n",
      "15/281, train_loss: 0.2496, step time: 0.2502\n",
      "16/281, train_loss: 0.0647, step time: 0.2487\n",
      "17/281, train_loss: 0.0945, step time: 0.2490\n",
      "18/281, train_loss: 0.0602, step time: 0.2504\n",
      "19/281, train_loss: 0.0910, step time: 0.2583\n",
      "20/281, train_loss: 0.0550, step time: 0.2473\n",
      "21/281, train_loss: 0.0619, step time: 0.2489\n",
      "22/281, train_loss: 0.1254, step time: 0.2499\n",
      "23/281, train_loss: 0.0724, step time: 0.2481\n",
      "24/281, train_loss: 0.0726, step time: 0.2505\n",
      "25/281, train_loss: 0.0508, step time: 0.2489\n",
      "26/281, train_loss: 0.0858, step time: 0.2394\n",
      "27/281, train_loss: 0.0978, step time: 0.2470\n",
      "28/281, train_loss: 0.0904, step time: 0.2481\n",
      "29/281, train_loss: 0.0692, step time: 0.2495\n",
      "30/281, train_loss: 0.2192, step time: 0.2530\n",
      "31/281, train_loss: 0.0398, step time: 0.2496\n",
      "32/281, train_loss: 0.0951, step time: 0.2510\n",
      "33/281, train_loss: 0.0362, step time: 0.2515\n",
      "34/281, train_loss: 0.0566, step time: 0.2508\n",
      "35/281, train_loss: 0.0640, step time: 0.2520\n",
      "36/281, train_loss: 0.0893, step time: 0.2480\n",
      "37/281, train_loss: 0.0829, step time: 0.2536\n",
      "38/281, train_loss: 0.0530, step time: 0.2526\n",
      "39/281, train_loss: 0.1110, step time: 0.2503\n",
      "40/281, train_loss: 0.0582, step time: 0.2538\n",
      "41/281, train_loss: 0.2391, step time: 0.2509\n",
      "42/281, train_loss: 0.0762, step time: 0.2552\n",
      "43/281, train_loss: 0.0897, step time: 0.2542\n",
      "44/281, train_loss: 0.1035, step time: 0.2514\n",
      "45/281, train_loss: 0.0645, step time: 0.2464\n",
      "46/281, train_loss: 0.0803, step time: 0.2511\n",
      "47/281, train_loss: 0.0884, step time: 0.2583\n",
      "48/281, train_loss: 0.0694, step time: 0.2816\n",
      "49/281, train_loss: 0.0536, step time: 0.2629\n",
      "50/281, train_loss: 0.0716, step time: 0.2564\n",
      "51/281, train_loss: 0.0544, step time: 0.2492\n",
      "52/281, train_loss: 0.0855, step time: 0.2535\n",
      "53/281, train_loss: 0.2422, step time: 0.2481\n",
      "54/281, train_loss: 0.2430, step time: 0.2491\n",
      "55/281, train_loss: 0.0667, step time: 0.2535\n",
      "56/281, train_loss: 0.0817, step time: 0.2506\n",
      "57/281, train_loss: 0.0738, step time: 0.2528\n",
      "58/281, train_loss: 0.2418, step time: 0.2515\n",
      "59/281, train_loss: 0.0691, step time: 0.2484\n",
      "60/281, train_loss: 0.0479, step time: 0.2453\n",
      "61/281, train_loss: 0.0919, step time: 0.2519\n",
      "62/281, train_loss: 0.2116, step time: 0.2548\n",
      "63/281, train_loss: 0.0562, step time: 0.2556\n",
      "64/281, train_loss: 0.0566, step time: 0.2594\n",
      "65/281, train_loss: 0.2398, step time: 0.2560\n",
      "66/281, train_loss: 0.0509, step time: 0.2550\n",
      "67/281, train_loss: 0.2334, step time: 0.2491\n",
      "68/281, train_loss: 0.0882, step time: 0.2499\n",
      "69/281, train_loss: 0.0674, step time: 0.2569\n",
      "70/281, train_loss: 0.0615, step time: 0.2632\n",
      "71/281, train_loss: 0.1037, step time: 0.2543\n",
      "72/281, train_loss: 0.0863, step time: 0.2574\n",
      "73/281, train_loss: 0.0371, step time: 0.2505\n",
      "74/281, train_loss: 0.0747, step time: 0.2498\n",
      "75/281, train_loss: 0.0721, step time: 0.2492\n",
      "76/281, train_loss: 0.2445, step time: 0.2533\n",
      "77/281, train_loss: 0.1051, step time: 0.2523\n",
      "78/281, train_loss: 0.1260, step time: 0.2558\n",
      "79/281, train_loss: 0.0515, step time: 0.2527\n",
      "80/281, train_loss: 0.0845, step time: 0.2539\n",
      "81/281, train_loss: 0.0865, step time: 0.2520\n",
      "82/281, train_loss: 0.0682, step time: 0.2535\n",
      "83/281, train_loss: 0.0605, step time: 0.2554\n",
      "84/281, train_loss: 0.0932, step time: 0.2517\n",
      "85/281, train_loss: 0.0603, step time: 0.2493\n",
      "86/281, train_loss: 0.0591, step time: 0.2516\n",
      "87/281, train_loss: 0.0588, step time: 0.2567\n",
      "88/281, train_loss: 0.1194, step time: 0.2548\n",
      "89/281, train_loss: 0.0928, step time: 0.2546\n",
      "90/281, train_loss: 0.0869, step time: 0.2524\n",
      "91/281, train_loss: 0.0777, step time: 0.2529\n",
      "92/281, train_loss: 0.1014, step time: 0.2503\n",
      "93/281, train_loss: 0.0849, step time: 0.2588\n",
      "94/281, train_loss: 0.0712, step time: 0.2483\n",
      "95/281, train_loss: 0.0438, step time: 0.2547\n",
      "96/281, train_loss: 0.0758, step time: 0.2511\n",
      "97/281, train_loss: 0.1544, step time: 0.2554\n",
      "98/281, train_loss: 0.1242, step time: 0.2559\n",
      "99/281, train_loss: 0.0732, step time: 0.2540\n",
      "100/281, train_loss: 0.0412, step time: 0.2533\n",
      "101/281, train_loss: 0.2382, step time: 0.2558\n",
      "102/281, train_loss: 0.2361, step time: 0.2525\n",
      "103/281, train_loss: 0.0648, step time: 0.2572\n",
      "104/281, train_loss: 0.0326, step time: 0.2513\n",
      "105/281, train_loss: 0.0467, step time: 0.2514\n",
      "106/281, train_loss: 0.0802, step time: 0.2514\n",
      "107/281, train_loss: 0.0749, step time: 0.2502\n",
      "108/281, train_loss: 0.2124, step time: 0.2516\n",
      "109/281, train_loss: 0.0830, step time: 0.2513\n",
      "110/281, train_loss: 0.0757, step time: 0.2524\n",
      "111/281, train_loss: 0.0816, step time: 0.2496\n",
      "112/281, train_loss: 0.0740, step time: 0.2534\n",
      "113/281, train_loss: 0.0943, step time: 0.2557\n",
      "114/281, train_loss: 0.0702, step time: 0.2570\n",
      "115/281, train_loss: 0.0665, step time: 0.2559\n",
      "116/281, train_loss: 0.0763, step time: 0.2522\n",
      "117/281, train_loss: 0.2710, step time: 0.2581\n",
      "118/281, train_loss: 0.0580, step time: 0.2594\n",
      "119/281, train_loss: 0.0985, step time: 0.2550\n",
      "120/281, train_loss: 0.0626, step time: 0.2516\n",
      "121/281, train_loss: 0.0788, step time: 0.2568\n",
      "122/281, train_loss: 0.2260, step time: 0.2506\n",
      "123/281, train_loss: 0.0839, step time: 0.2516\n",
      "124/281, train_loss: 0.0829, step time: 0.2658\n",
      "125/281, train_loss: 0.0570, step time: 0.2604\n",
      "126/281, train_loss: 0.0617, step time: 0.2516\n",
      "127/281, train_loss: 0.0552, step time: 0.2524\n",
      "128/281, train_loss: 0.0595, step time: 0.2576\n",
      "129/281, train_loss: 0.0476, step time: 0.2560\n",
      "130/281, train_loss: 0.0673, step time: 0.2525\n",
      "131/281, train_loss: 0.0696, step time: 0.2528\n",
      "132/281, train_loss: 0.0504, step time: 0.2512\n",
      "133/281, train_loss: 0.0515, step time: 0.2584\n",
      "134/281, train_loss: 0.0953, step time: 0.2506\n",
      "135/281, train_loss: 0.0404, step time: 0.2526\n",
      "136/281, train_loss: 0.0431, step time: 0.2554\n",
      "137/281, train_loss: 0.2166, step time: 0.2513\n",
      "138/281, train_loss: 0.0467, step time: 0.2503\n",
      "139/281, train_loss: 0.0843, step time: 0.2523\n",
      "140/281, train_loss: 0.0763, step time: 0.2552\n",
      "141/281, train_loss: 0.0802, step time: 0.2506\n",
      "142/281, train_loss: 0.0830, step time: 0.2503\n",
      "143/281, train_loss: 0.0686, step time: 0.2484\n",
      "144/281, train_loss: 0.1093, step time: 0.2578\n",
      "145/281, train_loss: 0.1081, step time: 0.2506\n",
      "146/281, train_loss: 0.0503, step time: 0.2524\n",
      "147/281, train_loss: 0.0789, step time: 0.2513\n",
      "148/281, train_loss: 0.0536, step time: 0.2540\n",
      "149/281, train_loss: 0.2052, step time: 0.2517\n",
      "150/281, train_loss: 0.0645, step time: 0.2542\n",
      "151/281, train_loss: 0.0526, step time: 0.2552\n",
      "152/281, train_loss: 0.0814, step time: 0.2539\n",
      "153/281, train_loss: 0.1018, step time: 0.2553\n",
      "154/281, train_loss: 0.0515, step time: 0.2536\n",
      "155/281, train_loss: 0.2312, step time: 0.2545\n",
      "156/281, train_loss: 0.0745, step time: 0.2519\n",
      "157/281, train_loss: 0.0946, step time: 0.2486\n",
      "158/281, train_loss: 0.1161, step time: 0.2568\n",
      "159/281, train_loss: 0.0582, step time: 0.2513\n",
      "160/281, train_loss: 0.0757, step time: 0.2525\n",
      "161/281, train_loss: 0.1123, step time: 0.2495\n",
      "162/281, train_loss: 0.0884, step time: 0.2454\n",
      "163/281, train_loss: 0.0467, step time: 0.2524\n",
      "164/281, train_loss: 0.0873, step time: 0.2593\n",
      "165/281, train_loss: 0.0642, step time: 0.2569\n",
      "166/281, train_loss: 0.0507, step time: 0.2491\n",
      "167/281, train_loss: 0.1307, step time: 0.2465\n",
      "168/281, train_loss: 0.2276, step time: 0.2525\n",
      "169/281, train_loss: 0.0640, step time: 0.2454\n",
      "170/281, train_loss: 0.0971, step time: 0.2454\n",
      "171/281, train_loss: 0.0617, step time: 0.2487\n",
      "172/281, train_loss: 0.0749, step time: 0.2505\n",
      "173/281, train_loss: 0.0580, step time: 0.2484\n",
      "174/281, train_loss: 0.0704, step time: 0.2545\n",
      "175/281, train_loss: 0.0748, step time: 0.2516\n",
      "176/281, train_loss: 0.0866, step time: 0.2489\n",
      "177/281, train_loss: 0.0717, step time: 0.2479\n",
      "178/281, train_loss: 0.0742, step time: 0.2527\n",
      "179/281, train_loss: 0.1001, step time: 0.2537\n",
      "180/281, train_loss: 0.0930, step time: 0.2538\n",
      "181/281, train_loss: 0.0803, step time: 0.2495\n",
      "182/281, train_loss: 0.2096, step time: 0.2473\n",
      "183/281, train_loss: 0.0474, step time: 0.2476\n",
      "184/281, train_loss: 0.1048, step time: 0.2496\n",
      "185/281, train_loss: 0.2271, step time: 0.2478\n",
      "186/281, train_loss: 0.0435, step time: 0.2458\n",
      "187/281, train_loss: 0.0759, step time: 0.2508\n",
      "188/281, train_loss: 0.1058, step time: 0.2501\n",
      "189/281, train_loss: 0.2263, step time: 0.2498\n",
      "190/281, train_loss: 0.0571, step time: 0.2448\n",
      "191/281, train_loss: 0.0713, step time: 0.2466\n",
      "192/281, train_loss: 0.0501, step time: 0.2516\n",
      "193/281, train_loss: 0.2355, step time: 0.2485\n",
      "194/281, train_loss: 0.0843, step time: 0.2526\n",
      "195/281, train_loss: 0.1161, step time: 0.2485\n",
      "196/281, train_loss: 0.2388, step time: 0.2512\n",
      "197/281, train_loss: 0.0870, step time: 0.2532\n",
      "198/281, train_loss: 0.0620, step time: 0.2458\n",
      "199/281, train_loss: 0.0547, step time: 0.2487\n",
      "200/281, train_loss: 0.0762, step time: 0.2479\n",
      "201/281, train_loss: 0.0635, step time: 0.2510\n",
      "202/281, train_loss: 0.1180, step time: 0.2525\n",
      "203/281, train_loss: 0.0532, step time: 0.2487\n",
      "204/281, train_loss: 0.2299, step time: 0.2493\n",
      "205/281, train_loss: 0.0839, step time: 0.2530\n",
      "206/281, train_loss: 0.0561, step time: 0.2484\n",
      "207/281, train_loss: 0.0643, step time: 0.2519\n",
      "208/281, train_loss: 0.0874, step time: 0.2489\n",
      "209/281, train_loss: 0.0525, step time: 0.2484\n",
      "210/281, train_loss: 0.0764, step time: 0.2543\n",
      "211/281, train_loss: 0.0744, step time: 0.2573\n",
      "212/281, train_loss: 0.0632, step time: 0.2481\n",
      "213/281, train_loss: 0.0719, step time: 0.2533\n",
      "214/281, train_loss: 0.0573, step time: 0.2502\n",
      "215/281, train_loss: 0.2273, step time: 0.2531\n",
      "216/281, train_loss: 0.2266, step time: 0.2533\n",
      "217/281, train_loss: 0.0637, step time: 0.2474\n",
      "218/281, train_loss: 0.1017, step time: 0.2484\n",
      "219/281, train_loss: 0.3979, step time: 0.2477\n",
      "220/281, train_loss: 0.1150, step time: 0.2572\n",
      "221/281, train_loss: 0.0787, step time: 0.2475\n",
      "222/281, train_loss: 0.0506, step time: 0.2513\n",
      "223/281, train_loss: 0.2319, step time: 0.2470\n",
      "224/281, train_loss: 0.0870, step time: 0.2499\n",
      "225/281, train_loss: 0.0551, step time: 0.2512\n",
      "226/281, train_loss: 0.0617, step time: 0.2527\n",
      "227/281, train_loss: 0.0714, step time: 0.2520\n",
      "228/281, train_loss: 0.0644, step time: 0.2485\n",
      "229/281, train_loss: 0.0609, step time: 0.2494\n",
      "230/281, train_loss: 0.0629, step time: 0.2485\n",
      "231/281, train_loss: 0.0765, step time: 0.2519\n",
      "232/281, train_loss: 0.0625, step time: 0.2488\n",
      "233/281, train_loss: 0.0449, step time: 0.2491\n",
      "234/281, train_loss: 0.0791, step time: 0.2517\n",
      "235/281, train_loss: 0.0635, step time: 0.2545\n",
      "236/281, train_loss: 0.0564, step time: 0.2498\n",
      "237/281, train_loss: 0.1199, step time: 0.2507\n",
      "238/281, train_loss: 0.0698, step time: 0.2468\n",
      "239/281, train_loss: 0.0557, step time: 0.2508\n",
      "240/281, train_loss: 0.0935, step time: 0.2462\n",
      "241/281, train_loss: 0.1140, step time: 0.2485\n",
      "242/281, train_loss: 0.0582, step time: 0.2468\n",
      "243/281, train_loss: 0.0814, step time: 0.2493\n",
      "244/281, train_loss: 0.0425, step time: 0.2473\n",
      "245/281, train_loss: 0.0528, step time: 0.2446\n",
      "246/281, train_loss: 0.2525, step time: 0.2481\n",
      "247/281, train_loss: 0.0730, step time: 0.2501\n",
      "248/281, train_loss: 0.0454, step time: 0.2466\n",
      "249/281, train_loss: 0.0427, step time: 0.2442\n",
      "250/281, train_loss: 0.0616, step time: 0.2490\n",
      "251/281, train_loss: 0.0718, step time: 0.2476\n",
      "252/281, train_loss: 0.2298, step time: 0.2477\n",
      "253/281, train_loss: 0.1261, step time: 0.2477\n",
      "254/281, train_loss: 0.0610, step time: 0.2486\n",
      "255/281, train_loss: 0.1061, step time: 0.2474\n",
      "256/281, train_loss: 0.0940, step time: 0.2535\n",
      "257/281, train_loss: 0.0784, step time: 0.2480\n",
      "258/281, train_loss: 0.1086, step time: 0.2450\n",
      "259/281, train_loss: 0.0642, step time: 0.2445\n",
      "260/281, train_loss: 0.1078, step time: 0.2510\n",
      "261/281, train_loss: 0.3685, step time: 0.2496\n",
      "262/281, train_loss: 0.0548, step time: 0.2438\n",
      "263/281, train_loss: 0.0449, step time: 0.2487\n",
      "264/281, train_loss: 0.2272, step time: 0.2527\n",
      "265/281, train_loss: 0.0770, step time: 0.2480\n",
      "266/281, train_loss: 0.2168, step time: 0.2497\n",
      "267/281, train_loss: 0.3746, step time: 0.2448\n",
      "268/281, train_loss: 0.0601, step time: 0.2502\n",
      "269/281, train_loss: 0.2376, step time: 0.2498\n",
      "270/281, train_loss: 0.0747, step time: 0.2457\n",
      "271/281, train_loss: 0.0716, step time: 0.2479\n",
      "272/281, train_loss: 0.0513, step time: 0.2463\n",
      "273/281, train_loss: 0.0612, step time: 0.2447\n",
      "274/281, train_loss: 0.3791, step time: 0.2438\n",
      "275/281, train_loss: 0.1019, step time: 0.2470\n",
      "276/281, train_loss: 0.0680, step time: 0.2486\n",
      "277/281, train_loss: 0.0859, step time: 0.2503\n",
      "278/281, train_loss: 0.0657, step time: 0.2482\n",
      "279/281, train_loss: 0.0822, step time: 0.2511\n",
      "280/281, train_loss: 0.0617, step time: 0.2455\n",
      "281/281, train_loss: 0.0482, step time: 0.2472\n",
      "282/281, train_loss: 0.1203, step time: 0.1510\n",
      "epoch 145 average loss: 0.0983\n",
      "current epoch: 145 current mean dice: 0.8839 tc: 0.8820 wt: 0.9051 et: 0.8796\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 145 is: 374.8251\n",
      "----------\n",
      "epoch 146/200\n",
      "1/281, train_loss: 0.2211, step time: 0.2605\n",
      "2/281, train_loss: 0.2249, step time: 0.2503\n",
      "3/281, train_loss: 0.0922, step time: 0.2487\n",
      "4/281, train_loss: 0.0563, step time: 0.2462\n",
      "5/281, train_loss: 0.0738, step time: 0.2487\n",
      "6/281, train_loss: 0.0513, step time: 0.2510\n",
      "7/281, train_loss: 0.0682, step time: 0.2410\n",
      "8/281, train_loss: 0.0831, step time: 0.2574\n",
      "9/281, train_loss: 0.1053, step time: 0.2492\n",
      "10/281, train_loss: 0.0663, step time: 0.2480\n",
      "11/281, train_loss: 0.0998, step time: 0.2528\n",
      "12/281, train_loss: 0.0620, step time: 0.2429\n",
      "13/281, train_loss: 0.0424, step time: 0.2502\n",
      "14/281, train_loss: 0.0558, step time: 0.2449\n",
      "15/281, train_loss: 0.2391, step time: 0.2436\n",
      "16/281, train_loss: 0.2382, step time: 0.2498\n",
      "17/281, train_loss: 0.2350, step time: 0.2489\n",
      "18/281, train_loss: 0.0649, step time: 0.2487\n",
      "19/281, train_loss: 0.0477, step time: 0.2522\n",
      "20/281, train_loss: 0.0508, step time: 0.2436\n",
      "21/281, train_loss: 0.0401, step time: 0.2472\n",
      "22/281, train_loss: 0.0919, step time: 0.2476\n",
      "23/281, train_loss: 0.0944, step time: 0.2470\n",
      "24/281, train_loss: 0.0756, step time: 0.2525\n",
      "25/281, train_loss: 0.0502, step time: 0.2460\n",
      "26/281, train_loss: 0.0869, step time: 0.2457\n",
      "27/281, train_loss: 0.0826, step time: 0.2421\n",
      "28/281, train_loss: 0.0576, step time: 0.2441\n",
      "29/281, train_loss: 0.0749, step time: 0.2515\n",
      "30/281, train_loss: 0.0929, step time: 0.2523\n",
      "31/281, train_loss: 0.0866, step time: 0.2493\n",
      "32/281, train_loss: 0.0552, step time: 0.2505\n",
      "33/281, train_loss: 0.0896, step time: 0.2458\n",
      "34/281, train_loss: 0.0802, step time: 0.2467\n",
      "35/281, train_loss: 0.0606, step time: 0.2503\n",
      "36/281, train_loss: 0.2566, step time: 0.2510\n",
      "37/281, train_loss: 0.0803, step time: 0.2493\n",
      "38/281, train_loss: 0.1282, step time: 0.2474\n",
      "39/281, train_loss: 0.0762, step time: 0.2471\n",
      "40/281, train_loss: 0.0762, step time: 0.2468\n",
      "41/281, train_loss: 0.2385, step time: 0.2473\n",
      "42/281, train_loss: 0.0883, step time: 0.2505\n",
      "43/281, train_loss: 0.0676, step time: 0.2499\n",
      "44/281, train_loss: 0.0852, step time: 0.2455\n",
      "45/281, train_loss: 0.0646, step time: 0.2495\n",
      "46/281, train_loss: 0.0490, step time: 0.2500\n",
      "47/281, train_loss: 0.0439, step time: 0.2512\n",
      "48/281, train_loss: 0.0716, step time: 0.2483\n",
      "49/281, train_loss: 0.0835, step time: 0.2465\n",
      "50/281, train_loss: 0.2149, step time: 0.2449\n",
      "51/281, train_loss: 0.2884, step time: 0.2465\n",
      "52/281, train_loss: 0.0860, step time: 0.2506\n",
      "53/281, train_loss: 0.0705, step time: 0.2446\n",
      "54/281, train_loss: 0.0420, step time: 0.2436\n",
      "55/281, train_loss: 0.0699, step time: 0.2433\n",
      "56/281, train_loss: 0.0724, step time: 0.2511\n",
      "57/281, train_loss: 0.1102, step time: 0.2468\n",
      "58/281, train_loss: 0.0652, step time: 0.2486\n",
      "59/281, train_loss: 0.0547, step time: 0.2473\n",
      "60/281, train_loss: 0.0637, step time: 0.2665\n",
      "61/281, train_loss: 0.2330, step time: 0.2457\n",
      "62/281, train_loss: 0.2214, step time: 0.2430\n",
      "63/281, train_loss: 0.0692, step time: 0.2490\n",
      "64/281, train_loss: 0.1010, step time: 0.2468\n",
      "65/281, train_loss: 0.0762, step time: 0.2408\n",
      "66/281, train_loss: 0.0747, step time: 0.2400\n",
      "67/281, train_loss: 0.0724, step time: 0.2400\n",
      "68/281, train_loss: 0.0411, step time: 0.2445\n",
      "69/281, train_loss: 0.0655, step time: 0.2705\n",
      "70/281, train_loss: 0.0812, step time: 0.2434\n",
      "71/281, train_loss: 0.0532, step time: 0.2412\n",
      "72/281, train_loss: 0.0532, step time: 0.2437\n",
      "73/281, train_loss: 0.0768, step time: 0.2466\n",
      "74/281, train_loss: 0.0460, step time: 0.2479\n",
      "75/281, train_loss: 0.2100, step time: 0.2455\n",
      "76/281, train_loss: 0.0868, step time: 0.2462\n",
      "77/281, train_loss: 0.0829, step time: 0.2400\n",
      "78/281, train_loss: 0.0584, step time: 0.2447\n",
      "79/281, train_loss: 0.0678, step time: 0.2440\n",
      "80/281, train_loss: 0.0862, step time: 0.2420\n",
      "81/281, train_loss: 0.0949, step time: 0.2508\n",
      "82/281, train_loss: 0.0405, step time: 0.2442\n",
      "83/281, train_loss: 0.0843, step time: 0.2482\n",
      "84/281, train_loss: 0.0840, step time: 0.2428\n",
      "85/281, train_loss: 0.0700, step time: 0.2454\n",
      "86/281, train_loss: 0.2164, step time: 0.2463\n",
      "87/281, train_loss: 0.0722, step time: 0.2482\n",
      "88/281, train_loss: 0.0668, step time: 0.2470\n",
      "89/281, train_loss: 0.0412, step time: 0.2462\n",
      "90/281, train_loss: 0.0975, step time: 0.2461\n",
      "91/281, train_loss: 0.0567, step time: 0.2504\n",
      "92/281, train_loss: 0.0473, step time: 0.2458\n",
      "93/281, train_loss: 0.0730, step time: 0.2481\n",
      "94/281, train_loss: 0.0898, step time: 0.2442\n",
      "95/281, train_loss: 0.1033, step time: 0.2496\n",
      "96/281, train_loss: 0.0671, step time: 0.2417\n",
      "97/281, train_loss: 0.1581, step time: 0.2487\n",
      "98/281, train_loss: 0.0597, step time: 0.2540\n",
      "99/281, train_loss: 0.0618, step time: 0.2475\n",
      "100/281, train_loss: 0.0401, step time: 0.2503\n",
      "101/281, train_loss: 0.0702, step time: 0.2500\n",
      "102/281, train_loss: 0.0694, step time: 0.2531\n",
      "103/281, train_loss: 0.0835, step time: 0.2476\n",
      "104/281, train_loss: 0.1088, step time: 0.2468\n",
      "105/281, train_loss: 0.0629, step time: 0.2510\n",
      "106/281, train_loss: 0.0680, step time: 0.2452\n",
      "107/281, train_loss: 0.0663, step time: 0.2464\n",
      "108/281, train_loss: 0.2099, step time: 0.2479\n",
      "109/281, train_loss: 0.0636, step time: 0.2513\n",
      "110/281, train_loss: 0.0632, step time: 0.2528\n",
      "111/281, train_loss: 0.0638, step time: 0.2503\n",
      "112/281, train_loss: 0.1113, step time: 0.2502\n",
      "113/281, train_loss: 0.1246, step time: 0.2565\n",
      "114/281, train_loss: 0.0558, step time: 0.2533\n",
      "115/281, train_loss: 0.0330, step time: 0.2440\n",
      "116/281, train_loss: 0.1209, step time: 0.2471\n",
      "117/281, train_loss: 0.1093, step time: 0.2435\n",
      "118/281, train_loss: 0.0556, step time: 0.2488\n",
      "119/281, train_loss: 0.0755, step time: 0.2417\n",
      "120/281, train_loss: 0.0878, step time: 0.2445\n",
      "121/281, train_loss: 0.1009, step time: 0.2505\n",
      "122/281, train_loss: 0.0898, step time: 0.2456\n",
      "123/281, train_loss: 0.0939, step time: 0.2429\n",
      "124/281, train_loss: 0.0752, step time: 0.2439\n",
      "125/281, train_loss: 0.0686, step time: 0.2445\n",
      "126/281, train_loss: 0.0476, step time: 0.2496\n",
      "127/281, train_loss: 0.1020, step time: 0.2492\n",
      "128/281, train_loss: 0.0603, step time: 0.2492\n",
      "129/281, train_loss: 0.0624, step time: 0.2460\n",
      "130/281, train_loss: 0.0771, step time: 0.2480\n",
      "131/281, train_loss: 0.0892, step time: 0.2669\n",
      "132/281, train_loss: 0.2213, step time: 0.2441\n",
      "133/281, train_loss: 0.0560, step time: 0.2442\n",
      "134/281, train_loss: 0.0564, step time: 0.2482\n",
      "135/281, train_loss: 0.1142, step time: 0.2479\n",
      "136/281, train_loss: 0.2384, step time: 0.2510\n",
      "137/281, train_loss: 0.0359, step time: 0.2515\n",
      "138/281, train_loss: 0.0576, step time: 0.2422\n",
      "139/281, train_loss: 0.0821, step time: 0.2481\n",
      "140/281, train_loss: 0.2298, step time: 0.2439\n",
      "141/281, train_loss: 0.0635, step time: 0.2493\n",
      "142/281, train_loss: 0.0655, step time: 0.2488\n",
      "143/281, train_loss: 0.2506, step time: 0.2535\n",
      "144/281, train_loss: 0.0446, step time: 0.2458\n",
      "145/281, train_loss: 0.0843, step time: 0.2465\n",
      "146/281, train_loss: 0.0695, step time: 0.2515\n",
      "147/281, train_loss: 0.0588, step time: 0.2514\n",
      "148/281, train_loss: 0.0779, step time: 0.2508\n",
      "149/281, train_loss: 0.0378, step time: 0.2498\n",
      "150/281, train_loss: 0.2363, step time: 0.2473\n",
      "151/281, train_loss: 0.0602, step time: 0.2550\n",
      "152/281, train_loss: 0.2257, step time: 0.2951\n",
      "153/281, train_loss: 0.2173, step time: 0.2507\n",
      "154/281, train_loss: 0.0880, step time: 0.2493\n",
      "155/281, train_loss: 0.0827, step time: 0.2501\n",
      "156/281, train_loss: 0.0542, step time: 0.2432\n",
      "157/281, train_loss: 0.0827, step time: 0.2432\n",
      "158/281, train_loss: 0.2320, step time: 0.2439\n",
      "159/281, train_loss: 0.0716, step time: 0.2497\n",
      "160/281, train_loss: 0.1097, step time: 0.2523\n",
      "161/281, train_loss: 0.0817, step time: 0.2459\n",
      "162/281, train_loss: 0.0651, step time: 0.2509\n",
      "163/281, train_loss: 0.0624, step time: 0.2488\n",
      "164/281, train_loss: 0.0583, step time: 0.2535\n",
      "165/281, train_loss: 0.0765, step time: 0.2467\n",
      "166/281, train_loss: 0.0637, step time: 0.2439\n",
      "167/281, train_loss: 0.0625, step time: 0.2435\n",
      "168/281, train_loss: 0.0821, step time: 0.2459\n",
      "169/281, train_loss: 0.0852, step time: 0.2521\n",
      "170/281, train_loss: 0.2479, step time: 0.2463\n",
      "171/281, train_loss: 0.0556, step time: 0.2532\n",
      "172/281, train_loss: 0.0777, step time: 0.2502\n",
      "173/281, train_loss: 0.0615, step time: 0.2511\n",
      "174/281, train_loss: 0.2122, step time: 0.2444\n",
      "175/281, train_loss: 0.2067, step time: 0.2477\n",
      "176/281, train_loss: 0.0352, step time: 0.2472\n",
      "177/281, train_loss: 0.0565, step time: 0.2536\n",
      "178/281, train_loss: 0.0584, step time: 0.2504\n",
      "179/281, train_loss: 0.0497, step time: 0.2402\n",
      "180/281, train_loss: 0.0916, step time: 0.2435\n",
      "181/281, train_loss: 0.2340, step time: 0.2502\n",
      "182/281, train_loss: 0.0609, step time: 0.2478\n",
      "183/281, train_loss: 0.2373, step time: 0.2436\n",
      "184/281, train_loss: 0.0510, step time: 0.2456\n",
      "185/281, train_loss: 0.0610, step time: 0.2460\n",
      "186/281, train_loss: 0.0755, step time: 0.2502\n",
      "187/281, train_loss: 0.0700, step time: 0.2489\n",
      "188/281, train_loss: 0.0755, step time: 0.2503\n",
      "189/281, train_loss: 0.0623, step time: 0.2518\n",
      "190/281, train_loss: 0.0698, step time: 0.2549\n",
      "191/281, train_loss: 0.0486, step time: 0.2479\n",
      "192/281, train_loss: 0.0997, step time: 0.2433\n",
      "193/281, train_loss: 0.0804, step time: 0.2436\n",
      "194/281, train_loss: 0.0618, step time: 0.2444\n",
      "195/281, train_loss: 0.0516, step time: 0.2434\n",
      "196/281, train_loss: 0.0646, step time: 0.2521\n",
      "197/281, train_loss: 0.2628, step time: 0.2420\n",
      "198/281, train_loss: 0.0406, step time: 0.2437\n",
      "199/281, train_loss: 0.0570, step time: 0.2458\n",
      "200/281, train_loss: 0.0992, step time: 0.2495\n",
      "201/281, train_loss: 0.2374, step time: 0.2517\n",
      "202/281, train_loss: 0.1054, step time: 0.2477\n",
      "203/281, train_loss: 0.0618, step time: 0.2498\n",
      "204/281, train_loss: 0.0784, step time: 0.2445\n",
      "205/281, train_loss: 0.0751, step time: 0.2454\n",
      "206/281, train_loss: 0.2186, step time: 0.2431\n",
      "207/281, train_loss: 0.0809, step time: 0.2495\n",
      "208/281, train_loss: 0.0981, step time: 0.2458\n",
      "209/281, train_loss: 0.0756, step time: 0.2395\n",
      "210/281, train_loss: 0.0972, step time: 0.2389\n",
      "211/281, train_loss: 0.0765, step time: 0.2453\n",
      "212/281, train_loss: 0.0540, step time: 0.2557\n",
      "213/281, train_loss: 0.0375, step time: 0.2513\n",
      "214/281, train_loss: 0.0631, step time: 0.2460\n",
      "215/281, train_loss: 0.0621, step time: 0.2477\n",
      "216/281, train_loss: 0.0667, step time: 0.2447\n",
      "217/281, train_loss: 0.1095, step time: 0.2418\n",
      "218/281, train_loss: 0.0651, step time: 0.2469\n",
      "219/281, train_loss: 0.0460, step time: 0.2465\n",
      "220/281, train_loss: 0.0689, step time: 0.2449\n",
      "221/281, train_loss: 0.0625, step time: 0.2479\n",
      "222/281, train_loss: 0.0769, step time: 0.2486\n",
      "223/281, train_loss: 0.2657, step time: 0.2472\n",
      "224/281, train_loss: 0.0802, step time: 0.2502\n",
      "225/281, train_loss: 0.0766, step time: 0.2502\n",
      "226/281, train_loss: 0.0934, step time: 0.2479\n",
      "227/281, train_loss: 0.2167, step time: 0.2450\n",
      "228/281, train_loss: 0.0389, step time: 0.2469\n",
      "229/281, train_loss: 0.0677, step time: 0.2449\n",
      "230/281, train_loss: 0.0506, step time: 0.2453\n",
      "231/281, train_loss: 0.0864, step time: 0.2489\n",
      "232/281, train_loss: 0.2327, step time: 0.2526\n",
      "233/281, train_loss: 0.0644, step time: 0.2521\n",
      "234/281, train_loss: 0.1013, step time: 0.2518\n",
      "235/281, train_loss: 0.1063, step time: 0.2515\n",
      "236/281, train_loss: 0.0854, step time: 0.2486\n",
      "237/281, train_loss: 0.0550, step time: 0.2426\n",
      "238/281, train_loss: 0.0522, step time: 0.2424\n",
      "239/281, train_loss: 0.0627, step time: 0.2444\n",
      "240/281, train_loss: 0.0867, step time: 0.2561\n",
      "241/281, train_loss: 0.0546, step time: 0.2581\n",
      "242/281, train_loss: 0.0626, step time: 0.2566\n",
      "243/281, train_loss: 0.0666, step time: 0.2723\n",
      "244/281, train_loss: 0.1199, step time: 0.2520\n",
      "245/281, train_loss: 0.0454, step time: 0.2722\n",
      "246/281, train_loss: 0.0653, step time: 0.2514\n",
      "247/281, train_loss: 0.0594, step time: 0.2471\n",
      "248/281, train_loss: 0.0869, step time: 0.2496\n",
      "249/281, train_loss: 0.0889, step time: 0.2573\n",
      "250/281, train_loss: 0.2346, step time: 0.2527\n",
      "251/281, train_loss: 0.1470, step time: 0.2583\n",
      "252/281, train_loss: 0.0711, step time: 0.2535\n",
      "253/281, train_loss: 0.2290, step time: 0.2567\n",
      "254/281, train_loss: 0.0687, step time: 0.2528\n",
      "255/281, train_loss: 0.0733, step time: 0.2460\n",
      "256/281, train_loss: 0.0758, step time: 0.2514\n",
      "257/281, train_loss: 0.2265, step time: 0.2529\n",
      "258/281, train_loss: 0.2797, step time: 0.2536\n",
      "259/281, train_loss: 0.0781, step time: 0.2493\n",
      "260/281, train_loss: 0.0763, step time: 0.2480\n",
      "261/281, train_loss: 0.2183, step time: 0.2518\n",
      "262/281, train_loss: 0.0707, step time: 0.2486\n",
      "263/281, train_loss: 0.1045, step time: 0.2497\n",
      "264/281, train_loss: 0.0648, step time: 0.2563\n",
      "265/281, train_loss: 0.0593, step time: 0.2454\n",
      "266/281, train_loss: 0.0557, step time: 0.2467\n",
      "267/281, train_loss: 0.0762, step time: 0.2486\n",
      "268/281, train_loss: 0.0925, step time: 0.2530\n",
      "269/281, train_loss: 0.0595, step time: 0.2473\n",
      "270/281, train_loss: 0.2385, step time: 0.2606\n",
      "271/281, train_loss: 0.0852, step time: 0.2790\n",
      "272/281, train_loss: 0.0871, step time: 0.2532\n",
      "273/281, train_loss: 0.0703, step time: 0.2469\n",
      "274/281, train_loss: 0.3815, step time: 0.2465\n",
      "275/281, train_loss: 0.0805, step time: 0.2475\n",
      "276/281, train_loss: 0.0808, step time: 0.2519\n",
      "277/281, train_loss: 0.1335, step time: 0.2458\n",
      "278/281, train_loss: 0.0941, step time: 0.2469\n",
      "279/281, train_loss: 0.0685, step time: 0.2520\n",
      "280/281, train_loss: 0.0880, step time: 0.2539\n",
      "281/281, train_loss: 0.3835, step time: 0.2516\n",
      "282/281, train_loss: 0.0928, step time: 0.1491\n",
      "epoch 146 average loss: 0.0975\n",
      "current epoch: 146 current mean dice: 0.8705 tc: 0.8720 wt: 0.8893 et: 0.8713\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 146 is: 378.4869\n",
      "----------\n",
      "epoch 147/200\n",
      "1/281, train_loss: 0.0577, step time: 0.2540\n",
      "2/281, train_loss: 0.0786, step time: 0.2500\n",
      "3/281, train_loss: 0.0897, step time: 0.2535\n",
      "4/281, train_loss: 0.0765, step time: 0.2573\n",
      "5/281, train_loss: 0.2345, step time: 0.2576\n",
      "6/281, train_loss: 0.2140, step time: 0.2601\n",
      "7/281, train_loss: 0.0977, step time: 0.2527\n",
      "8/281, train_loss: 0.0769, step time: 0.2572\n",
      "9/281, train_loss: 0.3908, step time: 0.2585\n",
      "10/281, train_loss: 0.2184, step time: 0.2721\n",
      "11/281, train_loss: 0.0684, step time: 0.2736\n",
      "12/281, train_loss: 0.0930, step time: 0.2611\n",
      "13/281, train_loss: 0.0636, step time: 0.2574\n",
      "14/281, train_loss: 0.0866, step time: 0.2538\n",
      "15/281, train_loss: 0.0724, step time: 0.2509\n",
      "16/281, train_loss: 0.0803, step time: 0.2511\n",
      "17/281, train_loss: 0.1073, step time: 0.2475\n",
      "18/281, train_loss: 0.0882, step time: 0.2451\n",
      "19/281, train_loss: 0.0524, step time: 0.2508\n",
      "20/281, train_loss: 0.0567, step time: 0.2530\n",
      "21/281, train_loss: 0.0742, step time: 0.2510\n",
      "22/281, train_loss: 0.1143, step time: 0.2523\n",
      "23/281, train_loss: 0.1053, step time: 0.2549\n",
      "24/281, train_loss: 0.0854, step time: 0.2489\n",
      "25/281, train_loss: 0.2094, step time: 0.2544\n",
      "26/281, train_loss: 0.0764, step time: 0.2568\n",
      "27/281, train_loss: 0.0663, step time: 0.2509\n",
      "28/281, train_loss: 0.0774, step time: 0.2518\n",
      "29/281, train_loss: 0.2191, step time: 0.2553\n",
      "30/281, train_loss: 0.0706, step time: 0.2500\n",
      "31/281, train_loss: 0.0839, step time: 0.2527\n",
      "32/281, train_loss: 0.0725, step time: 0.2540\n",
      "33/281, train_loss: 0.0750, step time: 0.2546\n",
      "34/281, train_loss: 0.0931, step time: 0.2490\n",
      "35/281, train_loss: 0.0862, step time: 0.2546\n",
      "36/281, train_loss: 0.1010, step time: 0.2487\n",
      "37/281, train_loss: 0.0918, step time: 0.2477\n",
      "38/281, train_loss: 0.0764, step time: 0.2553\n",
      "39/281, train_loss: 0.0515, step time: 0.2567\n",
      "40/281, train_loss: 0.0640, step time: 0.2490\n",
      "41/281, train_loss: 0.0519, step time: 0.2493\n",
      "42/281, train_loss: 0.0916, step time: 0.2503\n",
      "43/281, train_loss: 0.0662, step time: 0.2518\n",
      "44/281, train_loss: 0.0890, step time: 0.2557\n",
      "45/281, train_loss: 0.0852, step time: 0.2499\n",
      "46/281, train_loss: 0.0784, step time: 0.2524\n",
      "47/281, train_loss: 0.0650, step time: 0.2551\n",
      "48/281, train_loss: 0.0862, step time: 0.2557\n",
      "49/281, train_loss: 0.0577, step time: 0.2512\n",
      "50/281, train_loss: 0.0900, step time: 0.2501\n",
      "51/281, train_loss: 0.0677, step time: 0.2501\n",
      "52/281, train_loss: 0.2268, step time: 0.2669\n",
      "53/281, train_loss: 0.0591, step time: 0.2534\n",
      "54/281, train_loss: 0.0996, step time: 0.2543\n",
      "55/281, train_loss: 0.0819, step time: 0.2497\n",
      "56/281, train_loss: 0.0692, step time: 0.2473\n",
      "57/281, train_loss: 0.0742, step time: 0.2521\n",
      "58/281, train_loss: 0.0851, step time: 0.2551\n",
      "59/281, train_loss: 0.0636, step time: 0.2600\n",
      "60/281, train_loss: 0.0803, step time: 0.2566\n",
      "61/281, train_loss: 0.0873, step time: 0.2481\n",
      "62/281, train_loss: 0.0746, step time: 0.2557\n",
      "63/281, train_loss: 0.0774, step time: 0.2471\n",
      "64/281, train_loss: 0.0480, step time: 0.2436\n",
      "65/281, train_loss: 0.0732, step time: 0.2441\n",
      "66/281, train_loss: 0.0735, step time: 0.2496\n",
      "67/281, train_loss: 0.0850, step time: 0.2479\n",
      "68/281, train_loss: 0.0693, step time: 0.2592\n",
      "69/281, train_loss: 0.0858, step time: 0.2552\n",
      "70/281, train_loss: 0.0580, step time: 0.2530\n",
      "71/281, train_loss: 0.0543, step time: 0.2548\n",
      "72/281, train_loss: 0.0722, step time: 0.2583\n",
      "73/281, train_loss: 0.0922, step time: 0.2501\n",
      "74/281, train_loss: 0.1108, step time: 0.2530\n",
      "75/281, train_loss: 0.0619, step time: 0.2506\n",
      "76/281, train_loss: 0.0628, step time: 0.2482\n",
      "77/281, train_loss: 0.0687, step time: 0.2626\n",
      "78/281, train_loss: 0.0633, step time: 0.2556\n",
      "79/281, train_loss: 0.0738, step time: 0.2661\n",
      "80/281, train_loss: 0.0605, step time: 0.2583\n",
      "81/281, train_loss: 0.1066, step time: 0.2795\n",
      "82/281, train_loss: 0.0765, step time: 0.2637\n",
      "83/281, train_loss: 0.0528, step time: 0.2496\n",
      "84/281, train_loss: 0.2316, step time: 0.2487\n",
      "85/281, train_loss: 0.1009, step time: 0.2565\n",
      "86/281, train_loss: 0.2211, step time: 0.2612\n",
      "87/281, train_loss: 0.0637, step time: 0.2552\n",
      "88/281, train_loss: 0.2357, step time: 0.2445\n",
      "89/281, train_loss: 0.2064, step time: 0.2497\n",
      "90/281, train_loss: 0.2174, step time: 0.2534\n",
      "91/281, train_loss: 0.0559, step time: 0.2528\n",
      "92/281, train_loss: 0.1150, step time: 0.2524\n",
      "93/281, train_loss: 0.0798, step time: 0.2541\n",
      "94/281, train_loss: 0.0790, step time: 0.2619\n",
      "95/281, train_loss: 0.0777, step time: 0.2599\n",
      "96/281, train_loss: 0.1220, step time: 0.2532\n",
      "97/281, train_loss: 0.0828, step time: 0.2561\n",
      "98/281, train_loss: 0.2079, step time: 0.2566\n",
      "99/281, train_loss: 0.2231, step time: 0.2537\n",
      "100/281, train_loss: 0.0659, step time: 0.2538\n",
      "101/281, train_loss: 0.0514, step time: 0.2529\n",
      "102/281, train_loss: 0.0669, step time: 0.2540\n",
      "103/281, train_loss: 0.0731, step time: 0.2504\n",
      "104/281, train_loss: 0.0929, step time: 0.2530\n",
      "105/281, train_loss: 0.4033, step time: 0.2462\n",
      "106/281, train_loss: 0.0873, step time: 0.2426\n",
      "107/281, train_loss: 0.0575, step time: 0.2422\n",
      "108/281, train_loss: 0.0915, step time: 0.2480\n",
      "109/281, train_loss: 0.0934, step time: 0.2466\n",
      "110/281, train_loss: 0.0838, step time: 0.2491\n",
      "111/281, train_loss: 0.0704, step time: 0.2479\n",
      "112/281, train_loss: 0.2196, step time: 0.2469\n",
      "113/281, train_loss: 0.0972, step time: 0.2482\n",
      "114/281, train_loss: 0.0646, step time: 0.2498\n",
      "115/281, train_loss: 0.0563, step time: 0.2427\n",
      "116/281, train_loss: 0.0710, step time: 0.2480\n",
      "117/281, train_loss: 0.0686, step time: 0.2493\n",
      "118/281, train_loss: 0.0642, step time: 0.2563\n",
      "119/281, train_loss: 0.0908, step time: 0.2478\n",
      "120/281, train_loss: 0.0817, step time: 0.2538\n",
      "121/281, train_loss: 0.0490, step time: 0.2488\n",
      "122/281, train_loss: 0.0795, step time: 0.2501\n",
      "123/281, train_loss: 0.0961, step time: 0.2553\n",
      "124/281, train_loss: 0.0767, step time: 0.2511\n",
      "125/281, train_loss: 0.0854, step time: 0.2476\n",
      "126/281, train_loss: 0.0760, step time: 0.2509\n",
      "127/281, train_loss: 0.0398, step time: 0.2480\n",
      "128/281, train_loss: 0.0746, step time: 0.2769\n",
      "129/281, train_loss: 0.0458, step time: 0.2482\n",
      "130/281, train_loss: 0.1093, step time: 0.2519\n",
      "131/281, train_loss: 0.0505, step time: 0.2438\n",
      "132/281, train_loss: 0.0803, step time: 0.2522\n",
      "133/281, train_loss: 0.0596, step time: 0.2492\n",
      "134/281, train_loss: 0.0978, step time: 0.2489\n",
      "135/281, train_loss: 0.1026, step time: 0.2427\n",
      "136/281, train_loss: 0.0664, step time: 0.2464\n",
      "137/281, train_loss: 0.0547, step time: 0.2487\n",
      "138/281, train_loss: 0.0773, step time: 0.2513\n",
      "139/281, train_loss: 0.0800, step time: 0.2497\n",
      "140/281, train_loss: 0.0896, step time: 0.2473\n",
      "141/281, train_loss: 0.0638, step time: 0.2509\n",
      "142/281, train_loss: 0.0935, step time: 0.2479\n",
      "143/281, train_loss: 0.0799, step time: 0.2504\n",
      "144/281, train_loss: 0.0843, step time: 0.2520\n",
      "145/281, train_loss: 0.0779, step time: 0.2520\n",
      "146/281, train_loss: 0.0470, step time: 0.2489\n",
      "147/281, train_loss: 0.0736, step time: 0.2523\n",
      "148/281, train_loss: 0.0932, step time: 0.2519\n",
      "149/281, train_loss: 0.0527, step time: 0.2517\n",
      "150/281, train_loss: 0.1074, step time: 0.2522\n",
      "151/281, train_loss: 0.0761, step time: 0.2532\n",
      "152/281, train_loss: 0.0788, step time: 0.2508\n",
      "153/281, train_loss: 0.0775, step time: 0.2538\n",
      "154/281, train_loss: 0.0963, step time: 0.2477\n",
      "155/281, train_loss: 0.1240, step time: 0.2527\n",
      "156/281, train_loss: 0.0834, step time: 0.2522\n",
      "157/281, train_loss: 0.2581, step time: 0.2501\n",
      "158/281, train_loss: 0.0781, step time: 0.2496\n",
      "159/281, train_loss: 0.0629, step time: 0.2495\n",
      "160/281, train_loss: 0.0619, step time: 0.2538\n",
      "161/281, train_loss: 0.0516, step time: 0.2477\n",
      "162/281, train_loss: 0.2347, step time: 0.2497\n",
      "163/281, train_loss: 0.0774, step time: 0.2516\n",
      "164/281, train_loss: 0.0495, step time: 0.2572\n",
      "165/281, train_loss: 0.2443, step time: 0.2751\n",
      "166/281, train_loss: 0.0700, step time: 0.2516\n",
      "167/281, train_loss: 0.0772, step time: 0.2534\n",
      "168/281, train_loss: 0.0584, step time: 0.2501\n",
      "169/281, train_loss: 0.0571, step time: 0.2466\n",
      "170/281, train_loss: 0.1094, step time: 0.2491\n",
      "171/281, train_loss: 0.1019, step time: 0.2510\n",
      "172/281, train_loss: 0.0476, step time: 0.2486\n",
      "173/281, train_loss: 0.1134, step time: 0.2509\n",
      "174/281, train_loss: 0.2136, step time: 0.2510\n",
      "175/281, train_loss: 0.1338, step time: 0.2551\n",
      "176/281, train_loss: 0.1212, step time: 0.2502\n",
      "177/281, train_loss: 0.1285, step time: 0.2555\n",
      "178/281, train_loss: 0.0586, step time: 0.2568\n",
      "179/281, train_loss: 0.0535, step time: 0.2534\n",
      "180/281, train_loss: 0.0670, step time: 0.2517\n",
      "181/281, train_loss: 0.0699, step time: 0.2540\n",
      "182/281, train_loss: 0.0766, step time: 0.2497\n",
      "183/281, train_loss: 0.2301, step time: 0.2497\n",
      "184/281, train_loss: 0.0509, step time: 0.2507\n",
      "185/281, train_loss: 0.0847, step time: 0.2622\n",
      "186/281, train_loss: 0.0893, step time: 0.2581\n",
      "187/281, train_loss: 0.0627, step time: 0.2565\n",
      "188/281, train_loss: 0.0608, step time: 0.2574\n",
      "189/281, train_loss: 0.0885, step time: 0.2477\n",
      "190/281, train_loss: 0.0724, step time: 0.2491\n",
      "191/281, train_loss: 0.2209, step time: 0.2504\n",
      "192/281, train_loss: 0.2158, step time: 0.2462\n",
      "193/281, train_loss: 0.0954, step time: 0.2484\n",
      "194/281, train_loss: 0.0858, step time: 0.2534\n",
      "195/281, train_loss: 0.0997, step time: 0.2522\n",
      "196/281, train_loss: 0.2377, step time: 0.2504\n",
      "197/281, train_loss: 0.2324, step time: 0.2476\n",
      "198/281, train_loss: 0.0851, step time: 0.2504\n",
      "199/281, train_loss: 0.0468, step time: 0.2470\n",
      "200/281, train_loss: 0.0864, step time: 0.2481\n",
      "201/281, train_loss: 0.0742, step time: 0.2519\n",
      "202/281, train_loss: 0.0896, step time: 0.2501\n",
      "203/281, train_loss: 0.2000, step time: 0.2522\n",
      "204/281, train_loss: 0.2241, step time: 0.2577\n",
      "205/281, train_loss: 0.0604, step time: 0.2538\n",
      "206/281, train_loss: 0.0602, step time: 0.2585\n",
      "207/281, train_loss: 0.0643, step time: 0.2514\n",
      "208/281, train_loss: 0.2098, step time: 0.2513\n",
      "209/281, train_loss: 0.0886, step time: 0.2514\n",
      "210/281, train_loss: 0.0634, step time: 0.2544\n",
      "211/281, train_loss: 0.0418, step time: 0.2589\n",
      "212/281, train_loss: 0.2444, step time: 0.2556\n",
      "213/281, train_loss: 0.0719, step time: 0.2526\n",
      "214/281, train_loss: 0.1154, step time: 0.2506\n",
      "215/281, train_loss: 0.0570, step time: 0.2516\n",
      "216/281, train_loss: 0.0507, step time: 0.2487\n",
      "217/281, train_loss: 0.0724, step time: 0.2517\n",
      "218/281, train_loss: 0.0687, step time: 0.2468\n",
      "219/281, train_loss: 0.0955, step time: 0.2523\n",
      "220/281, train_loss: 0.0460, step time: 0.2468\n",
      "221/281, train_loss: 0.0403, step time: 0.2478\n",
      "222/281, train_loss: 0.0497, step time: 0.2487\n",
      "223/281, train_loss: 0.2230, step time: 0.2534\n",
      "224/281, train_loss: 0.0651, step time: 0.2532\n",
      "225/281, train_loss: 0.0714, step time: 0.2504\n",
      "226/281, train_loss: 0.0789, step time: 0.2500\n",
      "227/281, train_loss: 0.0662, step time: 0.2488\n",
      "228/281, train_loss: 0.0631, step time: 0.2456\n",
      "229/281, train_loss: 0.0563, step time: 0.2465\n",
      "230/281, train_loss: 0.0784, step time: 0.2449\n",
      "231/281, train_loss: 0.0497, step time: 0.2629\n",
      "232/281, train_loss: 0.0642, step time: 0.2861\n",
      "233/281, train_loss: 0.2420, step time: 0.2567\n",
      "234/281, train_loss: 0.2366, step time: 0.2526\n",
      "235/281, train_loss: 0.3878, step time: 0.2498\n",
      "236/281, train_loss: 0.0889, step time: 0.2477\n",
      "237/281, train_loss: 0.0859, step time: 0.2471\n",
      "238/281, train_loss: 0.0826, step time: 0.2439\n",
      "239/281, train_loss: 0.0725, step time: 0.2470\n",
      "240/281, train_loss: 0.0811, step time: 0.2467\n",
      "241/281, train_loss: 0.0476, step time: 0.2492\n",
      "242/281, train_loss: 0.0840, step time: 0.2480\n",
      "243/281, train_loss: 0.0635, step time: 0.2503\n",
      "244/281, train_loss: 0.0554, step time: 0.2498\n",
      "245/281, train_loss: 0.0595, step time: 0.2507\n",
      "246/281, train_loss: 0.0741, step time: 0.2458\n",
      "247/281, train_loss: 0.0685, step time: 0.2555\n",
      "248/281, train_loss: 0.0714, step time: 0.2492\n",
      "249/281, train_loss: 0.0806, step time: 0.2498\n",
      "250/281, train_loss: 0.0795, step time: 0.2508\n",
      "251/281, train_loss: 0.0586, step time: 0.2518\n",
      "252/281, train_loss: 0.0346, step time: 0.2505\n",
      "253/281, train_loss: 0.1989, step time: 0.2482\n",
      "254/281, train_loss: 0.0745, step time: 0.2510\n",
      "255/281, train_loss: 0.0584, step time: 0.2493\n",
      "256/281, train_loss: 0.0571, step time: 0.2477\n",
      "257/281, train_loss: 0.2288, step time: 0.2478\n",
      "258/281, train_loss: 0.0863, step time: 0.2493\n",
      "259/281, train_loss: 0.0514, step time: 0.2467\n",
      "260/281, train_loss: 0.0772, step time: 0.2467\n",
      "261/281, train_loss: 0.0731, step time: 0.2484\n",
      "262/281, train_loss: 0.0688, step time: 0.2447\n",
      "263/281, train_loss: 0.0651, step time: 0.2544\n",
      "264/281, train_loss: 0.2212, step time: 0.2568\n",
      "265/281, train_loss: 0.0863, step time: 0.2524\n",
      "266/281, train_loss: 0.0528, step time: 0.2543\n",
      "267/281, train_loss: 0.0975, step time: 0.2472\n",
      "268/281, train_loss: 0.2221, step time: 0.2514\n",
      "269/281, train_loss: 0.0511, step time: 0.2479\n",
      "270/281, train_loss: 0.2315, step time: 0.2514\n",
      "271/281, train_loss: 0.0579, step time: 0.2596\n",
      "272/281, train_loss: 0.0660, step time: 0.2564\n",
      "273/281, train_loss: 0.0498, step time: 0.2468\n",
      "274/281, train_loss: 0.0529, step time: 0.2433\n",
      "275/281, train_loss: 0.0588, step time: 0.2520\n",
      "276/281, train_loss: 0.0434, step time: 0.2510\n",
      "277/281, train_loss: 0.0559, step time: 0.2500\n",
      "278/281, train_loss: 0.0814, step time: 0.2490\n",
      "279/281, train_loss: 0.0561, step time: 0.2505\n",
      "280/281, train_loss: 0.2247, step time: 0.2478\n",
      "281/281, train_loss: 0.2045, step time: 0.2442\n",
      "282/281, train_loss: 0.0616, step time: 0.1473\n",
      "epoch 147 average loss: 0.0976\n",
      "current epoch: 147 current mean dice: 0.8832 tc: 0.8870 wt: 0.9023 et: 0.8743\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 147 is: 397.9159\n",
      "----------\n",
      "epoch 148/200\n",
      "1/281, train_loss: 0.0576, step time: 0.2513\n",
      "2/281, train_loss: 0.0800, step time: 0.2467\n",
      "3/281, train_loss: 0.0523, step time: 0.2439\n",
      "4/281, train_loss: 0.0735, step time: 0.2439\n",
      "5/281, train_loss: 0.0607, step time: 0.2486\n",
      "6/281, train_loss: 0.2732, step time: 0.2492\n",
      "7/281, train_loss: 0.0971, step time: 0.2485\n",
      "8/281, train_loss: 0.0823, step time: 0.2490\n",
      "9/281, train_loss: 0.2164, step time: 0.2407\n",
      "10/281, train_loss: 0.0825, step time: 0.2435\n",
      "11/281, train_loss: 0.1213, step time: 0.2411\n",
      "12/281, train_loss: 0.0576, step time: 0.2391\n",
      "13/281, train_loss: 0.0854, step time: 0.2486\n",
      "14/281, train_loss: 0.0391, step time: 0.2488\n",
      "15/281, train_loss: 0.1601, step time: 0.2547\n",
      "16/281, train_loss: 0.0703, step time: 0.2648\n",
      "17/281, train_loss: 0.0582, step time: 0.2484\n",
      "18/281, train_loss: 0.0699, step time: 0.2493\n",
      "19/281, train_loss: 0.0462, step time: 0.2465\n",
      "20/281, train_loss: 0.0782, step time: 0.2442\n",
      "21/281, train_loss: 0.0944, step time: 0.2423\n",
      "22/281, train_loss: 0.0562, step time: 0.2438\n",
      "23/281, train_loss: 0.0669, step time: 0.2454\n",
      "24/281, train_loss: 0.0803, step time: 0.2474\n",
      "25/281, train_loss: 0.0690, step time: 0.2494\n",
      "26/281, train_loss: 0.0952, step time: 0.2479\n",
      "27/281, train_loss: 0.0744, step time: 0.2487\n",
      "28/281, train_loss: 0.2129, step time: 0.2435\n",
      "29/281, train_loss: 0.0901, step time: 0.2494\n",
      "30/281, train_loss: 0.0590, step time: 0.2577\n",
      "31/281, train_loss: 0.0866, step time: 0.2500\n",
      "32/281, train_loss: 0.0603, step time: 0.2459\n",
      "33/281, train_loss: 0.0666, step time: 0.2495\n",
      "34/281, train_loss: 0.0704, step time: 0.2452\n",
      "35/281, train_loss: 0.0677, step time: 0.2416\n",
      "36/281, train_loss: 0.2549, step time: 0.2431\n",
      "37/281, train_loss: 0.0657, step time: 0.2451\n",
      "38/281, train_loss: 0.0592, step time: 0.2405\n",
      "39/281, train_loss: 0.0626, step time: 0.2441\n",
      "40/281, train_loss: 0.0482, step time: 0.2580\n",
      "41/281, train_loss: 0.0911, step time: 0.2499\n",
      "42/281, train_loss: 0.0824, step time: 0.2468\n",
      "43/281, train_loss: 0.0656, step time: 0.2497\n",
      "44/281, train_loss: 0.0736, step time: 0.2446\n",
      "45/281, train_loss: 0.2247, step time: 0.2483\n",
      "46/281, train_loss: 0.0834, step time: 0.2440\n",
      "47/281, train_loss: 0.2313, step time: 0.2462\n",
      "48/281, train_loss: 0.0541, step time: 0.2464\n",
      "49/281, train_loss: 0.0578, step time: 0.2520\n",
      "50/281, train_loss: 0.0741, step time: 0.2507\n",
      "51/281, train_loss: 0.2402, step time: 0.2484\n",
      "52/281, train_loss: 0.0838, step time: 0.2495\n",
      "53/281, train_loss: 0.0418, step time: 0.2463\n",
      "54/281, train_loss: 0.0560, step time: 0.2472\n",
      "55/281, train_loss: 0.0731, step time: 0.2508\n",
      "56/281, train_loss: 0.0650, step time: 0.2520\n",
      "57/281, train_loss: 0.0789, step time: 0.2529\n",
      "58/281, train_loss: 0.2449, step time: 0.2467\n",
      "59/281, train_loss: 0.0757, step time: 0.2506\n",
      "60/281, train_loss: 0.1125, step time: 0.2511\n",
      "61/281, train_loss: 0.0936, step time: 0.2430\n",
      "62/281, train_loss: 0.1062, step time: 0.2441\n",
      "63/281, train_loss: 0.2646, step time: 0.2501\n",
      "64/281, train_loss: 0.0884, step time: 0.2455\n",
      "65/281, train_loss: 0.0781, step time: 0.2434\n",
      "66/281, train_loss: 0.0695, step time: 0.2430\n",
      "67/281, train_loss: 0.0851, step time: 0.2516\n",
      "68/281, train_loss: 0.2057, step time: 0.2519\n",
      "69/281, train_loss: 0.0635, step time: 0.2518\n",
      "70/281, train_loss: 0.0574, step time: 0.2486\n",
      "71/281, train_loss: 0.0699, step time: 0.2482\n",
      "72/281, train_loss: 0.0858, step time: 0.2497\n",
      "73/281, train_loss: 0.0885, step time: 0.2454\n",
      "74/281, train_loss: 0.0454, step time: 0.2468\n",
      "75/281, train_loss: 0.0607, step time: 0.2534\n",
      "76/281, train_loss: 0.1946, step time: 0.2482\n",
      "77/281, train_loss: 0.2186, step time: 0.2468\n",
      "78/281, train_loss: 0.0780, step time: 0.2446\n",
      "79/281, train_loss: 0.0523, step time: 0.2523\n",
      "80/281, train_loss: 0.0826, step time: 0.2515\n",
      "81/281, train_loss: 0.0605, step time: 0.2492\n",
      "82/281, train_loss: 0.0365, step time: 0.2500\n",
      "83/281, train_loss: 0.0696, step time: 0.2492\n",
      "84/281, train_loss: 0.0550, step time: 0.2526\n",
      "85/281, train_loss: 0.0832, step time: 0.2530\n",
      "86/281, train_loss: 0.0660, step time: 0.2433\n",
      "87/281, train_loss: 0.2189, step time: 0.2427\n",
      "88/281, train_loss: 0.0645, step time: 0.2433\n",
      "89/281, train_loss: 0.0505, step time: 0.2440\n",
      "90/281, train_loss: 0.0722, step time: 0.2466\n",
      "91/281, train_loss: 0.2416, step time: 0.2548\n",
      "92/281, train_loss: 0.0606, step time: 0.2522\n",
      "93/281, train_loss: 0.2413, step time: 0.2460\n",
      "94/281, train_loss: 0.0680, step time: 0.2491\n",
      "95/281, train_loss: 0.0500, step time: 0.2508\n",
      "96/281, train_loss: 0.0810, step time: 0.2556\n",
      "97/281, train_loss: 0.0526, step time: 0.2552\n",
      "98/281, train_loss: 0.1163, step time: 0.2489\n",
      "99/281, train_loss: 0.0749, step time: 0.2506\n",
      "100/281, train_loss: 0.0790, step time: 0.2571\n",
      "101/281, train_loss: 0.0728, step time: 0.2487\n",
      "102/281, train_loss: 0.0597, step time: 0.2473\n",
      "103/281, train_loss: 0.0927, step time: 0.2516\n",
      "104/281, train_loss: 0.2420, step time: 0.2457\n",
      "105/281, train_loss: 0.1035, step time: 0.2453\n",
      "106/281, train_loss: 0.0655, step time: 0.2472\n",
      "107/281, train_loss: 0.2152, step time: 0.2520\n",
      "108/281, train_loss: 0.1050, step time: 0.2539\n",
      "109/281, train_loss: 0.0746, step time: 0.2459\n",
      "110/281, train_loss: 0.0844, step time: 0.2496\n",
      "111/281, train_loss: 0.0535, step time: 0.2421\n",
      "112/281, train_loss: 0.0592, step time: 0.2434\n",
      "113/281, train_loss: 0.0764, step time: 0.2437\n",
      "114/281, train_loss: 0.1088, step time: 0.2508\n",
      "115/281, train_loss: 0.0514, step time: 0.2367\n",
      "116/281, train_loss: 0.2321, step time: 0.2405\n",
      "117/281, train_loss: 0.2198, step time: 0.2401\n",
      "118/281, train_loss: 0.0741, step time: 0.2443\n",
      "119/281, train_loss: 0.1004, step time: 0.2494\n",
      "120/281, train_loss: 0.0894, step time: 0.2456\n",
      "121/281, train_loss: 0.0872, step time: 0.2483\n",
      "122/281, train_loss: 0.2026, step time: 0.2534\n",
      "123/281, train_loss: 0.0779, step time: 0.2457\n",
      "124/281, train_loss: 0.0752, step time: 0.2426\n",
      "125/281, train_loss: 0.2215, step time: 0.2430\n",
      "126/281, train_loss: 0.0642, step time: 0.2444\n",
      "127/281, train_loss: 0.0964, step time: 0.2499\n",
      "128/281, train_loss: 0.1041, step time: 0.2465\n",
      "129/281, train_loss: 0.2345, step time: 0.2417\n",
      "130/281, train_loss: 0.0991, step time: 0.2472\n",
      "131/281, train_loss: 0.2198, step time: 0.2482\n",
      "132/281, train_loss: 0.1074, step time: 0.2460\n",
      "133/281, train_loss: 0.0658, step time: 0.2439\n",
      "134/281, train_loss: 0.2096, step time: 0.2395\n",
      "135/281, train_loss: 0.0673, step time: 0.2504\n",
      "136/281, train_loss: 0.0525, step time: 0.2449\n",
      "137/281, train_loss: 0.0578, step time: 0.2480\n",
      "138/281, train_loss: 0.0484, step time: 0.2558\n",
      "139/281, train_loss: 0.0964, step time: 0.2511\n",
      "140/281, train_loss: 0.0562, step time: 0.2455\n",
      "141/281, train_loss: 0.2140, step time: 0.2436\n",
      "142/281, train_loss: 0.2771, step time: 0.2439\n",
      "143/281, train_loss: 0.0491, step time: 0.2455\n",
      "144/281, train_loss: 0.0684, step time: 0.2453\n",
      "145/281, train_loss: 0.2157, step time: 0.2424\n",
      "146/281, train_loss: 0.2405, step time: 0.2411\n",
      "147/281, train_loss: 0.0633, step time: 0.2397\n",
      "148/281, train_loss: 0.0745, step time: 0.2405\n",
      "149/281, train_loss: 0.0706, step time: 0.2387\n",
      "150/281, train_loss: 0.2134, step time: 0.2407\n",
      "151/281, train_loss: 0.0595, step time: 0.2413\n",
      "152/281, train_loss: 0.2246, step time: 0.2407\n",
      "153/281, train_loss: 0.0632, step time: 0.2412\n",
      "154/281, train_loss: 0.0704, step time: 0.2443\n",
      "155/281, train_loss: 0.0792, step time: 0.2446\n",
      "156/281, train_loss: 0.0773, step time: 0.2435\n",
      "157/281, train_loss: 0.0868, step time: 0.2442\n",
      "158/281, train_loss: 0.0452, step time: 0.2432\n",
      "159/281, train_loss: 0.0679, step time: 0.2490\n",
      "160/281, train_loss: 0.0751, step time: 0.2444\n",
      "161/281, train_loss: 0.0708, step time: 0.2435\n",
      "162/281, train_loss: 0.0392, step time: 0.2450\n",
      "163/281, train_loss: 0.0659, step time: 0.2473\n",
      "164/281, train_loss: 0.0902, step time: 0.2494\n",
      "165/281, train_loss: 0.0757, step time: 0.2454\n",
      "166/281, train_loss: 0.0856, step time: 0.2422\n",
      "167/281, train_loss: 0.1168, step time: 0.2618\n",
      "168/281, train_loss: 0.0840, step time: 0.2502\n",
      "169/281, train_loss: 0.0725, step time: 0.2480\n",
      "170/281, train_loss: 0.0747, step time: 0.2478\n",
      "171/281, train_loss: 0.0729, step time: 0.2517\n",
      "172/281, train_loss: 0.0477, step time: 0.2509\n",
      "173/281, train_loss: 0.0411, step time: 0.2487\n",
      "174/281, train_loss: 0.0380, step time: 0.2546\n",
      "175/281, train_loss: 0.1070, step time: 0.2528\n",
      "176/281, train_loss: 0.2184, step time: 0.2469\n",
      "177/281, train_loss: 0.0983, step time: 0.2402\n",
      "178/281, train_loss: 0.0605, step time: 0.2408\n",
      "179/281, train_loss: 0.0726, step time: 0.2478\n",
      "180/281, train_loss: 0.0667, step time: 0.2453\n",
      "181/281, train_loss: 0.0471, step time: 0.2430\n",
      "182/281, train_loss: 0.0616, step time: 0.2463\n",
      "183/281, train_loss: 0.2168, step time: 0.2501\n",
      "184/281, train_loss: 0.0739, step time: 0.2612\n",
      "185/281, train_loss: 0.0616, step time: 0.2426\n",
      "186/281, train_loss: 0.0455, step time: 0.2431\n",
      "187/281, train_loss: 0.2236, step time: 0.2396\n",
      "188/281, train_loss: 0.0508, step time: 0.2459\n",
      "189/281, train_loss: 0.0626, step time: 0.2436\n",
      "190/281, train_loss: 0.0761, step time: 0.2489\n",
      "191/281, train_loss: 0.2141, step time: 0.2460\n",
      "192/281, train_loss: 0.0739, step time: 0.2407\n",
      "193/281, train_loss: 0.0848, step time: 0.2402\n",
      "194/281, train_loss: 0.0442, step time: 0.2504\n",
      "195/281, train_loss: 0.0681, step time: 0.2503\n",
      "196/281, train_loss: 0.0768, step time: 0.2445\n",
      "197/281, train_loss: 0.0656, step time: 0.2453\n",
      "198/281, train_loss: 0.0703, step time: 0.2508\n",
      "199/281, train_loss: 0.0601, step time: 0.2464\n",
      "200/281, train_loss: 0.0868, step time: 0.2468\n",
      "201/281, train_loss: 0.2155, step time: 0.2477\n",
      "202/281, train_loss: 0.0765, step time: 0.2484\n",
      "203/281, train_loss: 0.0820, step time: 0.2473\n",
      "204/281, train_loss: 0.0771, step time: 0.2454\n",
      "205/281, train_loss: 0.0933, step time: 0.2434\n",
      "206/281, train_loss: 0.0806, step time: 0.2409\n",
      "207/281, train_loss: 0.0605, step time: 0.2477\n",
      "208/281, train_loss: 0.0766, step time: 0.2515\n",
      "209/281, train_loss: 0.1283, step time: 0.2286\n",
      "210/281, train_loss: 0.1078, step time: 0.2475\n",
      "211/281, train_loss: 0.0529, step time: 0.2484\n",
      "212/281, train_loss: 0.0594, step time: 0.2522\n",
      "213/281, train_loss: 0.0731, step time: 0.2639\n",
      "214/281, train_loss: 0.1014, step time: 0.2522\n",
      "215/281, train_loss: 0.0425, step time: 0.2490\n",
      "216/281, train_loss: 0.0651, step time: 0.2513\n",
      "217/281, train_loss: 0.0366, step time: 0.2485\n",
      "218/281, train_loss: 0.2285, step time: 0.2480\n",
      "219/281, train_loss: 0.1201, step time: 0.2463\n",
      "220/281, train_loss: 0.0629, step time: 0.2443\n",
      "221/281, train_loss: 0.0545, step time: 0.2486\n",
      "222/281, train_loss: 0.0779, step time: 0.2475\n",
      "223/281, train_loss: 0.1109, step time: 0.2487\n",
      "224/281, train_loss: 0.0686, step time: 0.2506\n",
      "225/281, train_loss: 0.0911, step time: 0.2497\n",
      "226/281, train_loss: 0.2451, step time: 0.2520\n",
      "227/281, train_loss: 0.2629, step time: 0.2479\n",
      "228/281, train_loss: 0.0969, step time: 0.2484\n",
      "229/281, train_loss: 0.1435, step time: 0.2529\n",
      "230/281, train_loss: 0.0846, step time: 0.2493\n",
      "231/281, train_loss: 0.0685, step time: 0.2486\n",
      "232/281, train_loss: 0.0819, step time: 0.2474\n",
      "233/281, train_loss: 0.0429, step time: 0.2501\n",
      "234/281, train_loss: 0.0518, step time: 0.2462\n",
      "235/281, train_loss: 0.0900, step time: 0.2439\n",
      "236/281, train_loss: 0.0856, step time: 0.2505\n",
      "237/281, train_loss: 0.0755, step time: 0.2470\n",
      "238/281, train_loss: 0.2499, step time: 0.2496\n",
      "239/281, train_loss: 0.0589, step time: 0.2477\n",
      "240/281, train_loss: 0.0650, step time: 0.2461\n",
      "241/281, train_loss: 0.0898, step time: 0.2455\n",
      "242/281, train_loss: 0.2452, step time: 0.2488\n",
      "243/281, train_loss: 0.1061, step time: 0.2436\n",
      "244/281, train_loss: 0.0750, step time: 0.2449\n",
      "245/281, train_loss: 0.0898, step time: 0.2467\n",
      "246/281, train_loss: 0.0456, step time: 0.2442\n",
      "247/281, train_loss: 0.0621, step time: 0.2430\n",
      "248/281, train_loss: 0.1042, step time: 0.2482\n",
      "249/281, train_loss: 0.0853, step time: 0.2434\n",
      "250/281, train_loss: 0.0493, step time: 0.2431\n",
      "251/281, train_loss: 0.0565, step time: 0.2431\n",
      "252/281, train_loss: 0.0709, step time: 0.2435\n",
      "253/281, train_loss: 0.0789, step time: 0.2487\n",
      "254/281, train_loss: 0.0819, step time: 0.2476\n",
      "255/281, train_loss: 0.0560, step time: 0.2514\n",
      "256/281, train_loss: 0.0469, step time: 0.2430\n",
      "257/281, train_loss: 0.1091, step time: 0.2432\n",
      "258/281, train_loss: 0.0826, step time: 0.2419\n",
      "259/281, train_loss: 0.0992, step time: 0.2454\n",
      "260/281, train_loss: 0.2246, step time: 0.2485\n",
      "261/281, train_loss: 0.0897, step time: 0.2469\n",
      "262/281, train_loss: 0.0897, step time: 0.2663\n",
      "263/281, train_loss: 0.0614, step time: 0.2444\n",
      "264/281, train_loss: 0.0497, step time: 0.2497\n",
      "265/281, train_loss: 0.0516, step time: 0.2478\n",
      "266/281, train_loss: 0.0417, step time: 0.2492\n",
      "267/281, train_loss: 0.0752, step time: 0.2505\n",
      "268/281, train_loss: 0.0994, step time: 0.2450\n",
      "269/281, train_loss: 0.0636, step time: 0.2436\n",
      "270/281, train_loss: 0.0968, step time: 0.2434\n",
      "271/281, train_loss: 0.0875, step time: 0.2546\n",
      "272/281, train_loss: 0.0666, step time: 0.2434\n",
      "273/281, train_loss: 0.1946, step time: 0.2419\n",
      "274/281, train_loss: 0.0511, step time: 0.2434\n",
      "275/281, train_loss: 0.0555, step time: 0.2442\n",
      "276/281, train_loss: 0.0509, step time: 0.2469\n",
      "277/281, train_loss: 0.0694, step time: 0.2490\n",
      "278/281, train_loss: 0.1241, step time: 0.2486\n",
      "279/281, train_loss: 0.0757, step time: 0.2489\n",
      "280/281, train_loss: 0.0898, step time: 0.2454\n",
      "281/281, train_loss: 0.0624, step time: 0.2444\n",
      "282/281, train_loss: 0.0422, step time: 0.1498\n",
      "epoch 148 average loss: 0.0967\n",
      "current epoch: 148 current mean dice: 0.8796 tc: 0.8833 wt: 0.9045 et: 0.8648\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 148 is: 381.4574\n",
      "----------\n",
      "epoch 149/200\n",
      "1/281, train_loss: 0.0712, step time: 0.2663\n",
      "2/281, train_loss: 0.0833, step time: 0.2516\n",
      "3/281, train_loss: 0.0522, step time: 0.2461\n",
      "4/281, train_loss: 0.0671, step time: 0.2462\n",
      "5/281, train_loss: 0.0735, step time: 0.2523\n",
      "6/281, train_loss: 0.0574, step time: 0.2577\n",
      "7/281, train_loss: 0.0412, step time: 0.2526\n",
      "8/281, train_loss: 0.0511, step time: 0.2568\n",
      "9/281, train_loss: 0.0704, step time: 0.2589\n",
      "10/281, train_loss: 0.2677, step time: 0.2614\n",
      "11/281, train_loss: 0.0890, step time: 0.2492\n",
      "12/281, train_loss: 0.0559, step time: 0.2478\n",
      "13/281, train_loss: 0.0794, step time: 0.2517\n",
      "14/281, train_loss: 0.0843, step time: 0.2452\n",
      "15/281, train_loss: 0.0835, step time: 0.2500\n",
      "16/281, train_loss: 0.0639, step time: 0.2531\n",
      "17/281, train_loss: 0.0569, step time: 0.2550\n",
      "18/281, train_loss: 0.0726, step time: 0.2527\n",
      "19/281, train_loss: 0.0468, step time: 0.2506\n",
      "20/281, train_loss: 0.0904, step time: 0.2530\n",
      "21/281, train_loss: 0.0595, step time: 0.2538\n",
      "22/281, train_loss: 0.0311, step time: 0.2512\n",
      "23/281, train_loss: 0.0794, step time: 0.2480\n",
      "24/281, train_loss: 0.0531, step time: 0.2475\n",
      "25/281, train_loss: 0.0499, step time: 0.2452\n",
      "26/281, train_loss: 0.0696, step time: 0.2472\n",
      "27/281, train_loss: 0.0837, step time: 0.2523\n",
      "28/281, train_loss: 0.0763, step time: 0.2538\n",
      "29/281, train_loss: 0.2348, step time: 0.2534\n",
      "30/281, train_loss: 0.0612, step time: 0.2458\n",
      "31/281, train_loss: 0.0674, step time: 0.2505\n",
      "32/281, train_loss: 0.0934, step time: 0.2508\n",
      "33/281, train_loss: 0.0514, step time: 0.2485\n",
      "34/281, train_loss: 0.1146, step time: 0.2502\n",
      "35/281, train_loss: 0.0845, step time: 0.2470\n",
      "36/281, train_loss: 0.0838, step time: 0.2518\n",
      "37/281, train_loss: 0.1034, step time: 0.2558\n",
      "38/281, train_loss: 0.2403, step time: 0.2577\n",
      "39/281, train_loss: 0.0517, step time: 0.2506\n",
      "40/281, train_loss: 0.2066, step time: 0.2594\n",
      "41/281, train_loss: 0.0494, step time: 0.2623\n",
      "42/281, train_loss: 0.0657, step time: 0.2558\n",
      "43/281, train_loss: 0.0818, step time: 0.2556\n",
      "44/281, train_loss: 0.0927, step time: 0.2511\n",
      "45/281, train_loss: 0.0704, step time: 0.2528\n",
      "46/281, train_loss: 0.0902, step time: 0.2563\n",
      "47/281, train_loss: 0.0848, step time: 0.2508\n",
      "48/281, train_loss: 0.0793, step time: 0.2507\n",
      "49/281, train_loss: 0.1123, step time: 0.2460\n",
      "50/281, train_loss: 0.2340, step time: 0.2468\n",
      "51/281, train_loss: 0.0547, step time: 0.2495\n",
      "52/281, train_loss: 0.2390, step time: 0.2528\n",
      "53/281, train_loss: 0.0779, step time: 0.2488\n",
      "54/281, train_loss: 0.0660, step time: 0.2525\n",
      "55/281, train_loss: 0.0870, step time: 0.2552\n",
      "56/281, train_loss: 0.0677, step time: 0.2542\n",
      "57/281, train_loss: 0.0582, step time: 0.2481\n",
      "58/281, train_loss: 0.0595, step time: 0.2503\n",
      "59/281, train_loss: 0.0806, step time: 0.2544\n",
      "60/281, train_loss: 0.0491, step time: 0.2490\n",
      "61/281, train_loss: 0.0449, step time: 0.2530\n",
      "62/281, train_loss: 0.0839, step time: 0.2476\n",
      "63/281, train_loss: 0.0487, step time: 0.2509\n",
      "64/281, train_loss: 0.2201, step time: 0.2488\n",
      "65/281, train_loss: 0.0812, step time: 0.2553\n",
      "66/281, train_loss: 0.0947, step time: 0.2543\n",
      "67/281, train_loss: 0.0576, step time: 0.2497\n",
      "68/281, train_loss: 0.0555, step time: 0.2572\n",
      "69/281, train_loss: 0.1091, step time: 0.2569\n",
      "70/281, train_loss: 0.0791, step time: 0.2523\n",
      "71/281, train_loss: 0.2135, step time: 0.2514\n",
      "72/281, train_loss: 0.0752, step time: 0.2603\n",
      "73/281, train_loss: 0.0943, step time: 0.2584\n",
      "74/281, train_loss: 0.0768, step time: 0.2528\n",
      "75/281, train_loss: 0.0764, step time: 0.2486\n",
      "76/281, train_loss: 0.0527, step time: 0.2538\n",
      "77/281, train_loss: 0.0774, step time: 0.2591\n",
      "78/281, train_loss: 0.0563, step time: 0.2693\n",
      "79/281, train_loss: 0.0602, step time: 0.2492\n",
      "80/281, train_loss: 0.0977, step time: 0.2589\n",
      "81/281, train_loss: 0.0373, step time: 0.2544\n",
      "82/281, train_loss: 0.0584, step time: 0.2529\n",
      "83/281, train_loss: 0.0677, step time: 0.2538\n",
      "84/281, train_loss: 0.0836, step time: 0.2517\n",
      "85/281, train_loss: 0.0705, step time: 0.2555\n",
      "86/281, train_loss: 0.0803, step time: 0.2564\n",
      "87/281, train_loss: 0.2102, step time: 0.2525\n",
      "88/281, train_loss: 0.2077, step time: 0.2485\n",
      "89/281, train_loss: 0.0790, step time: 0.2530\n",
      "90/281, train_loss: 0.0387, step time: 0.2500\n",
      "91/281, train_loss: 0.0589, step time: 0.2519\n",
      "92/281, train_loss: 0.2819, step time: 0.2532\n",
      "93/281, train_loss: 0.0660, step time: 0.2514\n",
      "94/281, train_loss: 0.0794, step time: 0.2578\n",
      "95/281, train_loss: 0.0674, step time: 0.2567\n",
      "96/281, train_loss: 0.0943, step time: 0.2686\n",
      "97/281, train_loss: 0.0669, step time: 0.2513\n",
      "98/281, train_loss: 0.0618, step time: 0.2563\n",
      "99/281, train_loss: 0.0821, step time: 0.2538\n",
      "100/281, train_loss: 0.0675, step time: 0.2528\n",
      "101/281, train_loss: 0.2251, step time: 0.2538\n",
      "102/281, train_loss: 0.0664, step time: 0.2555\n",
      "103/281, train_loss: 0.0748, step time: 0.2568\n",
      "104/281, train_loss: 0.2196, step time: 0.2498\n",
      "105/281, train_loss: 0.0549, step time: 0.2516\n",
      "106/281, train_loss: 0.0467, step time: 0.2543\n",
      "107/281, train_loss: 0.0714, step time: 0.2549\n",
      "108/281, train_loss: 0.0633, step time: 0.2544\n",
      "109/281, train_loss: 0.0593, step time: 0.2537\n",
      "110/281, train_loss: 0.0694, step time: 0.2868\n",
      "111/281, train_loss: 0.0726, step time: 0.2742\n",
      "112/281, train_loss: 0.0829, step time: 0.2522\n",
      "113/281, train_loss: 0.1992, step time: 0.2533\n",
      "114/281, train_loss: 0.1001, step time: 0.2502\n",
      "115/281, train_loss: 0.0882, step time: 0.2481\n",
      "116/281, train_loss: 0.0629, step time: 0.2447\n",
      "117/281, train_loss: 0.2302, step time: 0.2552\n",
      "118/281, train_loss: 0.0606, step time: 0.2520\n",
      "119/281, train_loss: 0.0629, step time: 0.2539\n",
      "120/281, train_loss: 0.2180, step time: 0.2456\n",
      "121/281, train_loss: 0.0751, step time: 0.2508\n",
      "122/281, train_loss: 0.2269, step time: 0.2545\n",
      "123/281, train_loss: 0.0474, step time: 0.2548\n",
      "124/281, train_loss: 0.0655, step time: 0.2538\n",
      "125/281, train_loss: 0.0683, step time: 0.2494\n",
      "126/281, train_loss: 0.0547, step time: 0.2501\n",
      "127/281, train_loss: 0.0754, step time: 0.2599\n",
      "128/281, train_loss: 0.0856, step time: 0.2580\n",
      "129/281, train_loss: 0.2247, step time: 0.2489\n",
      "130/281, train_loss: 0.0482, step time: 0.2480\n",
      "131/281, train_loss: 0.0544, step time: 0.2498\n",
      "132/281, train_loss: 0.0607, step time: 0.2496\n",
      "133/281, train_loss: 0.2200, step time: 0.2485\n",
      "134/281, train_loss: 0.0654, step time: 0.2554\n",
      "135/281, train_loss: 0.0775, step time: 0.2551\n",
      "136/281, train_loss: 0.2394, step time: 0.2509\n",
      "137/281, train_loss: 0.0530, step time: 0.2518\n",
      "138/281, train_loss: 0.1533, step time: 0.2557\n",
      "139/281, train_loss: 0.2186, step time: 0.2540\n",
      "140/281, train_loss: 0.0494, step time: 0.2518\n",
      "141/281, train_loss: 0.0885, step time: 0.2538\n",
      "142/281, train_loss: 0.0527, step time: 0.2607\n",
      "143/281, train_loss: 0.2206, step time: 0.2562\n",
      "144/281, train_loss: 0.0610, step time: 0.2556\n",
      "145/281, train_loss: 0.2523, step time: 0.2518\n",
      "146/281, train_loss: 0.0693, step time: 0.2548\n",
      "147/281, train_loss: 0.0524, step time: 0.2515\n",
      "148/281, train_loss: 0.0850, step time: 0.2573\n",
      "149/281, train_loss: 0.0653, step time: 0.2581\n",
      "150/281, train_loss: 0.0533, step time: 0.2549\n",
      "151/281, train_loss: 0.0888, step time: 0.2562\n",
      "152/281, train_loss: 0.0649, step time: 0.2542\n",
      "153/281, train_loss: 0.0649, step time: 0.2563\n",
      "154/281, train_loss: 0.0804, step time: 0.2532\n",
      "155/281, train_loss: 0.0638, step time: 0.2538\n",
      "156/281, train_loss: 0.0576, step time: 0.2575\n",
      "157/281, train_loss: 0.0486, step time: 0.2527\n",
      "158/281, train_loss: 0.2071, step time: 0.2515\n",
      "159/281, train_loss: 0.0729, step time: 0.2506\n",
      "160/281, train_loss: 0.0733, step time: 0.2479\n",
      "161/281, train_loss: 0.1212, step time: 0.2483\n",
      "162/281, train_loss: 0.2428, step time: 0.2464\n",
      "163/281, train_loss: 0.2143, step time: 0.2487\n",
      "164/281, train_loss: 0.0826, step time: 0.2495\n",
      "165/281, train_loss: 0.0577, step time: 0.2447\n",
      "166/281, train_loss: 0.0550, step time: 0.2500\n",
      "167/281, train_loss: 0.0508, step time: 0.2502\n",
      "168/281, train_loss: 0.0863, step time: 0.2483\n",
      "169/281, train_loss: 0.0377, step time: 0.2616\n",
      "170/281, train_loss: 0.0770, step time: 0.2536\n",
      "171/281, train_loss: 0.0556, step time: 0.2477\n",
      "172/281, train_loss: 0.0563, step time: 0.2506\n",
      "173/281, train_loss: 0.2726, step time: 0.2461\n",
      "174/281, train_loss: 0.0604, step time: 0.2477\n",
      "175/281, train_loss: 0.2309, step time: 0.2475\n",
      "176/281, train_loss: 0.0808, step time: 0.2494\n",
      "177/281, train_loss: 0.0637, step time: 0.2545\n",
      "178/281, train_loss: 0.0577, step time: 0.2485\n",
      "179/281, train_loss: 0.0579, step time: 0.2468\n",
      "180/281, train_loss: 0.1048, step time: 0.2433\n",
      "181/281, train_loss: 0.3778, step time: 0.2529\n",
      "182/281, train_loss: 0.0659, step time: 0.2471\n",
      "183/281, train_loss: 0.0471, step time: 0.2478\n",
      "184/281, train_loss: 0.0718, step time: 0.2431\n",
      "185/281, train_loss: 0.0938, step time: 0.2500\n",
      "186/281, train_loss: 0.0755, step time: 0.2464\n",
      "187/281, train_loss: 0.0691, step time: 0.2456\n",
      "188/281, train_loss: 0.0784, step time: 0.2520\n",
      "189/281, train_loss: 0.0813, step time: 0.2485\n",
      "190/281, train_loss: 0.0559, step time: 0.2519\n",
      "191/281, train_loss: 0.0819, step time: 0.2547\n",
      "192/281, train_loss: 0.0640, step time: 0.2536\n",
      "193/281, train_loss: 0.0422, step time: 0.2509\n",
      "194/281, train_loss: 0.0830, step time: 0.2518\n",
      "195/281, train_loss: 0.1012, step time: 0.2547\n",
      "196/281, train_loss: 0.1249, step time: 0.2578\n",
      "197/281, train_loss: 0.0615, step time: 0.2540\n",
      "198/281, train_loss: 0.0630, step time: 0.2524\n",
      "199/281, train_loss: 0.0703, step time: 0.2493\n",
      "200/281, train_loss: 0.0509, step time: 0.2528\n",
      "201/281, train_loss: 0.0609, step time: 0.2491\n",
      "202/281, train_loss: 0.2250, step time: 0.2507\n",
      "203/281, train_loss: 0.2169, step time: 0.2564\n",
      "204/281, train_loss: 0.0758, step time: 0.2473\n",
      "205/281, train_loss: 0.0853, step time: 0.2538\n",
      "206/281, train_loss: 0.0560, step time: 0.2554\n",
      "207/281, train_loss: 0.0593, step time: 0.2501\n",
      "208/281, train_loss: 0.0944, step time: 0.2506\n",
      "209/281, train_loss: 0.1125, step time: 0.2539\n",
      "210/281, train_loss: 0.0536, step time: 0.2499\n",
      "211/281, train_loss: 0.0694, step time: 0.2543\n",
      "212/281, train_loss: 0.0580, step time: 0.2500\n",
      "213/281, train_loss: 0.0810, step time: 0.2541\n",
      "214/281, train_loss: 0.0939, step time: 0.2521\n",
      "215/281, train_loss: 0.0588, step time: 0.2540\n",
      "216/281, train_loss: 0.0458, step time: 0.2529\n",
      "217/281, train_loss: 0.0895, step time: 0.2505\n",
      "218/281, train_loss: 0.0821, step time: 0.2504\n",
      "219/281, train_loss: 0.0782, step time: 0.2617\n",
      "220/281, train_loss: 0.0552, step time: 0.2554\n",
      "221/281, train_loss: 0.2404, step time: 0.2561\n",
      "222/281, train_loss: 0.0965, step time: 0.2541\n",
      "223/281, train_loss: 0.1155, step time: 0.2511\n",
      "224/281, train_loss: 0.0723, step time: 0.2509\n",
      "225/281, train_loss: 0.0756, step time: 0.2539\n",
      "226/281, train_loss: 0.2138, step time: 0.2515\n",
      "227/281, train_loss: 0.0576, step time: 0.2497\n",
      "228/281, train_loss: 0.0909, step time: 0.2471\n",
      "229/281, train_loss: 0.1382, step time: 0.2527\n",
      "230/281, train_loss: 0.0692, step time: 0.2468\n",
      "231/281, train_loss: 0.0620, step time: 0.2510\n",
      "232/281, train_loss: 0.0619, step time: 0.2508\n",
      "233/281, train_loss: 0.0592, step time: 0.2507\n",
      "234/281, train_loss: 0.0879, step time: 0.2460\n",
      "235/281, train_loss: 0.0652, step time: 0.2493\n",
      "236/281, train_loss: 0.0802, step time: 0.2482\n",
      "237/281, train_loss: 0.2559, step time: 0.2497\n",
      "238/281, train_loss: 0.0708, step time: 0.2470\n",
      "239/281, train_loss: 0.0684, step time: 0.2510\n",
      "240/281, train_loss: 0.0905, step time: 0.2505\n",
      "241/281, train_loss: 0.0690, step time: 0.2472\n",
      "242/281, train_loss: 0.0766, step time: 0.2437\n",
      "243/281, train_loss: 0.1110, step time: 0.2481\n",
      "244/281, train_loss: 0.2292, step time: 0.2476\n",
      "245/281, train_loss: 0.0739, step time: 0.2480\n",
      "246/281, train_loss: 0.2311, step time: 0.2503\n",
      "247/281, train_loss: 0.0495, step time: 0.2519\n",
      "248/281, train_loss: 0.2173, step time: 0.2529\n",
      "249/281, train_loss: 0.0480, step time: 0.2478\n",
      "250/281, train_loss: 0.2083, step time: 0.2482\n",
      "251/281, train_loss: 0.0744, step time: 0.2461\n",
      "252/281, train_loss: 0.0900, step time: 0.2490\n",
      "253/281, train_loss: 0.0716, step time: 0.2530\n",
      "254/281, train_loss: 0.0491, step time: 0.2522\n",
      "255/281, train_loss: 0.2149, step time: 0.2480\n",
      "256/281, train_loss: 0.0517, step time: 0.2517\n",
      "257/281, train_loss: 0.2030, step time: 0.2468\n",
      "258/281, train_loss: 0.1238, step time: 0.2447\n",
      "259/281, train_loss: 0.0686, step time: 0.2487\n",
      "260/281, train_loss: 0.0950, step time: 0.2461\n",
      "261/281, train_loss: 0.0893, step time: 0.2475\n",
      "262/281, train_loss: 0.0549, step time: 0.2444\n",
      "263/281, train_loss: 0.1164, step time: 0.2469\n",
      "264/281, train_loss: 0.0829, step time: 0.2445\n",
      "265/281, train_loss: 0.0748, step time: 0.2458\n",
      "266/281, train_loss: 0.0891, step time: 0.2481\n",
      "267/281, train_loss: 0.0933, step time: 0.2501\n",
      "268/281, train_loss: 0.0633, step time: 0.2485\n",
      "269/281, train_loss: 0.0578, step time: 0.2497\n",
      "270/281, train_loss: 0.0974, step time: 0.2498\n",
      "271/281, train_loss: 0.0649, step time: 0.2510\n",
      "272/281, train_loss: 0.2148, step time: 0.2523\n",
      "273/281, train_loss: 0.0583, step time: 0.2459\n",
      "274/281, train_loss: 0.0889, step time: 0.2479\n",
      "275/281, train_loss: 0.0895, step time: 0.2636\n",
      "276/281, train_loss: 0.1100, step time: 0.2498\n",
      "277/281, train_loss: 0.0585, step time: 0.2474\n",
      "278/281, train_loss: 0.0905, step time: 0.2507\n",
      "279/281, train_loss: 0.0501, step time: 0.2554\n",
      "280/281, train_loss: 0.0900, step time: 0.2585\n",
      "281/281, train_loss: 0.0877, step time: 0.2464\n",
      "282/281, train_loss: 0.0885, step time: 0.1486\n",
      "epoch 149 average loss: 0.0954\n",
      "current epoch: 149 current mean dice: 0.8865 tc: 0.8869 wt: 0.9063 et: 0.8810\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 149 is: 367.0428\n",
      "----------\n",
      "epoch 150/200\n",
      "1/281, train_loss: 0.0960, step time: 0.2534\n",
      "2/281, train_loss: 0.0473, step time: 0.2492\n",
      "3/281, train_loss: 0.0586, step time: 0.2442\n",
      "4/281, train_loss: 0.0718, step time: 0.2422\n",
      "5/281, train_loss: 0.0583, step time: 0.2616\n",
      "6/281, train_loss: 0.0358, step time: 0.2492\n",
      "7/281, train_loss: 0.2156, step time: 0.2442\n",
      "8/281, train_loss: 0.2363, step time: 0.2408\n",
      "9/281, train_loss: 0.0694, step time: 0.2545\n",
      "10/281, train_loss: 0.0793, step time: 0.2622\n",
      "11/281, train_loss: 0.2202, step time: 0.2745\n",
      "12/281, train_loss: 0.0383, step time: 0.2602\n",
      "13/281, train_loss: 0.0424, step time: 0.2496\n",
      "14/281, train_loss: 0.2484, step time: 0.2503\n",
      "15/281, train_loss: 0.2356, step time: 0.2478\n",
      "16/281, train_loss: 0.0546, step time: 0.2512\n",
      "17/281, train_loss: 0.0588, step time: 0.2598\n",
      "18/281, train_loss: 0.2241, step time: 0.2491\n",
      "19/281, train_loss: 0.0368, step time: 0.2481\n",
      "20/281, train_loss: 0.0495, step time: 0.2493\n",
      "21/281, train_loss: 0.0476, step time: 0.2552\n",
      "22/281, train_loss: 0.0745, step time: 0.2515\n",
      "23/281, train_loss: 0.0606, step time: 0.2513\n",
      "24/281, train_loss: 0.0757, step time: 0.2528\n",
      "25/281, train_loss: 0.0746, step time: 0.2518\n",
      "26/281, train_loss: 0.2279, step time: 0.2498\n",
      "27/281, train_loss: 0.0649, step time: 0.2483\n",
      "28/281, train_loss: 0.1125, step time: 0.2504\n",
      "29/281, train_loss: 0.0717, step time: 0.2523\n",
      "30/281, train_loss: 0.1293, step time: 0.2515\n",
      "31/281, train_loss: 0.0768, step time: 0.2557\n",
      "32/281, train_loss: 0.0945, step time: 0.2596\n",
      "33/281, train_loss: 0.2235, step time: 0.2520\n",
      "34/281, train_loss: 0.0803, step time: 0.2553\n",
      "35/281, train_loss: 0.0803, step time: 0.2478\n",
      "36/281, train_loss: 0.1506, step time: 0.2492\n",
      "37/281, train_loss: 0.0600, step time: 0.2559\n",
      "38/281, train_loss: 0.2519, step time: 0.2486\n",
      "39/281, train_loss: 0.0769, step time: 0.2474\n",
      "40/281, train_loss: 0.0737, step time: 0.2488\n",
      "41/281, train_loss: 0.0787, step time: 0.2530\n",
      "42/281, train_loss: 0.0710, step time: 0.2549\n",
      "43/281, train_loss: 0.0579, step time: 0.2520\n",
      "44/281, train_loss: 0.0707, step time: 0.2517\n",
      "45/281, train_loss: 0.0774, step time: 0.2453\n",
      "46/281, train_loss: 0.0780, step time: 0.2527\n",
      "47/281, train_loss: 0.0485, step time: 0.2506\n",
      "48/281, train_loss: 0.0350, step time: 0.2569\n",
      "49/281, train_loss: 0.0675, step time: 0.2582\n",
      "50/281, train_loss: 0.2309, step time: 0.2535\n",
      "51/281, train_loss: 0.0748, step time: 0.2575\n",
      "52/281, train_loss: 0.2073, step time: 0.2492\n",
      "53/281, train_loss: 0.0697, step time: 0.2496\n",
      "54/281, train_loss: 0.0560, step time: 0.2456\n",
      "55/281, train_loss: 0.0908, step time: 0.2529\n",
      "56/281, train_loss: 0.1277, step time: 0.2576\n",
      "57/281, train_loss: 0.0530, step time: 0.2457\n",
      "58/281, train_loss: 0.2122, step time: 0.2506\n",
      "59/281, train_loss: 0.0859, step time: 0.2501\n",
      "60/281, train_loss: 0.0525, step time: 0.2512\n",
      "61/281, train_loss: 0.2232, step time: 0.2547\n",
      "62/281, train_loss: 0.0921, step time: 0.2586\n",
      "63/281, train_loss: 0.1020, step time: 0.2510\n",
      "64/281, train_loss: 0.0481, step time: 0.2557\n",
      "65/281, train_loss: 0.0610, step time: 0.2582\n",
      "66/281, train_loss: 0.2299, step time: 0.2563\n",
      "67/281, train_loss: 0.2416, step time: 0.2493\n",
      "68/281, train_loss: 0.0791, step time: 0.2491\n",
      "69/281, train_loss: 0.0951, step time: 0.2554\n",
      "70/281, train_loss: 0.0939, step time: 0.2551\n",
      "71/281, train_loss: 0.0663, step time: 0.2537\n",
      "72/281, train_loss: 0.2232, step time: 0.2614\n",
      "73/281, train_loss: 0.1199, step time: 0.2566\n",
      "74/281, train_loss: 0.0604, step time: 0.2501\n",
      "75/281, train_loss: 0.0745, step time: 0.2621\n",
      "76/281, train_loss: 0.0944, step time: 0.2558\n",
      "77/281, train_loss: 0.0588, step time: 0.2526\n",
      "78/281, train_loss: 0.0900, step time: 0.2611\n",
      "79/281, train_loss: 0.0833, step time: 0.2535\n",
      "80/281, train_loss: 0.0523, step time: 0.2509\n",
      "81/281, train_loss: 0.0466, step time: 0.2536\n",
      "82/281, train_loss: 0.0733, step time: 0.2521\n",
      "83/281, train_loss: 0.0652, step time: 0.2592\n",
      "84/281, train_loss: 0.0726, step time: 0.2551\n",
      "85/281, train_loss: 0.1036, step time: 0.2556\n",
      "86/281, train_loss: 0.1056, step time: 0.2534\n",
      "87/281, train_loss: 0.0796, step time: 0.2520\n",
      "88/281, train_loss: 0.2403, step time: 0.2491\n",
      "89/281, train_loss: 0.1054, step time: 0.2551\n",
      "90/281, train_loss: 0.0672, step time: 0.2549\n",
      "91/281, train_loss: 0.0789, step time: 0.2559\n",
      "92/281, train_loss: 0.0643, step time: 0.2500\n",
      "93/281, train_loss: 0.0676, step time: 0.2582\n",
      "94/281, train_loss: 0.0819, step time: 0.2531\n",
      "95/281, train_loss: 0.0588, step time: 0.2489\n",
      "96/281, train_loss: 0.0831, step time: 0.2492\n",
      "97/281, train_loss: 0.0836, step time: 0.2504\n",
      "98/281, train_loss: 0.0709, step time: 0.2513\n",
      "99/281, train_loss: 0.0691, step time: 0.2491\n",
      "100/281, train_loss: 0.2392, step time: 0.2518\n",
      "101/281, train_loss: 0.0579, step time: 0.2475\n",
      "102/281, train_loss: 0.0714, step time: 0.2488\n",
      "103/281, train_loss: 0.0866, step time: 0.2518\n",
      "104/281, train_loss: 0.0593, step time: 0.2545\n",
      "105/281, train_loss: 0.0634, step time: 0.2556\n",
      "106/281, train_loss: 0.0747, step time: 0.2558\n",
      "107/281, train_loss: 0.0817, step time: 0.2562\n",
      "108/281, train_loss: 0.0773, step time: 0.2529\n",
      "109/281, train_loss: 0.0529, step time: 0.2517\n",
      "110/281, train_loss: 0.0623, step time: 0.2542\n",
      "111/281, train_loss: 0.0446, step time: 0.2490\n",
      "112/281, train_loss: 0.0930, step time: 0.2490\n",
      "113/281, train_loss: 0.0766, step time: 0.2524\n",
      "114/281, train_loss: 0.0632, step time: 0.2568\n",
      "115/281, train_loss: 0.0758, step time: 0.2519\n",
      "116/281, train_loss: 0.0796, step time: 0.2550\n",
      "117/281, train_loss: 0.0471, step time: 0.2482\n",
      "118/281, train_loss: 0.1019, step time: 0.2495\n",
      "119/281, train_loss: 0.0690, step time: 0.2534\n",
      "120/281, train_loss: 0.2477, step time: 0.2566\n",
      "121/281, train_loss: 0.0664, step time: 0.2488\n",
      "122/281, train_loss: 0.0800, step time: 0.2487\n",
      "123/281, train_loss: 0.0563, step time: 0.2567\n",
      "124/281, train_loss: 0.0423, step time: 0.2582\n",
      "125/281, train_loss: 0.0716, step time: 0.2586\n",
      "126/281, train_loss: 0.0483, step time: 0.2492\n",
      "127/281, train_loss: 0.0663, step time: 0.2511\n",
      "128/281, train_loss: 0.0397, step time: 0.2449\n",
      "129/281, train_loss: 0.0917, step time: 0.2440\n",
      "130/281, train_loss: 0.0769, step time: 0.2481\n",
      "131/281, train_loss: 0.1224, step time: 0.2482\n",
      "132/281, train_loss: 0.0633, step time: 0.2469\n",
      "133/281, train_loss: 0.2367, step time: 0.2481\n",
      "134/281, train_loss: 0.0976, step time: 0.2479\n",
      "135/281, train_loss: 0.0615, step time: 0.2481\n",
      "136/281, train_loss: 0.0628, step time: 0.2409\n",
      "137/281, train_loss: 0.0473, step time: 0.2408\n",
      "138/281, train_loss: 0.0725, step time: 0.2465\n",
      "139/281, train_loss: 0.0613, step time: 0.2451\n",
      "140/281, train_loss: 0.2287, step time: 0.2463\n",
      "141/281, train_loss: 0.0684, step time: 0.2489\n",
      "142/281, train_loss: 0.2503, step time: 0.2506\n",
      "143/281, train_loss: 0.0750, step time: 0.2424\n",
      "144/281, train_loss: 0.1041, step time: 0.2420\n",
      "145/281, train_loss: 0.0582, step time: 0.2420\n",
      "146/281, train_loss: 0.0765, step time: 0.2492\n",
      "147/281, train_loss: 0.1138, step time: 0.2487\n",
      "148/281, train_loss: 0.0347, step time: 0.2456\n",
      "149/281, train_loss: 0.2275, step time: 0.2429\n",
      "150/281, train_loss: 0.0448, step time: 0.2463\n",
      "151/281, train_loss: 0.0611, step time: 0.2446\n",
      "152/281, train_loss: 0.0769, step time: 0.2432\n",
      "153/281, train_loss: 0.0558, step time: 0.2425\n",
      "154/281, train_loss: 0.0846, step time: 0.2427\n",
      "155/281, train_loss: 0.0454, step time: 0.2405\n",
      "156/281, train_loss: 0.0546, step time: 0.2413\n",
      "157/281, train_loss: 0.0329, step time: 0.2427\n",
      "158/281, train_loss: 0.0544, step time: 0.2434\n",
      "159/281, train_loss: 0.0668, step time: 0.2578\n",
      "160/281, train_loss: 0.0770, step time: 0.2430\n",
      "161/281, train_loss: 0.0628, step time: 0.2433\n",
      "162/281, train_loss: 0.0766, step time: 0.2451\n",
      "163/281, train_loss: 0.0743, step time: 0.2399\n",
      "164/281, train_loss: 0.2105, step time: 0.2417\n",
      "165/281, train_loss: 0.0672, step time: 0.2435\n",
      "166/281, train_loss: 0.0797, step time: 0.2526\n",
      "167/281, train_loss: 0.0727, step time: 0.2484\n",
      "168/281, train_loss: 0.0399, step time: 0.2451\n",
      "169/281, train_loss: 0.0620, step time: 0.2442\n",
      "170/281, train_loss: 0.0981, step time: 0.2469\n",
      "171/281, train_loss: 0.0424, step time: 0.2500\n",
      "172/281, train_loss: 0.1028, step time: 0.2465\n",
      "173/281, train_loss: 0.0882, step time: 0.2423\n",
      "174/281, train_loss: 0.0850, step time: 0.2430\n",
      "175/281, train_loss: 0.0847, step time: 0.2476\n",
      "176/281, train_loss: 0.0877, step time: 0.2440\n",
      "177/281, train_loss: 0.0674, step time: 0.2424\n",
      "178/281, train_loss: 0.0450, step time: 0.2394\n",
      "179/281, train_loss: 0.0555, step time: 0.2481\n",
      "180/281, train_loss: 0.1353, step time: 0.2468\n",
      "181/281, train_loss: 0.1178, step time: 0.2402\n",
      "182/281, train_loss: 0.0740, step time: 0.2415\n",
      "183/281, train_loss: 0.2330, step time: 0.2498\n",
      "184/281, train_loss: 0.1166, step time: 0.2499\n",
      "185/281, train_loss: 0.2200, step time: 0.2491\n",
      "186/281, train_loss: 0.0732, step time: 0.2457\n",
      "187/281, train_loss: 0.0898, step time: 0.2502\n",
      "188/281, train_loss: 0.2078, step time: 0.2443\n",
      "189/281, train_loss: 0.0947, step time: 0.2568\n",
      "190/281, train_loss: 0.0782, step time: 0.2541\n",
      "191/281, train_loss: 0.0552, step time: 0.2518\n",
      "192/281, train_loss: 0.0553, step time: 0.2429\n",
      "193/281, train_loss: 0.0432, step time: 0.2487\n",
      "194/281, train_loss: 0.0767, step time: 0.2491\n",
      "195/281, train_loss: 0.0539, step time: 0.2445\n",
      "196/281, train_loss: 0.0756, step time: 0.2437\n",
      "197/281, train_loss: 0.0770, step time: 0.2432\n",
      "198/281, train_loss: 0.2438, step time: 0.2427\n",
      "199/281, train_loss: 0.2287, step time: 0.2471\n",
      "200/281, train_loss: 0.0659, step time: 0.2479\n",
      "201/281, train_loss: 0.0797, step time: 0.2443\n",
      "202/281, train_loss: 0.0497, step time: 0.2399\n",
      "203/281, train_loss: 0.0629, step time: 0.2434\n",
      "204/281, train_loss: 0.0577, step time: 0.2446\n",
      "205/281, train_loss: 0.0548, step time: 0.2469\n",
      "206/281, train_loss: 0.0505, step time: 0.2460\n",
      "207/281, train_loss: 0.1054, step time: 0.2434\n",
      "208/281, train_loss: 0.0651, step time: 0.2399\n",
      "209/281, train_loss: 0.0659, step time: 0.2490\n",
      "210/281, train_loss: 0.0958, step time: 0.2395\n",
      "211/281, train_loss: 0.0655, step time: 0.2417\n",
      "212/281, train_loss: 0.0658, step time: 0.2408\n",
      "213/281, train_loss: 0.0438, step time: 0.2409\n",
      "214/281, train_loss: 0.0597, step time: 0.2403\n",
      "215/281, train_loss: 0.0569, step time: 0.2408\n",
      "216/281, train_loss: 0.0542, step time: 0.2467\n",
      "217/281, train_loss: 0.1192, step time: 0.2443\n",
      "218/281, train_loss: 0.0885, step time: 0.2450\n",
      "219/281, train_loss: 0.0594, step time: 0.2424\n",
      "220/281, train_loss: 0.0672, step time: 0.2438\n",
      "221/281, train_loss: 0.0933, step time: 0.2513\n",
      "222/281, train_loss: 0.0524, step time: 0.2460\n",
      "223/281, train_loss: 0.0377, step time: 0.2512\n",
      "224/281, train_loss: 0.0603, step time: 0.2429\n",
      "225/281, train_loss: 0.0858, step time: 0.2413\n",
      "226/281, train_loss: 0.2269, step time: 0.2438\n",
      "227/281, train_loss: 0.0631, step time: 0.2420\n",
      "228/281, train_loss: 0.1029, step time: 0.2489\n",
      "229/281, train_loss: 0.0460, step time: 0.2458\n",
      "230/281, train_loss: 0.0601, step time: 0.2436\n",
      "231/281, train_loss: 0.2351, step time: 0.2420\n",
      "232/281, train_loss: 0.2780, step time: 0.2445\n",
      "233/281, train_loss: 0.1191, step time: 0.2472\n",
      "234/281, train_loss: 0.2192, step time: 0.2529\n",
      "235/281, train_loss: 0.2149, step time: 0.2479\n",
      "236/281, train_loss: 0.1340, step time: 0.2459\n",
      "237/281, train_loss: 0.0728, step time: 0.2481\n",
      "238/281, train_loss: 0.0757, step time: 0.2445\n",
      "239/281, train_loss: 0.0694, step time: 0.2463\n",
      "240/281, train_loss: 0.3054, step time: 0.2446\n",
      "241/281, train_loss: 0.0406, step time: 0.2416\n",
      "242/281, train_loss: 0.0508, step time: 0.2479\n",
      "243/281, train_loss: 0.0973, step time: 0.2479\n",
      "244/281, train_loss: 0.0729, step time: 0.2442\n",
      "245/281, train_loss: 0.0758, step time: 0.2451\n",
      "246/281, train_loss: 0.0741, step time: 0.2505\n",
      "247/281, train_loss: 0.2221, step time: 0.2456\n",
      "248/281, train_loss: 0.2104, step time: 0.2509\n",
      "249/281, train_loss: 0.0586, step time: 0.2441\n",
      "250/281, train_loss: 0.0694, step time: 0.2506\n",
      "251/281, train_loss: 0.2363, step time: 0.2422\n",
      "252/281, train_loss: 0.0905, step time: 0.2458\n",
      "253/281, train_loss: 0.2208, step time: 0.2499\n",
      "254/281, train_loss: 0.0910, step time: 0.2480\n",
      "255/281, train_loss: 0.0607, step time: 0.2475\n",
      "256/281, train_loss: 0.0368, step time: 0.2528\n",
      "257/281, train_loss: 0.0787, step time: 0.2458\n",
      "258/281, train_loss: 0.0795, step time: 0.2525\n",
      "259/281, train_loss: 0.0728, step time: 0.2503\n",
      "260/281, train_loss: 0.0530, step time: 0.2485\n",
      "261/281, train_loss: 0.0744, step time: 0.2468\n",
      "262/281, train_loss: 0.0812, step time: 0.2494\n",
      "263/281, train_loss: 0.0519, step time: 0.2471\n",
      "264/281, train_loss: 0.0497, step time: 0.2470\n",
      "265/281, train_loss: 0.0692, step time: 0.2502\n",
      "266/281, train_loss: 0.2344, step time: 0.2446\n",
      "267/281, train_loss: 0.0644, step time: 0.2487\n",
      "268/281, train_loss: 0.0491, step time: 0.2773\n",
      "269/281, train_loss: 0.0803, step time: 0.2464\n",
      "270/281, train_loss: 0.1431, step time: 0.2468\n",
      "271/281, train_loss: 0.0619, step time: 0.2469\n",
      "272/281, train_loss: 0.0685, step time: 0.2415\n",
      "273/281, train_loss: 0.0478, step time: 0.2581\n",
      "274/281, train_loss: 0.0962, step time: 0.2426\n",
      "275/281, train_loss: 0.0805, step time: 0.2441\n",
      "276/281, train_loss: 0.0899, step time: 0.2475\n",
      "277/281, train_loss: 0.2362, step time: 0.2472\n",
      "278/281, train_loss: 0.0790, step time: 0.2501\n",
      "279/281, train_loss: 0.0599, step time: 0.2430\n",
      "280/281, train_loss: 0.0824, step time: 0.2415\n",
      "281/281, train_loss: 0.0374, step time: 0.2370\n",
      "282/281, train_loss: 0.4003, step time: 0.1444\n",
      "epoch 150 average loss: 0.0964\n",
      "current epoch: 150 current mean dice: 0.8830 tc: 0.8861 wt: 0.9010 et: 0.8757\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 150 is: 393.7691\n",
      "----------\n",
      "epoch 151/200\n",
      "1/281, train_loss: 0.0760, step time: 0.2619\n",
      "2/281, train_loss: 0.0631, step time: 0.2762\n",
      "3/281, train_loss: 0.0783, step time: 0.2471\n",
      "4/281, train_loss: 0.0823, step time: 0.2477\n",
      "5/281, train_loss: 0.0631, step time: 0.2493\n",
      "6/281, train_loss: 0.0790, step time: 0.2521\n",
      "7/281, train_loss: 0.0534, step time: 0.2450\n",
      "8/281, train_loss: 0.0717, step time: 0.2463\n",
      "9/281, train_loss: 0.0563, step time: 0.2511\n",
      "10/281, train_loss: 0.1007, step time: 0.2488\n",
      "11/281, train_loss: 0.2495, step time: 0.2507\n",
      "12/281, train_loss: 0.0428, step time: 0.2469\n",
      "13/281, train_loss: 0.0752, step time: 0.2501\n",
      "14/281, train_loss: 0.2218, step time: 0.2480\n",
      "15/281, train_loss: 0.0507, step time: 0.2491\n",
      "16/281, train_loss: 0.0683, step time: 0.2446\n",
      "17/281, train_loss: 0.2476, step time: 0.2506\n",
      "18/281, train_loss: 0.0770, step time: 0.2500\n",
      "19/281, train_loss: 0.0725, step time: 0.2457\n",
      "20/281, train_loss: 0.0509, step time: 0.2431\n",
      "21/281, train_loss: 0.0945, step time: 0.2493\n",
      "22/281, train_loss: 0.0645, step time: 0.2491\n",
      "23/281, train_loss: 0.1326, step time: 0.2541\n",
      "24/281, train_loss: 0.0662, step time: 0.2475\n",
      "25/281, train_loss: 0.0672, step time: 0.2528\n",
      "26/281, train_loss: 0.2044, step time: 0.2498\n",
      "27/281, train_loss: 0.1180, step time: 0.2536\n",
      "28/281, train_loss: 0.0714, step time: 0.2563\n",
      "29/281, train_loss: 0.0705, step time: 0.2488\n",
      "30/281, train_loss: 0.0785, step time: 0.2461\n",
      "31/281, train_loss: 0.0508, step time: 0.2491\n",
      "32/281, train_loss: 0.0653, step time: 0.2460\n",
      "33/281, train_loss: 0.0744, step time: 0.2493\n",
      "34/281, train_loss: 0.0594, step time: 0.2539\n",
      "35/281, train_loss: 0.0617, step time: 0.2459\n",
      "36/281, train_loss: 0.0790, step time: 0.2498\n",
      "37/281, train_loss: 0.0739, step time: 0.2536\n",
      "38/281, train_loss: 0.0604, step time: 0.2492\n",
      "39/281, train_loss: 0.2340, step time: 0.2518\n",
      "40/281, train_loss: 0.0738, step time: 0.2497\n",
      "41/281, train_loss: 0.0938, step time: 0.2508\n",
      "42/281, train_loss: 0.0557, step time: 0.2511\n",
      "43/281, train_loss: 0.0607, step time: 0.2505\n",
      "44/281, train_loss: 0.0614, step time: 0.2431\n",
      "45/281, train_loss: 0.0736, step time: 0.2489\n",
      "46/281, train_loss: 0.0575, step time: 0.2461\n",
      "47/281, train_loss: 0.2205, step time: 0.2528\n",
      "48/281, train_loss: 0.0708, step time: 0.2494\n",
      "49/281, train_loss: 0.2181, step time: 0.2495\n",
      "50/281, train_loss: 0.2383, step time: 0.2459\n",
      "51/281, train_loss: 0.1219, step time: 0.2525\n",
      "52/281, train_loss: 0.0826, step time: 0.2465\n",
      "53/281, train_loss: 0.0475, step time: 0.2529\n",
      "54/281, train_loss: 0.2413, step time: 0.2489\n",
      "55/281, train_loss: 0.0980, step time: 0.2536\n",
      "56/281, train_loss: 0.1014, step time: 0.2519\n",
      "57/281, train_loss: 0.0607, step time: 0.2507\n",
      "58/281, train_loss: 0.2124, step time: 0.2488\n",
      "59/281, train_loss: 0.0428, step time: 0.2514\n",
      "60/281, train_loss: 0.0687, step time: 0.2488\n",
      "61/281, train_loss: 0.0701, step time: 0.2494\n",
      "62/281, train_loss: 0.0346, step time: 0.2537\n",
      "63/281, train_loss: 0.0850, step time: 0.2517\n",
      "64/281, train_loss: 0.0660, step time: 0.2499\n",
      "65/281, train_loss: 0.2293, step time: 0.2539\n",
      "66/281, train_loss: 0.0415, step time: 0.2517\n",
      "67/281, train_loss: 0.0809, step time: 0.2476\n",
      "68/281, train_loss: 0.0605, step time: 0.2460\n",
      "69/281, train_loss: 0.0863, step time: 0.2514\n",
      "70/281, train_loss: 0.0629, step time: 0.2507\n",
      "71/281, train_loss: 0.2168, step time: 0.2496\n",
      "72/281, train_loss: 0.0992, step time: 0.2509\n",
      "73/281, train_loss: 0.0534, step time: 0.2473\n",
      "74/281, train_loss: 0.0856, step time: 0.2541\n",
      "75/281, train_loss: 0.0472, step time: 0.2554\n",
      "76/281, train_loss: 0.0570, step time: 0.2533\n",
      "77/281, train_loss: 0.0589, step time: 0.2568\n",
      "78/281, train_loss: 0.0769, step time: 0.2548\n",
      "79/281, train_loss: 0.0709, step time: 0.2488\n",
      "80/281, train_loss: 0.1055, step time: 0.2436\n",
      "81/281, train_loss: 0.0613, step time: 0.2525\n",
      "82/281, train_loss: 0.3796, step time: 0.2523\n",
      "83/281, train_loss: 0.0765, step time: 0.2506\n",
      "84/281, train_loss: 0.2247, step time: 0.2590\n",
      "85/281, train_loss: 0.0417, step time: 0.2555\n",
      "86/281, train_loss: 0.0791, step time: 0.2491\n",
      "87/281, train_loss: 0.0702, step time: 0.2509\n",
      "88/281, train_loss: 0.0411, step time: 0.2482\n",
      "89/281, train_loss: 0.0671, step time: 0.2503\n",
      "90/281, train_loss: 0.0579, step time: 0.2515\n",
      "91/281, train_loss: 0.0688, step time: 0.2489\n",
      "92/281, train_loss: 0.2269, step time: 0.2484\n",
      "93/281, train_loss: 0.0749, step time: 0.2552\n",
      "94/281, train_loss: 0.0615, step time: 0.2486\n",
      "95/281, train_loss: 0.1270, step time: 0.2551\n",
      "96/281, train_loss: 0.2786, step time: 0.2527\n",
      "97/281, train_loss: 0.0669, step time: 0.2525\n",
      "98/281, train_loss: 0.0533, step time: 0.2480\n",
      "99/281, train_loss: 0.0686, step time: 0.2507\n",
      "100/281, train_loss: 0.0446, step time: 0.2611\n",
      "101/281, train_loss: 0.0582, step time: 0.2534\n",
      "102/281, train_loss: 0.0687, step time: 0.2504\n",
      "103/281, train_loss: 0.0883, step time: 0.2558\n",
      "104/281, train_loss: 0.0904, step time: 0.2509\n",
      "105/281, train_loss: 0.0792, step time: 0.2588\n",
      "106/281, train_loss: 0.0978, step time: 0.2513\n",
      "107/281, train_loss: 0.0673, step time: 0.2493\n",
      "108/281, train_loss: 0.0970, step time: 0.2492\n",
      "109/281, train_loss: 0.0536, step time: 0.2532\n",
      "110/281, train_loss: 0.0912, step time: 0.2557\n",
      "111/281, train_loss: 0.0496, step time: 0.2482\n",
      "112/281, train_loss: 0.1129, step time: 0.2522\n",
      "113/281, train_loss: 0.0871, step time: 0.2549\n",
      "114/281, train_loss: 0.2086, step time: 0.2528\n",
      "115/281, train_loss: 0.0725, step time: 0.2462\n",
      "116/281, train_loss: 0.0398, step time: 0.2500\n",
      "117/281, train_loss: 0.0759, step time: 0.2646\n",
      "118/281, train_loss: 0.0800, step time: 0.2494\n",
      "119/281, train_loss: 0.0455, step time: 0.2496\n",
      "120/281, train_loss: 0.0872, step time: 0.2486\n",
      "121/281, train_loss: 0.0480, step time: 0.2549\n",
      "122/281, train_loss: 0.0723, step time: 0.2493\n",
      "123/281, train_loss: 0.2246, step time: 0.2582\n",
      "124/281, train_loss: 0.0653, step time: 0.2473\n",
      "125/281, train_loss: 0.0630, step time: 0.2528\n",
      "126/281, train_loss: 0.0643, step time: 0.2480\n",
      "127/281, train_loss: 0.0754, step time: 0.2541\n",
      "128/281, train_loss: 0.2120, step time: 0.2611\n",
      "129/281, train_loss: 0.0949, step time: 0.2505\n",
      "130/281, train_loss: 0.0642, step time: 0.2484\n",
      "131/281, train_loss: 0.0564, step time: 0.2510\n",
      "132/281, train_loss: 0.0462, step time: 0.2481\n",
      "133/281, train_loss: 0.2096, step time: 0.2592\n",
      "134/281, train_loss: 0.1031, step time: 0.2587\n",
      "135/281, train_loss: 0.0458, step time: 0.2509\n",
      "136/281, train_loss: 0.0635, step time: 0.2505\n",
      "137/281, train_loss: 0.0877, step time: 0.2476\n",
      "138/281, train_loss: 0.1148, step time: 0.2551\n",
      "139/281, train_loss: 0.0413, step time: 0.2486\n",
      "140/281, train_loss: 0.0588, step time: 0.2537\n",
      "141/281, train_loss: 0.2178, step time: 0.2543\n",
      "142/281, train_loss: 0.2467, step time: 0.2521\n",
      "143/281, train_loss: 0.0506, step time: 0.2505\n",
      "144/281, train_loss: 0.0601, step time: 0.2480\n",
      "145/281, train_loss: 0.0824, step time: 0.2536\n",
      "146/281, train_loss: 0.0767, step time: 0.2529\n",
      "147/281, train_loss: 0.0706, step time: 0.2532\n",
      "148/281, train_loss: 0.0847, step time: 0.2475\n",
      "149/281, train_loss: 0.0653, step time: 0.2530\n",
      "150/281, train_loss: 0.0428, step time: 0.2587\n",
      "151/281, train_loss: 0.2249, step time: 0.2569\n",
      "152/281, train_loss: 0.0834, step time: 0.2533\n",
      "153/281, train_loss: 0.0766, step time: 0.2573\n",
      "154/281, train_loss: 0.1120, step time: 0.2475\n",
      "155/281, train_loss: 0.0483, step time: 0.2523\n",
      "156/281, train_loss: 0.1182, step time: 0.2507\n",
      "157/281, train_loss: 0.0731, step time: 0.2556\n",
      "158/281, train_loss: 0.0663, step time: 0.2509\n",
      "159/281, train_loss: 0.0609, step time: 0.2537\n",
      "160/281, train_loss: 0.1086, step time: 0.2513\n",
      "161/281, train_loss: 0.0785, step time: 0.2499\n",
      "162/281, train_loss: 0.0747, step time: 0.2500\n",
      "163/281, train_loss: 0.0596, step time: 0.2545\n",
      "164/281, train_loss: 0.0749, step time: 0.2522\n",
      "165/281, train_loss: 0.0472, step time: 0.2516\n",
      "166/281, train_loss: 0.0588, step time: 0.2542\n",
      "167/281, train_loss: 0.0587, step time: 0.2558\n",
      "168/281, train_loss: 0.0887, step time: 0.2530\n",
      "169/281, train_loss: 0.2371, step time: 0.2575\n",
      "170/281, train_loss: 0.0734, step time: 0.2497\n",
      "171/281, train_loss: 0.2087, step time: 0.2511\n",
      "172/281, train_loss: 0.0587, step time: 0.2513\n",
      "173/281, train_loss: 0.2101, step time: 0.2593\n",
      "174/281, train_loss: 0.1226, step time: 0.2528\n",
      "175/281, train_loss: 0.0627, step time: 0.2519\n",
      "176/281, train_loss: 0.0689, step time: 0.2546\n",
      "177/281, train_loss: 0.0810, step time: 0.2489\n",
      "178/281, train_loss: 0.1032, step time: 0.2530\n",
      "179/281, train_loss: 0.0737, step time: 0.2537\n",
      "180/281, train_loss: 0.0959, step time: 0.2522\n",
      "181/281, train_loss: 0.0743, step time: 0.2540\n",
      "182/281, train_loss: 0.0518, step time: 0.2550\n",
      "183/281, train_loss: 0.0651, step time: 0.2512\n",
      "184/281, train_loss: 0.0616, step time: 0.2514\n",
      "185/281, train_loss: 0.2077, step time: 0.2507\n",
      "186/281, train_loss: 0.0700, step time: 0.2528\n",
      "187/281, train_loss: 0.0862, step time: 0.2539\n",
      "188/281, train_loss: 0.1384, step time: 0.2515\n",
      "189/281, train_loss: 0.0715, step time: 0.2534\n",
      "190/281, train_loss: 0.0851, step time: 0.2481\n",
      "191/281, train_loss: 0.0684, step time: 0.2495\n",
      "192/281, train_loss: 0.0572, step time: 0.2513\n",
      "193/281, train_loss: 0.2330, step time: 0.2502\n",
      "194/281, train_loss: 0.1271, step time: 0.2549\n",
      "195/281, train_loss: 0.0657, step time: 0.2563\n",
      "196/281, train_loss: 0.0809, step time: 0.2539\n",
      "197/281, train_loss: 0.0939, step time: 0.2549\n",
      "198/281, train_loss: 0.0447, step time: 0.2575\n",
      "199/281, train_loss: 0.2260, step time: 0.2530\n",
      "200/281, train_loss: 0.0729, step time: 0.2536\n",
      "201/281, train_loss: 0.0511, step time: 0.2567\n",
      "202/281, train_loss: 0.0848, step time: 0.2491\n",
      "203/281, train_loss: 0.0533, step time: 0.2521\n",
      "204/281, train_loss: 0.0698, step time: 0.2533\n",
      "205/281, train_loss: 0.1034, step time: 0.2459\n",
      "206/281, train_loss: 0.2173, step time: 0.2500\n",
      "207/281, train_loss: 0.0603, step time: 0.2452\n",
      "208/281, train_loss: 0.0862, step time: 0.2505\n",
      "209/281, train_loss: 0.0567, step time: 0.2495\n",
      "210/281, train_loss: 0.0887, step time: 0.2491\n",
      "211/281, train_loss: 0.0621, step time: 0.2568\n",
      "212/281, train_loss: 0.0551, step time: 0.2505\n",
      "213/281, train_loss: 0.0641, step time: 0.2460\n",
      "214/281, train_loss: 0.0557, step time: 0.2496\n",
      "215/281, train_loss: 0.0859, step time: 0.2451\n",
      "216/281, train_loss: 0.0551, step time: 0.2429\n",
      "217/281, train_loss: 0.0799, step time: 0.2208\n",
      "218/281, train_loss: 0.0500, step time: 0.2449\n",
      "219/281, train_loss: 0.0678, step time: 0.2448\n",
      "220/281, train_loss: 0.0369, step time: 0.2462\n",
      "221/281, train_loss: 0.0763, step time: 0.2460\n",
      "222/281, train_loss: 0.0604, step time: 0.2486\n",
      "223/281, train_loss: 0.0704, step time: 0.2483\n",
      "224/281, train_loss: 0.0814, step time: 0.2507\n",
      "225/281, train_loss: 0.2190, step time: 0.2505\n",
      "226/281, train_loss: 0.0608, step time: 0.2515\n",
      "227/281, train_loss: 0.0672, step time: 0.2439\n",
      "228/281, train_loss: 0.0752, step time: 0.2459\n",
      "229/281, train_loss: 0.0583, step time: 0.2478\n",
      "230/281, train_loss: 0.2260, step time: 0.2494\n",
      "231/281, train_loss: 0.0363, step time: 0.2531\n",
      "232/281, train_loss: 0.0792, step time: 0.2504\n",
      "233/281, train_loss: 0.0642, step time: 0.2486\n",
      "234/281, train_loss: 0.0505, step time: 0.2433\n",
      "235/281, train_loss: 0.0622, step time: 0.2481\n",
      "236/281, train_loss: 0.1141, step time: 0.2485\n",
      "237/281, train_loss: 0.0682, step time: 0.2490\n",
      "238/281, train_loss: 0.0784, step time: 0.2495\n",
      "239/281, train_loss: 0.0574, step time: 0.2581\n",
      "240/281, train_loss: 0.0671, step time: 0.2572\n",
      "241/281, train_loss: 0.2296, step time: 0.2541\n",
      "242/281, train_loss: 0.2365, step time: 0.2560\n",
      "243/281, train_loss: 0.0709, step time: 0.2496\n",
      "244/281, train_loss: 0.0883, step time: 0.2516\n",
      "245/281, train_loss: 0.2157, step time: 0.2514\n",
      "246/281, train_loss: 0.0672, step time: 0.2496\n",
      "247/281, train_loss: 0.0675, step time: 0.2525\n",
      "248/281, train_loss: 0.0512, step time: 0.2526\n",
      "249/281, train_loss: 0.0509, step time: 0.2482\n",
      "250/281, train_loss: 0.0386, step time: 0.2482\n",
      "251/281, train_loss: 0.0796, step time: 0.2475\n",
      "252/281, train_loss: 0.0720, step time: 0.2552\n",
      "253/281, train_loss: 0.0693, step time: 0.2513\n",
      "254/281, train_loss: 0.2211, step time: 0.2511\n",
      "255/281, train_loss: 0.0748, step time: 0.2501\n",
      "256/281, train_loss: 0.0479, step time: 0.2512\n",
      "257/281, train_loss: 0.2066, step time: 0.2500\n",
      "258/281, train_loss: 0.0725, step time: 0.2510\n",
      "259/281, train_loss: 0.0720, step time: 0.2536\n",
      "260/281, train_loss: 0.0575, step time: 0.2513\n",
      "261/281, train_loss: 0.2448, step time: 0.2513\n",
      "262/281, train_loss: 0.0517, step time: 0.2504\n",
      "263/281, train_loss: 0.0753, step time: 0.2525\n",
      "264/281, train_loss: 0.0468, step time: 0.2538\n",
      "265/281, train_loss: 0.2162, step time: 0.2504\n",
      "266/281, train_loss: 0.0760, step time: 0.2511\n",
      "267/281, train_loss: 0.1132, step time: 0.2474\n",
      "268/281, train_loss: 0.0880, step time: 0.2493\n",
      "269/281, train_loss: 0.0534, step time: 0.2469\n",
      "270/281, train_loss: 0.0932, step time: 0.2470\n",
      "271/281, train_loss: 0.0744, step time: 0.2508\n",
      "272/281, train_loss: 0.0758, step time: 0.2532\n",
      "273/281, train_loss: 0.0435, step time: 0.2488\n",
      "274/281, train_loss: 0.0586, step time: 0.2497\n",
      "275/281, train_loss: 0.1158, step time: 0.2558\n",
      "276/281, train_loss: 0.0749, step time: 0.2507\n",
      "277/281, train_loss: 0.0766, step time: 0.2537\n",
      "278/281, train_loss: 0.0585, step time: 0.2564\n",
      "279/281, train_loss: 0.0749, step time: 0.2543\n",
      "280/281, train_loss: 0.0854, step time: 0.2514\n",
      "281/281, train_loss: 0.0948, step time: 0.2462\n",
      "282/281, train_loss: 0.3876, step time: 0.1528\n",
      "epoch 151 average loss: 0.0945\n",
      "current epoch: 151 current mean dice: 0.8803 tc: 0.8790 wt: 0.9045 et: 0.8713\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 151 is: 384.6772\n",
      "----------\n",
      "epoch 152/200\n",
      "1/281, train_loss: 0.1182, step time: 0.2509\n",
      "2/281, train_loss: 0.0588, step time: 0.2550\n",
      "3/281, train_loss: 0.0710, step time: 0.2512\n",
      "4/281, train_loss: 0.0684, step time: 0.2542\n",
      "5/281, train_loss: 0.0614, step time: 0.2473\n",
      "6/281, train_loss: 0.0812, step time: 0.2494\n",
      "7/281, train_loss: 0.0728, step time: 0.2520\n",
      "8/281, train_loss: 0.0584, step time: 0.2482\n",
      "9/281, train_loss: 0.0747, step time: 0.2545\n",
      "10/281, train_loss: 0.0803, step time: 0.2486\n",
      "11/281, train_loss: 0.2659, step time: 0.2517\n",
      "12/281, train_loss: 0.0392, step time: 0.2477\n",
      "13/281, train_loss: 0.0451, step time: 0.2466\n",
      "14/281, train_loss: 0.1145, step time: 0.2461\n",
      "15/281, train_loss: 0.0688, step time: 0.2489\n",
      "16/281, train_loss: 0.0518, step time: 0.2459\n",
      "17/281, train_loss: 0.0694, step time: 0.2510\n",
      "18/281, train_loss: 0.0511, step time: 0.2568\n",
      "19/281, train_loss: 0.0696, step time: 0.2617\n",
      "20/281, train_loss: 0.0583, step time: 0.2778\n",
      "21/281, train_loss: 0.0443, step time: 0.2516\n",
      "22/281, train_loss: 0.0780, step time: 0.2453\n",
      "23/281, train_loss: 0.0926, step time: 0.2465\n",
      "24/281, train_loss: 0.2739, step time: 0.2492\n",
      "25/281, train_loss: 0.0428, step time: 0.2528\n",
      "26/281, train_loss: 0.0701, step time: 0.2520\n",
      "27/281, train_loss: 0.1175, step time: 0.2487\n",
      "28/281, train_loss: 0.0539, step time: 0.2468\n",
      "29/281, train_loss: 0.0468, step time: 0.2517\n",
      "30/281, train_loss: 0.0565, step time: 0.2499\n",
      "31/281, train_loss: 0.0666, step time: 0.2467\n",
      "32/281, train_loss: 0.0604, step time: 0.2433\n",
      "33/281, train_loss: 0.0881, step time: 0.2551\n",
      "34/281, train_loss: 0.0385, step time: 0.2489\n",
      "35/281, train_loss: 0.0893, step time: 0.2646\n",
      "36/281, train_loss: 0.0668, step time: 0.2545\n",
      "37/281, train_loss: 0.0607, step time: 0.2566\n",
      "38/281, train_loss: 0.1170, step time: 0.2583\n",
      "39/281, train_loss: 0.0690, step time: 0.2542\n",
      "40/281, train_loss: 0.0863, step time: 0.2489\n",
      "41/281, train_loss: 0.0905, step time: 0.2502\n",
      "42/281, train_loss: 0.0894, step time: 0.2517\n",
      "43/281, train_loss: 0.0436, step time: 0.2474\n",
      "44/281, train_loss: 0.0713, step time: 0.2486\n",
      "45/281, train_loss: 0.0303, step time: 0.2451\n",
      "46/281, train_loss: 0.0914, step time: 0.2453\n",
      "47/281, train_loss: 0.0835, step time: 0.2558\n",
      "48/281, train_loss: 0.0561, step time: 0.2474\n",
      "49/281, train_loss: 0.0567, step time: 0.2653\n",
      "50/281, train_loss: 0.0760, step time: 0.2643\n",
      "51/281, train_loss: 0.0492, step time: 0.2484\n",
      "52/281, train_loss: 0.0602, step time: 0.2514\n",
      "53/281, train_loss: 0.0773, step time: 0.2459\n",
      "54/281, train_loss: 0.0743, step time: 0.2482\n",
      "55/281, train_loss: 0.2487, step time: 0.2469\n",
      "56/281, train_loss: 0.2127, step time: 0.2466\n",
      "57/281, train_loss: 0.0698, step time: 0.2531\n",
      "58/281, train_loss: 0.0736, step time: 0.2476\n",
      "59/281, train_loss: 0.0612, step time: 0.2500\n",
      "60/281, train_loss: 0.0815, step time: 0.2491\n",
      "61/281, train_loss: 0.2346, step time: 0.2569\n",
      "62/281, train_loss: 0.0492, step time: 0.2544\n",
      "63/281, train_loss: 0.0668, step time: 0.2470\n",
      "64/281, train_loss: 0.0448, step time: 0.2487\n",
      "65/281, train_loss: 0.0453, step time: 0.2591\n",
      "66/281, train_loss: 0.1321, step time: 0.2548\n",
      "67/281, train_loss: 0.2484, step time: 0.2505\n",
      "68/281, train_loss: 0.0452, step time: 0.2546\n",
      "69/281, train_loss: 0.0648, step time: 0.2480\n",
      "70/281, train_loss: 0.1135, step time: 0.2531\n",
      "71/281, train_loss: 0.0580, step time: 0.2488\n",
      "72/281, train_loss: 0.2295, step time: 0.2496\n",
      "73/281, train_loss: 0.0889, step time: 0.2498\n",
      "74/281, train_loss: 0.0772, step time: 0.2523\n",
      "75/281, train_loss: 0.0687, step time: 0.2479\n",
      "76/281, train_loss: 0.0478, step time: 0.2493\n",
      "77/281, train_loss: 0.0949, step time: 0.2588\n",
      "78/281, train_loss: 0.0964, step time: 0.2486\n",
      "79/281, train_loss: 0.0802, step time: 0.2514\n",
      "80/281, train_loss: 0.0590, step time: 0.2585\n",
      "81/281, train_loss: 0.0756, step time: 0.2470\n",
      "82/281, train_loss: 0.0805, step time: 0.2536\n",
      "83/281, train_loss: 0.0643, step time: 0.2455\n",
      "84/281, train_loss: 0.0458, step time: 0.2489\n",
      "85/281, train_loss: 0.0827, step time: 0.2506\n",
      "86/281, train_loss: 0.0530, step time: 0.2475\n",
      "87/281, train_loss: 0.0891, step time: 0.2531\n",
      "88/281, train_loss: 0.0585, step time: 0.2494\n",
      "89/281, train_loss: 0.0520, step time: 0.2497\n",
      "90/281, train_loss: 0.0544, step time: 0.2478\n",
      "91/281, train_loss: 0.0859, step time: 0.2462\n",
      "92/281, train_loss: 0.0588, step time: 0.2476\n",
      "93/281, train_loss: 0.0888, step time: 0.2498\n",
      "94/281, train_loss: 0.0634, step time: 0.2502\n",
      "95/281, train_loss: 0.0766, step time: 0.2562\n",
      "96/281, train_loss: 0.0709, step time: 0.2424\n",
      "97/281, train_loss: 0.0531, step time: 0.2432\n",
      "98/281, train_loss: 0.0577, step time: 0.2501\n",
      "99/281, train_loss: 0.0372, step time: 0.2588\n",
      "100/281, train_loss: 0.0508, step time: 0.2465\n",
      "101/281, train_loss: 0.0767, step time: 0.2499\n",
      "102/281, train_loss: 0.2355, step time: 0.2457\n",
      "103/281, train_loss: 0.0746, step time: 0.2523\n",
      "104/281, train_loss: 0.0616, step time: 0.2496\n",
      "105/281, train_loss: 0.2423, step time: 0.2488\n",
      "106/281, train_loss: 0.2304, step time: 0.2503\n",
      "107/281, train_loss: 0.0856, step time: 0.2502\n",
      "108/281, train_loss: 0.0930, step time: 0.2470\n",
      "109/281, train_loss: 0.0654, step time: 0.2496\n",
      "110/281, train_loss: 0.0722, step time: 0.2505\n",
      "111/281, train_loss: 0.0495, step time: 0.2488\n",
      "112/281, train_loss: 0.0680, step time: 0.2463\n",
      "113/281, train_loss: 0.2161, step time: 0.2521\n",
      "114/281, train_loss: 0.0648, step time: 0.2463\n",
      "115/281, train_loss: 0.0708, step time: 0.2537\n",
      "116/281, train_loss: 0.2386, step time: 0.2505\n",
      "117/281, train_loss: 0.0715, step time: 0.2500\n",
      "118/281, train_loss: 0.0791, step time: 0.2456\n",
      "119/281, train_loss: 0.0578, step time: 0.2527\n",
      "120/281, train_loss: 0.0527, step time: 0.2518\n",
      "121/281, train_loss: 0.0571, step time: 0.2482\n",
      "122/281, train_loss: 0.2105, step time: 0.2500\n",
      "123/281, train_loss: 0.0396, step time: 0.2501\n",
      "124/281, train_loss: 0.0699, step time: 0.2484\n",
      "125/281, train_loss: 0.0713, step time: 0.2420\n",
      "126/281, train_loss: 0.0678, step time: 0.2418\n",
      "127/281, train_loss: 0.0850, step time: 0.2572\n",
      "128/281, train_loss: 0.0612, step time: 0.2455\n",
      "129/281, train_loss: 0.0421, step time: 0.2451\n",
      "130/281, train_loss: 0.0582, step time: 0.2448\n",
      "131/281, train_loss: 0.2178, step time: 0.2500\n",
      "132/281, train_loss: 0.2244, step time: 0.2525\n",
      "133/281, train_loss: 0.2141, step time: 0.2497\n",
      "134/281, train_loss: 0.0674, step time: 0.2507\n",
      "135/281, train_loss: 0.0695, step time: 0.2527\n",
      "136/281, train_loss: 0.0828, step time: 0.2529\n",
      "137/281, train_loss: 0.0638, step time: 0.2459\n",
      "138/281, train_loss: 0.0648, step time: 0.2444\n",
      "139/281, train_loss: 0.0598, step time: 0.2479\n",
      "140/281, train_loss: 0.2239, step time: 0.2492\n",
      "141/281, train_loss: 0.1999, step time: 0.2464\n",
      "142/281, train_loss: 0.2172, step time: 0.2441\n",
      "143/281, train_loss: 0.0730, step time: 0.2553\n",
      "144/281, train_loss: 0.2114, step time: 0.2526\n",
      "145/281, train_loss: 0.0436, step time: 0.2473\n",
      "146/281, train_loss: 0.0631, step time: 0.2470\n",
      "147/281, train_loss: 0.0703, step time: 0.2464\n",
      "148/281, train_loss: 0.0878, step time: 0.2525\n",
      "149/281, train_loss: 0.0755, step time: 0.2472\n",
      "150/281, train_loss: 0.0705, step time: 0.2520\n",
      "151/281, train_loss: 0.0734, step time: 0.2522\n",
      "152/281, train_loss: 0.0792, step time: 0.2528\n",
      "153/281, train_loss: 0.0582, step time: 0.2508\n",
      "154/281, train_loss: 0.2460, step time: 0.2474\n",
      "155/281, train_loss: 0.0861, step time: 0.2458\n",
      "156/281, train_loss: 0.0562, step time: 0.2479\n",
      "157/281, train_loss: 0.0587, step time: 0.2446\n",
      "158/281, train_loss: 0.0897, step time: 0.2441\n",
      "159/281, train_loss: 0.0600, step time: 0.2549\n",
      "160/281, train_loss: 0.0573, step time: 0.2505\n",
      "161/281, train_loss: 0.0466, step time: 0.2485\n",
      "162/281, train_loss: 0.2061, step time: 0.2451\n",
      "163/281, train_loss: 0.0479, step time: 0.2506\n",
      "164/281, train_loss: 0.0648, step time: 0.2484\n",
      "165/281, train_loss: 0.0353, step time: 0.2510\n",
      "166/281, train_loss: 0.2199, step time: 0.2472\n",
      "167/281, train_loss: 0.0641, step time: 0.2391\n",
      "168/281, train_loss: 0.0981, step time: 0.2433\n",
      "169/281, train_loss: 0.0648, step time: 0.2455\n",
      "170/281, train_loss: 0.1058, step time: 0.2473\n",
      "171/281, train_loss: 0.0752, step time: 0.2457\n",
      "172/281, train_loss: 0.0970, step time: 0.2483\n",
      "173/281, train_loss: 0.0548, step time: 0.2513\n",
      "174/281, train_loss: 0.0953, step time: 0.2550\n",
      "175/281, train_loss: 0.0818, step time: 0.2436\n",
      "176/281, train_loss: 0.0500, step time: 0.2507\n",
      "177/281, train_loss: 0.3965, step time: 0.2441\n",
      "178/281, train_loss: 0.0903, step time: 0.2416\n",
      "179/281, train_loss: 0.0869, step time: 0.2475\n",
      "180/281, train_loss: 0.0467, step time: 0.2451\n",
      "181/281, train_loss: 0.0886, step time: 0.2435\n",
      "182/281, train_loss: 0.0904, step time: 0.2465\n",
      "183/281, train_loss: 0.2441, step time: 0.2447\n",
      "184/281, train_loss: 0.0492, step time: 0.2503\n",
      "185/281, train_loss: 0.0493, step time: 0.2520\n",
      "186/281, train_loss: 0.0767, step time: 0.2480\n",
      "187/281, train_loss: 0.1052, step time: 0.2500\n",
      "188/281, train_loss: 0.0861, step time: 0.2498\n",
      "189/281, train_loss: 0.0831, step time: 0.2479\n",
      "190/281, train_loss: 0.0463, step time: 0.2476\n",
      "191/281, train_loss: 0.0654, step time: 0.2444\n",
      "192/281, train_loss: 0.0842, step time: 0.2401\n",
      "193/281, train_loss: 0.0924, step time: 0.2438\n",
      "194/281, train_loss: 0.2031, step time: 0.2422\n",
      "195/281, train_loss: 0.0674, step time: 0.2405\n",
      "196/281, train_loss: 0.0549, step time: 0.2508\n",
      "197/281, train_loss: 0.0505, step time: 0.2447\n",
      "198/281, train_loss: 0.0916, step time: 0.2576\n",
      "199/281, train_loss: 0.0731, step time: 0.2553\n",
      "200/281, train_loss: 0.2379, step time: 0.2535\n",
      "201/281, train_loss: 0.0553, step time: 0.2494\n",
      "202/281, train_loss: 0.0986, step time: 0.2464\n",
      "203/281, train_loss: 0.0733, step time: 0.2445\n",
      "204/281, train_loss: 0.0552, step time: 0.2471\n",
      "205/281, train_loss: 0.0678, step time: 0.2506\n",
      "206/281, train_loss: 0.0736, step time: 0.2456\n",
      "207/281, train_loss: 0.2036, step time: 0.2470\n",
      "208/281, train_loss: 0.0816, step time: 0.2518\n",
      "209/281, train_loss: 0.0474, step time: 0.2622\n",
      "210/281, train_loss: 0.0661, step time: 0.2454\n",
      "211/281, train_loss: 0.0690, step time: 0.2448\n",
      "212/281, train_loss: 0.0822, step time: 0.2452\n",
      "213/281, train_loss: 0.1183, step time: 0.2426\n",
      "214/281, train_loss: 0.0604, step time: 0.2475\n",
      "215/281, train_loss: 0.0959, step time: 0.2410\n",
      "216/281, train_loss: 0.0599, step time: 0.2447\n",
      "217/281, train_loss: 0.0712, step time: 0.2480\n",
      "218/281, train_loss: 0.2548, step time: 0.2492\n",
      "219/281, train_loss: 0.1106, step time: 0.2466\n",
      "220/281, train_loss: 0.0545, step time: 0.2510\n",
      "221/281, train_loss: 0.1791, step time: 0.2457\n",
      "222/281, train_loss: 0.3767, step time: 0.2443\n",
      "223/281, train_loss: 0.0854, step time: 0.2497\n",
      "224/281, train_loss: 0.0764, step time: 0.2476\n",
      "225/281, train_loss: 0.0868, step time: 0.2476\n",
      "226/281, train_loss: 0.0907, step time: 0.2404\n",
      "227/281, train_loss: 0.0908, step time: 0.2508\n",
      "228/281, train_loss: 0.0655, step time: 0.2449\n",
      "229/281, train_loss: 0.0707, step time: 0.2491\n",
      "230/281, train_loss: 0.0397, step time: 0.2456\n",
      "231/281, train_loss: 0.0430, step time: 0.2506\n",
      "232/281, train_loss: 0.0939, step time: 0.2433\n",
      "233/281, train_loss: 0.0606, step time: 0.2404\n",
      "234/281, train_loss: 0.0658, step time: 0.2422\n",
      "235/281, train_loss: 0.0702, step time: 0.2451\n",
      "236/281, train_loss: 0.0745, step time: 0.2456\n",
      "237/281, train_loss: 0.1016, step time: 0.2453\n",
      "238/281, train_loss: 0.2255, step time: 0.2404\n",
      "239/281, train_loss: 0.0900, step time: 0.2462\n",
      "240/281, train_loss: 0.0706, step time: 0.2446\n",
      "241/281, train_loss: 0.0507, step time: 0.2419\n",
      "242/281, train_loss: 0.1413, step time: 0.2441\n",
      "243/281, train_loss: 0.0724, step time: 0.2449\n",
      "244/281, train_loss: 0.2209, step time: 0.2461\n",
      "245/281, train_loss: 0.2127, step time: 0.2419\n",
      "246/281, train_loss: 0.0985, step time: 0.2377\n",
      "247/281, train_loss: 0.0572, step time: 0.2439\n",
      "248/281, train_loss: 0.0566, step time: 0.2453\n",
      "249/281, train_loss: 0.0693, step time: 0.2456\n",
      "250/281, train_loss: 0.0804, step time: 0.2476\n",
      "251/281, train_loss: 0.0595, step time: 0.2440\n",
      "252/281, train_loss: 0.0506, step time: 0.2454\n",
      "253/281, train_loss: 0.0826, step time: 0.2526\n",
      "254/281, train_loss: 0.0741, step time: 0.2483\n",
      "255/281, train_loss: 0.2449, step time: 0.2448\n",
      "256/281, train_loss: 0.0645, step time: 0.2408\n",
      "257/281, train_loss: 0.0619, step time: 0.2396\n",
      "258/281, train_loss: 0.1221, step time: 0.2457\n",
      "259/281, train_loss: 0.0733, step time: 0.2470\n",
      "260/281, train_loss: 0.0845, step time: 0.2401\n",
      "261/281, train_loss: 0.0577, step time: 0.2375\n",
      "262/281, train_loss: 0.0484, step time: 0.2445\n",
      "263/281, train_loss: 0.0813, step time: 0.2504\n",
      "264/281, train_loss: 0.0493, step time: 0.2462\n",
      "265/281, train_loss: 0.0830, step time: 0.2429\n",
      "266/281, train_loss: 0.0632, step time: 0.2436\n",
      "267/281, train_loss: 0.0666, step time: 0.2200\n",
      "268/281, train_loss: 0.2425, step time: 0.2481\n",
      "269/281, train_loss: 0.2467, step time: 0.2401\n",
      "270/281, train_loss: 0.0481, step time: 0.2456\n",
      "271/281, train_loss: 0.0450, step time: 0.2443\n",
      "272/281, train_loss: 0.0912, step time: 0.2476\n",
      "273/281, train_loss: 0.0469, step time: 0.2502\n",
      "274/281, train_loss: 0.0813, step time: 0.2444\n",
      "275/281, train_loss: 0.0717, step time: 0.2466\n",
      "276/281, train_loss: 0.1056, step time: 0.2447\n",
      "277/281, train_loss: 0.2308, step time: 0.2403\n",
      "278/281, train_loss: 0.0936, step time: 0.2485\n",
      "279/281, train_loss: 0.2034, step time: 0.2474\n",
      "280/281, train_loss: 0.0679, step time: 0.2448\n",
      "281/281, train_loss: 0.2350, step time: 0.2414\n",
      "282/281, train_loss: 0.0713, step time: 0.1447\n",
      "epoch 152 average loss: 0.0940\n",
      "current epoch: 152 current mean dice: 0.8859 tc: 0.8876 wt: 0.9026 et: 0.8820\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 152 is: 386.7264\n",
      "----------\n",
      "epoch 153/200\n",
      "1/281, train_loss: 0.2288, step time: 0.2537\n",
      "2/281, train_loss: 0.0834, step time: 0.2449\n",
      "3/281, train_loss: 0.0760, step time: 0.2450\n",
      "4/281, train_loss: 0.0919, step time: 0.2489\n",
      "5/281, train_loss: 0.0372, step time: 0.2504\n",
      "6/281, train_loss: 0.0578, step time: 0.2473\n",
      "7/281, train_loss: 0.0660, step time: 0.2547\n",
      "8/281, train_loss: 0.0586, step time: 0.2479\n",
      "9/281, train_loss: 0.0518, step time: 0.2421\n",
      "10/281, train_loss: 0.2172, step time: 0.2493\n",
      "11/281, train_loss: 0.0562, step time: 0.2487\n",
      "12/281, train_loss: 0.0817, step time: 0.2491\n",
      "13/281, train_loss: 0.0951, step time: 0.2483\n",
      "14/281, train_loss: 0.2122, step time: 0.2541\n",
      "15/281, train_loss: 0.0916, step time: 0.2501\n",
      "16/281, train_loss: 0.0515, step time: 0.2565\n",
      "17/281, train_loss: 0.2209, step time: 0.2482\n",
      "18/281, train_loss: 0.0784, step time: 0.2470\n",
      "19/281, train_loss: 0.0825, step time: 0.2472\n",
      "20/281, train_loss: 0.0617, step time: 0.2484\n",
      "21/281, train_loss: 0.0871, step time: 0.2516\n",
      "22/281, train_loss: 0.1239, step time: 0.2499\n",
      "23/281, train_loss: 0.0585, step time: 0.2502\n",
      "24/281, train_loss: 0.2280, step time: 0.2463\n",
      "25/281, train_loss: 0.2208, step time: 0.2421\n",
      "26/281, train_loss: 0.0961, step time: 0.2414\n",
      "27/281, train_loss: 0.2324, step time: 0.2515\n",
      "28/281, train_loss: 0.0692, step time: 0.2513\n",
      "29/281, train_loss: 0.0909, step time: 0.2499\n",
      "30/281, train_loss: 0.0792, step time: 0.2484\n",
      "31/281, train_loss: 0.0757, step time: 0.2539\n",
      "32/281, train_loss: 0.0570, step time: 0.2483\n",
      "33/281, train_loss: 0.0700, step time: 0.2495\n",
      "34/281, train_loss: 0.0528, step time: 0.2500\n",
      "35/281, train_loss: 0.0976, step time: 0.2575\n",
      "36/281, train_loss: 0.0772, step time: 0.2514\n",
      "37/281, train_loss: 0.0730, step time: 0.2448\n",
      "38/281, train_loss: 0.0935, step time: 0.2492\n",
      "39/281, train_loss: 0.0946, step time: 0.2512\n",
      "40/281, train_loss: 0.0544, step time: 0.2513\n",
      "41/281, train_loss: 0.0474, step time: 0.2485\n",
      "42/281, train_loss: 0.0497, step time: 0.2538\n",
      "43/281, train_loss: 0.0648, step time: 0.2499\n",
      "44/281, train_loss: 0.0453, step time: 0.2525\n",
      "45/281, train_loss: 0.0627, step time: 0.2525\n",
      "46/281, train_loss: 0.0829, step time: 0.2554\n",
      "47/281, train_loss: 0.0615, step time: 0.2481\n",
      "48/281, train_loss: 0.0724, step time: 0.2511\n",
      "49/281, train_loss: 0.0776, step time: 0.2518\n",
      "50/281, train_loss: 0.0903, step time: 0.2503\n",
      "51/281, train_loss: 0.0932, step time: 0.2508\n",
      "52/281, train_loss: 0.0526, step time: 0.2541\n",
      "53/281, train_loss: 0.1066, step time: 0.2507\n",
      "54/281, train_loss: 0.0935, step time: 0.2532\n",
      "55/281, train_loss: 0.2208, step time: 0.2555\n",
      "56/281, train_loss: 0.2130, step time: 0.2528\n",
      "57/281, train_loss: 0.0832, step time: 0.2524\n",
      "58/281, train_loss: 0.0546, step time: 0.2524\n",
      "59/281, train_loss: 0.0725, step time: 0.2564\n",
      "60/281, train_loss: 0.0377, step time: 0.2536\n",
      "61/281, train_loss: 0.0610, step time: 0.2535\n",
      "62/281, train_loss: 0.2302, step time: 0.2495\n",
      "63/281, train_loss: 0.0945, step time: 0.2676\n",
      "64/281, train_loss: 0.2341, step time: 0.2435\n",
      "65/281, train_loss: 0.0496, step time: 0.2507\n",
      "66/281, train_loss: 0.0298, step time: 0.2496\n",
      "67/281, train_loss: 0.0748, step time: 0.2510\n",
      "68/281, train_loss: 0.2171, step time: 0.2619\n",
      "69/281, train_loss: 0.0614, step time: 0.2533\n",
      "70/281, train_loss: 0.0626, step time: 0.2533\n",
      "71/281, train_loss: 0.0518, step time: 0.2563\n",
      "72/281, train_loss: 0.1073, step time: 0.2511\n",
      "73/281, train_loss: 0.0399, step time: 0.2477\n",
      "74/281, train_loss: 0.0436, step time: 0.2487\n",
      "75/281, train_loss: 0.0648, step time: 0.2541\n",
      "76/281, train_loss: 0.0697, step time: 0.2530\n",
      "77/281, train_loss: 0.0930, step time: 0.2532\n",
      "78/281, train_loss: 0.0822, step time: 0.2510\n",
      "79/281, train_loss: 0.0574, step time: 0.2510\n",
      "80/281, train_loss: 0.0698, step time: 0.2511\n",
      "81/281, train_loss: 0.0605, step time: 0.2481\n",
      "82/281, train_loss: 0.0880, step time: 0.2523\n",
      "83/281, train_loss: 0.0657, step time: 0.2490\n",
      "84/281, train_loss: 0.0835, step time: 0.2564\n",
      "85/281, train_loss: 0.0678, step time: 0.2472\n",
      "86/281, train_loss: 0.0763, step time: 0.2541\n",
      "87/281, train_loss: 0.2249, step time: 0.2499\n",
      "88/281, train_loss: 0.0540, step time: 0.2486\n",
      "89/281, train_loss: 0.0661, step time: 0.2466\n",
      "90/281, train_loss: 0.0472, step time: 0.2540\n",
      "91/281, train_loss: 0.0760, step time: 0.2533\n",
      "92/281, train_loss: 0.0530, step time: 0.2530\n",
      "93/281, train_loss: 0.0489, step time: 0.2502\n",
      "94/281, train_loss: 0.0625, step time: 0.2478\n",
      "95/281, train_loss: 0.1008, step time: 0.2545\n",
      "96/281, train_loss: 0.0625, step time: 0.2575\n",
      "97/281, train_loss: 0.0609, step time: 0.2499\n",
      "98/281, train_loss: 0.0577, step time: 0.2515\n",
      "99/281, train_loss: 0.1102, step time: 0.2552\n",
      "100/281, train_loss: 0.0552, step time: 0.2574\n",
      "101/281, train_loss: 0.0555, step time: 0.2511\n",
      "102/281, train_loss: 0.0485, step time: 0.2523\n",
      "103/281, train_loss: 0.0468, step time: 0.2522\n",
      "104/281, train_loss: 0.2474, step time: 0.2504\n",
      "105/281, train_loss: 0.0641, step time: 0.2512\n",
      "106/281, train_loss: 0.0899, step time: 0.2537\n",
      "107/281, train_loss: 0.0622, step time: 0.2591\n",
      "108/281, train_loss: 0.0530, step time: 0.2540\n",
      "109/281, train_loss: 0.0597, step time: 0.2569\n",
      "110/281, train_loss: 0.0878, step time: 0.2542\n",
      "111/281, train_loss: 0.0696, step time: 0.2502\n",
      "112/281, train_loss: 0.0968, step time: 0.2517\n",
      "113/281, train_loss: 0.0891, step time: 0.2530\n",
      "114/281, train_loss: 0.0958, step time: 0.2481\n",
      "115/281, train_loss: 0.0576, step time: 0.2501\n",
      "116/281, train_loss: 0.0563, step time: 0.2565\n",
      "117/281, train_loss: 0.0788, step time: 0.2478\n",
      "118/281, train_loss: 0.0644, step time: 0.2573\n",
      "119/281, train_loss: 0.0554, step time: 0.2510\n",
      "120/281, train_loss: 0.0717, step time: 0.2559\n",
      "121/281, train_loss: 0.0690, step time: 0.2516\n",
      "122/281, train_loss: 0.0599, step time: 0.2525\n",
      "123/281, train_loss: 0.0549, step time: 0.2536\n",
      "124/281, train_loss: 0.0415, step time: 0.2524\n",
      "125/281, train_loss: 0.0601, step time: 0.2517\n",
      "126/281, train_loss: 0.0631, step time: 0.2544\n",
      "127/281, train_loss: 0.0663, step time: 0.2532\n",
      "128/281, train_loss: 0.0780, step time: 0.2537\n",
      "129/281, train_loss: 0.0700, step time: 0.2553\n",
      "130/281, train_loss: 0.0811, step time: 0.2535\n",
      "131/281, train_loss: 0.2133, step time: 0.2569\n",
      "132/281, train_loss: 0.2494, step time: 0.2513\n",
      "133/281, train_loss: 0.0976, step time: 0.2500\n",
      "134/281, train_loss: 0.0530, step time: 0.2503\n",
      "135/281, train_loss: 0.0812, step time: 0.2596\n",
      "136/281, train_loss: 0.0661, step time: 0.2599\n",
      "137/281, train_loss: 0.0836, step time: 0.2585\n",
      "138/281, train_loss: 0.0739, step time: 0.2563\n",
      "139/281, train_loss: 0.0804, step time: 0.2616\n",
      "140/281, train_loss: 0.0957, step time: 0.2512\n",
      "141/281, train_loss: 0.0482, step time: 0.2548\n",
      "142/281, train_loss: 0.0553, step time: 0.2594\n",
      "143/281, train_loss: 0.0824, step time: 0.2544\n",
      "144/281, train_loss: 0.2536, step time: 0.2609\n",
      "145/281, train_loss: 0.0862, step time: 0.2525\n",
      "146/281, train_loss: 0.0684, step time: 0.2574\n",
      "147/281, train_loss: 0.0614, step time: 0.2550\n",
      "148/281, train_loss: 0.0417, step time: 0.2520\n",
      "149/281, train_loss: 0.1076, step time: 0.2498\n",
      "150/281, train_loss: 0.0512, step time: 0.2493\n",
      "151/281, train_loss: 0.0952, step time: 0.2546\n",
      "152/281, train_loss: 0.1099, step time: 0.2537\n",
      "153/281, train_loss: 0.0807, step time: 0.2534\n",
      "154/281, train_loss: 0.0661, step time: 0.2501\n",
      "155/281, train_loss: 0.0562, step time: 0.2460\n",
      "156/281, train_loss: 0.0611, step time: 0.2503\n",
      "157/281, train_loss: 0.0638, step time: 0.2539\n",
      "158/281, train_loss: 0.0718, step time: 0.2520\n",
      "159/281, train_loss: 0.0613, step time: 0.2518\n",
      "160/281, train_loss: 0.2127, step time: 0.2472\n",
      "161/281, train_loss: 0.1067, step time: 0.2517\n",
      "162/281, train_loss: 0.0480, step time: 0.2519\n",
      "163/281, train_loss: 0.2306, step time: 0.2500\n",
      "164/281, train_loss: 0.1106, step time: 0.2460\n",
      "165/281, train_loss: 0.0613, step time: 0.2550\n",
      "166/281, train_loss: 0.0519, step time: 0.2494\n",
      "167/281, train_loss: 0.2052, step time: 0.2495\n",
      "168/281, train_loss: 0.0737, step time: 0.2485\n",
      "169/281, train_loss: 0.0435, step time: 0.2543\n",
      "170/281, train_loss: 0.0626, step time: 0.2487\n",
      "171/281, train_loss: 0.2390, step time: 0.2469\n",
      "172/281, train_loss: 0.2081, step time: 0.2495\n",
      "173/281, train_loss: 0.2202, step time: 0.2550\n",
      "174/281, train_loss: 0.0840, step time: 0.2537\n",
      "175/281, train_loss: 0.2163, step time: 0.2501\n",
      "176/281, train_loss: 0.2507, step time: 0.2458\n",
      "177/281, train_loss: 0.0636, step time: 0.2539\n",
      "178/281, train_loss: 0.0564, step time: 0.2553\n",
      "179/281, train_loss: 0.0835, step time: 0.2497\n",
      "180/281, train_loss: 0.0616, step time: 0.2525\n",
      "181/281, train_loss: 0.0572, step time: 0.2536\n",
      "182/281, train_loss: 0.0724, step time: 0.2505\n",
      "183/281, train_loss: 0.0690, step time: 0.2490\n",
      "184/281, train_loss: 0.0559, step time: 0.2453\n",
      "185/281, train_loss: 0.0341, step time: 0.2556\n",
      "186/281, train_loss: 0.0736, step time: 0.2632\n",
      "187/281, train_loss: 0.0551, step time: 0.2546\n",
      "188/281, train_loss: 0.0413, step time: 0.2489\n",
      "189/281, train_loss: 0.0582, step time: 0.2491\n",
      "190/281, train_loss: 0.0624, step time: 0.2529\n",
      "191/281, train_loss: 0.0510, step time: 0.2504\n",
      "192/281, train_loss: 0.2387, step time: 0.2550\n",
      "193/281, train_loss: 0.0648, step time: 0.2518\n",
      "194/281, train_loss: 0.0563, step time: 0.2472\n",
      "195/281, train_loss: 0.0409, step time: 0.2438\n",
      "196/281, train_loss: 0.0573, step time: 0.2491\n",
      "197/281, train_loss: 0.0591, step time: 0.2457\n",
      "198/281, train_loss: 0.2555, step time: 0.2484\n",
      "199/281, train_loss: 0.0692, step time: 0.2512\n",
      "200/281, train_loss: 0.0904, step time: 0.2475\n",
      "201/281, train_loss: 0.0383, step time: 0.2549\n",
      "202/281, train_loss: 0.0477, step time: 0.2505\n",
      "203/281, train_loss: 0.0604, step time: 0.2468\n",
      "204/281, train_loss: 0.0552, step time: 0.2522\n",
      "205/281, train_loss: 0.2360, step time: 0.2498\n",
      "206/281, train_loss: 0.2170, step time: 0.2500\n",
      "207/281, train_loss: 0.0660, step time: 0.2549\n",
      "208/281, train_loss: 0.0623, step time: 0.2490\n",
      "209/281, train_loss: 0.0927, step time: 0.2519\n",
      "210/281, train_loss: 0.0777, step time: 0.2484\n",
      "211/281, train_loss: 0.0817, step time: 0.2503\n",
      "212/281, train_loss: 0.0506, step time: 0.2515\n",
      "213/281, train_loss: 0.0516, step time: 0.2486\n",
      "214/281, train_loss: 0.2552, step time: 0.2485\n",
      "215/281, train_loss: 0.0632, step time: 0.2525\n",
      "216/281, train_loss: 0.2120, step time: 0.2523\n",
      "217/281, train_loss: 0.0602, step time: 0.2535\n",
      "218/281, train_loss: 0.0427, step time: 0.2499\n",
      "219/281, train_loss: 0.0881, step time: 0.2474\n",
      "220/281, train_loss: 0.0549, step time: 0.2532\n",
      "221/281, train_loss: 0.0925, step time: 0.2515\n",
      "222/281, train_loss: 0.2634, step time: 0.2570\n",
      "223/281, train_loss: 0.0728, step time: 0.2496\n",
      "224/281, train_loss: 0.0696, step time: 0.2544\n",
      "225/281, train_loss: 0.0561, step time: 0.2506\n",
      "226/281, train_loss: 0.0518, step time: 0.2557\n",
      "227/281, train_loss: 0.0653, step time: 0.2515\n",
      "228/281, train_loss: 0.2317, step time: 0.2506\n",
      "229/281, train_loss: 0.0726, step time: 0.2527\n",
      "230/281, train_loss: 0.0936, step time: 0.2514\n",
      "231/281, train_loss: 0.3828, step time: 0.2433\n",
      "232/281, train_loss: 0.0783, step time: 0.2535\n",
      "233/281, train_loss: 0.0536, step time: 0.2469\n",
      "234/281, train_loss: 0.0786, step time: 0.2513\n",
      "235/281, train_loss: 0.0572, step time: 0.2518\n",
      "236/281, train_loss: 0.0653, step time: 0.2544\n",
      "237/281, train_loss: 0.0744, step time: 0.2499\n",
      "238/281, train_loss: 0.0425, step time: 0.2500\n",
      "239/281, train_loss: 0.0488, step time: 0.2504\n",
      "240/281, train_loss: 0.2154, step time: 0.2510\n",
      "241/281, train_loss: 0.0604, step time: 0.2515\n",
      "242/281, train_loss: 0.0529, step time: 0.2510\n",
      "243/281, train_loss: 0.1061, step time: 0.2474\n",
      "244/281, train_loss: 0.0537, step time: 0.2502\n",
      "245/281, train_loss: 0.0732, step time: 0.2491\n",
      "246/281, train_loss: 0.2302, step time: 0.2444\n",
      "247/281, train_loss: 0.0844, step time: 0.2438\n",
      "248/281, train_loss: 0.1115, step time: 0.2536\n",
      "249/281, train_loss: 0.0838, step time: 0.2485\n",
      "250/281, train_loss: 0.0818, step time: 0.2490\n",
      "251/281, train_loss: 0.0730, step time: 0.2538\n",
      "252/281, train_loss: 0.0637, step time: 0.2587\n",
      "253/281, train_loss: 0.0380, step time: 0.2553\n",
      "254/281, train_loss: 0.1116, step time: 0.2553\n",
      "255/281, train_loss: 0.2271, step time: 0.2498\n",
      "256/281, train_loss: 0.2262, step time: 0.2468\n",
      "257/281, train_loss: 0.2624, step time: 0.2519\n",
      "258/281, train_loss: 0.0686, step time: 0.2479\n",
      "259/281, train_loss: 0.0810, step time: 0.2485\n",
      "260/281, train_loss: 0.0884, step time: 0.2592\n",
      "261/281, train_loss: 0.2034, step time: 0.2565\n",
      "262/281, train_loss: 0.0671, step time: 0.2516\n",
      "263/281, train_loss: 0.0818, step time: 0.2493\n",
      "264/281, train_loss: 0.0984, step time: 0.2475\n",
      "265/281, train_loss: 0.0637, step time: 0.2467\n",
      "266/281, train_loss: 0.0686, step time: 0.2499\n",
      "267/281, train_loss: 0.0598, step time: 0.2577\n",
      "268/281, train_loss: 0.0668, step time: 0.2473\n",
      "269/281, train_loss: 0.0514, step time: 0.2524\n",
      "270/281, train_loss: 0.0330, step time: 0.2490\n",
      "271/281, train_loss: 0.0686, step time: 0.2513\n",
      "272/281, train_loss: 0.0844, step time: 0.2492\n",
      "273/281, train_loss: 0.0739, step time: 0.2444\n",
      "274/281, train_loss: 0.1006, step time: 0.2474\n",
      "275/281, train_loss: 0.0891, step time: 0.2479\n",
      "276/281, train_loss: 0.0796, step time: 0.2470\n",
      "277/281, train_loss: 0.1064, step time: 0.2453\n",
      "278/281, train_loss: 0.0674, step time: 0.2495\n",
      "279/281, train_loss: 0.0526, step time: 0.2492\n",
      "280/281, train_loss: 0.0705, step time: 0.2503\n",
      "281/281, train_loss: 0.0608, step time: 0.2471\n",
      "282/281, train_loss: 0.0447, step time: 0.1473\n",
      "epoch 153 average loss: 0.0924\n",
      "current epoch: 153 current mean dice: 0.8687 tc: 0.8655 wt: 0.8840 et: 0.8721\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 153 is: 394.1702\n",
      "----------\n",
      "epoch 154/200\n",
      "1/281, train_loss: 0.0727, step time: 0.2563\n",
      "2/281, train_loss: 0.0762, step time: 0.2504\n",
      "3/281, train_loss: 0.0440, step time: 0.2395\n",
      "4/281, train_loss: 0.0445, step time: 0.2453\n",
      "5/281, train_loss: 0.2339, step time: 0.2532\n",
      "6/281, train_loss: 0.0934, step time: 0.2462\n",
      "7/281, train_loss: 0.0416, step time: 0.2457\n",
      "8/281, train_loss: 0.0686, step time: 0.2537\n",
      "9/281, train_loss: 0.0589, step time: 0.2494\n",
      "10/281, train_loss: 0.3669, step time: 0.2425\n",
      "11/281, train_loss: 0.0868, step time: 0.2477\n",
      "12/281, train_loss: 0.2261, step time: 0.2492\n",
      "13/281, train_loss: 0.0538, step time: 0.2446\n",
      "14/281, train_loss: 0.0565, step time: 0.2448\n",
      "15/281, train_loss: 0.0768, step time: 0.2455\n",
      "16/281, train_loss: 0.2517, step time: 0.2495\n",
      "17/281, train_loss: 0.0491, step time: 0.2479\n",
      "18/281, train_loss: 0.0980, step time: 0.2538\n",
      "19/281, train_loss: 0.0471, step time: 0.2493\n",
      "20/281, train_loss: 0.0545, step time: 0.2487\n",
      "21/281, train_loss: 0.0711, step time: 0.2448\n",
      "22/281, train_loss: 0.0809, step time: 0.2497\n",
      "23/281, train_loss: 0.0568, step time: 0.2505\n",
      "24/281, train_loss: 0.0802, step time: 0.2507\n",
      "25/281, train_loss: 0.0697, step time: 0.2509\n",
      "26/281, train_loss: 0.2279, step time: 0.2463\n",
      "27/281, train_loss: 0.0979, step time: 0.2503\n",
      "28/281, train_loss: 0.0792, step time: 0.2456\n",
      "29/281, train_loss: 0.2192, step time: 0.2448\n",
      "30/281, train_loss: 0.0687, step time: 0.2773\n",
      "31/281, train_loss: 0.0834, step time: 0.2516\n",
      "32/281, train_loss: 0.2210, step time: 0.2557\n",
      "33/281, train_loss: 0.1115, step time: 0.2466\n",
      "34/281, train_loss: 0.0474, step time: 0.2469\n",
      "35/281, train_loss: 0.0781, step time: 0.2489\n",
      "36/281, train_loss: 0.0705, step time: 0.2445\n",
      "37/281, train_loss: 0.0640, step time: 0.2455\n",
      "38/281, train_loss: 0.1198, step time: 0.2466\n",
      "39/281, train_loss: 0.2183, step time: 0.2516\n",
      "40/281, train_loss: 0.0620, step time: 0.2441\n",
      "41/281, train_loss: 0.0762, step time: 0.2527\n",
      "42/281, train_loss: 0.0851, step time: 0.2454\n",
      "43/281, train_loss: 0.0847, step time: 0.2478\n",
      "44/281, train_loss: 0.0651, step time: 0.2441\n",
      "45/281, train_loss: 0.1022, step time: 0.2465\n",
      "46/281, train_loss: 0.0557, step time: 0.2450\n",
      "47/281, train_loss: 0.2123, step time: 0.2451\n",
      "48/281, train_loss: 0.0671, step time: 0.2460\n",
      "49/281, train_loss: 0.0722, step time: 0.2503\n",
      "50/281, train_loss: 0.0753, step time: 0.2520\n",
      "51/281, train_loss: 0.0649, step time: 0.2486\n",
      "52/281, train_loss: 0.2262, step time: 0.2449\n",
      "53/281, train_loss: 0.0540, step time: 0.2430\n",
      "54/281, train_loss: 0.0672, step time: 0.2444\n",
      "55/281, train_loss: 0.0838, step time: 0.2456\n",
      "56/281, train_loss: 0.2298, step time: 0.2549\n",
      "57/281, train_loss: 0.2177, step time: 0.2499\n",
      "58/281, train_loss: 0.2417, step time: 0.2452\n",
      "59/281, train_loss: 0.0611, step time: 0.2464\n",
      "60/281, train_loss: 0.0644, step time: 0.2435\n",
      "61/281, train_loss: 0.0620, step time: 0.2544\n",
      "62/281, train_loss: 0.0569, step time: 0.2501\n",
      "63/281, train_loss: 0.0695, step time: 0.2479\n",
      "64/281, train_loss: 0.0976, step time: 0.2500\n",
      "65/281, train_loss: 0.0633, step time: 0.2469\n",
      "66/281, train_loss: 0.2293, step time: 0.2530\n",
      "67/281, train_loss: 0.0743, step time: 0.2529\n",
      "68/281, train_loss: 0.0664, step time: 0.2500\n",
      "69/281, train_loss: 0.0459, step time: 0.2545\n",
      "70/281, train_loss: 0.0601, step time: 0.2466\n",
      "71/281, train_loss: 0.0732, step time: 0.2495\n",
      "72/281, train_loss: 0.0608, step time: 0.2488\n",
      "73/281, train_loss: 0.0478, step time: 0.2476\n",
      "74/281, train_loss: 0.0485, step time: 0.2482\n",
      "75/281, train_loss: 0.0595, step time: 0.2429\n",
      "76/281, train_loss: 0.2049, step time: 0.2515\n",
      "77/281, train_loss: 0.0642, step time: 0.2446\n",
      "78/281, train_loss: 0.0725, step time: 0.2461\n",
      "79/281, train_loss: 0.0495, step time: 0.2503\n",
      "80/281, train_loss: 0.2215, step time: 0.2483\n",
      "81/281, train_loss: 0.0457, step time: 0.2509\n",
      "82/281, train_loss: 0.2350, step time: 0.2493\n",
      "83/281, train_loss: 0.0741, step time: 0.2498\n",
      "84/281, train_loss: 0.0555, step time: 0.2497\n",
      "85/281, train_loss: 0.0624, step time: 0.2517\n",
      "86/281, train_loss: 0.0771, step time: 0.2488\n",
      "87/281, train_loss: 0.0933, step time: 0.2461\n",
      "88/281, train_loss: 0.0556, step time: 0.2495\n",
      "89/281, train_loss: 0.0347, step time: 0.2471\n",
      "90/281, train_loss: 0.0520, step time: 0.2463\n",
      "91/281, train_loss: 0.0780, step time: 0.2456\n",
      "92/281, train_loss: 0.0650, step time: 0.2511\n",
      "93/281, train_loss: 0.0650, step time: 0.2473\n",
      "94/281, train_loss: 0.0865, step time: 0.2617\n",
      "95/281, train_loss: 0.2319, step time: 0.2514\n",
      "96/281, train_loss: 0.0773, step time: 0.2429\n",
      "97/281, train_loss: 0.2132, step time: 0.2436\n",
      "98/281, train_loss: 0.0490, step time: 0.2484\n",
      "99/281, train_loss: 0.0331, step time: 0.2510\n",
      "100/281, train_loss: 0.0612, step time: 0.2474\n",
      "101/281, train_loss: 0.0592, step time: 0.2458\n",
      "102/281, train_loss: 0.0513, step time: 0.2504\n",
      "103/281, train_loss: 0.0733, step time: 0.2505\n",
      "104/281, train_loss: 0.0621, step time: 0.2445\n",
      "105/281, train_loss: 0.0419, step time: 0.2419\n",
      "106/281, train_loss: 0.0887, step time: 0.2487\n",
      "107/281, train_loss: 0.2103, step time: 0.2564\n",
      "108/281, train_loss: 0.0371, step time: 0.2446\n",
      "109/281, train_loss: 0.0563, step time: 0.2413\n",
      "110/281, train_loss: 0.2102, step time: 0.2446\n",
      "111/281, train_loss: 0.2242, step time: 0.2467\n",
      "112/281, train_loss: 0.0598, step time: 0.2495\n",
      "113/281, train_loss: 0.0494, step time: 0.2455\n",
      "114/281, train_loss: 0.0808, step time: 0.2548\n",
      "115/281, train_loss: 0.0936, step time: 0.2556\n",
      "116/281, train_loss: 0.0895, step time: 0.2488\n",
      "117/281, train_loss: 0.1207, step time: 0.2474\n",
      "118/281, train_loss: 0.0957, step time: 0.2447\n",
      "119/281, train_loss: 0.0503, step time: 0.2480\n",
      "120/281, train_loss: 0.0691, step time: 0.2509\n",
      "121/281, train_loss: 0.0498, step time: 0.2509\n",
      "122/281, train_loss: 0.0723, step time: 0.2445\n",
      "123/281, train_loss: 0.0703, step time: 0.2427\n",
      "124/281, train_loss: 0.0652, step time: 0.2448\n",
      "125/281, train_loss: 0.0812, step time: 0.2421\n",
      "126/281, train_loss: 0.0712, step time: 0.2473\n",
      "127/281, train_loss: 0.0457, step time: 0.2439\n",
      "128/281, train_loss: 0.0953, step time: 0.2493\n",
      "129/281, train_loss: 0.0931, step time: 0.2422\n",
      "130/281, train_loss: 0.0619, step time: 0.2492\n",
      "131/281, train_loss: 0.1026, step time: 0.2466\n",
      "132/281, train_loss: 0.0507, step time: 0.2468\n",
      "133/281, train_loss: 0.0902, step time: 0.2475\n",
      "134/281, train_loss: 0.2493, step time: 0.2495\n",
      "135/281, train_loss: 0.2440, step time: 0.2433\n",
      "136/281, train_loss: 0.0767, step time: 0.2472\n",
      "137/281, train_loss: 0.0712, step time: 0.2402\n",
      "138/281, train_loss: 0.0567, step time: 0.2501\n",
      "139/281, train_loss: 0.0442, step time: 0.2482\n",
      "140/281, train_loss: 0.2344, step time: 0.2542\n",
      "141/281, train_loss: 0.0800, step time: 0.2459\n",
      "142/281, train_loss: 0.0695, step time: 0.2527\n",
      "143/281, train_loss: 0.0567, step time: 0.2480\n",
      "144/281, train_loss: 0.0599, step time: 0.2483\n",
      "145/281, train_loss: 0.0887, step time: 0.2485\n",
      "146/281, train_loss: 0.0890, step time: 0.2516\n",
      "147/281, train_loss: 0.0975, step time: 0.2458\n",
      "148/281, train_loss: 0.2251, step time: 0.2449\n",
      "149/281, train_loss: 0.0870, step time: 0.2418\n",
      "150/281, train_loss: 0.0632, step time: 0.2484\n",
      "151/281, train_loss: 0.0726, step time: 0.2479\n",
      "152/281, train_loss: 0.0639, step time: 0.2474\n",
      "153/281, train_loss: 0.0773, step time: 0.2466\n",
      "154/281, train_loss: 0.0809, step time: 0.2483\n",
      "155/281, train_loss: 0.0498, step time: 0.2455\n",
      "156/281, train_loss: 0.0712, step time: 0.2407\n",
      "157/281, train_loss: 0.0893, step time: 0.2437\n",
      "158/281, train_loss: 0.0498, step time: 0.2455\n",
      "159/281, train_loss: 0.0419, step time: 0.2496\n",
      "160/281, train_loss: 0.0875, step time: 0.2468\n",
      "161/281, train_loss: 0.0483, step time: 0.2474\n",
      "162/281, train_loss: 0.0541, step time: 0.2426\n",
      "163/281, train_loss: 0.1472, step time: 0.2422\n",
      "164/281, train_loss: 0.0435, step time: 0.2447\n",
      "165/281, train_loss: 0.1044, step time: 0.2462\n",
      "166/281, train_loss: 0.2265, step time: 0.2489\n",
      "167/281, train_loss: 0.0541, step time: 0.2461\n",
      "168/281, train_loss: 0.0558, step time: 0.2419\n",
      "169/281, train_loss: 0.1098, step time: 0.2440\n",
      "170/281, train_loss: 0.0283, step time: 0.2475\n",
      "171/281, train_loss: 0.2252, step time: 0.2433\n",
      "172/281, train_loss: 0.0842, step time: 0.2499\n",
      "173/281, train_loss: 0.0919, step time: 0.2494\n",
      "174/281, train_loss: 0.0961, step time: 0.2470\n",
      "175/281, train_loss: 0.2101, step time: 0.2404\n",
      "176/281, train_loss: 0.2373, step time: 0.2405\n",
      "177/281, train_loss: 0.0577, step time: 0.2414\n",
      "178/281, train_loss: 0.0854, step time: 0.2461\n",
      "179/281, train_loss: 0.0651, step time: 0.2485\n",
      "180/281, train_loss: 0.0512, step time: 0.2457\n",
      "181/281, train_loss: 0.0545, step time: 0.2507\n",
      "182/281, train_loss: 0.2362, step time: 0.2467\n",
      "183/281, train_loss: 0.0659, step time: 0.2450\n",
      "184/281, train_loss: 0.0780, step time: 0.2491\n",
      "185/281, train_loss: 0.1585, step time: 0.2476\n",
      "186/281, train_loss: 0.0491, step time: 0.2496\n",
      "187/281, train_loss: 0.2030, step time: 0.2522\n",
      "188/281, train_loss: 0.0800, step time: 0.2466\n",
      "189/281, train_loss: 0.0590, step time: 0.2423\n",
      "190/281, train_loss: 0.0420, step time: 0.2542\n",
      "191/281, train_loss: 0.2108, step time: 0.2556\n",
      "192/281, train_loss: 0.0420, step time: 0.2471\n",
      "193/281, train_loss: 0.0724, step time: 0.2451\n",
      "194/281, train_loss: 0.0854, step time: 0.2435\n",
      "195/281, train_loss: 0.1114, step time: 0.2398\n",
      "196/281, train_loss: 0.0697, step time: 0.2387\n",
      "197/281, train_loss: 0.0795, step time: 0.2449\n",
      "198/281, train_loss: 0.0639, step time: 0.2460\n",
      "199/281, train_loss: 0.0844, step time: 0.2500\n",
      "200/281, train_loss: 0.0662, step time: 0.2502\n",
      "201/281, train_loss: 0.0658, step time: 0.2484\n",
      "202/281, train_loss: 0.0798, step time: 0.2431\n",
      "203/281, train_loss: 0.0601, step time: 0.2415\n",
      "204/281, train_loss: 0.0767, step time: 0.2464\n",
      "205/281, train_loss: 0.0676, step time: 0.2484\n",
      "206/281, train_loss: 0.0508, step time: 0.2519\n",
      "207/281, train_loss: 0.2172, step time: 0.2465\n",
      "208/281, train_loss: 0.0454, step time: 0.2414\n",
      "209/281, train_loss: 0.0623, step time: 0.2425\n",
      "210/281, train_loss: 0.0546, step time: 0.2492\n",
      "211/281, train_loss: 0.0368, step time: 0.2503\n",
      "212/281, train_loss: 0.2265, step time: 0.2418\n",
      "213/281, train_loss: 0.0726, step time: 0.2416\n",
      "214/281, train_loss: 0.0391, step time: 0.2511\n",
      "215/281, train_loss: 0.0800, step time: 0.2454\n",
      "216/281, train_loss: 0.0793, step time: 0.2453\n",
      "217/281, train_loss: 0.1004, step time: 0.2477\n",
      "218/281, train_loss: 0.0722, step time: 0.2501\n",
      "219/281, train_loss: 0.0627, step time: 0.2468\n",
      "220/281, train_loss: 0.0385, step time: 0.2440\n",
      "221/281, train_loss: 0.0725, step time: 0.2415\n",
      "222/281, train_loss: 0.0760, step time: 0.2430\n",
      "223/281, train_loss: 0.2518, step time: 0.2425\n",
      "224/281, train_loss: 0.0516, step time: 0.2410\n",
      "225/281, train_loss: 0.0558, step time: 0.2395\n",
      "226/281, train_loss: 0.0963, step time: 0.2462\n",
      "227/281, train_loss: 0.0475, step time: 0.2496\n",
      "228/281, train_loss: 0.0531, step time: 0.2412\n",
      "229/281, train_loss: 0.0476, step time: 0.2422\n",
      "230/281, train_loss: 0.1123, step time: 0.2425\n",
      "231/281, train_loss: 0.0894, step time: 0.2457\n",
      "232/281, train_loss: 0.0433, step time: 0.2444\n",
      "233/281, train_loss: 0.0442, step time: 0.2442\n",
      "234/281, train_loss: 0.0855, step time: 0.2469\n",
      "235/281, train_loss: 0.0616, step time: 0.2469\n",
      "236/281, train_loss: 0.0610, step time: 0.2437\n",
      "237/281, train_loss: 0.0709, step time: 0.2439\n",
      "238/281, train_loss: 0.0765, step time: 0.2434\n",
      "239/281, train_loss: 0.0668, step time: 0.2428\n",
      "240/281, train_loss: 0.0516, step time: 0.2393\n",
      "241/281, train_loss: 0.2143, step time: 0.2515\n",
      "242/281, train_loss: 0.0602, step time: 0.2444\n",
      "243/281, train_loss: 0.0608, step time: 0.2442\n",
      "244/281, train_loss: 0.0688, step time: 0.2427\n",
      "245/281, train_loss: 0.0576, step time: 0.2444\n",
      "246/281, train_loss: 0.0602, step time: 0.2484\n",
      "247/281, train_loss: 0.1101, step time: 0.2451\n",
      "248/281, train_loss: 0.0603, step time: 0.2437\n",
      "249/281, train_loss: 0.0838, step time: 0.2514\n",
      "250/281, train_loss: 0.0607, step time: 0.2531\n",
      "251/281, train_loss: 0.0659, step time: 0.2452\n",
      "252/281, train_loss: 0.2077, step time: 0.2489\n",
      "253/281, train_loss: 0.0735, step time: 0.2489\n",
      "254/281, train_loss: 0.0659, step time: 0.2442\n",
      "255/281, train_loss: 0.0788, step time: 0.2437\n",
      "256/281, train_loss: 0.0843, step time: 0.2498\n",
      "257/281, train_loss: 0.0589, step time: 0.2519\n",
      "258/281, train_loss: 0.1016, step time: 0.2449\n",
      "259/281, train_loss: 0.0691, step time: 0.2416\n",
      "260/281, train_loss: 0.2064, step time: 0.2427\n",
      "261/281, train_loss: 0.0735, step time: 0.2551\n",
      "262/281, train_loss: 0.2089, step time: 0.2468\n",
      "263/281, train_loss: 0.0684, step time: 0.2420\n",
      "264/281, train_loss: 0.0538, step time: 0.2428\n",
      "265/281, train_loss: 0.0542, step time: 0.2497\n",
      "266/281, train_loss: 0.0624, step time: 0.2515\n",
      "267/281, train_loss: 0.0501, step time: 0.2473\n",
      "268/281, train_loss: 0.0542, step time: 0.2440\n",
      "269/281, train_loss: 0.0383, step time: 0.2476\n",
      "270/281, train_loss: 0.0570, step time: 0.2479\n",
      "271/281, train_loss: 0.0713, step time: 0.2544\n",
      "272/281, train_loss: 0.0716, step time: 0.2487\n",
      "273/281, train_loss: 0.0873, step time: 0.2485\n",
      "274/281, train_loss: 0.1035, step time: 0.2536\n",
      "275/281, train_loss: 0.0899, step time: 0.2467\n",
      "276/281, train_loss: 0.0929, step time: 0.2438\n",
      "277/281, train_loss: 0.0494, step time: 0.2492\n",
      "278/281, train_loss: 0.0586, step time: 0.2447\n",
      "279/281, train_loss: 0.0775, step time: 0.2467\n",
      "280/281, train_loss: 0.0659, step time: 0.2510\n",
      "281/281, train_loss: 0.0612, step time: 0.2456\n",
      "282/281, train_loss: 0.0525, step time: 0.1515\n",
      "epoch 154 average loss: 0.0915\n",
      "current epoch: 154 current mean dice: 0.8806 tc: 0.8689 wt: 0.9054 et: 0.8834\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 154 is: 380.7754\n",
      "----------\n",
      "epoch 155/200\n",
      "1/281, train_loss: 0.0384, step time: 0.2575\n",
      "2/281, train_loss: 0.0731, step time: 0.2546\n",
      "3/281, train_loss: 0.0807, step time: 0.2587\n",
      "4/281, train_loss: 0.0708, step time: 0.2620\n",
      "5/281, train_loss: 0.0806, step time: 0.2482\n",
      "6/281, train_loss: 0.0687, step time: 0.2529\n",
      "7/281, train_loss: 0.0591, step time: 0.2461\n",
      "8/281, train_loss: 0.0760, step time: 0.2452\n",
      "9/281, train_loss: 0.0561, step time: 0.2518\n",
      "10/281, train_loss: 0.0642, step time: 0.2484\n",
      "11/281, train_loss: 0.0622, step time: 0.2495\n",
      "12/281, train_loss: 0.0696, step time: 0.2765\n",
      "13/281, train_loss: 0.0631, step time: 0.2615\n",
      "14/281, train_loss: 0.0705, step time: 0.2568\n",
      "15/281, train_loss: 0.2270, step time: 0.2532\n",
      "16/281, train_loss: 0.1020, step time: 0.2523\n",
      "17/281, train_loss: 0.2321, step time: 0.2466\n",
      "18/281, train_loss: 0.0822, step time: 0.2522\n",
      "19/281, train_loss: 0.0769, step time: 0.2542\n",
      "20/281, train_loss: 0.0680, step time: 0.2525\n",
      "21/281, train_loss: 0.2136, step time: 0.2462\n",
      "22/281, train_loss: 0.2396, step time: 0.2496\n",
      "23/281, train_loss: 0.0842, step time: 0.2504\n",
      "24/281, train_loss: 0.2350, step time: 0.2497\n",
      "25/281, train_loss: 0.0607, step time: 0.2464\n",
      "26/281, train_loss: 0.0516, step time: 0.2491\n",
      "27/281, train_loss: 0.0383, step time: 0.2457\n",
      "28/281, train_loss: 0.0664, step time: 0.2504\n",
      "29/281, train_loss: 0.0988, step time: 0.2553\n",
      "30/281, train_loss: 0.0417, step time: 0.2569\n",
      "31/281, train_loss: 0.0486, step time: 0.2575\n",
      "32/281, train_loss: 0.0811, step time: 0.2519\n",
      "33/281, train_loss: 0.0521, step time: 0.2435\n",
      "34/281, train_loss: 0.0661, step time: 0.2490\n",
      "35/281, train_loss: 0.0687, step time: 0.2495\n",
      "36/281, train_loss: 0.0416, step time: 0.2524\n",
      "37/281, train_loss: 0.0759, step time: 0.2526\n",
      "38/281, train_loss: 0.0546, step time: 0.2532\n",
      "39/281, train_loss: 0.2178, step time: 0.2482\n",
      "40/281, train_loss: 0.0685, step time: 0.2469\n",
      "41/281, train_loss: 0.0597, step time: 0.2520\n",
      "42/281, train_loss: 0.0473, step time: 0.2561\n",
      "43/281, train_loss: 0.1123, step time: 0.2515\n",
      "44/281, train_loss: 0.0585, step time: 0.2478\n",
      "45/281, train_loss: 0.0467, step time: 0.2516\n",
      "46/281, train_loss: 0.1132, step time: 0.2511\n",
      "47/281, train_loss: 0.0941, step time: 0.2589\n",
      "48/281, train_loss: 0.0737, step time: 0.2546\n",
      "49/281, train_loss: 0.0718, step time: 0.2541\n",
      "50/281, train_loss: 0.2150, step time: 0.2545\n",
      "51/281, train_loss: 0.2005, step time: 0.2621\n",
      "52/281, train_loss: 0.0668, step time: 0.2555\n",
      "53/281, train_loss: 0.2121, step time: 0.2529\n",
      "54/281, train_loss: 0.0714, step time: 0.2484\n",
      "55/281, train_loss: 0.0465, step time: 0.2522\n",
      "56/281, train_loss: 0.0631, step time: 0.2437\n",
      "57/281, train_loss: 0.0493, step time: 0.2466\n",
      "58/281, train_loss: 0.0624, step time: 0.2502\n",
      "59/281, train_loss: 0.0606, step time: 0.2508\n",
      "60/281, train_loss: 0.0638, step time: 0.2533\n",
      "61/281, train_loss: 0.0583, step time: 0.2527\n",
      "62/281, train_loss: 0.0848, step time: 0.2551\n",
      "63/281, train_loss: 0.0479, step time: 0.2593\n",
      "64/281, train_loss: 0.0376, step time: 0.2521\n",
      "65/281, train_loss: 0.0423, step time: 0.2521\n",
      "66/281, train_loss: 0.0696, step time: 0.2516\n",
      "67/281, train_loss: 0.0718, step time: 0.2536\n",
      "68/281, train_loss: 0.0747, step time: 0.2589\n",
      "69/281, train_loss: 0.0569, step time: 0.2536\n",
      "70/281, train_loss: 0.1274, step time: 0.2517\n",
      "71/281, train_loss: 0.0938, step time: 0.2602\n",
      "72/281, train_loss: 0.0485, step time: 0.2561\n",
      "73/281, train_loss: 0.0912, step time: 0.2504\n",
      "74/281, train_loss: 0.0707, step time: 0.2514\n",
      "75/281, train_loss: 0.0662, step time: 0.2512\n",
      "76/281, train_loss: 0.1044, step time: 0.2515\n",
      "77/281, train_loss: 0.0658, step time: 0.2480\n",
      "78/281, train_loss: 0.2242, step time: 0.2443\n",
      "79/281, train_loss: 0.3869, step time: 0.2554\n",
      "80/281, train_loss: 0.2088, step time: 0.2562\n",
      "81/281, train_loss: 0.0657, step time: 0.2560\n",
      "82/281, train_loss: 0.0539, step time: 0.2505\n",
      "83/281, train_loss: 0.0565, step time: 0.2573\n",
      "84/281, train_loss: 0.2111, step time: 0.2733\n",
      "85/281, train_loss: 0.2251, step time: 0.2550\n",
      "86/281, train_loss: 0.2281, step time: 0.2530\n",
      "87/281, train_loss: 0.0400, step time: 0.2626\n",
      "88/281, train_loss: 0.0548, step time: 0.2643\n",
      "89/281, train_loss: 0.0849, step time: 0.2466\n",
      "90/281, train_loss: 0.0736, step time: 0.2533\n",
      "91/281, train_loss: 0.0653, step time: 0.2462\n",
      "92/281, train_loss: 0.0506, step time: 0.2434\n",
      "93/281, train_loss: 0.1064, step time: 0.2472\n",
      "94/281, train_loss: 0.0908, step time: 0.2451\n",
      "95/281, train_loss: 0.2296, step time: 0.2547\n",
      "96/281, train_loss: 0.0921, step time: 0.2532\n",
      "97/281, train_loss: 0.0653, step time: 0.2505\n",
      "98/281, train_loss: 0.0955, step time: 0.2498\n",
      "99/281, train_loss: 0.0609, step time: 0.2535\n",
      "100/281, train_loss: 0.0539, step time: 0.2558\n",
      "101/281, train_loss: 0.0483, step time: 0.2547\n",
      "102/281, train_loss: 0.0990, step time: 0.2517\n",
      "103/281, train_loss: 0.0670, step time: 0.2489\n",
      "104/281, train_loss: 0.0662, step time: 0.2503\n",
      "105/281, train_loss: 0.2333, step time: 0.2494\n",
      "106/281, train_loss: 0.0436, step time: 0.2492\n",
      "107/281, train_loss: 0.0739, step time: 0.2520\n",
      "108/281, train_loss: 0.0666, step time: 0.2570\n",
      "109/281, train_loss: 0.2550, step time: 0.2465\n",
      "110/281, train_loss: 0.0442, step time: 0.2544\n",
      "111/281, train_loss: 0.0524, step time: 0.2562\n",
      "112/281, train_loss: 0.2450, step time: 0.2560\n",
      "113/281, train_loss: 0.0561, step time: 0.2590\n",
      "114/281, train_loss: 0.0801, step time: 0.2561\n",
      "115/281, train_loss: 0.0497, step time: 0.2516\n",
      "116/281, train_loss: 0.0605, step time: 0.2473\n",
      "117/281, train_loss: 0.2330, step time: 0.2470\n",
      "118/281, train_loss: 0.0806, step time: 0.2610\n",
      "119/281, train_loss: 0.1070, step time: 0.2571\n",
      "120/281, train_loss: 0.0924, step time: 0.2521\n",
      "121/281, train_loss: 0.0553, step time: 0.2508\n",
      "122/281, train_loss: 0.1606, step time: 0.2540\n",
      "123/281, train_loss: 0.0836, step time: 0.2509\n",
      "124/281, train_loss: 0.0734, step time: 0.2542\n",
      "125/281, train_loss: 0.0954, step time: 0.2534\n",
      "126/281, train_loss: 0.0717, step time: 0.2564\n",
      "127/281, train_loss: 0.0833, step time: 0.2577\n",
      "128/281, train_loss: 0.2298, step time: 0.2516\n",
      "129/281, train_loss: 0.0482, step time: 0.2538\n",
      "130/281, train_loss: 0.1333, step time: 0.2566\n",
      "131/281, train_loss: 0.0558, step time: 0.2506\n",
      "132/281, train_loss: 0.0859, step time: 0.2500\n",
      "133/281, train_loss: 0.0419, step time: 0.2493\n",
      "134/281, train_loss: 0.0443, step time: 0.2595\n",
      "135/281, train_loss: 0.0709, step time: 0.2495\n",
      "136/281, train_loss: 0.0473, step time: 0.2515\n",
      "137/281, train_loss: 0.0770, step time: 0.2522\n",
      "138/281, train_loss: 0.0466, step time: 0.2487\n",
      "139/281, train_loss: 0.0823, step time: 0.2477\n",
      "140/281, train_loss: 0.0529, step time: 0.2470\n",
      "141/281, train_loss: 0.0511, step time: 0.2540\n",
      "142/281, train_loss: 0.0913, step time: 0.2576\n",
      "143/281, train_loss: 0.1297, step time: 0.2589\n",
      "144/281, train_loss: 0.0800, step time: 0.2508\n",
      "145/281, train_loss: 0.0836, step time: 0.2486\n",
      "146/281, train_loss: 0.0806, step time: 0.2521\n",
      "147/281, train_loss: 0.0718, step time: 0.2516\n",
      "148/281, train_loss: 0.0678, step time: 0.2549\n",
      "149/281, train_loss: 0.1142, step time: 0.2585\n",
      "150/281, train_loss: 0.0504, step time: 0.2549\n",
      "151/281, train_loss: 0.1168, step time: 0.2591\n",
      "152/281, train_loss: 0.0636, step time: 0.2597\n",
      "153/281, train_loss: 0.0532, step time: 0.2484\n",
      "154/281, train_loss: 0.0907, step time: 0.2516\n",
      "155/281, train_loss: 0.1014, step time: 0.2565\n",
      "156/281, train_loss: 0.2089, step time: 0.2493\n",
      "157/281, train_loss: 0.0724, step time: 0.2507\n",
      "158/281, train_loss: 0.0853, step time: 0.2587\n",
      "159/281, train_loss: 0.0651, step time: 0.2497\n",
      "160/281, train_loss: 0.2244, step time: 0.2505\n",
      "161/281, train_loss: 0.0626, step time: 0.2499\n",
      "162/281, train_loss: 0.0797, step time: 0.2555\n",
      "163/281, train_loss: 0.0641, step time: 0.2514\n",
      "164/281, train_loss: 0.3981, step time: 0.2477\n",
      "165/281, train_loss: 0.2159, step time: 0.2471\n",
      "166/281, train_loss: 0.1046, step time: 0.2614\n",
      "167/281, train_loss: 0.0848, step time: 0.2857\n",
      "168/281, train_loss: 0.0763, step time: 0.2563\n",
      "169/281, train_loss: 0.0576, step time: 0.2572\n",
      "170/281, train_loss: 0.0549, step time: 0.2506\n",
      "171/281, train_loss: 0.0596, step time: 0.2499\n",
      "172/281, train_loss: 0.0897, step time: 0.2478\n",
      "173/281, train_loss: 0.0770, step time: 0.2550\n",
      "174/281, train_loss: 0.0771, step time: 0.2539\n",
      "175/281, train_loss: 0.0778, step time: 0.2547\n",
      "176/281, train_loss: 0.0599, step time: 0.2581\n",
      "177/281, train_loss: 0.0641, step time: 0.2819\n",
      "178/281, train_loss: 0.2415, step time: 0.2504\n",
      "179/281, train_loss: 0.0790, step time: 0.2513\n",
      "180/281, train_loss: 0.0574, step time: 0.2510\n",
      "181/281, train_loss: 0.0727, step time: 0.2578\n",
      "182/281, train_loss: 0.0671, step time: 0.2483\n",
      "183/281, train_loss: 0.0652, step time: 0.2486\n",
      "184/281, train_loss: 0.0676, step time: 0.2520\n",
      "185/281, train_loss: 0.0573, step time: 0.2557\n",
      "186/281, train_loss: 0.0408, step time: 0.2522\n",
      "187/281, train_loss: 0.0691, step time: 0.2513\n",
      "188/281, train_loss: 0.0820, step time: 0.2501\n",
      "189/281, train_loss: 0.0496, step time: 0.2545\n",
      "190/281, train_loss: 0.0813, step time: 0.2557\n",
      "191/281, train_loss: 0.0993, step time: 0.2460\n",
      "192/281, train_loss: 0.2242, step time: 0.2512\n",
      "193/281, train_loss: 0.1319, step time: 0.2541\n",
      "194/281, train_loss: 0.0883, step time: 0.2499\n",
      "195/281, train_loss: 0.2092, step time: 0.2498\n",
      "196/281, train_loss: 0.0670, step time: 0.2519\n",
      "197/281, train_loss: 0.0704, step time: 0.2481\n",
      "198/281, train_loss: 0.0407, step time: 0.2511\n",
      "199/281, train_loss: 0.0823, step time: 0.2592\n",
      "200/281, train_loss: 0.0658, step time: 0.2525\n",
      "201/281, train_loss: 0.0460, step time: 0.2485\n",
      "202/281, train_loss: 0.0613, step time: 0.2511\n",
      "203/281, train_loss: 0.0609, step time: 0.2499\n",
      "204/281, train_loss: 0.0490, step time: 0.2528\n",
      "205/281, train_loss: 0.0546, step time: 0.2514\n",
      "206/281, train_loss: 0.0501, step time: 0.2494\n",
      "207/281, train_loss: 0.0767, step time: 0.2488\n",
      "208/281, train_loss: 0.0597, step time: 0.2548\n",
      "209/281, train_loss: 0.2358, step time: 0.2522\n",
      "210/281, train_loss: 0.0870, step time: 0.2560\n",
      "211/281, train_loss: 0.0747, step time: 0.2535\n",
      "212/281, train_loss: 0.0695, step time: 0.2483\n",
      "213/281, train_loss: 0.0972, step time: 0.2563\n",
      "214/281, train_loss: 0.0419, step time: 0.2485\n",
      "215/281, train_loss: 0.0639, step time: 0.2507\n",
      "216/281, train_loss: 0.0608, step time: 0.2506\n",
      "217/281, train_loss: 0.0847, step time: 0.2531\n",
      "218/281, train_loss: 0.0527, step time: 0.2472\n",
      "219/281, train_loss: 0.0630, step time: 0.2502\n",
      "220/281, train_loss: 0.0656, step time: 0.2496\n",
      "221/281, train_loss: 0.2113, step time: 0.2488\n",
      "222/281, train_loss: 0.0684, step time: 0.2458\n",
      "223/281, train_loss: 0.0515, step time: 0.2443\n",
      "224/281, train_loss: 0.0636, step time: 0.2516\n",
      "225/281, train_loss: 0.0467, step time: 0.2544\n",
      "226/281, train_loss: 0.0874, step time: 0.2540\n",
      "227/281, train_loss: 0.2174, step time: 0.2514\n",
      "228/281, train_loss: 0.0959, step time: 0.2509\n",
      "229/281, train_loss: 0.0779, step time: 0.2533\n",
      "230/281, train_loss: 0.2121, step time: 0.2574\n",
      "231/281, train_loss: 0.0683, step time: 0.2529\n",
      "232/281, train_loss: 0.0765, step time: 0.2521\n",
      "233/281, train_loss: 0.2231, step time: 0.2533\n",
      "234/281, train_loss: 0.0877, step time: 0.2544\n",
      "235/281, train_loss: 0.0772, step time: 0.2513\n",
      "236/281, train_loss: 0.0724, step time: 0.2495\n",
      "237/281, train_loss: 0.0524, step time: 0.2507\n",
      "238/281, train_loss: 0.0924, step time: 0.2497\n",
      "239/281, train_loss: 0.0731, step time: 0.2467\n",
      "240/281, train_loss: 0.2426, step time: 0.2553\n",
      "241/281, train_loss: 0.0766, step time: 0.2481\n",
      "242/281, train_loss: 0.1643, step time: 0.2510\n",
      "243/281, train_loss: 0.0571, step time: 0.2574\n",
      "244/281, train_loss: 0.0606, step time: 0.2503\n",
      "245/281, train_loss: 0.0894, step time: 0.2499\n",
      "246/281, train_loss: 0.0853, step time: 0.2573\n",
      "247/281, train_loss: 0.0517, step time: 0.2542\n",
      "248/281, train_loss: 0.0450, step time: 0.2575\n",
      "249/281, train_loss: 0.0894, step time: 0.2544\n",
      "250/281, train_loss: 0.0492, step time: 0.2519\n",
      "251/281, train_loss: 0.0568, step time: 0.2518\n",
      "252/281, train_loss: 0.0472, step time: 0.2524\n",
      "253/281, train_loss: 0.0402, step time: 0.2474\n",
      "254/281, train_loss: 0.0952, step time: 0.2498\n",
      "255/281, train_loss: 0.2145, step time: 0.2508\n",
      "256/281, train_loss: 0.0907, step time: 0.2533\n",
      "257/281, train_loss: 0.2493, step time: 0.2543\n",
      "258/281, train_loss: 0.0529, step time: 0.2508\n",
      "259/281, train_loss: 0.0968, step time: 0.2467\n",
      "260/281, train_loss: 0.0519, step time: 0.2476\n",
      "261/281, train_loss: 0.0702, step time: 0.2512\n",
      "262/281, train_loss: 0.0524, step time: 0.2504\n",
      "263/281, train_loss: 0.2022, step time: 0.2518\n",
      "264/281, train_loss: 0.0870, step time: 0.2529\n",
      "265/281, train_loss: 0.0491, step time: 0.2554\n",
      "266/281, train_loss: 0.0465, step time: 0.2519\n",
      "267/281, train_loss: 0.0797, step time: 0.2524\n",
      "268/281, train_loss: 0.1062, step time: 0.2486\n",
      "269/281, train_loss: 0.2143, step time: 0.2500\n",
      "270/281, train_loss: 0.0806, step time: 0.2525\n",
      "271/281, train_loss: 0.0554, step time: 0.2541\n",
      "272/281, train_loss: 0.0714, step time: 0.2506\n",
      "273/281, train_loss: 0.1087, step time: 0.2506\n",
      "274/281, train_loss: 0.0638, step time: 0.2549\n",
      "275/281, train_loss: 0.0849, step time: 0.2573\n",
      "276/281, train_loss: 0.0484, step time: 0.2508\n",
      "277/281, train_loss: 0.0592, step time: 0.2515\n",
      "278/281, train_loss: 0.0456, step time: 0.2529\n",
      "279/281, train_loss: 0.0718, step time: 0.2542\n",
      "280/281, train_loss: 0.0863, step time: 0.2543\n",
      "281/281, train_loss: 0.0631, step time: 0.2514\n",
      "282/281, train_loss: 0.3645, step time: 0.1504\n",
      "epoch 155 average loss: 0.0936\n",
      "current epoch: 155 current mean dice: 0.8797 tc: 0.8781 wt: 0.8968 et: 0.8790\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 155 is: 372.8968\n",
      "----------\n",
      "epoch 156/200\n",
      "1/281, train_loss: 0.0536, step time: 0.2509\n",
      "2/281, train_loss: 0.0716, step time: 0.2482\n",
      "3/281, train_loss: 0.0675, step time: 0.2478\n",
      "4/281, train_loss: 0.2290, step time: 0.2439\n",
      "5/281, train_loss: 0.2083, step time: 0.2464\n",
      "6/281, train_loss: 0.0710, step time: 0.2491\n",
      "7/281, train_loss: 0.0736, step time: 0.2529\n",
      "8/281, train_loss: 0.0585, step time: 0.2564\n",
      "9/281, train_loss: 0.0689, step time: 0.2521\n",
      "10/281, train_loss: 0.0791, step time: 0.2456\n",
      "11/281, train_loss: 0.0711, step time: 0.2531\n",
      "12/281, train_loss: 0.2337, step time: 0.2501\n",
      "13/281, train_loss: 0.0382, step time: 0.2490\n",
      "14/281, train_loss: 0.0686, step time: 0.2474\n",
      "15/281, train_loss: 0.0473, step time: 0.2489\n",
      "16/281, train_loss: 0.0662, step time: 0.2494\n",
      "17/281, train_loss: 0.0293, step time: 0.2515\n",
      "18/281, train_loss: 0.0651, step time: 0.2481\n",
      "19/281, train_loss: 0.0518, step time: 0.2492\n",
      "20/281, train_loss: 0.0684, step time: 0.2430\n",
      "21/281, train_loss: 0.0472, step time: 0.2450\n",
      "22/281, train_loss: 0.0815, step time: 0.2448\n",
      "23/281, train_loss: 0.0706, step time: 0.2506\n",
      "24/281, train_loss: 0.0739, step time: 0.2464\n",
      "25/281, train_loss: 0.0623, step time: 0.2500\n",
      "26/281, train_loss: 0.0796, step time: 0.2526\n",
      "27/281, train_loss: 0.0708, step time: 0.2541\n",
      "28/281, train_loss: 0.0794, step time: 0.2488\n",
      "29/281, train_loss: 0.0691, step time: 0.2615\n",
      "30/281, train_loss: 0.0983, step time: 0.2603\n",
      "31/281, train_loss: 0.1000, step time: 0.2594\n",
      "32/281, train_loss: 0.0551, step time: 0.2579\n",
      "33/281, train_loss: 0.0372, step time: 0.2485\n",
      "34/281, train_loss: 0.2065, step time: 0.2516\n",
      "35/281, train_loss: 0.0770, step time: 0.2515\n",
      "36/281, train_loss: 0.0608, step time: 0.2452\n",
      "37/281, train_loss: 0.0793, step time: 0.2489\n",
      "38/281, train_loss: 0.0597, step time: 0.2487\n",
      "39/281, train_loss: 0.2199, step time: 0.2526\n",
      "40/281, train_loss: 0.0543, step time: 0.2539\n",
      "41/281, train_loss: 0.0920, step time: 0.2483\n",
      "42/281, train_loss: 0.2094, step time: 0.2535\n",
      "43/281, train_loss: 0.0528, step time: 0.2500\n",
      "44/281, train_loss: 0.0604, step time: 0.2602\n",
      "45/281, train_loss: 0.0733, step time: 0.2473\n",
      "46/281, train_loss: 0.0841, step time: 0.2447\n",
      "47/281, train_loss: 0.0605, step time: 0.2468\n",
      "48/281, train_loss: 0.0399, step time: 0.2520\n",
      "49/281, train_loss: 0.2459, step time: 0.2560\n",
      "50/281, train_loss: 0.0531, step time: 0.2499\n",
      "51/281, train_loss: 0.2417, step time: 0.2491\n",
      "52/281, train_loss: 0.1058, step time: 0.2534\n",
      "53/281, train_loss: 0.1037, step time: 0.2454\n",
      "54/281, train_loss: 0.0813, step time: 0.2501\n",
      "55/281, train_loss: 0.2344, step time: 0.2464\n",
      "56/281, train_loss: 0.0781, step time: 0.2509\n",
      "57/281, train_loss: 0.0442, step time: 0.2493\n",
      "58/281, train_loss: 0.0469, step time: 0.2567\n",
      "59/281, train_loss: 0.0819, step time: 0.2506\n",
      "60/281, train_loss: 0.0759, step time: 0.2661\n",
      "61/281, train_loss: 0.0893, step time: 0.2542\n",
      "62/281, train_loss: 0.0515, step time: 0.2539\n",
      "63/281, train_loss: 0.0553, step time: 0.2501\n",
      "64/281, train_loss: 0.1059, step time: 0.2510\n",
      "65/281, train_loss: 0.0524, step time: 0.2494\n",
      "66/281, train_loss: 0.0501, step time: 0.2541\n",
      "67/281, train_loss: 0.0573, step time: 0.2514\n",
      "68/281, train_loss: 0.2089, step time: 0.2559\n",
      "69/281, train_loss: 0.0740, step time: 0.2492\n",
      "70/281, train_loss: 0.0676, step time: 0.2523\n",
      "71/281, train_loss: 0.0763, step time: 0.2526\n",
      "72/281, train_loss: 0.0699, step time: 0.2490\n",
      "73/281, train_loss: 0.1012, step time: 0.2513\n",
      "74/281, train_loss: 0.0654, step time: 0.2548\n",
      "75/281, train_loss: 0.0618, step time: 0.2539\n",
      "76/281, train_loss: 0.0420, step time: 0.2517\n",
      "77/281, train_loss: 0.2387, step time: 0.2580\n",
      "78/281, train_loss: 0.0688, step time: 0.2543\n",
      "79/281, train_loss: 0.0557, step time: 0.2563\n",
      "80/281, train_loss: 0.0658, step time: 0.2511\n",
      "81/281, train_loss: 0.0673, step time: 0.2514\n",
      "82/281, train_loss: 0.0967, step time: 0.2515\n",
      "83/281, train_loss: 0.0718, step time: 0.2531\n",
      "84/281, train_loss: 0.0656, step time: 0.2542\n",
      "85/281, train_loss: 0.2145, step time: 0.2586\n",
      "86/281, train_loss: 0.0711, step time: 0.2544\n",
      "87/281, train_loss: 0.2794, step time: 0.2537\n",
      "88/281, train_loss: 0.2179, step time: 0.2508\n",
      "89/281, train_loss: 0.0698, step time: 0.2543\n",
      "90/281, train_loss: 0.0543, step time: 0.2489\n",
      "91/281, train_loss: 0.0724, step time: 0.2491\n",
      "92/281, train_loss: 0.0385, step time: 0.2475\n",
      "93/281, train_loss: 0.2329, step time: 0.2454\n",
      "94/281, train_loss: 0.0775, step time: 0.2537\n",
      "95/281, train_loss: 0.0710, step time: 0.2608\n",
      "96/281, train_loss: 0.0806, step time: 0.2457\n",
      "97/281, train_loss: 0.1205, step time: 0.2473\n",
      "98/281, train_loss: 0.0755, step time: 0.2467\n",
      "99/281, train_loss: 0.0506, step time: 0.2468\n",
      "100/281, train_loss: 0.0885, step time: 0.2514\n",
      "101/281, train_loss: 0.0565, step time: 0.2491\n",
      "102/281, train_loss: 0.0784, step time: 0.2506\n",
      "103/281, train_loss: 0.0769, step time: 0.2510\n",
      "104/281, train_loss: 0.0669, step time: 0.2540\n",
      "105/281, train_loss: 0.0670, step time: 0.2408\n",
      "106/281, train_loss: 0.0773, step time: 0.2401\n",
      "107/281, train_loss: 0.0907, step time: 0.2418\n",
      "108/281, train_loss: 0.0602, step time: 0.2479\n",
      "109/281, train_loss: 0.0735, step time: 0.2445\n",
      "110/281, train_loss: 0.0652, step time: 0.2469\n",
      "111/281, train_loss: 0.0884, step time: 0.2428\n",
      "112/281, train_loss: 0.0864, step time: 0.2444\n",
      "113/281, train_loss: 0.0782, step time: 0.2425\n",
      "114/281, train_loss: 0.0725, step time: 0.2428\n",
      "115/281, train_loss: 0.0684, step time: 0.2476\n",
      "116/281, train_loss: 0.0694, step time: 0.2565\n",
      "117/281, train_loss: 0.0624, step time: 0.2487\n",
      "118/281, train_loss: 0.0592, step time: 0.2464\n",
      "119/281, train_loss: 0.0610, step time: 0.2442\n",
      "120/281, train_loss: 0.0608, step time: 0.2457\n",
      "121/281, train_loss: 0.0499, step time: 0.2463\n",
      "122/281, train_loss: 0.0800, step time: 0.2537\n",
      "123/281, train_loss: 0.0518, step time: 0.2454\n",
      "124/281, train_loss: 0.0802, step time: 0.2466\n",
      "125/281, train_loss: 0.0665, step time: 0.2499\n",
      "126/281, train_loss: 0.0625, step time: 0.2465\n",
      "127/281, train_loss: 0.0606, step time: 0.2457\n",
      "128/281, train_loss: 0.0573, step time: 0.2445\n",
      "129/281, train_loss: 0.2203, step time: 0.2493\n",
      "130/281, train_loss: 0.0585, step time: 0.2431\n",
      "131/281, train_loss: 0.0546, step time: 0.2395\n",
      "132/281, train_loss: 0.2074, step time: 0.2700\n",
      "133/281, train_loss: 0.2313, step time: 0.2423\n",
      "134/281, train_loss: 0.0636, step time: 0.2464\n",
      "135/281, train_loss: 0.0479, step time: 0.2374\n",
      "136/281, train_loss: 0.0808, step time: 0.2461\n",
      "137/281, train_loss: 0.0662, step time: 0.2476\n",
      "138/281, train_loss: 0.0863, step time: 0.2707\n",
      "139/281, train_loss: 0.0806, step time: 0.2480\n",
      "140/281, train_loss: 0.0714, step time: 0.2473\n",
      "141/281, train_loss: 0.1602, step time: 0.2462\n",
      "142/281, train_loss: 0.0576, step time: 0.2465\n",
      "143/281, train_loss: 0.0541, step time: 0.2521\n",
      "144/281, train_loss: 0.0723, step time: 0.2483\n",
      "145/281, train_loss: 0.0402, step time: 0.2482\n",
      "146/281, train_loss: 0.2288, step time: 0.2426\n",
      "147/281, train_loss: 0.0723, step time: 0.2488\n",
      "148/281, train_loss: 0.0538, step time: 0.2600\n",
      "149/281, train_loss: 0.0838, step time: 0.2462\n",
      "150/281, train_loss: 0.0992, step time: 0.2435\n",
      "151/281, train_loss: 0.2235, step time: 0.2471\n",
      "152/281, train_loss: 0.0698, step time: 0.2413\n",
      "153/281, train_loss: 0.0834, step time: 0.2441\n",
      "154/281, train_loss: 0.0491, step time: 0.2500\n",
      "155/281, train_loss: 0.2451, step time: 0.2491\n",
      "156/281, train_loss: 0.0907, step time: 0.2496\n",
      "157/281, train_loss: 0.0672, step time: 0.2446\n",
      "158/281, train_loss: 0.0728, step time: 0.2436\n",
      "159/281, train_loss: 0.0438, step time: 0.2464\n",
      "160/281, train_loss: 0.0577, step time: 0.2468\n",
      "161/281, train_loss: 0.0487, step time: 0.2466\n",
      "162/281, train_loss: 0.0435, step time: 0.2514\n",
      "163/281, train_loss: 0.2202, step time: 0.2430\n",
      "164/281, train_loss: 0.0914, step time: 0.2428\n",
      "165/281, train_loss: 0.0858, step time: 0.2444\n",
      "166/281, train_loss: 0.0656, step time: 0.2388\n",
      "167/281, train_loss: 0.0622, step time: 0.2462\n",
      "168/281, train_loss: 0.0474, step time: 0.2445\n",
      "169/281, train_loss: 0.0936, step time: 0.2407\n",
      "170/281, train_loss: 0.0543, step time: 0.2427\n",
      "171/281, train_loss: 0.0595, step time: 0.2496\n",
      "172/281, train_loss: 0.0961, step time: 0.2474\n",
      "173/281, train_loss: 0.0587, step time: 0.2421\n",
      "174/281, train_loss: 0.0806, step time: 0.2535\n",
      "175/281, train_loss: 0.0879, step time: 0.2414\n",
      "176/281, train_loss: 0.0520, step time: 0.2444\n",
      "177/281, train_loss: 0.2191, step time: 0.2517\n",
      "178/281, train_loss: 0.0591, step time: 0.2587\n",
      "179/281, train_loss: 0.0347, step time: 0.2488\n",
      "180/281, train_loss: 0.0662, step time: 0.2430\n",
      "181/281, train_loss: 0.2082, step time: 0.2431\n",
      "182/281, train_loss: 0.0743, step time: 0.2380\n",
      "183/281, train_loss: 0.1033, step time: 0.2445\n",
      "184/281, train_loss: 0.2243, step time: 0.2490\n",
      "185/281, train_loss: 0.0476, step time: 0.2462\n",
      "186/281, train_loss: 0.1054, step time: 0.2429\n",
      "187/281, train_loss: 0.0632, step time: 0.2439\n",
      "188/281, train_loss: 0.0777, step time: 0.2430\n",
      "189/281, train_loss: 0.0507, step time: 0.2446\n",
      "190/281, train_loss: 0.0602, step time: 0.2480\n",
      "191/281, train_loss: 0.0804, step time: 0.2458\n",
      "192/281, train_loss: 0.0620, step time: 0.2432\n",
      "193/281, train_loss: 0.0809, step time: 0.2440\n",
      "194/281, train_loss: 0.1043, step time: 0.2531\n",
      "195/281, train_loss: 0.1144, step time: 0.2491\n",
      "196/281, train_loss: 0.2465, step time: 0.2449\n",
      "197/281, train_loss: 0.0660, step time: 0.2545\n",
      "198/281, train_loss: 0.0786, step time: 0.3119\n",
      "199/281, train_loss: 0.2246, step time: 0.2466\n",
      "200/281, train_loss: 0.0643, step time: 0.2495\n",
      "201/281, train_loss: 0.0524, step time: 0.2542\n",
      "202/281, train_loss: 0.1289, step time: 0.2469\n",
      "203/281, train_loss: 0.0497, step time: 0.2448\n",
      "204/281, train_loss: 0.0594, step time: 0.2417\n",
      "205/281, train_loss: 0.0638, step time: 0.2371\n",
      "206/281, train_loss: 0.0902, step time: 0.2440\n",
      "207/281, train_loss: 0.0402, step time: 0.2491\n",
      "208/281, train_loss: 0.0447, step time: 0.2492\n",
      "209/281, train_loss: 0.0510, step time: 0.2480\n",
      "210/281, train_loss: 0.0724, step time: 0.2498\n",
      "211/281, train_loss: 0.0947, step time: 0.2421\n",
      "212/281, train_loss: 0.2156, step time: 0.2454\n",
      "213/281, train_loss: 0.0871, step time: 0.2465\n",
      "214/281, train_loss: 0.2283, step time: 0.2533\n",
      "215/281, train_loss: 0.2448, step time: 0.2497\n",
      "216/281, train_loss: 0.0490, step time: 0.2498\n",
      "217/281, train_loss: 0.0734, step time: 0.2539\n",
      "218/281, train_loss: 0.0633, step time: 0.2467\n",
      "219/281, train_loss: 0.0612, step time: 0.2477\n",
      "220/281, train_loss: 0.0744, step time: 0.2487\n",
      "221/281, train_loss: 0.0741, step time: 0.2476\n",
      "222/281, train_loss: 0.1241, step time: 0.2463\n",
      "223/281, train_loss: 0.0670, step time: 0.2509\n",
      "224/281, train_loss: 0.2238, step time: 0.2484\n",
      "225/281, train_loss: 0.0887, step time: 0.2476\n",
      "226/281, train_loss: 0.0690, step time: 0.2527\n",
      "227/281, train_loss: 0.0858, step time: 0.2491\n",
      "228/281, train_loss: 0.0586, step time: 0.2475\n",
      "229/281, train_loss: 0.0921, step time: 0.2480\n",
      "230/281, train_loss: 0.0783, step time: 0.2519\n",
      "231/281, train_loss: 0.0811, step time: 0.2483\n",
      "232/281, train_loss: 0.0725, step time: 0.2445\n",
      "233/281, train_loss: 0.0376, step time: 0.2464\n",
      "234/281, train_loss: 0.0866, step time: 0.2450\n",
      "235/281, train_loss: 0.0573, step time: 0.2493\n",
      "236/281, train_loss: 0.0461, step time: 0.2437\n",
      "237/281, train_loss: 0.2235, step time: 0.2493\n",
      "238/281, train_loss: 0.0484, step time: 0.2460\n",
      "239/281, train_loss: 0.0803, step time: 0.2449\n",
      "240/281, train_loss: 0.0944, step time: 0.2494\n",
      "241/281, train_loss: 0.0614, step time: 0.2397\n",
      "242/281, train_loss: 0.0492, step time: 0.2392\n",
      "243/281, train_loss: 0.0661, step time: 0.2418\n",
      "244/281, train_loss: 0.2034, step time: 0.2425\n",
      "245/281, train_loss: 0.1207, step time: 0.2466\n",
      "246/281, train_loss: 0.0766, step time: 0.2427\n",
      "247/281, train_loss: 0.2319, step time: 0.2439\n",
      "248/281, train_loss: 0.2224, step time: 0.2381\n",
      "249/281, train_loss: 0.1288, step time: 0.2447\n",
      "250/281, train_loss: 0.0464, step time: 0.2445\n",
      "251/281, train_loss: 0.0742, step time: 0.2511\n",
      "252/281, train_loss: 0.0927, step time: 0.2431\n",
      "253/281, train_loss: 0.0609, step time: 0.2433\n",
      "254/281, train_loss: 0.2346, step time: 0.2481\n",
      "255/281, train_loss: 0.0692, step time: 0.2427\n",
      "256/281, train_loss: 0.0452, step time: 0.2429\n",
      "257/281, train_loss: 0.0700, step time: 0.2387\n",
      "258/281, train_loss: 0.0591, step time: 0.2478\n",
      "259/281, train_loss: 0.0744, step time: 0.2427\n",
      "260/281, train_loss: 0.0794, step time: 0.2472\n",
      "261/281, train_loss: 0.2191, step time: 0.2471\n",
      "262/281, train_loss: 0.0528, step time: 0.2471\n",
      "263/281, train_loss: 0.0717, step time: 0.2438\n",
      "264/281, train_loss: 0.0974, step time: 0.2453\n",
      "265/281, train_loss: 0.0687, step time: 0.2416\n",
      "266/281, train_loss: 0.0485, step time: 0.2494\n",
      "267/281, train_loss: 0.0629, step time: 0.2463\n",
      "268/281, train_loss: 0.2372, step time: 0.2449\n",
      "269/281, train_loss: 0.1938, step time: 0.2430\n",
      "270/281, train_loss: 0.0755, step time: 0.2535\n",
      "271/281, train_loss: 0.3966, step time: 0.2418\n",
      "272/281, train_loss: 0.0523, step time: 0.2430\n",
      "273/281, train_loss: 0.0430, step time: 0.2450\n",
      "274/281, train_loss: 0.0501, step time: 0.2464\n",
      "275/281, train_loss: 0.0753, step time: 0.2493\n",
      "276/281, train_loss: 0.0458, step time: 0.2446\n",
      "277/281, train_loss: 0.0619, step time: 0.2421\n",
      "278/281, train_loss: 0.0775, step time: 0.2435\n",
      "279/281, train_loss: 0.0641, step time: 0.2442\n",
      "280/281, train_loss: 0.0491, step time: 0.2491\n",
      "281/281, train_loss: 0.1018, step time: 0.2474\n",
      "282/281, train_loss: 0.0552, step time: 0.1479\n",
      "epoch 156 average loss: 0.0923\n",
      "current epoch: 156 current mean dice: 0.8825 tc: 0.8739 wt: 0.9049 et: 0.8845\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 156 is: 354.1518\n",
      "----------\n",
      "epoch 157/200\n",
      "1/281, train_loss: 0.0880, step time: 0.2477\n",
      "2/281, train_loss: 0.0799, step time: 0.2445\n",
      "3/281, train_loss: 0.0830, step time: 0.2429\n",
      "4/281, train_loss: 0.0823, step time: 0.2448\n",
      "5/281, train_loss: 0.1064, step time: 0.2464\n",
      "6/281, train_loss: 0.0776, step time: 0.2445\n",
      "7/281, train_loss: 0.0465, step time: 0.2456\n",
      "8/281, train_loss: 0.0536, step time: 0.2431\n",
      "9/281, train_loss: 0.2146, step time: 0.2511\n",
      "10/281, train_loss: 0.0536, step time: 0.2476\n",
      "11/281, train_loss: 0.0458, step time: 0.2565\n",
      "12/281, train_loss: 0.0703, step time: 0.2509\n",
      "13/281, train_loss: 0.0792, step time: 0.2471\n",
      "14/281, train_loss: 0.1960, step time: 0.2501\n",
      "15/281, train_loss: 0.0718, step time: 0.2594\n",
      "16/281, train_loss: 0.0575, step time: 0.2535\n",
      "17/281, train_loss: 0.2263, step time: 0.2517\n",
      "18/281, train_loss: 0.0864, step time: 0.2558\n",
      "19/281, train_loss: 0.1019, step time: 0.2537\n",
      "20/281, train_loss: 0.0725, step time: 0.2488\n",
      "21/281, train_loss: 0.0455, step time: 0.2485\n",
      "22/281, train_loss: 0.0514, step time: 0.2507\n",
      "23/281, train_loss: 0.0496, step time: 0.2490\n",
      "24/281, train_loss: 0.0949, step time: 0.2501\n",
      "25/281, train_loss: 0.0583, step time: 0.2461\n",
      "26/281, train_loss: 0.0421, step time: 0.2472\n",
      "27/281, train_loss: 0.0932, step time: 0.2439\n",
      "28/281, train_loss: 0.0586, step time: 0.2430\n",
      "29/281, train_loss: 0.0764, step time: 0.2486\n",
      "30/281, train_loss: 0.2227, step time: 0.2507\n",
      "31/281, train_loss: 0.0462, step time: 0.2465\n",
      "32/281, train_loss: 0.2227, step time: 0.2427\n",
      "33/281, train_loss: 0.2374, step time: 0.2470\n",
      "34/281, train_loss: 0.0536, step time: 0.2514\n",
      "35/281, train_loss: 0.0848, step time: 0.2721\n",
      "36/281, train_loss: 0.0837, step time: 0.2512\n",
      "37/281, train_loss: 0.0829, step time: 0.2461\n",
      "38/281, train_loss: 0.1270, step time: 0.2584\n",
      "39/281, train_loss: 0.0948, step time: 0.2452\n",
      "40/281, train_loss: 0.0760, step time: 0.2515\n",
      "41/281, train_loss: 0.0853, step time: 0.2543\n",
      "42/281, train_loss: 0.0512, step time: 0.2479\n",
      "43/281, train_loss: 0.0694, step time: 0.2450\n",
      "44/281, train_loss: 0.0441, step time: 0.2474\n",
      "45/281, train_loss: 0.0721, step time: 0.2459\n",
      "46/281, train_loss: 0.0903, step time: 0.2485\n",
      "47/281, train_loss: 0.0797, step time: 0.2585\n",
      "48/281, train_loss: 0.0673, step time: 0.2567\n",
      "49/281, train_loss: 0.3875, step time: 0.2472\n",
      "50/281, train_loss: 0.2007, step time: 0.2451\n",
      "51/281, train_loss: 0.0562, step time: 0.2455\n",
      "52/281, train_loss: 0.0830, step time: 0.2473\n",
      "53/281, train_loss: 0.0478, step time: 0.2468\n",
      "54/281, train_loss: 0.0915, step time: 0.2473\n",
      "55/281, train_loss: 0.0613, step time: 0.2434\n",
      "56/281, train_loss: 0.0653, step time: 0.2462\n",
      "57/281, train_loss: 0.0474, step time: 0.2447\n",
      "58/281, train_loss: 0.0873, step time: 0.2517\n",
      "59/281, train_loss: 0.2074, step time: 0.2478\n",
      "60/281, train_loss: 0.0603, step time: 0.2491\n",
      "61/281, train_loss: 0.2214, step time: 0.2485\n",
      "62/281, train_loss: 0.0694, step time: 0.2430\n",
      "63/281, train_loss: 0.0513, step time: 0.2477\n",
      "64/281, train_loss: 0.0674, step time: 0.2524\n",
      "65/281, train_loss: 0.0482, step time: 0.2454\n",
      "66/281, train_loss: 0.0833, step time: 0.2467\n",
      "67/281, train_loss: 0.0964, step time: 0.2445\n",
      "68/281, train_loss: 0.0968, step time: 0.2468\n",
      "69/281, train_loss: 0.0637, step time: 0.2501\n",
      "70/281, train_loss: 0.1068, step time: 0.2485\n",
      "71/281, train_loss: 0.0714, step time: 0.2518\n",
      "72/281, train_loss: 0.0293, step time: 0.2471\n",
      "73/281, train_loss: 0.0645, step time: 0.2538\n",
      "74/281, train_loss: 0.0489, step time: 0.2488\n",
      "75/281, train_loss: 0.0741, step time: 0.2450\n",
      "76/281, train_loss: 0.0847, step time: 0.2423\n",
      "77/281, train_loss: 0.1250, step time: 0.2450\n",
      "78/281, train_loss: 0.0660, step time: 0.2496\n",
      "79/281, train_loss: 0.1054, step time: 0.2507\n",
      "80/281, train_loss: 0.2077, step time: 0.2500\n",
      "81/281, train_loss: 0.2291, step time: 0.2519\n",
      "82/281, train_loss: 0.0742, step time: 0.2457\n",
      "83/281, train_loss: 0.2240, step time: 0.2452\n",
      "84/281, train_loss: 0.2086, step time: 0.2501\n",
      "85/281, train_loss: 0.0586, step time: 0.2536\n",
      "86/281, train_loss: 0.0739, step time: 0.2492\n",
      "87/281, train_loss: 0.0839, step time: 0.2563\n",
      "88/281, train_loss: 0.0550, step time: 0.2521\n",
      "89/281, train_loss: 0.0533, step time: 0.2518\n",
      "90/281, train_loss: 0.0738, step time: 0.2506\n",
      "91/281, train_loss: 0.0598, step time: 0.2456\n",
      "92/281, train_loss: 0.0818, step time: 0.2660\n",
      "93/281, train_loss: 0.0899, step time: 0.2507\n",
      "94/281, train_loss: 0.0800, step time: 0.2527\n",
      "95/281, train_loss: 0.0700, step time: 0.2522\n",
      "96/281, train_loss: 0.0602, step time: 0.2531\n",
      "97/281, train_loss: 0.0322, step time: 0.2508\n",
      "98/281, train_loss: 0.0729, step time: 0.2467\n",
      "99/281, train_loss: 0.0548, step time: 0.2475\n",
      "100/281, train_loss: 0.2247, step time: 0.2496\n",
      "101/281, train_loss: 0.0570, step time: 0.2479\n",
      "102/281, train_loss: 0.0741, step time: 0.2568\n",
      "103/281, train_loss: 0.0708, step time: 0.2537\n",
      "104/281, train_loss: 0.2238, step time: 0.2469\n",
      "105/281, train_loss: 0.0708, step time: 0.2508\n",
      "106/281, train_loss: 0.0886, step time: 0.2516\n",
      "107/281, train_loss: 0.0731, step time: 0.2524\n",
      "108/281, train_loss: 0.0706, step time: 0.2551\n",
      "109/281, train_loss: 0.0441, step time: 0.2542\n",
      "110/281, train_loss: 0.0676, step time: 0.2533\n",
      "111/281, train_loss: 0.0873, step time: 0.2493\n",
      "112/281, train_loss: 0.0541, step time: 0.2481\n",
      "113/281, train_loss: 0.2323, step time: 0.2540\n",
      "114/281, train_loss: 0.0521, step time: 0.2551\n",
      "115/281, train_loss: 0.0619, step time: 0.2455\n",
      "116/281, train_loss: 0.0373, step time: 0.2493\n",
      "117/281, train_loss: 0.0758, step time: 0.2520\n",
      "118/281, train_loss: 0.0656, step time: 0.2484\n",
      "119/281, train_loss: 0.0566, step time: 0.2487\n",
      "120/281, train_loss: 0.0635, step time: 0.2540\n",
      "121/281, train_loss: 0.0633, step time: 0.2549\n",
      "122/281, train_loss: 0.0877, step time: 0.2522\n",
      "123/281, train_loss: 0.0650, step time: 0.2518\n",
      "124/281, train_loss: 0.1000, step time: 0.2519\n",
      "125/281, train_loss: 0.2275, step time: 0.2483\n",
      "126/281, train_loss: 0.0547, step time: 0.2570\n",
      "127/281, train_loss: 0.0629, step time: 0.2536\n",
      "128/281, train_loss: 0.0769, step time: 0.2582\n",
      "129/281, train_loss: 0.0815, step time: 0.2504\n",
      "130/281, train_loss: 0.2986, step time: 0.2511\n",
      "131/281, train_loss: 0.0888, step time: 0.2493\n",
      "132/281, train_loss: 0.0413, step time: 0.2530\n",
      "133/281, train_loss: 0.0732, step time: 0.2504\n",
      "134/281, train_loss: 0.0658, step time: 0.2526\n",
      "135/281, train_loss: 0.0717, step time: 0.2579\n",
      "136/281, train_loss: 0.0581, step time: 0.2518\n",
      "137/281, train_loss: 0.0981, step time: 0.2511\n",
      "138/281, train_loss: 0.1127, step time: 0.2512\n",
      "139/281, train_loss: 0.0643, step time: 0.2533\n",
      "140/281, train_loss: 0.0530, step time: 0.2577\n",
      "141/281, train_loss: 0.0639, step time: 0.2530\n",
      "142/281, train_loss: 0.0459, step time: 0.2511\n",
      "143/281, train_loss: 0.0988, step time: 0.2489\n",
      "144/281, train_loss: 0.0514, step time: 0.2497\n",
      "145/281, train_loss: 0.0919, step time: 0.2470\n",
      "146/281, train_loss: 0.0698, step time: 0.2438\n",
      "147/281, train_loss: 0.0910, step time: 0.2453\n",
      "148/281, train_loss: 0.0417, step time: 0.2492\n",
      "149/281, train_loss: 0.0503, step time: 0.2477\n",
      "150/281, train_loss: 0.0739, step time: 0.2533\n",
      "151/281, train_loss: 0.0598, step time: 0.2614\n",
      "152/281, train_loss: 0.0661, step time: 0.2549\n",
      "153/281, train_loss: 0.0520, step time: 0.2521\n",
      "154/281, train_loss: 0.0542, step time: 0.2499\n",
      "155/281, train_loss: 0.0738, step time: 0.2450\n",
      "156/281, train_loss: 0.0852, step time: 0.2484\n",
      "157/281, train_loss: 0.0870, step time: 0.2552\n",
      "158/281, train_loss: 0.0881, step time: 0.2536\n",
      "159/281, train_loss: 0.2472, step time: 0.2508\n",
      "160/281, train_loss: 0.0627, step time: 0.2511\n",
      "161/281, train_loss: 0.0821, step time: 0.2600\n",
      "162/281, train_loss: 0.2428, step time: 0.2534\n",
      "163/281, train_loss: 0.0722, step time: 0.2514\n",
      "164/281, train_loss: 0.0922, step time: 0.2506\n",
      "165/281, train_loss: 0.0435, step time: 0.2547\n",
      "166/281, train_loss: 0.0868, step time: 0.2544\n",
      "167/281, train_loss: 0.0935, step time: 0.2527\n",
      "168/281, train_loss: 0.0736, step time: 0.2513\n",
      "169/281, train_loss: 0.0740, step time: 0.2537\n",
      "170/281, train_loss: 0.1001, step time: 0.2427\n",
      "171/281, train_loss: 0.0838, step time: 0.2469\n",
      "172/281, train_loss: 0.0818, step time: 0.2521\n",
      "173/281, train_loss: 0.1153, step time: 0.2520\n",
      "174/281, train_loss: 0.2306, step time: 0.2555\n",
      "175/281, train_loss: 0.2222, step time: 0.2527\n",
      "176/281, train_loss: 0.0593, step time: 0.2530\n",
      "177/281, train_loss: 0.0609, step time: 0.2554\n",
      "178/281, train_loss: 0.0451, step time: 0.2521\n",
      "179/281, train_loss: 0.2644, step time: 0.2541\n",
      "180/281, train_loss: 0.0561, step time: 0.2497\n",
      "181/281, train_loss: 0.0628, step time: 0.2567\n",
      "182/281, train_loss: 0.0928, step time: 0.2632\n",
      "183/281, train_loss: 0.0936, step time: 0.2493\n",
      "184/281, train_loss: 0.0712, step time: 0.2504\n",
      "185/281, train_loss: 0.0947, step time: 0.2476\n",
      "186/281, train_loss: 0.0667, step time: 0.2463\n",
      "187/281, train_loss: 0.1286, step time: 0.2538\n",
      "188/281, train_loss: 0.2378, step time: 0.2440\n",
      "189/281, train_loss: 0.0542, step time: 0.2465\n",
      "190/281, train_loss: 0.1002, step time: 0.2416\n",
      "191/281, train_loss: 0.0523, step time: 0.2497\n",
      "192/281, train_loss: 0.0798, step time: 0.2483\n",
      "193/281, train_loss: 0.0563, step time: 0.2567\n",
      "194/281, train_loss: 0.1216, step time: 0.2515\n",
      "195/281, train_loss: 0.0630, step time: 0.2531\n",
      "196/281, train_loss: 0.0597, step time: 0.2483\n",
      "197/281, train_loss: 0.2434, step time: 0.2487\n",
      "198/281, train_loss: 0.0666, step time: 0.2475\n",
      "199/281, train_loss: 0.0646, step time: 0.2483\n",
      "200/281, train_loss: 0.0755, step time: 0.2543\n",
      "201/281, train_loss: 0.0460, step time: 0.2481\n",
      "202/281, train_loss: 0.2269, step time: 0.2499\n",
      "203/281, train_loss: 0.0721, step time: 0.2565\n",
      "204/281, train_loss: 0.0667, step time: 0.2510\n",
      "205/281, train_loss: 0.0918, step time: 0.2481\n",
      "206/281, train_loss: 0.0754, step time: 0.2496\n",
      "207/281, train_loss: 0.0487, step time: 0.2535\n",
      "208/281, train_loss: 0.0676, step time: 0.2505\n",
      "209/281, train_loss: 0.1393, step time: 0.2526\n",
      "210/281, train_loss: 0.1115, step time: 0.2529\n",
      "211/281, train_loss: 0.0597, step time: 0.2524\n",
      "212/281, train_loss: 0.0791, step time: 0.2501\n",
      "213/281, train_loss: 0.0665, step time: 0.2466\n",
      "214/281, train_loss: 0.1175, step time: 0.2492\n",
      "215/281, train_loss: 0.0449, step time: 0.2564\n",
      "216/281, train_loss: 0.0998, step time: 0.2504\n",
      "217/281, train_loss: 0.2594, step time: 0.2494\n",
      "218/281, train_loss: 0.0475, step time: 0.2540\n",
      "219/281, train_loss: 0.0804, step time: 0.2569\n",
      "220/281, train_loss: 0.0867, step time: 0.2490\n",
      "221/281, train_loss: 0.0522, step time: 0.2504\n",
      "222/281, train_loss: 0.0957, step time: 0.2484\n",
      "223/281, train_loss: 0.0750, step time: 0.2508\n",
      "224/281, train_loss: 0.2268, step time: 0.2477\n",
      "225/281, train_loss: 0.0729, step time: 0.2536\n",
      "226/281, train_loss: 0.0665, step time: 0.2484\n",
      "227/281, train_loss: 0.0676, step time: 0.2530\n",
      "228/281, train_loss: 0.0623, step time: 0.2491\n",
      "229/281, train_loss: 0.0541, step time: 0.2530\n",
      "230/281, train_loss: 0.0996, step time: 0.2502\n",
      "231/281, train_loss: 0.3801, step time: 0.2508\n",
      "232/281, train_loss: 0.0725, step time: 0.2532\n",
      "233/281, train_loss: 0.0357, step time: 0.2481\n",
      "234/281, train_loss: 0.0549, step time: 0.2535\n",
      "235/281, train_loss: 0.0708, step time: 0.2525\n",
      "236/281, train_loss: 0.2652, step time: 0.2506\n",
      "237/281, train_loss: 0.2295, step time: 0.2483\n",
      "238/281, train_loss: 0.0582, step time: 0.2528\n",
      "239/281, train_loss: 0.0839, step time: 0.2493\n",
      "240/281, train_loss: 0.0744, step time: 0.2502\n",
      "241/281, train_loss: 0.0708, step time: 0.2493\n",
      "242/281, train_loss: 0.1161, step time: 0.2489\n",
      "243/281, train_loss: 0.0854, step time: 0.2500\n",
      "244/281, train_loss: 0.0450, step time: 0.2538\n",
      "245/281, train_loss: 0.0678, step time: 0.2533\n",
      "246/281, train_loss: 0.1002, step time: 0.2513\n",
      "247/281, train_loss: 0.1198, step time: 0.2482\n",
      "248/281, train_loss: 0.0975, step time: 0.2456\n",
      "249/281, train_loss: 0.0681, step time: 0.2483\n",
      "250/281, train_loss: 0.0675, step time: 0.2506\n",
      "251/281, train_loss: 0.0515, step time: 0.2482\n",
      "252/281, train_loss: 0.0882, step time: 0.2511\n",
      "253/281, train_loss: 0.2254, step time: 0.2525\n",
      "254/281, train_loss: 0.2384, step time: 0.2548\n",
      "255/281, train_loss: 0.0663, step time: 0.2523\n",
      "256/281, train_loss: 0.0957, step time: 0.2517\n",
      "257/281, train_loss: 0.0584, step time: 0.2525\n",
      "258/281, train_loss: 0.1085, step time: 0.2529\n",
      "259/281, train_loss: 0.1298, step time: 0.2547\n",
      "260/281, train_loss: 0.0905, step time: 0.2465\n",
      "261/281, train_loss: 0.2282, step time: 0.2490\n",
      "262/281, train_loss: 0.2399, step time: 0.2462\n",
      "263/281, train_loss: 0.0682, step time: 0.2554\n",
      "264/281, train_loss: 0.0675, step time: 0.2497\n",
      "265/281, train_loss: 0.0982, step time: 0.2541\n",
      "266/281, train_loss: 0.1039, step time: 0.2473\n",
      "267/281, train_loss: 0.0563, step time: 0.2493\n",
      "268/281, train_loss: 0.0604, step time: 0.2502\n",
      "269/281, train_loss: 0.0649, step time: 0.2572\n",
      "270/281, train_loss: 0.0660, step time: 0.2491\n",
      "271/281, train_loss: 0.2232, step time: 0.2774\n",
      "272/281, train_loss: 0.0743, step time: 0.2582\n",
      "273/281, train_loss: 0.2361, step time: 0.2574\n",
      "274/281, train_loss: 0.1168, step time: 0.2523\n",
      "275/281, train_loss: 0.1101, step time: 0.2478\n",
      "276/281, train_loss: 0.1207, step time: 0.2492\n",
      "277/281, train_loss: 0.0672, step time: 0.2529\n",
      "278/281, train_loss: 0.2132, step time: 0.2547\n",
      "279/281, train_loss: 0.0607, step time: 0.2553\n",
      "280/281, train_loss: 0.2250, step time: 0.2538\n",
      "281/281, train_loss: 0.0482, step time: 0.2466\n",
      "282/281, train_loss: 0.0920, step time: 0.1519\n",
      "epoch 157 average loss: 0.0968\n",
      "current epoch: 157 current mean dice: 0.8758 tc: 0.8743 wt: 0.8903 et: 0.8771\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 157 is: 368.1961\n",
      "----------\n",
      "epoch 158/200\n",
      "1/281, train_loss: 0.1041, step time: 0.2637\n",
      "2/281, train_loss: 0.0873, step time: 0.2633\n",
      "3/281, train_loss: 0.0881, step time: 0.2650\n",
      "4/281, train_loss: 0.1535, step time: 0.2589\n",
      "5/281, train_loss: 0.1047, step time: 0.2485\n",
      "6/281, train_loss: 0.0611, step time: 0.2483\n",
      "7/281, train_loss: 0.0538, step time: 0.2556\n",
      "8/281, train_loss: 0.1108, step time: 0.2548\n",
      "9/281, train_loss: 0.1325, step time: 0.2592\n",
      "10/281, train_loss: 0.0666, step time: 0.2585\n",
      "11/281, train_loss: 0.0601, step time: 0.2546\n",
      "12/281, train_loss: 0.0766, step time: 0.2574\n",
      "13/281, train_loss: 0.0905, step time: 0.2536\n",
      "14/281, train_loss: 0.0770, step time: 0.2501\n",
      "15/281, train_loss: 0.0442, step time: 0.2538\n",
      "16/281, train_loss: 0.0749, step time: 0.2514\n",
      "17/281, train_loss: 0.1198, step time: 0.2524\n",
      "18/281, train_loss: 0.0772, step time: 0.2488\n",
      "19/281, train_loss: 0.0546, step time: 0.2491\n",
      "20/281, train_loss: 0.0541, step time: 0.2560\n",
      "21/281, train_loss: 0.1662, step time: 0.2594\n",
      "22/281, train_loss: 0.0938, step time: 0.2547\n",
      "23/281, train_loss: 0.0626, step time: 0.2514\n",
      "24/281, train_loss: 0.2285, step time: 0.2542\n",
      "25/281, train_loss: 0.0850, step time: 0.2596\n",
      "26/281, train_loss: 0.1090, step time: 0.2520\n",
      "27/281, train_loss: 0.1205, step time: 0.2523\n",
      "28/281, train_loss: 0.0599, step time: 0.2480\n",
      "29/281, train_loss: 0.0703, step time: 0.2527\n",
      "30/281, train_loss: 0.0938, step time: 0.2578\n",
      "31/281, train_loss: 0.2560, step time: 0.2613\n",
      "32/281, train_loss: 0.0597, step time: 0.2502\n",
      "33/281, train_loss: 0.2100, step time: 0.2486\n",
      "34/281, train_loss: 0.0997, step time: 0.2535\n",
      "35/281, train_loss: 0.0698, step time: 0.2550\n",
      "36/281, train_loss: 0.0531, step time: 0.2412\n",
      "37/281, train_loss: 0.0662, step time: 0.2532\n",
      "38/281, train_loss: 0.0566, step time: 0.2491\n",
      "39/281, train_loss: 0.1158, step time: 0.2509\n",
      "40/281, train_loss: 0.2365, step time: 0.2508\n",
      "41/281, train_loss: 0.0565, step time: 0.2468\n",
      "42/281, train_loss: 0.0716, step time: 0.2499\n",
      "43/281, train_loss: 0.0601, step time: 0.2484\n",
      "44/281, train_loss: 0.0695, step time: 0.2454\n",
      "45/281, train_loss: 0.2095, step time: 0.2514\n",
      "46/281, train_loss: 0.0715, step time: 0.2567\n",
      "47/281, train_loss: 0.2533, step time: 0.2520\n",
      "48/281, train_loss: 0.0614, step time: 0.2572\n",
      "49/281, train_loss: 0.0894, step time: 0.2464\n",
      "50/281, train_loss: 0.0375, step time: 0.2445\n",
      "51/281, train_loss: 0.0638, step time: 0.2468\n",
      "52/281, train_loss: 0.0780, step time: 0.2443\n",
      "53/281, train_loss: 0.0896, step time: 0.2562\n",
      "54/281, train_loss: 0.1332, step time: 0.2611\n",
      "55/281, train_loss: 0.0908, step time: 0.2537\n",
      "56/281, train_loss: 0.0616, step time: 0.2491\n",
      "57/281, train_loss: 0.0599, step time: 0.2485\n",
      "58/281, train_loss: 0.0654, step time: 0.2497\n",
      "59/281, train_loss: 0.0539, step time: 0.2507\n",
      "60/281, train_loss: 0.0504, step time: 0.2513\n",
      "61/281, train_loss: 0.1117, step time: 0.2578\n",
      "62/281, train_loss: 0.0712, step time: 0.2520\n",
      "63/281, train_loss: 0.0751, step time: 0.2511\n",
      "64/281, train_loss: 0.2078, step time: 0.2501\n",
      "65/281, train_loss: 0.2262, step time: 0.2473\n",
      "66/281, train_loss: 0.0963, step time: 0.2520\n",
      "67/281, train_loss: 0.0682, step time: 0.2515\n",
      "68/281, train_loss: 0.0895, step time: 0.2498\n",
      "69/281, train_loss: 0.1097, step time: 0.2516\n",
      "70/281, train_loss: 0.0760, step time: 0.2430\n",
      "71/281, train_loss: 0.0972, step time: 0.2587\n",
      "72/281, train_loss: 0.2289, step time: 0.2698\n",
      "73/281, train_loss: 0.0972, step time: 0.2781\n",
      "74/281, train_loss: 0.0528, step time: 0.2593\n",
      "75/281, train_loss: 0.2176, step time: 0.2493\n",
      "76/281, train_loss: 0.0433, step time: 0.2484\n",
      "77/281, train_loss: 0.0994, step time: 0.2441\n",
      "78/281, train_loss: 0.2732, step time: 0.2515\n",
      "79/281, train_loss: 0.2736, step time: 0.2521\n",
      "80/281, train_loss: 0.0991, step time: 0.2499\n",
      "81/281, train_loss: 0.2318, step time: 0.2503\n",
      "82/281, train_loss: 0.0643, step time: 0.2506\n",
      "83/281, train_loss: 0.2403, step time: 0.2544\n",
      "84/281, train_loss: 0.0462, step time: 0.2547\n",
      "85/281, train_loss: 0.1082, step time: 0.2569\n",
      "86/281, train_loss: 0.1089, step time: 0.2481\n",
      "87/281, train_loss: 0.2399, step time: 0.2484\n",
      "88/281, train_loss: 0.0725, step time: 0.2541\n",
      "89/281, train_loss: 0.0988, step time: 0.2706\n",
      "90/281, train_loss: 0.0704, step time: 0.2769\n",
      "91/281, train_loss: 0.0951, step time: 0.2567\n",
      "92/281, train_loss: 0.0893, step time: 0.2448\n",
      "93/281, train_loss: 0.0670, step time: 0.2443\n",
      "94/281, train_loss: 0.0972, step time: 0.2490\n",
      "95/281, train_loss: 0.0548, step time: 0.2477\n",
      "96/281, train_loss: 0.0585, step time: 0.2453\n",
      "97/281, train_loss: 0.2318, step time: 0.2482\n",
      "98/281, train_loss: 0.0523, step time: 0.2544\n",
      "99/281, train_loss: 0.1314, step time: 0.2553\n",
      "100/281, train_loss: 0.0873, step time: 0.2501\n",
      "101/281, train_loss: 0.0797, step time: 0.2626\n",
      "102/281, train_loss: 0.0853, step time: 0.2527\n",
      "103/281, train_loss: 0.0786, step time: 0.2464\n",
      "104/281, train_loss: 0.0974, step time: 0.2456\n",
      "105/281, train_loss: 0.0900, step time: 0.2487\n",
      "106/281, train_loss: 0.2212, step time: 0.2531\n",
      "107/281, train_loss: 0.0574, step time: 0.2494\n",
      "108/281, train_loss: 0.1125, step time: 0.2497\n",
      "109/281, train_loss: 0.0477, step time: 0.2467\n",
      "110/281, train_loss: 0.0473, step time: 0.2452\n",
      "111/281, train_loss: 0.0764, step time: 0.2469\n",
      "112/281, train_loss: 0.0899, step time: 0.2554\n",
      "113/281, train_loss: 0.0684, step time: 0.2568\n",
      "114/281, train_loss: 0.0511, step time: 0.2540\n",
      "115/281, train_loss: 0.0612, step time: 0.2461\n",
      "116/281, train_loss: 0.0599, step time: 0.2492\n",
      "117/281, train_loss: 0.0559, step time: 0.2580\n",
      "118/281, train_loss: 0.0616, step time: 0.2497\n",
      "119/281, train_loss: 0.0727, step time: 0.2562\n",
      "120/281, train_loss: 0.0621, step time: 0.2456\n",
      "121/281, train_loss: 0.0813, step time: 0.2466\n",
      "122/281, train_loss: 0.3909, step time: 0.2451\n",
      "123/281, train_loss: 0.2600, step time: 0.2557\n",
      "124/281, train_loss: 0.0571, step time: 0.2536\n",
      "125/281, train_loss: 0.0709, step time: 0.2551\n",
      "126/281, train_loss: 0.0534, step time: 0.2567\n",
      "127/281, train_loss: 0.0852, step time: 0.2534\n",
      "128/281, train_loss: 0.0934, step time: 0.2492\n",
      "129/281, train_loss: 0.1044, step time: 0.2497\n",
      "130/281, train_loss: 0.0490, step time: 0.2528\n",
      "131/281, train_loss: 0.4097, step time: 0.2584\n",
      "132/281, train_loss: 0.0509, step time: 0.2526\n",
      "133/281, train_loss: 0.0625, step time: 0.2467\n",
      "134/281, train_loss: 0.0764, step time: 0.2450\n",
      "135/281, train_loss: 0.0881, step time: 0.2425\n",
      "136/281, train_loss: 0.2181, step time: 0.2458\n",
      "137/281, train_loss: 0.0500, step time: 0.2499\n",
      "138/281, train_loss: 0.0825, step time: 0.2504\n",
      "139/281, train_loss: 0.0762, step time: 0.2552\n",
      "140/281, train_loss: 0.2204, step time: 0.2473\n",
      "141/281, train_loss: 0.0637, step time: 0.2478\n",
      "142/281, train_loss: 0.0936, step time: 0.2463\n",
      "143/281, train_loss: 0.0430, step time: 0.2537\n",
      "144/281, train_loss: 0.1438, step time: 0.2470\n",
      "145/281, train_loss: 0.0847, step time: 0.2502\n",
      "146/281, train_loss: 0.1068, step time: 0.2488\n",
      "147/281, train_loss: 0.0817, step time: 0.2511\n",
      "148/281, train_loss: 0.0751, step time: 0.2509\n",
      "149/281, train_loss: 0.0682, step time: 0.2477\n",
      "150/281, train_loss: 0.0848, step time: 0.2536\n",
      "151/281, train_loss: 0.0719, step time: 0.2485\n",
      "152/281, train_loss: 0.0838, step time: 0.2459\n",
      "153/281, train_loss: 0.0832, step time: 0.2518\n",
      "154/281, train_loss: 0.3764, step time: 0.2498\n",
      "155/281, train_loss: 0.1209, step time: 0.2478\n",
      "156/281, train_loss: 0.0546, step time: 0.2531\n",
      "157/281, train_loss: 0.0706, step time: 0.2547\n",
      "158/281, train_loss: 0.1399, step time: 0.2582\n",
      "159/281, train_loss: 0.0645, step time: 0.2508\n",
      "160/281, train_loss: 0.0617, step time: 0.2515\n",
      "161/281, train_loss: 0.0606, step time: 0.2476\n",
      "162/281, train_loss: 0.0912, step time: 0.2434\n",
      "163/281, train_loss: 0.0529, step time: 0.2459\n",
      "164/281, train_loss: 0.0414, step time: 0.2505\n",
      "165/281, train_loss: 0.2239, step time: 0.2482\n",
      "166/281, train_loss: 0.0605, step time: 0.2524\n",
      "167/281, train_loss: 0.0593, step time: 0.2444\n",
      "168/281, train_loss: 0.0911, step time: 0.2471\n",
      "169/281, train_loss: 0.0697, step time: 0.2516\n",
      "170/281, train_loss: 0.2012, step time: 0.2458\n",
      "171/281, train_loss: 0.0724, step time: 0.2422\n",
      "172/281, train_loss: 0.0675, step time: 0.2474\n",
      "173/281, train_loss: 0.0781, step time: 0.2530\n",
      "174/281, train_loss: 0.0779, step time: 0.2564\n",
      "175/281, train_loss: 0.0863, step time: 0.2485\n",
      "176/281, train_loss: 0.0767, step time: 0.2480\n",
      "177/281, train_loss: 0.2214, step time: 0.2559\n",
      "178/281, train_loss: 0.0652, step time: 0.2480\n",
      "179/281, train_loss: 0.0896, step time: 0.2456\n",
      "180/281, train_loss: 0.2305, step time: 0.2511\n",
      "181/281, train_loss: 0.0557, step time: 0.2452\n",
      "182/281, train_loss: 0.0933, step time: 0.2487\n",
      "183/281, train_loss: 0.0510, step time: 0.2726\n",
      "184/281, train_loss: 0.0768, step time: 0.2487\n",
      "185/281, train_loss: 0.0904, step time: 0.2514\n",
      "186/281, train_loss: 0.0685, step time: 0.2529\n",
      "187/281, train_loss: 0.0361, step time: 0.2462\n",
      "188/281, train_loss: 0.0790, step time: 0.2453\n",
      "189/281, train_loss: 0.2429, step time: 0.2444\n",
      "190/281, train_loss: 0.0713, step time: 0.2510\n",
      "191/281, train_loss: 0.0502, step time: 0.2466\n",
      "192/281, train_loss: 0.2163, step time: 0.2477\n",
      "193/281, train_loss: 0.0796, step time: 0.2503\n",
      "194/281, train_loss: 0.0709, step time: 0.2471\n",
      "195/281, train_loss: 0.0856, step time: 0.2467\n",
      "196/281, train_loss: 0.0800, step time: 0.2512\n",
      "197/281, train_loss: 0.0786, step time: 0.2483\n",
      "198/281, train_loss: 0.1019, step time: 0.2473\n",
      "199/281, train_loss: 0.0896, step time: 0.2474\n",
      "200/281, train_loss: 0.1084, step time: 0.2488\n",
      "201/281, train_loss: 0.0802, step time: 0.2498\n",
      "202/281, train_loss: 0.0697, step time: 0.2467\n",
      "203/281, train_loss: 0.0643, step time: 0.2492\n",
      "204/281, train_loss: 0.0588, step time: 0.2541\n",
      "205/281, train_loss: 0.0403, step time: 0.2485\n",
      "206/281, train_loss: 0.0976, step time: 0.2483\n",
      "207/281, train_loss: 0.0691, step time: 0.2443\n",
      "208/281, train_loss: 0.0690, step time: 0.2459\n",
      "209/281, train_loss: 0.1111, step time: 0.2467\n",
      "210/281, train_loss: 0.2510, step time: 0.2465\n",
      "211/281, train_loss: 0.1257, step time: 0.2542\n",
      "212/281, train_loss: 0.0865, step time: 0.2473\n",
      "213/281, train_loss: 0.0522, step time: 0.2445\n",
      "214/281, train_loss: 0.1707, step time: 0.2469\n",
      "215/281, train_loss: 0.0944, step time: 0.2462\n",
      "216/281, train_loss: 0.0619, step time: 0.2479\n",
      "217/281, train_loss: 0.0731, step time: 0.2415\n",
      "218/281, train_loss: 0.0825, step time: 0.2402\n",
      "219/281, train_loss: 0.0887, step time: 0.2495\n",
      "220/281, train_loss: 0.1463, step time: 0.2484\n",
      "221/281, train_loss: 0.0706, step time: 0.2498\n",
      "222/281, train_loss: 0.0777, step time: 0.2453\n",
      "223/281, train_loss: 0.0902, step time: 0.2490\n",
      "224/281, train_loss: 0.2870, step time: 0.2491\n",
      "225/281, train_loss: 0.0667, step time: 0.2485\n",
      "226/281, train_loss: 0.0741, step time: 0.2517\n",
      "227/281, train_loss: 0.1665, step time: 0.2475\n",
      "228/281, train_loss: 0.2330, step time: 0.2475\n",
      "229/281, train_loss: 0.0632, step time: 0.2490\n",
      "230/281, train_loss: 0.2455, step time: 0.2459\n",
      "231/281, train_loss: 0.0812, step time: 0.2424\n",
      "232/281, train_loss: 0.0814, step time: 0.2453\n",
      "233/281, train_loss: 0.0968, step time: 0.2501\n",
      "234/281, train_loss: 0.1052, step time: 0.2458\n",
      "235/281, train_loss: 0.2395, step time: 0.2438\n",
      "236/281, train_loss: 0.3336, step time: 0.2497\n",
      "237/281, train_loss: 0.1086, step time: 0.2466\n",
      "238/281, train_loss: 0.0733, step time: 0.2574\n",
      "239/281, train_loss: 0.1014, step time: 0.2425\n",
      "240/281, train_loss: 0.2413, step time: 0.2387\n",
      "241/281, train_loss: 0.0817, step time: 0.2399\n",
      "242/281, train_loss: 0.0587, step time: 0.2449\n",
      "243/281, train_loss: 0.0839, step time: 0.2470\n",
      "244/281, train_loss: 0.1405, step time: 0.2397\n",
      "245/281, train_loss: 0.1365, step time: 0.2442\n",
      "246/281, train_loss: 0.1121, step time: 0.2494\n",
      "247/281, train_loss: 0.0776, step time: 0.2517\n",
      "248/281, train_loss: 0.0635, step time: 0.2479\n",
      "249/281, train_loss: 0.2272, step time: 0.2495\n",
      "250/281, train_loss: 0.1122, step time: 0.2515\n",
      "251/281, train_loss: 0.0716, step time: 0.2445\n",
      "252/281, train_loss: 0.0906, step time: 0.2543\n",
      "253/281, train_loss: 0.1313, step time: 0.2480\n",
      "254/281, train_loss: 0.0680, step time: 0.2502\n",
      "255/281, train_loss: 0.2144, step time: 0.2437\n",
      "256/281, train_loss: 0.1275, step time: 0.2445\n",
      "257/281, train_loss: 0.0816, step time: 0.2456\n",
      "258/281, train_loss: 0.1369, step time: 0.2526\n",
      "259/281, train_loss: 0.0910, step time: 0.2522\n",
      "260/281, train_loss: 0.0641, step time: 0.2439\n",
      "261/281, train_loss: 0.0919, step time: 0.2433\n",
      "262/281, train_loss: 0.1078, step time: 0.2484\n",
      "263/281, train_loss: 0.0626, step time: 0.2485\n",
      "264/281, train_loss: 0.0631, step time: 0.2494\n",
      "265/281, train_loss: 0.0849, step time: 0.2453\n",
      "266/281, train_loss: 0.0627, step time: 0.2511\n",
      "267/281, train_loss: 0.0764, step time: 0.2518\n",
      "268/281, train_loss: 0.0632, step time: 0.2450\n",
      "269/281, train_loss: 0.0687, step time: 0.2484\n",
      "270/281, train_loss: 0.0834, step time: 0.2485\n",
      "271/281, train_loss: 0.0734, step time: 0.2488\n",
      "272/281, train_loss: 0.1100, step time: 0.2501\n",
      "273/281, train_loss: 0.0783, step time: 0.2497\n",
      "274/281, train_loss: 0.0893, step time: 0.2470\n",
      "275/281, train_loss: 0.1001, step time: 0.2465\n",
      "276/281, train_loss: 0.0770, step time: 0.2418\n",
      "277/281, train_loss: 0.0725, step time: 0.2456\n",
      "278/281, train_loss: 0.2498, step time: 0.2473\n",
      "279/281, train_loss: 0.0547, step time: 0.2439\n",
      "280/281, train_loss: 0.0544, step time: 0.2433\n",
      "281/281, train_loss: 0.1084, step time: 0.2425\n",
      "282/281, train_loss: 0.0623, step time: 0.1471\n",
      "epoch 158 average loss: 0.1041\n",
      "current epoch: 158 current mean dice: 0.8656 tc: 0.8669 wt: 0.8709 et: 0.8745\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 158 is: 357.7068\n",
      "----------\n",
      "epoch 159/200\n",
      "1/281, train_loss: 0.0816, step time: 0.2577\n",
      "2/281, train_loss: 0.0999, step time: 0.2456\n",
      "3/281, train_loss: 0.0965, step time: 0.2436\n",
      "4/281, train_loss: 0.0920, step time: 0.2472\n",
      "5/281, train_loss: 0.0682, step time: 0.2525\n",
      "6/281, train_loss: 0.1018, step time: 0.2543\n",
      "7/281, train_loss: 0.0639, step time: 0.2490\n",
      "8/281, train_loss: 0.0888, step time: 0.2456\n",
      "9/281, train_loss: 0.0764, step time: 0.2485\n",
      "10/281, train_loss: 0.0863, step time: 0.2476\n",
      "11/281, train_loss: 0.0795, step time: 0.2558\n",
      "12/281, train_loss: 0.0897, step time: 0.2560\n",
      "13/281, train_loss: 0.0941, step time: 0.2473\n",
      "14/281, train_loss: 0.0923, step time: 0.2429\n",
      "15/281, train_loss: 0.2577, step time: 0.2483\n",
      "16/281, train_loss: 0.1335, step time: 0.2469\n",
      "17/281, train_loss: 0.2591, step time: 0.2412\n",
      "18/281, train_loss: 0.2248, step time: 0.2505\n",
      "19/281, train_loss: 0.2843, step time: 0.2509\n",
      "20/281, train_loss: 0.0770, step time: 0.2488\n",
      "21/281, train_loss: 0.1161, step time: 0.2485\n",
      "22/281, train_loss: 0.0625, step time: 0.2451\n",
      "23/281, train_loss: 0.0674, step time: 0.2548\n",
      "24/281, train_loss: 0.2428, step time: 0.2465\n",
      "25/281, train_loss: 0.0697, step time: 0.2485\n",
      "26/281, train_loss: 0.2415, step time: 0.2456\n",
      "27/281, train_loss: 0.0688, step time: 0.2497\n",
      "28/281, train_loss: 0.2231, step time: 0.2472\n",
      "29/281, train_loss: 0.1116, step time: 0.2439\n",
      "30/281, train_loss: 0.0720, step time: 0.2469\n",
      "31/281, train_loss: 0.1123, step time: 0.2519\n",
      "32/281, train_loss: 0.0769, step time: 0.2477\n",
      "33/281, train_loss: 0.0756, step time: 0.2440\n",
      "34/281, train_loss: 0.0825, step time: 0.2468\n",
      "35/281, train_loss: 0.0466, step time: 0.2441\n",
      "36/281, train_loss: 0.0633, step time: 0.2486\n",
      "37/281, train_loss: 0.0959, step time: 0.2522\n",
      "38/281, train_loss: 0.0892, step time: 0.2527\n",
      "39/281, train_loss: 0.2234, step time: 0.2444\n",
      "40/281, train_loss: 0.0908, step time: 0.2472\n",
      "41/281, train_loss: 0.0503, step time: 0.2523\n",
      "42/281, train_loss: 0.0753, step time: 0.2477\n",
      "43/281, train_loss: 0.0695, step time: 0.2572\n",
      "44/281, train_loss: 0.1079, step time: 0.2448\n",
      "45/281, train_loss: 0.2304, step time: 0.2429\n",
      "46/281, train_loss: 0.0562, step time: 0.2397\n",
      "47/281, train_loss: 0.0746, step time: 0.2503\n",
      "48/281, train_loss: 0.1545, step time: 0.2466\n",
      "49/281, train_loss: 0.2243, step time: 0.2460\n",
      "50/281, train_loss: 0.0523, step time: 0.2480\n",
      "51/281, train_loss: 0.0844, step time: 0.2478\n",
      "52/281, train_loss: 0.2146, step time: 0.2415\n",
      "53/281, train_loss: 0.0804, step time: 0.2426\n",
      "54/281, train_loss: 0.0558, step time: 0.2457\n",
      "55/281, train_loss: 0.2471, step time: 0.2458\n",
      "56/281, train_loss: 0.0668, step time: 0.2466\n",
      "57/281, train_loss: 0.0747, step time: 0.2423\n",
      "58/281, train_loss: 0.0722, step time: 0.2510\n",
      "59/281, train_loss: 0.0832, step time: 0.2457\n",
      "60/281, train_loss: 0.0686, step time: 0.2440\n",
      "61/281, train_loss: 0.0831, step time: 0.2411\n",
      "62/281, train_loss: 0.1000, step time: 0.2480\n",
      "63/281, train_loss: 0.1091, step time: 0.2514\n",
      "64/281, train_loss: 0.0732, step time: 0.2427\n",
      "65/281, train_loss: 0.0504, step time: 0.2458\n",
      "66/281, train_loss: 0.2673, step time: 0.2457\n",
      "67/281, train_loss: 0.0660, step time: 0.2426\n",
      "68/281, train_loss: 0.0956, step time: 0.2464\n",
      "69/281, train_loss: 0.0931, step time: 0.2406\n",
      "70/281, train_loss: 0.1087, step time: 0.2504\n",
      "71/281, train_loss: 0.2764, step time: 0.2445\n",
      "72/281, train_loss: 0.1063, step time: 0.2472\n",
      "73/281, train_loss: 0.0776, step time: 0.2483\n",
      "74/281, train_loss: 0.1278, step time: 0.2457\n",
      "75/281, train_loss: 0.3092, step time: 0.2428\n",
      "76/281, train_loss: 0.0584, step time: 0.2485\n",
      "77/281, train_loss: 0.0727, step time: 0.2451\n",
      "78/281, train_loss: 0.0519, step time: 0.2451\n",
      "79/281, train_loss: 0.0876, step time: 0.2468\n",
      "80/281, train_loss: 0.0379, step time: 0.2488\n",
      "81/281, train_loss: 0.0724, step time: 0.2439\n",
      "82/281, train_loss: 0.0848, step time: 0.2526\n",
      "83/281, train_loss: 0.0863, step time: 0.2479\n",
      "84/281, train_loss: 0.0557, step time: 0.2385\n",
      "85/281, train_loss: 0.0935, step time: 0.2428\n",
      "86/281, train_loss: 0.2049, step time: 0.2441\n",
      "87/281, train_loss: 0.2679, step time: 0.2406\n",
      "88/281, train_loss: 0.0503, step time: 0.2408\n",
      "89/281, train_loss: 0.0832, step time: 0.2502\n",
      "90/281, train_loss: 0.0501, step time: 0.2494\n",
      "91/281, train_loss: 0.0634, step time: 0.2433\n",
      "92/281, train_loss: 0.0475, step time: 0.2442\n",
      "93/281, train_loss: 0.0822, step time: 0.2436\n",
      "94/281, train_loss: 0.2244, step time: 0.2518\n",
      "95/281, train_loss: 0.1021, step time: 0.2488\n",
      "96/281, train_loss: 0.1157, step time: 0.2489\n",
      "97/281, train_loss: 0.1198, step time: 0.2443\n",
      "98/281, train_loss: 0.0864, step time: 0.2421\n",
      "99/281, train_loss: 0.2220, step time: 0.2442\n",
      "100/281, train_loss: 0.0534, step time: 0.2462\n",
      "101/281, train_loss: 0.0845, step time: 0.2514\n",
      "102/281, train_loss: 0.0723, step time: 0.2483\n",
      "103/281, train_loss: 0.0993, step time: 0.2432\n",
      "104/281, train_loss: 0.0853, step time: 0.2448\n",
      "105/281, train_loss: 0.0680, step time: 0.2504\n",
      "106/281, train_loss: 0.0760, step time: 0.2462\n",
      "107/281, train_loss: 0.0945, step time: 0.2539\n",
      "108/281, train_loss: 0.0643, step time: 0.2478\n",
      "109/281, train_loss: 0.0702, step time: 0.2460\n",
      "110/281, train_loss: 0.2198, step time: 0.2476\n",
      "111/281, train_loss: 0.0700, step time: 0.2491\n",
      "112/281, train_loss: 0.0603, step time: 0.2444\n",
      "113/281, train_loss: 0.0540, step time: 0.2416\n",
      "114/281, train_loss: 0.0834, step time: 0.2481\n",
      "115/281, train_loss: 0.0867, step time: 0.2457\n",
      "116/281, train_loss: 0.1056, step time: 0.2464\n",
      "117/281, train_loss: 0.1070, step time: 0.2485\n",
      "118/281, train_loss: 0.0433, step time: 0.2450\n",
      "119/281, train_loss: 0.0791, step time: 0.2536\n",
      "120/281, train_loss: 0.1341, step time: 0.2501\n",
      "121/281, train_loss: 0.0757, step time: 0.2410\n",
      "122/281, train_loss: 0.0819, step time: 0.2430\n",
      "123/281, train_loss: 0.3879, step time: 0.2525\n",
      "124/281, train_loss: 0.0832, step time: 0.2536\n",
      "125/281, train_loss: 0.0714, step time: 0.2581\n",
      "126/281, train_loss: 0.2241, step time: 0.2536\n",
      "127/281, train_loss: 0.0695, step time: 0.2494\n",
      "128/281, train_loss: 0.1179, step time: 0.2435\n",
      "129/281, train_loss: 0.1064, step time: 0.2520\n",
      "130/281, train_loss: 0.0805, step time: 0.2473\n",
      "131/281, train_loss: 0.0708, step time: 0.2463\n",
      "132/281, train_loss: 0.0896, step time: 0.2556\n",
      "133/281, train_loss: 0.1054, step time: 0.2462\n",
      "134/281, train_loss: 0.1389, step time: 0.2559\n",
      "135/281, train_loss: 0.0454, step time: 0.2499\n",
      "136/281, train_loss: 0.0641, step time: 0.2520\n",
      "137/281, train_loss: 0.2461, step time: 0.2514\n",
      "138/281, train_loss: 0.2195, step time: 0.2431\n",
      "139/281, train_loss: 0.0720, step time: 0.2480\n",
      "140/281, train_loss: 0.0622, step time: 0.2445\n",
      "141/281, train_loss: 0.0819, step time: 0.2481\n",
      "142/281, train_loss: 0.0940, step time: 0.2457\n",
      "143/281, train_loss: 0.1143, step time: 0.2432\n",
      "144/281, train_loss: 0.0652, step time: 0.2494\n",
      "145/281, train_loss: 0.2131, step time: 0.2447\n",
      "146/281, train_loss: 0.0982, step time: 0.2386\n",
      "147/281, train_loss: 0.0811, step time: 0.2447\n",
      "148/281, train_loss: 0.0802, step time: 0.2508\n",
      "149/281, train_loss: 0.0738, step time: 0.2686\n",
      "150/281, train_loss: 0.0774, step time: 0.2461\n",
      "151/281, train_loss: 0.1155, step time: 0.2465\n",
      "152/281, train_loss: 0.0949, step time: 0.2468\n",
      "153/281, train_loss: 0.2273, step time: 0.2497\n",
      "154/281, train_loss: 0.0759, step time: 0.2466\n",
      "155/281, train_loss: 0.1008, step time: 0.2455\n",
      "156/281, train_loss: 0.0946, step time: 0.2505\n",
      "157/281, train_loss: 0.0596, step time: 0.2468\n",
      "158/281, train_loss: 0.0804, step time: 0.2509\n",
      "159/281, train_loss: 0.0932, step time: 0.2464\n",
      "160/281, train_loss: 0.0636, step time: 0.2510\n",
      "161/281, train_loss: 0.2504, step time: 0.2555\n",
      "162/281, train_loss: 0.0908, step time: 0.2458\n",
      "163/281, train_loss: 0.0602, step time: 0.2418\n",
      "164/281, train_loss: 0.0987, step time: 0.2499\n",
      "165/281, train_loss: 0.0603, step time: 0.2492\n",
      "166/281, train_loss: 0.0762, step time: 0.2538\n",
      "167/281, train_loss: 0.0741, step time: 0.2540\n",
      "168/281, train_loss: 0.1659, step time: 0.2523\n",
      "169/281, train_loss: 0.2419, step time: 0.2511\n",
      "170/281, train_loss: 0.0716, step time: 0.2513\n",
      "171/281, train_loss: 0.0582, step time: 0.2459\n",
      "172/281, train_loss: 0.1103, step time: 0.2482\n",
      "173/281, train_loss: 0.0509, step time: 0.2523\n",
      "174/281, train_loss: 0.2232, step time: 0.2494\n",
      "175/281, train_loss: 0.0759, step time: 0.2480\n",
      "176/281, train_loss: 0.0686, step time: 0.2523\n",
      "177/281, train_loss: 0.0681, step time: 0.2509\n",
      "178/281, train_loss: 0.0690, step time: 0.2512\n",
      "179/281, train_loss: 0.0698, step time: 0.2537\n",
      "180/281, train_loss: 0.0929, step time: 0.2512\n",
      "181/281, train_loss: 0.0551, step time: 0.2516\n",
      "182/281, train_loss: 0.1405, step time: 0.2483\n",
      "183/281, train_loss: 0.0947, step time: 0.2552\n",
      "184/281, train_loss: 0.0728, step time: 0.2553\n",
      "185/281, train_loss: 0.0537, step time: 0.2572\n",
      "186/281, train_loss: 0.1298, step time: 0.2552\n",
      "187/281, train_loss: 0.0691, step time: 0.2536\n",
      "188/281, train_loss: 0.0709, step time: 0.2480\n",
      "189/281, train_loss: 0.2267, step time: 0.2503\n",
      "190/281, train_loss: 0.3530, step time: 0.2525\n",
      "191/281, train_loss: 0.0891, step time: 0.2530\n",
      "192/281, train_loss: 0.0671, step time: 0.2488\n",
      "193/281, train_loss: 0.2292, step time: 0.2501\n",
      "194/281, train_loss: 0.2199, step time: 0.2484\n",
      "195/281, train_loss: 0.0793, step time: 0.2476\n",
      "196/281, train_loss: 0.1002, step time: 0.2482\n",
      "197/281, train_loss: 0.1087, step time: 0.2494\n",
      "198/281, train_loss: 0.0761, step time: 0.2455\n",
      "199/281, train_loss: 0.0855, step time: 0.2589\n",
      "200/281, train_loss: 0.0661, step time: 0.2485\n",
      "201/281, train_loss: 0.1220, step time: 0.2480\n",
      "202/281, train_loss: 0.0748, step time: 0.2467\n",
      "203/281, train_loss: 0.0920, step time: 0.2496\n",
      "204/281, train_loss: 0.2474, step time: 0.2504\n",
      "205/281, train_loss: 0.2053, step time: 0.2470\n",
      "206/281, train_loss: 0.0591, step time: 0.2495\n",
      "207/281, train_loss: 0.0923, step time: 0.2506\n",
      "208/281, train_loss: 0.0662, step time: 0.2484\n",
      "209/281, train_loss: 0.1280, step time: 0.2517\n",
      "210/281, train_loss: 0.0526, step time: 0.2461\n",
      "211/281, train_loss: 0.0978, step time: 0.2588\n",
      "212/281, train_loss: 0.0808, step time: 0.2515\n",
      "213/281, train_loss: 0.0573, step time: 0.2551\n",
      "214/281, train_loss: 0.0820, step time: 0.2515\n",
      "215/281, train_loss: 0.0478, step time: 0.2531\n",
      "216/281, train_loss: 0.0773, step time: 0.2480\n",
      "217/281, train_loss: 0.0823, step time: 0.2465\n",
      "218/281, train_loss: 0.0790, step time: 0.2531\n",
      "219/281, train_loss: 0.0781, step time: 0.2564\n",
      "220/281, train_loss: 0.0971, step time: 0.2542\n",
      "221/281, train_loss: 0.0711, step time: 0.2496\n",
      "222/281, train_loss: 0.0850, step time: 0.2450\n",
      "223/281, train_loss: 0.0672, step time: 0.2549\n",
      "224/281, train_loss: 0.0727, step time: 0.2567\n",
      "225/281, train_loss: 0.2179, step time: 0.2547\n",
      "226/281, train_loss: 0.0923, step time: 0.2552\n",
      "227/281, train_loss: 0.0783, step time: 0.2513\n",
      "228/281, train_loss: 0.0824, step time: 0.2488\n",
      "229/281, train_loss: 0.1009, step time: 0.2528\n",
      "230/281, train_loss: 0.0795, step time: 0.2568\n",
      "231/281, train_loss: 0.0760, step time: 0.2520\n",
      "232/281, train_loss: 0.0813, step time: 0.2519\n",
      "233/281, train_loss: 0.1097, step time: 0.2538\n",
      "234/281, train_loss: 0.0700, step time: 0.2528\n",
      "235/281, train_loss: 0.0556, step time: 0.2506\n",
      "236/281, train_loss: 0.0615, step time: 0.2508\n",
      "237/281, train_loss: 0.1254, step time: 0.2514\n",
      "238/281, train_loss: 0.0394, step time: 0.2539\n",
      "239/281, train_loss: 0.0918, step time: 0.2514\n",
      "240/281, train_loss: 0.0617, step time: 0.2516\n",
      "241/281, train_loss: 0.1159, step time: 0.2537\n",
      "242/281, train_loss: 0.1213, step time: 0.2501\n",
      "243/281, train_loss: 0.0852, step time: 0.2490\n",
      "244/281, train_loss: 0.0888, step time: 0.2507\n",
      "245/281, train_loss: 0.2413, step time: 0.2496\n",
      "246/281, train_loss: 0.1120, step time: 0.2484\n",
      "247/281, train_loss: 0.2320, step time: 0.2552\n",
      "248/281, train_loss: 0.0715, step time: 0.2495\n",
      "249/281, train_loss: 0.0985, step time: 0.2499\n",
      "250/281, train_loss: 0.0917, step time: 0.2565\n",
      "251/281, train_loss: 0.0565, step time: 0.2512\n",
      "252/281, train_loss: 0.0759, step time: 0.2534\n",
      "253/281, train_loss: 0.0706, step time: 0.2520\n",
      "254/281, train_loss: 0.0544, step time: 0.2492\n",
      "255/281, train_loss: 0.0836, step time: 0.2474\n",
      "256/281, train_loss: 0.0859, step time: 0.2471\n",
      "257/281, train_loss: 0.0871, step time: 0.2473\n",
      "258/281, train_loss: 0.0778, step time: 0.2456\n",
      "259/281, train_loss: 0.0634, step time: 0.2510\n",
      "260/281, train_loss: 0.0746, step time: 0.2482\n",
      "261/281, train_loss: 0.0638, step time: 0.2497\n",
      "262/281, train_loss: 0.1256, step time: 0.2515\n",
      "263/281, train_loss: 0.0658, step time: 0.2519\n",
      "264/281, train_loss: 0.0725, step time: 0.2480\n",
      "265/281, train_loss: 0.0719, step time: 0.2446\n",
      "266/281, train_loss: 0.2455, step time: 0.2479\n",
      "267/281, train_loss: 0.0626, step time: 0.2487\n",
      "268/281, train_loss: 0.0668, step time: 0.2451\n",
      "269/281, train_loss: 0.2350, step time: 0.2454\n",
      "270/281, train_loss: 0.0713, step time: 0.2436\n",
      "271/281, train_loss: 0.2228, step time: 0.2516\n",
      "272/281, train_loss: 0.0736, step time: 0.2530\n",
      "273/281, train_loss: 0.0812, step time: 0.2504\n",
      "274/281, train_loss: 0.0939, step time: 0.2506\n",
      "275/281, train_loss: 0.1011, step time: 0.2562\n",
      "276/281, train_loss: 0.0787, step time: 0.2531\n",
      "277/281, train_loss: 0.0845, step time: 0.2511\n",
      "278/281, train_loss: 0.0803, step time: 0.2544\n",
      "279/281, train_loss: 0.1157, step time: 0.2519\n",
      "280/281, train_loss: 0.0551, step time: 0.2499\n",
      "281/281, train_loss: 0.0910, step time: 0.2525\n",
      "282/281, train_loss: 0.0465, step time: 0.1539\n",
      "epoch 159 average loss: 0.1053\n",
      "current epoch: 159 current mean dice: 0.8767 tc: 0.8701 wt: 0.8997 et: 0.8750\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 159 is: 397.3403\n",
      "----------\n",
      "epoch 160/200\n",
      "1/281, train_loss: 0.0737, step time: 0.2526\n",
      "2/281, train_loss: 0.0815, step time: 0.2529\n",
      "3/281, train_loss: 0.0775, step time: 0.2565\n",
      "4/281, train_loss: 0.1069, step time: 0.2539\n",
      "5/281, train_loss: 0.0808, step time: 0.2505\n",
      "6/281, train_loss: 0.2672, step time: 0.2550\n",
      "7/281, train_loss: 0.0581, step time: 0.2535\n",
      "8/281, train_loss: 0.0552, step time: 0.2532\n",
      "9/281, train_loss: 0.2150, step time: 0.2574\n",
      "10/281, train_loss: 0.0756, step time: 0.2547\n",
      "11/281, train_loss: 0.0847, step time: 0.2522\n",
      "12/281, train_loss: 0.2536, step time: 0.2582\n",
      "13/281, train_loss: 0.0897, step time: 0.2543\n",
      "14/281, train_loss: 0.2286, step time: 0.2503\n",
      "15/281, train_loss: 0.2589, step time: 0.2523\n",
      "16/281, train_loss: 0.0816, step time: 0.2522\n",
      "17/281, train_loss: 0.1126, step time: 0.2577\n",
      "18/281, train_loss: 0.0788, step time: 0.2558\n",
      "19/281, train_loss: 0.1489, step time: 0.2575\n",
      "20/281, train_loss: 0.1209, step time: 0.2625\n",
      "21/281, train_loss: 0.0976, step time: 0.2522\n",
      "22/281, train_loss: 0.1110, step time: 0.2542\n",
      "23/281, train_loss: 0.0686, step time: 0.2483\n",
      "24/281, train_loss: 0.0867, step time: 0.2483\n",
      "25/281, train_loss: 0.2289, step time: 0.2468\n",
      "26/281, train_loss: 0.0728, step time: 0.2496\n",
      "27/281, train_loss: 0.2792, step time: 0.2521\n",
      "28/281, train_loss: 0.0672, step time: 0.2498\n",
      "29/281, train_loss: 0.1049, step time: 0.2510\n",
      "30/281, train_loss: 0.0635, step time: 0.2522\n",
      "31/281, train_loss: 0.0650, step time: 0.2512\n",
      "32/281, train_loss: 0.0780, step time: 0.2584\n",
      "33/281, train_loss: 0.2291, step time: 0.2561\n",
      "34/281, train_loss: 0.0679, step time: 0.2549\n",
      "35/281, train_loss: 0.0637, step time: 0.2488\n",
      "36/281, train_loss: 0.2209, step time: 0.2474\n",
      "37/281, train_loss: 0.0867, step time: 0.2561\n",
      "38/281, train_loss: 0.0869, step time: 0.2513\n",
      "39/281, train_loss: 0.0806, step time: 0.2525\n",
      "40/281, train_loss: 0.0486, step time: 0.2563\n",
      "41/281, train_loss: 0.0700, step time: 0.2500\n",
      "42/281, train_loss: 0.0718, step time: 0.2506\n",
      "43/281, train_loss: 0.1012, step time: 0.2485\n",
      "44/281, train_loss: 0.1048, step time: 0.2487\n",
      "45/281, train_loss: 0.0527, step time: 0.2489\n",
      "46/281, train_loss: 0.0599, step time: 0.2543\n",
      "47/281, train_loss: 0.0931, step time: 0.2546\n",
      "48/281, train_loss: 0.0536, step time: 0.2488\n",
      "49/281, train_loss: 0.1055, step time: 0.2554\n",
      "50/281, train_loss: 0.0547, step time: 0.2501\n",
      "51/281, train_loss: 0.0688, step time: 0.2497\n",
      "52/281, train_loss: 0.0796, step time: 0.2525\n",
      "53/281, train_loss: 0.2313, step time: 0.2516\n",
      "54/281, train_loss: 0.0817, step time: 0.2533\n",
      "55/281, train_loss: 0.0529, step time: 0.2517\n",
      "56/281, train_loss: 0.0860, step time: 0.2589\n",
      "57/281, train_loss: 0.0688, step time: 0.2530\n",
      "58/281, train_loss: 0.2298, step time: 0.2559\n",
      "59/281, train_loss: 0.2258, step time: 0.2543\n",
      "60/281, train_loss: 0.0651, step time: 0.2515\n",
      "61/281, train_loss: 0.0881, step time: 0.2545\n",
      "62/281, train_loss: 0.0957, step time: 0.2557\n",
      "63/281, train_loss: 0.0864, step time: 0.2589\n",
      "64/281, train_loss: 0.2659, step time: 0.2559\n",
      "65/281, train_loss: 0.0958, step time: 0.2573\n",
      "66/281, train_loss: 0.0615, step time: 0.2541\n",
      "67/281, train_loss: 0.0890, step time: 0.2526\n",
      "68/281, train_loss: 0.0931, step time: 0.2520\n",
      "69/281, train_loss: 0.0405, step time: 0.2642\n",
      "70/281, train_loss: 0.0703, step time: 0.2571\n",
      "71/281, train_loss: 0.0831, step time: 0.2545\n",
      "72/281, train_loss: 0.0573, step time: 0.2481\n",
      "73/281, train_loss: 0.0777, step time: 0.2504\n",
      "74/281, train_loss: 0.2329, step time: 0.2529\n",
      "75/281, train_loss: 0.0687, step time: 0.2531\n",
      "76/281, train_loss: 0.0579, step time: 0.2481\n",
      "77/281, train_loss: 0.0879, step time: 0.2587\n",
      "78/281, train_loss: 0.0733, step time: 0.2523\n",
      "79/281, train_loss: 0.0837, step time: 0.2460\n",
      "80/281, train_loss: 0.0820, step time: 0.2454\n",
      "81/281, train_loss: 0.1032, step time: 0.2461\n",
      "82/281, train_loss: 0.2172, step time: 0.2605\n",
      "83/281, train_loss: 0.0536, step time: 0.2618\n",
      "84/281, train_loss: 0.0504, step time: 0.2564\n",
      "85/281, train_loss: 0.0905, step time: 0.2436\n",
      "86/281, train_loss: 0.0651, step time: 0.2577\n",
      "87/281, train_loss: 0.0553, step time: 0.2554\n",
      "88/281, train_loss: 0.0490, step time: 0.2461\n",
      "89/281, train_loss: 0.0818, step time: 0.2498\n",
      "90/281, train_loss: 0.1016, step time: 0.2562\n",
      "91/281, train_loss: 0.0981, step time: 0.2593\n",
      "92/281, train_loss: 0.2136, step time: 0.2563\n",
      "93/281, train_loss: 0.1265, step time: 0.2506\n",
      "94/281, train_loss: 0.0908, step time: 0.2537\n",
      "95/281, train_loss: 0.2463, step time: 0.2478\n",
      "96/281, train_loss: 0.0829, step time: 0.2484\n",
      "97/281, train_loss: 0.0334, step time: 0.2500\n",
      "98/281, train_loss: 0.0969, step time: 0.2471\n",
      "99/281, train_loss: 0.1106, step time: 0.2539\n",
      "100/281, train_loss: 0.2250, step time: 0.2488\n",
      "101/281, train_loss: 0.0913, step time: 0.2500\n",
      "102/281, train_loss: 0.0719, step time: 0.2471\n",
      "103/281, train_loss: 0.0841, step time: 0.2452\n",
      "104/281, train_loss: 0.0751, step time: 0.2485\n",
      "105/281, train_loss: 0.1272, step time: 0.2569\n",
      "106/281, train_loss: 0.0518, step time: 0.2497\n",
      "107/281, train_loss: 0.0600, step time: 0.2474\n",
      "108/281, train_loss: 0.0690, step time: 0.2522\n",
      "109/281, train_loss: 0.0636, step time: 0.2493\n",
      "110/281, train_loss: 0.0959, step time: 0.2506\n",
      "111/281, train_loss: 0.0583, step time: 0.2500\n",
      "112/281, train_loss: 0.0679, step time: 0.2494\n",
      "113/281, train_loss: 0.0739, step time: 0.2516\n",
      "114/281, train_loss: 0.0834, step time: 0.2527\n",
      "115/281, train_loss: 0.2688, step time: 0.2477\n",
      "116/281, train_loss: 0.0954, step time: 0.2494\n",
      "117/281, train_loss: 0.0855, step time: 0.2455\n",
      "118/281, train_loss: 0.0803, step time: 0.2416\n",
      "119/281, train_loss: 0.1908, step time: 0.2450\n",
      "120/281, train_loss: 0.0737, step time: 0.2443\n",
      "121/281, train_loss: 0.0966, step time: 0.2527\n",
      "122/281, train_loss: 0.1272, step time: 0.2531\n",
      "123/281, train_loss: 0.0842, step time: 0.2476\n",
      "124/281, train_loss: 0.0886, step time: 0.2509\n",
      "125/281, train_loss: 0.0566, step time: 0.2474\n",
      "126/281, train_loss: 0.0824, step time: 0.2489\n",
      "127/281, train_loss: 0.0901, step time: 0.2489\n",
      "128/281, train_loss: 0.2148, step time: 0.2515\n",
      "129/281, train_loss: 0.0681, step time: 0.2512\n",
      "130/281, train_loss: 0.0987, step time: 0.2522\n",
      "131/281, train_loss: 0.0544, step time: 0.2498\n",
      "132/281, train_loss: 0.0552, step time: 0.2497\n",
      "133/281, train_loss: 0.0834, step time: 0.2479\n",
      "134/281, train_loss: 0.1160, step time: 0.2628\n",
      "135/281, train_loss: 0.0753, step time: 0.2550\n",
      "136/281, train_loss: 0.0727, step time: 0.2481\n",
      "137/281, train_loss: 0.0632, step time: 0.2412\n",
      "138/281, train_loss: 0.0753, step time: 0.2552\n",
      "139/281, train_loss: 0.2515, step time: 0.2517\n",
      "140/281, train_loss: 0.0571, step time: 0.2455\n",
      "141/281, train_loss: 0.0655, step time: 0.2460\n",
      "142/281, train_loss: 0.0790, step time: 0.2521\n",
      "143/281, train_loss: 0.0864, step time: 0.2457\n",
      "144/281, train_loss: 0.2270, step time: 0.2448\n",
      "145/281, train_loss: 0.0550, step time: 0.2500\n",
      "146/281, train_loss: 0.0592, step time: 0.2487\n",
      "147/281, train_loss: 0.0865, step time: 0.2507\n",
      "148/281, train_loss: 0.0615, step time: 0.2484\n",
      "149/281, train_loss: 0.0755, step time: 0.2461\n",
      "150/281, train_loss: 0.0670, step time: 0.2424\n",
      "151/281, train_loss: 0.0923, step time: 0.2417\n",
      "152/281, train_loss: 0.0872, step time: 0.2509\n",
      "153/281, train_loss: 0.0642, step time: 0.2501\n",
      "154/281, train_loss: 0.1416, step time: 0.2534\n",
      "155/281, train_loss: 0.0640, step time: 0.2545\n",
      "156/281, train_loss: 0.0756, step time: 0.2443\n",
      "157/281, train_loss: 0.0370, step time: 0.2485\n",
      "158/281, train_loss: 0.0491, step time: 0.2475\n",
      "159/281, train_loss: 0.0432, step time: 0.2427\n",
      "160/281, train_loss: 0.0764, step time: 0.2456\n",
      "161/281, train_loss: 0.1291, step time: 0.2434\n",
      "162/281, train_loss: 0.0680, step time: 0.2447\n",
      "163/281, train_loss: 0.2404, step time: 0.2414\n",
      "164/281, train_loss: 0.0632, step time: 0.2512\n",
      "165/281, train_loss: 0.1027, step time: 0.2506\n",
      "166/281, train_loss: 0.0737, step time: 0.2593\n",
      "167/281, train_loss: 0.0803, step time: 0.2637\n",
      "168/281, train_loss: 0.0765, step time: 0.2545\n",
      "169/281, train_loss: 0.1076, step time: 0.2469\n",
      "170/281, train_loss: 0.0436, step time: 0.2437\n",
      "171/281, train_loss: 0.0941, step time: 0.2431\n",
      "172/281, train_loss: 0.0747, step time: 0.2497\n",
      "173/281, train_loss: 0.0666, step time: 0.2525\n",
      "174/281, train_loss: 0.1181, step time: 0.2516\n",
      "175/281, train_loss: 0.0921, step time: 0.2483\n",
      "176/281, train_loss: 0.0516, step time: 0.2502\n",
      "177/281, train_loss: 0.0847, step time: 0.2503\n",
      "178/281, train_loss: 0.0605, step time: 0.2493\n",
      "179/281, train_loss: 0.0354, step time: 0.2497\n",
      "180/281, train_loss: 0.2614, step time: 0.2466\n",
      "181/281, train_loss: 0.0511, step time: 0.2471\n",
      "182/281, train_loss: 0.0914, step time: 0.2477\n",
      "183/281, train_loss: 0.0843, step time: 0.2467\n",
      "184/281, train_loss: 0.0915, step time: 0.2546\n",
      "185/281, train_loss: 0.0655, step time: 0.2496\n",
      "186/281, train_loss: 0.2247, step time: 0.2484\n",
      "187/281, train_loss: 0.0586, step time: 0.2441\n",
      "188/281, train_loss: 0.0933, step time: 0.2569\n",
      "189/281, train_loss: 0.0880, step time: 0.2458\n",
      "190/281, train_loss: 0.0897, step time: 0.2490\n",
      "191/281, train_loss: 0.0928, step time: 0.2583\n",
      "192/281, train_loss: 0.0654, step time: 0.2537\n",
      "193/281, train_loss: 0.2399, step time: 0.2474\n",
      "194/281, train_loss: 0.0664, step time: 0.2504\n",
      "195/281, train_loss: 0.0843, step time: 0.2516\n",
      "196/281, train_loss: 0.0801, step time: 0.2564\n",
      "197/281, train_loss: 0.0998, step time: 0.2481\n",
      "198/281, train_loss: 0.0820, step time: 0.2483\n",
      "199/281, train_loss: 0.0468, step time: 0.2501\n",
      "200/281, train_loss: 0.0683, step time: 0.2469\n",
      "201/281, train_loss: 0.0697, step time: 0.2487\n",
      "202/281, train_loss: 0.0590, step time: 0.2484\n",
      "203/281, train_loss: 0.1100, step time: 0.2463\n",
      "204/281, train_loss: 0.0666, step time: 0.2473\n",
      "205/281, train_loss: 0.0682, step time: 0.2540\n",
      "206/281, train_loss: 0.0889, step time: 0.2524\n",
      "207/281, train_loss: 0.2413, step time: 0.2533\n",
      "208/281, train_loss: 0.0608, step time: 0.2443\n",
      "209/281, train_loss: 0.0556, step time: 0.2449\n",
      "210/281, train_loss: 0.2360, step time: 0.2451\n",
      "211/281, train_loss: 0.2183, step time: 0.2516\n",
      "212/281, train_loss: 0.2188, step time: 0.2493\n",
      "213/281, train_loss: 0.0744, step time: 0.2490\n",
      "214/281, train_loss: 0.0643, step time: 0.2458\n",
      "215/281, train_loss: 0.0715, step time: 0.2469\n",
      "216/281, train_loss: 0.0632, step time: 0.2539\n",
      "217/281, train_loss: 0.0611, step time: 0.2532\n",
      "218/281, train_loss: 0.1085, step time: 0.2484\n",
      "219/281, train_loss: 0.0893, step time: 0.2491\n",
      "220/281, train_loss: 0.0550, step time: 0.2485\n",
      "221/281, train_loss: 0.1023, step time: 0.2489\n",
      "222/281, train_loss: 0.0787, step time: 0.2519\n",
      "223/281, train_loss: 0.0455, step time: 0.2470\n",
      "224/281, train_loss: 0.0683, step time: 0.2505\n",
      "225/281, train_loss: 0.1284, step time: 0.2491\n",
      "226/281, train_loss: 0.0934, step time: 0.2475\n",
      "227/281, train_loss: 0.0926, step time: 0.2465\n",
      "228/281, train_loss: 0.0713, step time: 0.2511\n",
      "229/281, train_loss: 0.2297, step time: 0.2446\n",
      "230/281, train_loss: 0.0689, step time: 0.2465\n",
      "231/281, train_loss: 0.1379, step time: 0.2457\n",
      "232/281, train_loss: 0.0923, step time: 0.2443\n",
      "233/281, train_loss: 0.1056, step time: 0.2443\n",
      "234/281, train_loss: 0.1175, step time: 0.2494\n",
      "235/281, train_loss: 0.1341, step time: 0.2513\n",
      "236/281, train_loss: 0.2240, step time: 0.2467\n",
      "237/281, train_loss: 0.2313, step time: 0.2454\n",
      "238/281, train_loss: 0.0828, step time: 0.2434\n",
      "239/281, train_loss: 0.1013, step time: 0.2442\n",
      "240/281, train_loss: 0.3802, step time: 0.2456\n",
      "241/281, train_loss: 0.0672, step time: 0.2487\n",
      "242/281, train_loss: 0.0894, step time: 0.2542\n",
      "243/281, train_loss: 0.0909, step time: 0.2547\n",
      "244/281, train_loss: 0.0765, step time: 0.2542\n",
      "245/281, train_loss: 0.0859, step time: 0.2456\n",
      "246/281, train_loss: 0.2555, step time: 0.2498\n",
      "247/281, train_loss: 0.0533, step time: 0.2483\n",
      "248/281, train_loss: 0.0684, step time: 0.2498\n",
      "249/281, train_loss: 0.0848, step time: 0.2476\n",
      "250/281, train_loss: 0.2310, step time: 0.2556\n",
      "251/281, train_loss: 0.1257, step time: 0.2470\n",
      "252/281, train_loss: 0.0730, step time: 0.2447\n",
      "253/281, train_loss: 0.0577, step time: 0.2436\n",
      "254/281, train_loss: 0.2243, step time: 0.2485\n",
      "255/281, train_loss: 0.1289, step time: 0.2479\n",
      "256/281, train_loss: 0.2465, step time: 0.2454\n",
      "257/281, train_loss: 0.0900, step time: 0.2452\n",
      "258/281, train_loss: 0.0635, step time: 0.2523\n",
      "259/281, train_loss: 0.0557, step time: 0.2514\n",
      "260/281, train_loss: 0.0841, step time: 0.2441\n",
      "261/281, train_loss: 0.0871, step time: 0.2445\n",
      "262/281, train_loss: 0.0631, step time: 0.2477\n",
      "263/281, train_loss: 0.0952, step time: 0.2467\n",
      "264/281, train_loss: 0.0498, step time: 0.2444\n",
      "265/281, train_loss: 0.2167, step time: 0.2488\n",
      "266/281, train_loss: 0.0918, step time: 0.2491\n",
      "267/281, train_loss: 0.0998, step time: 0.2467\n",
      "268/281, train_loss: 0.0891, step time: 0.2456\n",
      "269/281, train_loss: 0.2298, step time: 0.2521\n",
      "270/281, train_loss: 0.0813, step time: 0.2492\n",
      "271/281, train_loss: 0.0928, step time: 0.2436\n",
      "272/281, train_loss: 0.0688, step time: 0.2429\n",
      "273/281, train_loss: 0.2413, step time: 0.2446\n",
      "274/281, train_loss: 0.0722, step time: 0.2447\n",
      "275/281, train_loss: 0.0508, step time: 0.2401\n",
      "276/281, train_loss: 0.0802, step time: 0.2417\n",
      "277/281, train_loss: 0.0786, step time: 0.2445\n",
      "278/281, train_loss: 0.0834, step time: 0.2484\n",
      "279/281, train_loss: 0.0423, step time: 0.2444\n",
      "280/281, train_loss: 0.1439, step time: 0.2452\n",
      "281/281, train_loss: 0.0665, step time: 0.2462\n",
      "282/281, train_loss: 0.0690, step time: 0.1451\n",
      "epoch 160 average loss: 0.1031\n",
      "current epoch: 160 current mean dice: 0.8732 tc: 0.8710 wt: 0.8933 et: 0.8695\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 160 is: 376.2759\n",
      "----------\n",
      "epoch 161/200\n",
      "1/281, train_loss: 0.0775, step time: 0.2570\n",
      "2/281, train_loss: 0.2111, step time: 0.2461\n",
      "3/281, train_loss: 0.0782, step time: 0.2467\n",
      "4/281, train_loss: 0.1119, step time: 0.2480\n",
      "5/281, train_loss: 0.2319, step time: 0.2460\n",
      "6/281, train_loss: 0.0707, step time: 0.2435\n",
      "7/281, train_loss: 0.0471, step time: 0.2442\n",
      "8/281, train_loss: 0.0453, step time: 0.2491\n",
      "9/281, train_loss: 0.0602, step time: 0.2423\n",
      "10/281, train_loss: 0.0766, step time: 0.2463\n",
      "11/281, train_loss: 0.0851, step time: 0.2488\n",
      "12/281, train_loss: 0.1278, step time: 0.2420\n",
      "13/281, train_loss: 0.0809, step time: 0.2415\n",
      "14/281, train_loss: 0.1983, step time: 0.2551\n",
      "15/281, train_loss: 0.2190, step time: 0.2528\n",
      "16/281, train_loss: 0.0912, step time: 0.2533\n",
      "17/281, train_loss: 0.1186, step time: 0.2404\n",
      "18/281, train_loss: 0.1368, step time: 0.2438\n",
      "19/281, train_loss: 0.2239, step time: 0.2602\n",
      "20/281, train_loss: 0.0891, step time: 0.2558\n",
      "21/281, train_loss: 0.0937, step time: 0.2586\n",
      "22/281, train_loss: 0.0734, step time: 0.2469\n",
      "23/281, train_loss: 0.0627, step time: 0.2468\n",
      "24/281, train_loss: 0.0889, step time: 0.2528\n",
      "25/281, train_loss: 0.0730, step time: 0.2560\n",
      "26/281, train_loss: 0.0729, step time: 0.2576\n",
      "27/281, train_loss: 0.2501, step time: 0.2518\n",
      "28/281, train_loss: 0.0728, step time: 0.2487\n",
      "29/281, train_loss: 0.0706, step time: 0.2518\n",
      "30/281, train_loss: 0.0813, step time: 0.2503\n",
      "31/281, train_loss: 0.0855, step time: 0.2425\n",
      "32/281, train_loss: 0.2459, step time: 0.2524\n",
      "33/281, train_loss: 0.0634, step time: 0.2491\n",
      "34/281, train_loss: 0.0674, step time: 0.2488\n",
      "35/281, train_loss: 0.2465, step time: 0.2507\n",
      "36/281, train_loss: 0.0605, step time: 0.2486\n",
      "37/281, train_loss: 0.0898, step time: 0.2475\n",
      "38/281, train_loss: 0.0619, step time: 0.2552\n",
      "39/281, train_loss: 0.2235, step time: 0.2540\n",
      "40/281, train_loss: 0.0808, step time: 0.2501\n",
      "41/281, train_loss: 0.1005, step time: 0.2475\n",
      "42/281, train_loss: 0.0864, step time: 0.2519\n",
      "43/281, train_loss: 0.2114, step time: 0.2513\n",
      "44/281, train_loss: 0.0906, step time: 0.2469\n",
      "45/281, train_loss: 0.0664, step time: 0.2507\n",
      "46/281, train_loss: 0.0785, step time: 0.2544\n",
      "47/281, train_loss: 0.0668, step time: 0.2492\n",
      "48/281, train_loss: 0.0679, step time: 0.2516\n",
      "49/281, train_loss: 0.0686, step time: 0.2438\n",
      "50/281, train_loss: 0.0758, step time: 0.2426\n",
      "51/281, train_loss: 0.0731, step time: 0.2430\n",
      "52/281, train_loss: 0.0787, step time: 0.2425\n",
      "53/281, train_loss: 0.0554, step time: 0.2420\n",
      "54/281, train_loss: 0.0560, step time: 0.2403\n",
      "55/281, train_loss: 0.2698, step time: 0.2416\n",
      "56/281, train_loss: 0.0559, step time: 0.2573\n",
      "57/281, train_loss: 0.2241, step time: 0.2539\n",
      "58/281, train_loss: 0.0914, step time: 0.2543\n",
      "59/281, train_loss: 0.0889, step time: 0.2520\n",
      "60/281, train_loss: 0.2711, step time: 0.2425\n",
      "61/281, train_loss: 0.0900, step time: 0.2442\n",
      "62/281, train_loss: 0.2208, step time: 0.2469\n",
      "63/281, train_loss: 0.1035, step time: 0.2472\n",
      "64/281, train_loss: 0.0920, step time: 0.2445\n",
      "65/281, train_loss: 0.0782, step time: 0.2411\n",
      "66/281, train_loss: 0.0763, step time: 0.2425\n",
      "67/281, train_loss: 0.0761, step time: 0.2425\n",
      "68/281, train_loss: 0.1038, step time: 0.2476\n",
      "69/281, train_loss: 0.0587, step time: 0.2481\n",
      "70/281, train_loss: 0.0782, step time: 0.2468\n",
      "71/281, train_loss: 0.0844, step time: 0.2482\n",
      "72/281, train_loss: 0.0726, step time: 0.2461\n",
      "73/281, train_loss: 0.2662, step time: 0.2530\n",
      "74/281, train_loss: 0.0697, step time: 0.2524\n",
      "75/281, train_loss: 0.0871, step time: 0.2532\n",
      "76/281, train_loss: 0.0769, step time: 0.2516\n",
      "77/281, train_loss: 0.0559, step time: 0.2508\n",
      "78/281, train_loss: 0.0951, step time: 0.2440\n",
      "79/281, train_loss: 0.0734, step time: 0.2438\n",
      "80/281, train_loss: 0.0745, step time: 0.2511\n",
      "81/281, train_loss: 0.0421, step time: 0.2470\n",
      "82/281, train_loss: 0.0996, step time: 0.2480\n",
      "83/281, train_loss: 0.0713, step time: 0.2495\n",
      "84/281, train_loss: 0.0618, step time: 0.2469\n",
      "85/281, train_loss: 0.0508, step time: 0.2487\n",
      "86/281, train_loss: 0.0687, step time: 0.2491\n",
      "87/281, train_loss: 0.0858, step time: 0.2500\n",
      "88/281, train_loss: 0.1061, step time: 0.2470\n",
      "89/281, train_loss: 0.0789, step time: 0.2437\n",
      "90/281, train_loss: 0.0863, step time: 0.2438\n",
      "91/281, train_loss: 0.0678, step time: 0.2510\n",
      "92/281, train_loss: 0.2034, step time: 0.2530\n",
      "93/281, train_loss: 0.0644, step time: 0.2513\n",
      "94/281, train_loss: 0.0473, step time: 0.2490\n",
      "95/281, train_loss: 0.2471, step time: 0.2528\n",
      "96/281, train_loss: 0.0983, step time: 0.2538\n",
      "97/281, train_loss: 0.1147, step time: 0.2494\n",
      "98/281, train_loss: 0.0806, step time: 0.2520\n",
      "99/281, train_loss: 0.1172, step time: 0.2473\n",
      "100/281, train_loss: 0.0944, step time: 0.2455\n",
      "101/281, train_loss: 0.0794, step time: 0.2481\n",
      "102/281, train_loss: 0.0406, step time: 0.2511\n",
      "103/281, train_loss: 0.0580, step time: 0.2505\n",
      "104/281, train_loss: 0.0842, step time: 0.2465\n",
      "105/281, train_loss: 0.0664, step time: 0.2451\n",
      "106/281, train_loss: 0.2623, step time: 0.2480\n",
      "107/281, train_loss: 0.2705, step time: 0.2496\n",
      "108/281, train_loss: 0.0649, step time: 0.2460\n",
      "109/281, train_loss: 0.0866, step time: 0.2462\n",
      "110/281, train_loss: 0.1035, step time: 0.2520\n",
      "111/281, train_loss: 0.0644, step time: 0.2485\n",
      "112/281, train_loss: 0.2441, step time: 0.2477\n",
      "113/281, train_loss: 0.0576, step time: 0.2538\n",
      "114/281, train_loss: 0.2245, step time: 0.2571\n",
      "115/281, train_loss: 0.2064, step time: 0.2558\n",
      "116/281, train_loss: 0.0581, step time: 0.2623\n",
      "117/281, train_loss: 0.2338, step time: 0.2454\n",
      "118/281, train_loss: 0.0563, step time: 0.2464\n",
      "119/281, train_loss: 0.0669, step time: 0.2460\n",
      "120/281, train_loss: 0.0780, step time: 0.2508\n",
      "121/281, train_loss: 0.0760, step time: 0.2448\n",
      "122/281, train_loss: 0.2309, step time: 0.2499\n",
      "123/281, train_loss: 0.0756, step time: 0.2489\n",
      "124/281, train_loss: 0.0736, step time: 0.2471\n",
      "125/281, train_loss: 0.1099, step time: 0.2534\n",
      "126/281, train_loss: 0.1072, step time: 0.2485\n",
      "127/281, train_loss: 0.0502, step time: 0.2505\n",
      "128/281, train_loss: 0.0780, step time: 0.2513\n",
      "129/281, train_loss: 0.1147, step time: 0.2429\n",
      "130/281, train_loss: 0.2471, step time: 0.2488\n",
      "131/281, train_loss: 0.0648, step time: 0.2466\n",
      "132/281, train_loss: 0.0697, step time: 0.2480\n",
      "133/281, train_loss: 0.2090, step time: 0.2453\n",
      "134/281, train_loss: 0.0744, step time: 0.2455\n",
      "135/281, train_loss: 0.1106, step time: 0.2440\n",
      "136/281, train_loss: 0.0467, step time: 0.2426\n",
      "137/281, train_loss: 0.2352, step time: 0.2437\n",
      "138/281, train_loss: 0.0652, step time: 0.2517\n",
      "139/281, train_loss: 0.0961, step time: 0.2511\n",
      "140/281, train_loss: 0.0815, step time: 0.2479\n",
      "141/281, train_loss: 0.2328, step time: 0.2431\n",
      "142/281, train_loss: 0.0696, step time: 0.2526\n",
      "143/281, train_loss: 0.0641, step time: 0.2469\n",
      "144/281, train_loss: 0.0702, step time: 0.2421\n",
      "145/281, train_loss: 0.1092, step time: 0.2458\n",
      "146/281, train_loss: 0.2312, step time: 0.2477\n",
      "147/281, train_loss: 0.0959, step time: 0.2503\n",
      "148/281, train_loss: 0.0726, step time: 0.2460\n",
      "149/281, train_loss: 0.1143, step time: 0.2420\n",
      "150/281, train_loss: 0.2413, step time: 0.2543\n",
      "151/281, train_loss: 0.1024, step time: 0.2846\n",
      "152/281, train_loss: 0.1118, step time: 0.2463\n",
      "153/281, train_loss: 0.1044, step time: 0.2453\n",
      "154/281, train_loss: 0.0463, step time: 0.2479\n",
      "155/281, train_loss: 0.2404, step time: 0.2610\n",
      "156/281, train_loss: 0.0629, step time: 0.2843\n",
      "157/281, train_loss: 0.2214, step time: 0.2449\n",
      "158/281, train_loss: 0.0846, step time: 0.2530\n",
      "159/281, train_loss: 0.2092, step time: 0.2664\n",
      "160/281, train_loss: 0.0700, step time: 0.2538\n",
      "161/281, train_loss: 0.0634, step time: 0.2485\n",
      "162/281, train_loss: 0.0644, step time: 0.2492\n",
      "163/281, train_loss: 0.1202, step time: 0.2463\n",
      "164/281, train_loss: 0.0732, step time: 0.2440\n",
      "165/281, train_loss: 0.0948, step time: 0.2425\n",
      "166/281, train_loss: 0.0589, step time: 0.2502\n",
      "167/281, train_loss: 0.0573, step time: 0.2499\n",
      "168/281, train_loss: 0.0536, step time: 0.2447\n",
      "169/281, train_loss: 0.0694, step time: 0.2434\n",
      "170/281, train_loss: 0.1051, step time: 0.2489\n",
      "171/281, train_loss: 0.0939, step time: 0.2502\n",
      "172/281, train_loss: 0.2295, step time: 0.2519\n",
      "173/281, train_loss: 0.0691, step time: 0.2444\n",
      "174/281, train_loss: 0.1085, step time: 0.2526\n",
      "175/281, train_loss: 0.0648, step time: 0.2493\n",
      "176/281, train_loss: 0.0815, step time: 0.2493\n",
      "177/281, train_loss: 0.0760, step time: 0.2486\n",
      "178/281, train_loss: 0.2852, step time: 0.2479\n",
      "179/281, train_loss: 0.0879, step time: 0.2498\n",
      "180/281, train_loss: 0.0733, step time: 0.2460\n",
      "181/281, train_loss: 0.1108, step time: 0.2442\n",
      "182/281, train_loss: 0.1046, step time: 0.2460\n",
      "183/281, train_loss: 0.0894, step time: 0.2427\n",
      "184/281, train_loss: 0.0914, step time: 0.2504\n",
      "185/281, train_loss: 0.0502, step time: 0.2492\n",
      "186/281, train_loss: 0.1032, step time: 0.2484\n",
      "187/281, train_loss: 0.0634, step time: 0.2437\n",
      "188/281, train_loss: 0.0639, step time: 0.2448\n",
      "189/281, train_loss: 0.0615, step time: 0.2489\n",
      "190/281, train_loss: 0.1173, step time: 0.2448\n",
      "191/281, train_loss: 0.0711, step time: 0.2454\n",
      "192/281, train_loss: 0.1679, step time: 0.2498\n",
      "193/281, train_loss: 0.0782, step time: 0.2463\n",
      "194/281, train_loss: 0.0954, step time: 0.2492\n",
      "195/281, train_loss: 0.0852, step time: 0.2404\n",
      "196/281, train_loss: 0.0689, step time: 0.2449\n",
      "197/281, train_loss: 0.0715, step time: 0.2385\n",
      "198/281, train_loss: 0.0918, step time: 0.2465\n",
      "199/281, train_loss: 0.0670, step time: 0.2460\n",
      "200/281, train_loss: 0.0587, step time: 0.2469\n",
      "201/281, train_loss: 0.1598, step time: 0.2449\n",
      "202/281, train_loss: 0.0977, step time: 0.2485\n",
      "203/281, train_loss: 0.0986, step time: 0.2478\n",
      "204/281, train_loss: 0.0688, step time: 0.2448\n",
      "205/281, train_loss: 0.0817, step time: 0.2475\n",
      "206/281, train_loss: 0.0845, step time: 0.2469\n",
      "207/281, train_loss: 0.0952, step time: 0.2430\n",
      "208/281, train_loss: 0.0723, step time: 0.2413\n",
      "209/281, train_loss: 0.0703, step time: 0.2449\n",
      "210/281, train_loss: 0.0716, step time: 0.2454\n",
      "211/281, train_loss: 0.0583, step time: 0.2425\n",
      "212/281, train_loss: 0.0488, step time: 0.2496\n",
      "213/281, train_loss: 0.0758, step time: 0.2493\n",
      "214/281, train_loss: 0.1081, step time: 0.2459\n",
      "215/281, train_loss: 0.2372, step time: 0.2495\n",
      "216/281, train_loss: 0.0815, step time: 0.2474\n",
      "217/281, train_loss: 0.2565, step time: 0.2474\n",
      "218/281, train_loss: 0.0878, step time: 0.2462\n",
      "219/281, train_loss: 0.0901, step time: 0.2478\n",
      "220/281, train_loss: 0.0726, step time: 0.2493\n",
      "221/281, train_loss: 0.0572, step time: 0.2453\n",
      "222/281, train_loss: 0.1032, step time: 0.2511\n",
      "223/281, train_loss: 0.0834, step time: 0.2516\n",
      "224/281, train_loss: 0.0748, step time: 0.2412\n",
      "225/281, train_loss: 0.0506, step time: 0.2458\n",
      "226/281, train_loss: 0.0685, step time: 0.2427\n",
      "227/281, train_loss: 0.0732, step time: 0.2495\n",
      "228/281, train_loss: 0.1253, step time: 0.2496\n",
      "229/281, train_loss: 0.1257, step time: 0.2515\n",
      "230/281, train_loss: 0.2145, step time: 0.2473\n",
      "231/281, train_loss: 0.0688, step time: 0.2495\n",
      "232/281, train_loss: 0.0630, step time: 0.2475\n",
      "233/281, train_loss: 0.0720, step time: 0.2490\n",
      "234/281, train_loss: 0.1082, step time: 0.2553\n",
      "235/281, train_loss: 0.0642, step time: 0.2512\n",
      "236/281, train_loss: 0.0781, step time: 0.2492\n",
      "237/281, train_loss: 0.0602, step time: 0.2518\n",
      "238/281, train_loss: 0.0598, step time: 0.2538\n",
      "239/281, train_loss: 0.0625, step time: 0.2481\n",
      "240/281, train_loss: 0.1052, step time: 0.2521\n",
      "241/281, train_loss: 0.0764, step time: 0.2533\n",
      "242/281, train_loss: 0.0777, step time: 0.2523\n",
      "243/281, train_loss: 0.0855, step time: 0.2525\n",
      "244/281, train_loss: 0.0963, step time: 0.2567\n",
      "245/281, train_loss: 0.0992, step time: 0.2534\n",
      "246/281, train_loss: 0.0512, step time: 0.2526\n",
      "247/281, train_loss: 0.0666, step time: 0.2485\n",
      "248/281, train_loss: 0.0708, step time: 0.2541\n",
      "249/281, train_loss: 0.0652, step time: 0.2524\n",
      "250/281, train_loss: 0.0747, step time: 0.2516\n",
      "251/281, train_loss: 0.0766, step time: 0.2468\n",
      "252/281, train_loss: 0.0678, step time: 0.2543\n",
      "253/281, train_loss: 0.0616, step time: 0.2541\n",
      "254/281, train_loss: 0.0767, step time: 0.2479\n",
      "255/281, train_loss: 0.1259, step time: 0.2582\n",
      "256/281, train_loss: 0.0909, step time: 0.2517\n",
      "257/281, train_loss: 0.2214, step time: 0.2472\n",
      "258/281, train_loss: 0.0790, step time: 0.2475\n",
      "259/281, train_loss: 0.0927, step time: 0.2482\n",
      "260/281, train_loss: 0.0548, step time: 0.2489\n",
      "261/281, train_loss: 0.0712, step time: 0.2481\n",
      "262/281, train_loss: 0.0598, step time: 0.2476\n",
      "263/281, train_loss: 0.0655, step time: 0.2578\n",
      "264/281, train_loss: 0.0662, step time: 0.2708\n",
      "265/281, train_loss: 0.0525, step time: 0.2521\n",
      "266/281, train_loss: 0.1059, step time: 0.2487\n",
      "267/281, train_loss: 0.0632, step time: 0.2555\n",
      "268/281, train_loss: 0.0616, step time: 0.2511\n",
      "269/281, train_loss: 0.0893, step time: 0.2559\n",
      "270/281, train_loss: 0.0682, step time: 0.2520\n",
      "271/281, train_loss: 0.0717, step time: 0.2542\n",
      "272/281, train_loss: 0.2326, step time: 0.2524\n",
      "273/281, train_loss: 0.2411, step time: 0.2548\n",
      "274/281, train_loss: 0.0839, step time: 0.2503\n",
      "275/281, train_loss: 0.0690, step time: 0.2528\n",
      "276/281, train_loss: 0.0833, step time: 0.2521\n",
      "277/281, train_loss: 0.0576, step time: 0.2494\n",
      "278/281, train_loss: 0.0728, step time: 0.2528\n",
      "279/281, train_loss: 0.0492, step time: 0.2543\n",
      "280/281, train_loss: 0.0591, step time: 0.2521\n",
      "281/281, train_loss: 0.2202, step time: 0.2495\n",
      "282/281, train_loss: 0.1428, step time: 0.1525\n",
      "epoch 161 average loss: 0.1025\n",
      "current epoch: 161 current mean dice: 0.8789 tc: 0.8722 wt: 0.9045 et: 0.8743\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 161 is: 403.0426\n",
      "----------\n",
      "epoch 162/200\n",
      "1/281, train_loss: 0.0900, step time: 0.2556\n",
      "2/281, train_loss: 0.2290, step time: 0.2478\n",
      "3/281, train_loss: 0.0566, step time: 0.2437\n",
      "4/281, train_loss: 0.0551, step time: 0.2489\n",
      "5/281, train_loss: 0.0609, step time: 0.2482\n",
      "6/281, train_loss: 0.0691, step time: 0.2521\n",
      "7/281, train_loss: 0.0818, step time: 0.2473\n",
      "8/281, train_loss: 0.0476, step time: 0.2534\n",
      "9/281, train_loss: 0.0762, step time: 0.2531\n",
      "10/281, train_loss: 0.1624, step time: 0.2533\n",
      "11/281, train_loss: 0.0799, step time: 0.2482\n",
      "12/281, train_loss: 0.0690, step time: 0.2449\n",
      "13/281, train_loss: 0.0585, step time: 0.2463\n",
      "14/281, train_loss: 0.0788, step time: 0.2479\n",
      "15/281, train_loss: 0.1144, step time: 0.2474\n",
      "16/281, train_loss: 0.0811, step time: 0.2467\n",
      "17/281, train_loss: 0.0895, step time: 0.2434\n",
      "18/281, train_loss: 0.0745, step time: 0.2494\n",
      "19/281, train_loss: 0.0712, step time: 0.2438\n",
      "20/281, train_loss: 0.0674, step time: 0.2434\n",
      "21/281, train_loss: 0.0473, step time: 0.2497\n",
      "22/281, train_loss: 0.0699, step time: 0.2468\n",
      "23/281, train_loss: 0.0802, step time: 0.2568\n",
      "24/281, train_loss: 0.0590, step time: 0.2471\n",
      "25/281, train_loss: 0.0765, step time: 0.2451\n",
      "26/281, train_loss: 0.0614, step time: 0.2506\n",
      "27/281, train_loss: 0.0504, step time: 0.2488\n",
      "28/281, train_loss: 0.0737, step time: 0.2444\n",
      "29/281, train_loss: 0.0792, step time: 0.2480\n",
      "30/281, train_loss: 0.2070, step time: 0.2509\n",
      "31/281, train_loss: 0.0594, step time: 0.2535\n",
      "32/281, train_loss: 0.0726, step time: 0.2482\n",
      "33/281, train_loss: 0.0527, step time: 0.2456\n",
      "34/281, train_loss: 0.0730, step time: 0.2474\n",
      "35/281, train_loss: 0.0769, step time: 0.2509\n",
      "36/281, train_loss: 0.0950, step time: 0.2431\n",
      "37/281, train_loss: 0.0592, step time: 0.2443\n",
      "38/281, train_loss: 0.0792, step time: 0.2484\n",
      "39/281, train_loss: 0.0516, step time: 0.2458\n",
      "40/281, train_loss: 0.0934, step time: 0.2516\n",
      "41/281, train_loss: 0.2558, step time: 0.2488\n",
      "42/281, train_loss: 0.0791, step time: 0.2428\n",
      "43/281, train_loss: 0.0833, step time: 0.2436\n",
      "44/281, train_loss: 0.0624, step time: 0.2440\n",
      "45/281, train_loss: 0.0788, step time: 0.2462\n",
      "46/281, train_loss: 0.0681, step time: 0.2497\n",
      "47/281, train_loss: 0.0665, step time: 0.2454\n",
      "48/281, train_loss: 0.0885, step time: 0.2508\n",
      "49/281, train_loss: 0.0465, step time: 0.2462\n",
      "50/281, train_loss: 0.2577, step time: 0.2463\n",
      "51/281, train_loss: 0.0786, step time: 0.2494\n",
      "52/281, train_loss: 0.0771, step time: 0.2498\n",
      "53/281, train_loss: 0.0613, step time: 0.2439\n",
      "54/281, train_loss: 0.0581, step time: 0.2507\n",
      "55/281, train_loss: 0.2537, step time: 0.2493\n",
      "56/281, train_loss: 0.0738, step time: 0.2222\n",
      "57/281, train_loss: 0.0945, step time: 0.2501\n",
      "58/281, train_loss: 0.0507, step time: 0.2478\n",
      "59/281, train_loss: 0.2313, step time: 0.2459\n",
      "60/281, train_loss: 0.0467, step time: 0.2492\n",
      "61/281, train_loss: 0.2679, step time: 0.2497\n",
      "62/281, train_loss: 0.0520, step time: 0.2489\n",
      "63/281, train_loss: 0.0413, step time: 0.2484\n",
      "64/281, train_loss: 0.0698, step time: 0.2460\n",
      "65/281, train_loss: 0.0567, step time: 0.2490\n",
      "66/281, train_loss: 0.0636, step time: 0.2513\n",
      "67/281, train_loss: 0.0765, step time: 0.2532\n",
      "68/281, train_loss: 0.0770, step time: 0.2533\n",
      "69/281, train_loss: 0.0572, step time: 0.2500\n",
      "70/281, train_loss: 0.0820, step time: 0.2490\n",
      "71/281, train_loss: 0.0705, step time: 0.2496\n",
      "72/281, train_loss: 0.0495, step time: 0.2435\n",
      "73/281, train_loss: 0.0755, step time: 0.2469\n",
      "74/281, train_loss: 0.0844, step time: 0.2458\n",
      "75/281, train_loss: 0.1144, step time: 0.2501\n",
      "76/281, train_loss: 0.0669, step time: 0.2509\n",
      "77/281, train_loss: 0.2228, step time: 0.2499\n",
      "78/281, train_loss: 0.0632, step time: 0.2465\n",
      "79/281, train_loss: 0.0937, step time: 0.2534\n",
      "80/281, train_loss: 0.0625, step time: 0.2479\n",
      "81/281, train_loss: 0.0846, step time: 0.2488\n",
      "82/281, train_loss: 0.0655, step time: 0.2607\n",
      "83/281, train_loss: 0.0552, step time: 0.2517\n",
      "84/281, train_loss: 0.0421, step time: 0.2527\n",
      "85/281, train_loss: 0.0689, step time: 0.2530\n",
      "86/281, train_loss: 0.0774, step time: 0.2478\n",
      "87/281, train_loss: 0.2384, step time: 0.2566\n",
      "88/281, train_loss: 0.0638, step time: 0.2554\n",
      "89/281, train_loss: 0.2218, step time: 0.2547\n",
      "90/281, train_loss: 0.0754, step time: 0.2550\n",
      "91/281, train_loss: 0.0734, step time: 0.2504\n",
      "92/281, train_loss: 0.0577, step time: 0.2556\n",
      "93/281, train_loss: 0.0449, step time: 0.2464\n",
      "94/281, train_loss: 0.0926, step time: 0.2458\n",
      "95/281, train_loss: 0.0751, step time: 0.2461\n",
      "96/281, train_loss: 0.0495, step time: 0.2469\n",
      "97/281, train_loss: 0.0683, step time: 0.2424\n",
      "98/281, train_loss: 0.0643, step time: 0.2438\n",
      "99/281, train_loss: 0.0722, step time: 0.2519\n",
      "100/281, train_loss: 0.0914, step time: 0.2510\n",
      "101/281, train_loss: 0.0914, step time: 0.2465\n",
      "102/281, train_loss: 0.0546, step time: 0.2499\n",
      "103/281, train_loss: 0.0614, step time: 0.2486\n",
      "104/281, train_loss: 0.0605, step time: 0.2500\n",
      "105/281, train_loss: 0.2324, step time: 0.2545\n",
      "106/281, train_loss: 0.1014, step time: 0.2490\n",
      "107/281, train_loss: 0.0937, step time: 0.2502\n",
      "108/281, train_loss: 0.0820, step time: 0.2509\n",
      "109/281, train_loss: 0.0541, step time: 0.2598\n",
      "110/281, train_loss: 0.0714, step time: 0.2504\n",
      "111/281, train_loss: 0.2508, step time: 0.2463\n",
      "112/281, train_loss: 0.2336, step time: 0.2485\n",
      "113/281, train_loss: 0.2290, step time: 0.2477\n",
      "114/281, train_loss: 0.0778, step time: 0.2507\n",
      "115/281, train_loss: 0.0912, step time: 0.2520\n",
      "116/281, train_loss: 0.0909, step time: 0.2494\n",
      "117/281, train_loss: 0.0829, step time: 0.2498\n",
      "118/281, train_loss: 0.0562, step time: 0.2491\n",
      "119/281, train_loss: 0.1046, step time: 0.2422\n",
      "120/281, train_loss: 0.0590, step time: 0.2422\n",
      "121/281, train_loss: 0.0749, step time: 0.2449\n",
      "122/281, train_loss: 0.2053, step time: 0.2495\n",
      "123/281, train_loss: 0.0757, step time: 0.2463\n",
      "124/281, train_loss: 0.3837, step time: 0.2472\n",
      "125/281, train_loss: 0.0609, step time: 0.2444\n",
      "126/281, train_loss: 0.0895, step time: 0.2456\n",
      "127/281, train_loss: 0.0994, step time: 0.2498\n",
      "128/281, train_loss: 0.0819, step time: 0.2511\n",
      "129/281, train_loss: 0.0602, step time: 0.2487\n",
      "130/281, train_loss: 0.0631, step time: 0.2432\n",
      "131/281, train_loss: 0.0724, step time: 0.2549\n",
      "132/281, train_loss: 0.0667, step time: 0.2487\n",
      "133/281, train_loss: 0.0776, step time: 0.2458\n",
      "134/281, train_loss: 0.0513, step time: 0.2533\n",
      "135/281, train_loss: 0.0676, step time: 0.2762\n",
      "136/281, train_loss: 0.0564, step time: 0.2548\n",
      "137/281, train_loss: 0.0569, step time: 0.2533\n",
      "138/281, train_loss: 0.0471, step time: 0.2497\n",
      "139/281, train_loss: 0.1097, step time: 0.2555\n",
      "140/281, train_loss: 0.0634, step time: 0.2513\n",
      "141/281, train_loss: 0.2441, step time: 0.2467\n",
      "142/281, train_loss: 0.0587, step time: 0.2511\n",
      "143/281, train_loss: 0.2464, step time: 0.2499\n",
      "144/281, train_loss: 0.2152, step time: 0.2455\n",
      "145/281, train_loss: 0.0507, step time: 0.2488\n",
      "146/281, train_loss: 0.0763, step time: 0.2527\n",
      "147/281, train_loss: 0.1124, step time: 0.2489\n",
      "148/281, train_loss: 0.0884, step time: 0.2509\n",
      "149/281, train_loss: 0.0493, step time: 0.2482\n",
      "150/281, train_loss: 0.0915, step time: 0.2529\n",
      "151/281, train_loss: 0.0901, step time: 0.2506\n",
      "152/281, train_loss: 0.1062, step time: 0.2601\n",
      "153/281, train_loss: 0.1019, step time: 0.2553\n",
      "154/281, train_loss: 0.0699, step time: 0.2470\n",
      "155/281, train_loss: 0.2334, step time: 0.2513\n",
      "156/281, train_loss: 0.0673, step time: 0.2515\n",
      "157/281, train_loss: 0.0719, step time: 0.2489\n",
      "158/281, train_loss: 0.2155, step time: 0.2504\n",
      "159/281, train_loss: 0.0526, step time: 0.2485\n",
      "160/281, train_loss: 0.0671, step time: 0.2515\n",
      "161/281, train_loss: 0.2247, step time: 0.2485\n",
      "162/281, train_loss: 0.0585, step time: 0.2466\n",
      "163/281, train_loss: 0.0857, step time: 0.2519\n",
      "164/281, train_loss: 0.0622, step time: 0.2500\n",
      "165/281, train_loss: 0.0651, step time: 0.2473\n",
      "166/281, train_loss: 0.0418, step time: 0.2482\n",
      "167/281, train_loss: 0.0886, step time: 0.2523\n",
      "168/281, train_loss: 0.0936, step time: 0.2560\n",
      "169/281, train_loss: 0.1012, step time: 0.2527\n",
      "170/281, train_loss: 0.2414, step time: 0.2478\n",
      "171/281, train_loss: 0.2415, step time: 0.2425\n",
      "172/281, train_loss: 0.2351, step time: 0.2436\n",
      "173/281, train_loss: 0.1030, step time: 0.2436\n",
      "174/281, train_loss: 0.1008, step time: 0.2495\n",
      "175/281, train_loss: 0.0921, step time: 0.2506\n",
      "176/281, train_loss: 0.0586, step time: 0.2509\n",
      "177/281, train_loss: 0.0746, step time: 0.2459\n",
      "178/281, train_loss: 0.0871, step time: 0.2446\n",
      "179/281, train_loss: 0.2361, step time: 0.2501\n",
      "180/281, train_loss: 0.0557, step time: 0.2438\n",
      "181/281, train_loss: 0.0669, step time: 0.2487\n",
      "182/281, train_loss: 0.0862, step time: 0.2453\n",
      "183/281, train_loss: 0.0672, step time: 0.2550\n",
      "184/281, train_loss: 0.0569, step time: 0.2574\n",
      "185/281, train_loss: 0.0683, step time: 0.2415\n",
      "186/281, train_loss: 0.0877, step time: 0.2454\n",
      "187/281, train_loss: 0.0786, step time: 0.2440\n",
      "188/281, train_loss: 0.0766, step time: 0.2497\n",
      "189/281, train_loss: 0.0548, step time: 0.2491\n",
      "190/281, train_loss: 0.0811, step time: 0.2449\n",
      "191/281, train_loss: 0.0526, step time: 0.2443\n",
      "192/281, train_loss: 0.0673, step time: 0.2538\n",
      "193/281, train_loss: 0.0505, step time: 0.2481\n",
      "194/281, train_loss: 0.0817, step time: 0.2500\n",
      "195/281, train_loss: 0.0873, step time: 0.2424\n",
      "196/281, train_loss: 0.1087, step time: 0.2462\n",
      "197/281, train_loss: 0.2041, step time: 0.2524\n",
      "198/281, train_loss: 0.0533, step time: 0.2558\n",
      "199/281, train_loss: 0.0821, step time: 0.2515\n",
      "200/281, train_loss: 0.1058, step time: 0.2491\n",
      "201/281, train_loss: 0.0627, step time: 0.2452\n",
      "202/281, train_loss: 0.0649, step time: 0.2493\n",
      "203/281, train_loss: 0.1411, step time: 0.2478\n",
      "204/281, train_loss: 0.0730, step time: 0.2410\n",
      "205/281, train_loss: 0.0744, step time: 0.2413\n",
      "206/281, train_loss: 0.0670, step time: 0.2491\n",
      "207/281, train_loss: 0.0848, step time: 0.2474\n",
      "208/281, train_loss: 0.0772, step time: 0.2446\n",
      "209/281, train_loss: 0.0626, step time: 0.2530\n",
      "210/281, train_loss: 0.0453, step time: 0.2456\n",
      "211/281, train_loss: 0.0889, step time: 0.2499\n",
      "212/281, train_loss: 0.0606, step time: 0.2546\n",
      "213/281, train_loss: 0.1293, step time: 0.2551\n",
      "214/281, train_loss: 0.2165, step time: 0.2466\n",
      "215/281, train_loss: 0.0456, step time: 0.2448\n",
      "216/281, train_loss: 0.2301, step time: 0.2468\n",
      "217/281, train_loss: 0.0728, step time: 0.2416\n",
      "218/281, train_loss: 0.0600, step time: 0.2447\n",
      "219/281, train_loss: 0.0685, step time: 0.2481\n",
      "220/281, train_loss: 0.0962, step time: 0.2485\n",
      "221/281, train_loss: 0.0881, step time: 0.2510\n",
      "222/281, train_loss: 0.0494, step time: 0.2446\n",
      "223/281, train_loss: 0.1088, step time: 0.2442\n",
      "224/281, train_loss: 0.0899, step time: 0.2494\n",
      "225/281, train_loss: 0.0918, step time: 0.2483\n",
      "226/281, train_loss: 0.0721, step time: 0.2484\n",
      "227/281, train_loss: 0.0688, step time: 0.2466\n",
      "228/281, train_loss: 0.1135, step time: 0.2507\n",
      "229/281, train_loss: 0.1054, step time: 0.2538\n",
      "230/281, train_loss: 0.0939, step time: 0.2478\n",
      "231/281, train_loss: 0.0839, step time: 0.2494\n",
      "232/281, train_loss: 0.0844, step time: 0.2477\n",
      "233/281, train_loss: 0.1038, step time: 0.2457\n",
      "234/281, train_loss: 0.2166, step time: 0.2462\n",
      "235/281, train_loss: 0.0909, step time: 0.2443\n",
      "236/281, train_loss: 0.0681, step time: 0.2501\n",
      "237/281, train_loss: 0.0568, step time: 0.2469\n",
      "238/281, train_loss: 0.2248, step time: 0.2489\n",
      "239/281, train_loss: 0.0656, step time: 0.2522\n",
      "240/281, train_loss: 0.0983, step time: 0.2491\n",
      "241/281, train_loss: 0.2149, step time: 0.2416\n",
      "242/281, train_loss: 0.1169, step time: 0.2407\n",
      "243/281, train_loss: 0.1292, step time: 0.2430\n",
      "244/281, train_loss: 0.0645, step time: 0.2495\n",
      "245/281, train_loss: 0.0543, step time: 0.2465\n",
      "246/281, train_loss: 0.0676, step time: 0.2562\n",
      "247/281, train_loss: 0.0909, step time: 0.2516\n",
      "248/281, train_loss: 0.0713, step time: 0.2479\n",
      "249/281, train_loss: 0.0748, step time: 0.2460\n",
      "250/281, train_loss: 0.2506, step time: 0.2507\n",
      "251/281, train_loss: 0.0630, step time: 0.2540\n",
      "252/281, train_loss: 0.0842, step time: 0.2499\n",
      "253/281, train_loss: 0.2425, step time: 0.2463\n",
      "254/281, train_loss: 0.0396, step time: 0.2458\n",
      "255/281, train_loss: 0.2715, step time: 0.2502\n",
      "256/281, train_loss: 0.0817, step time: 0.2540\n",
      "257/281, train_loss: 0.0949, step time: 0.2550\n",
      "258/281, train_loss: 0.1210, step time: 0.2511\n",
      "259/281, train_loss: 0.2230, step time: 0.2486\n",
      "260/281, train_loss: 0.0823, step time: 0.2511\n",
      "261/281, train_loss: 0.1020, step time: 0.2474\n",
      "262/281, train_loss: 0.0616, step time: 0.2428\n",
      "263/281, train_loss: 0.2125, step time: 0.2434\n",
      "264/281, train_loss: 0.0665, step time: 0.2514\n",
      "265/281, train_loss: 0.0711, step time: 0.2611\n",
      "266/281, train_loss: 0.0814, step time: 0.2444\n",
      "267/281, train_loss: 0.0766, step time: 0.2424\n",
      "268/281, train_loss: 0.0892, step time: 0.2521\n",
      "269/281, train_loss: 0.0946, step time: 0.2462\n",
      "270/281, train_loss: 0.0662, step time: 0.2524\n",
      "271/281, train_loss: 0.2258, step time: 0.2481\n",
      "272/281, train_loss: 0.0685, step time: 0.2509\n",
      "273/281, train_loss: 0.3820, step time: 0.2466\n",
      "274/281, train_loss: 0.0637, step time: 0.2485\n",
      "275/281, train_loss: 0.1081, step time: 0.2456\n",
      "276/281, train_loss: 0.0841, step time: 0.2483\n",
      "277/281, train_loss: 0.0654, step time: 0.2542\n",
      "278/281, train_loss: 0.0733, step time: 0.2566\n",
      "279/281, train_loss: 0.0799, step time: 0.2485\n",
      "280/281, train_loss: 0.0815, step time: 0.2445\n",
      "281/281, train_loss: 0.0696, step time: 0.2458\n",
      "282/281, train_loss: 0.0409, step time: 0.1470\n",
      "epoch 162 average loss: 0.0978\n",
      "current epoch: 162 current mean dice: 0.8772 tc: 0.8698 wt: 0.9028 et: 0.8739\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 162 is: 407.4884\n",
      "----------\n",
      "epoch 163/200\n",
      "1/281, train_loss: 0.0638, step time: 0.2424\n",
      "2/281, train_loss: 0.0631, step time: 0.2426\n",
      "3/281, train_loss: 0.0814, step time: 0.2476\n",
      "4/281, train_loss: 0.0710, step time: 0.2488\n",
      "5/281, train_loss: 0.1013, step time: 0.2503\n",
      "6/281, train_loss: 0.0920, step time: 0.2452\n",
      "7/281, train_loss: 0.0901, step time: 0.2456\n",
      "8/281, train_loss: 0.0572, step time: 0.2413\n",
      "9/281, train_loss: 0.2041, step time: 0.2490\n",
      "10/281, train_loss: 0.0679, step time: 0.2503\n",
      "11/281, train_loss: 0.2470, step time: 0.2477\n",
      "12/281, train_loss: 0.0765, step time: 0.2479\n",
      "13/281, train_loss: 0.0526, step time: 0.2481\n",
      "14/281, train_loss: 0.0709, step time: 0.2459\n",
      "15/281, train_loss: 0.0923, step time: 0.2470\n",
      "16/281, train_loss: 0.0661, step time: 0.2410\n",
      "17/281, train_loss: 0.2550, step time: 0.2434\n",
      "18/281, train_loss: 0.0399, step time: 0.2517\n",
      "19/281, train_loss: 0.0670, step time: 0.2453\n",
      "20/281, train_loss: 0.0503, step time: 0.2418\n",
      "21/281, train_loss: 0.0633, step time: 0.2431\n",
      "22/281, train_loss: 0.0568, step time: 0.2527\n",
      "23/281, train_loss: 0.0656, step time: 0.2475\n",
      "24/281, train_loss: 0.0762, step time: 0.2459\n",
      "25/281, train_loss: 0.0732, step time: 0.2525\n",
      "26/281, train_loss: 0.0880, step time: 0.2429\n",
      "27/281, train_loss: 0.0725, step time: 0.2426\n",
      "28/281, train_loss: 0.2361, step time: 0.2465\n",
      "29/281, train_loss: 0.0623, step time: 0.2501\n",
      "30/281, train_loss: 0.0812, step time: 0.2532\n",
      "31/281, train_loss: 0.0699, step time: 0.2580\n",
      "32/281, train_loss: 0.0658, step time: 0.2492\n",
      "33/281, train_loss: 0.2487, step time: 0.2491\n",
      "34/281, train_loss: 0.0735, step time: 0.2472\n",
      "35/281, train_loss: 0.0807, step time: 0.2484\n",
      "36/281, train_loss: 0.0568, step time: 0.2696\n",
      "37/281, train_loss: 0.0594, step time: 0.2507\n",
      "38/281, train_loss: 0.0479, step time: 0.2444\n",
      "39/281, train_loss: 0.0583, step time: 0.2431\n",
      "40/281, train_loss: 0.0396, step time: 0.2515\n",
      "41/281, train_loss: 0.0913, step time: 0.2493\n",
      "42/281, train_loss: 0.0570, step time: 0.2611\n",
      "43/281, train_loss: 0.0861, step time: 0.2554\n",
      "44/281, train_loss: 0.2371, step time: 0.2474\n",
      "45/281, train_loss: 0.3930, step time: 0.2491\n",
      "46/281, train_loss: 0.0677, step time: 0.2523\n",
      "47/281, train_loss: 0.0721, step time: 0.2505\n",
      "48/281, train_loss: 0.0649, step time: 0.2457\n",
      "49/281, train_loss: 0.2254, step time: 0.2444\n",
      "50/281, train_loss: 0.0713, step time: 0.2500\n",
      "51/281, train_loss: 0.0737, step time: 0.2497\n",
      "52/281, train_loss: 0.0747, step time: 0.2423\n",
      "53/281, train_loss: 0.1058, step time: 0.2458\n",
      "54/281, train_loss: 0.1007, step time: 0.2713\n",
      "55/281, train_loss: 0.1035, step time: 0.2579\n",
      "56/281, train_loss: 0.0749, step time: 0.2556\n",
      "57/281, train_loss: 0.0718, step time: 0.2511\n",
      "58/281, train_loss: 0.0844, step time: 0.2476\n",
      "59/281, train_loss: 0.0918, step time: 0.2449\n",
      "60/281, train_loss: 0.0846, step time: 0.2467\n",
      "61/281, train_loss: 0.0978, step time: 0.2554\n",
      "62/281, train_loss: 0.2351, step time: 0.2403\n",
      "63/281, train_loss: 0.2344, step time: 0.2443\n",
      "64/281, train_loss: 0.2172, step time: 0.2501\n",
      "65/281, train_loss: 0.0704, step time: 0.2484\n",
      "66/281, train_loss: 0.0452, step time: 0.2480\n",
      "67/281, train_loss: 0.0701, step time: 0.2456\n",
      "68/281, train_loss: 0.0697, step time: 0.2488\n",
      "69/281, train_loss: 0.2343, step time: 0.2463\n",
      "70/281, train_loss: 0.0505, step time: 0.2427\n",
      "71/281, train_loss: 0.0569, step time: 0.2427\n",
      "72/281, train_loss: 0.1167, step time: 0.2522\n",
      "73/281, train_loss: 0.0605, step time: 0.2456\n",
      "74/281, train_loss: 0.0731, step time: 0.2518\n",
      "75/281, train_loss: 0.2422, step time: 0.2478\n",
      "76/281, train_loss: 0.0751, step time: 0.2488\n",
      "77/281, train_loss: 0.0813, step time: 0.2529\n",
      "78/281, train_loss: 0.0812, step time: 0.2795\n",
      "79/281, train_loss: 0.1176, step time: 0.2675\n",
      "80/281, train_loss: 0.0724, step time: 0.2442\n",
      "81/281, train_loss: 0.0752, step time: 0.2527\n",
      "82/281, train_loss: 0.0585, step time: 0.2469\n",
      "83/281, train_loss: 0.0800, step time: 0.2446\n",
      "84/281, train_loss: 0.0556, step time: 0.2479\n",
      "85/281, train_loss: 0.0652, step time: 0.2469\n",
      "86/281, train_loss: 0.0994, step time: 0.2516\n",
      "87/281, train_loss: 0.0668, step time: 0.2503\n",
      "88/281, train_loss: 0.1119, step time: 0.2530\n",
      "89/281, train_loss: 0.0748, step time: 0.2489\n",
      "90/281, train_loss: 0.0766, step time: 0.2447\n",
      "91/281, train_loss: 0.0871, step time: 0.2458\n",
      "92/281, train_loss: 0.0924, step time: 0.2417\n",
      "93/281, train_loss: 0.2296, step time: 0.2450\n",
      "94/281, train_loss: 0.0677, step time: 0.2461\n",
      "95/281, train_loss: 0.1058, step time: 0.2470\n",
      "96/281, train_loss: 0.2601, step time: 0.2446\n",
      "97/281, train_loss: 0.0895, step time: 0.2465\n",
      "98/281, train_loss: 0.0522, step time: 0.2427\n",
      "99/281, train_loss: 0.0631, step time: 0.2453\n",
      "100/281, train_loss: 0.0738, step time: 0.2456\n",
      "101/281, train_loss: 0.0741, step time: 0.2490\n",
      "102/281, train_loss: 0.0812, step time: 0.2517\n",
      "103/281, train_loss: 0.0584, step time: 0.2486\n",
      "104/281, train_loss: 0.0806, step time: 0.2462\n",
      "105/281, train_loss: 0.0525, step time: 0.2425\n",
      "106/281, train_loss: 0.0898, step time: 0.2511\n",
      "107/281, train_loss: 0.0841, step time: 0.2489\n",
      "108/281, train_loss: 0.0603, step time: 0.2442\n",
      "109/281, train_loss: 0.2337, step time: 0.2448\n",
      "110/281, train_loss: 0.0802, step time: 0.2553\n",
      "111/281, train_loss: 0.0611, step time: 0.2605\n",
      "112/281, train_loss: 0.1002, step time: 0.2459\n",
      "113/281, train_loss: 0.0889, step time: 0.2466\n",
      "114/281, train_loss: 0.0832, step time: 0.2453\n",
      "115/281, train_loss: 0.2388, step time: 0.2496\n",
      "116/281, train_loss: 0.0586, step time: 0.2515\n",
      "117/281, train_loss: 0.0561, step time: 0.2478\n",
      "118/281, train_loss: 0.0864, step time: 0.2422\n",
      "119/281, train_loss: 0.2558, step time: 0.2443\n",
      "120/281, train_loss: 0.0769, step time: 0.2450\n",
      "121/281, train_loss: 0.0513, step time: 0.2507\n",
      "122/281, train_loss: 0.0700, step time: 0.2504\n",
      "123/281, train_loss: 0.1001, step time: 0.2519\n",
      "124/281, train_loss: 0.0866, step time: 0.2509\n",
      "125/281, train_loss: 0.2119, step time: 0.2514\n",
      "126/281, train_loss: 0.0753, step time: 0.2524\n",
      "127/281, train_loss: 0.0867, step time: 0.2482\n",
      "128/281, train_loss: 0.0834, step time: 0.2436\n",
      "129/281, train_loss: 0.0648, step time: 0.2501\n",
      "130/281, train_loss: 0.0709, step time: 0.2441\n",
      "131/281, train_loss: 0.0604, step time: 0.2489\n",
      "132/281, train_loss: 0.2461, step time: 0.2516\n",
      "133/281, train_loss: 0.0496, step time: 0.2442\n",
      "134/281, train_loss: 0.2539, step time: 0.2416\n",
      "135/281, train_loss: 0.0651, step time: 0.2453\n",
      "136/281, train_loss: 0.0929, step time: 0.2418\n",
      "137/281, train_loss: 0.1049, step time: 0.2490\n",
      "138/281, train_loss: 0.0641, step time: 0.2520\n",
      "139/281, train_loss: 0.0956, step time: 0.2474\n",
      "140/281, train_loss: 0.1082, step time: 0.2500\n",
      "141/281, train_loss: 0.0630, step time: 0.2452\n",
      "142/281, train_loss: 0.2317, step time: 0.2442\n",
      "143/281, train_loss: 0.0525, step time: 0.2482\n",
      "144/281, train_loss: 0.0721, step time: 0.2457\n",
      "145/281, train_loss: 0.0639, step time: 0.2491\n",
      "146/281, train_loss: 0.0533, step time: 0.2458\n",
      "147/281, train_loss: 0.0577, step time: 0.2533\n",
      "148/281, train_loss: 0.0766, step time: 0.2464\n",
      "149/281, train_loss: 0.1475, step time: 0.2484\n",
      "150/281, train_loss: 0.1022, step time: 0.2446\n",
      "151/281, train_loss: 0.1012, step time: 0.2515\n",
      "152/281, train_loss: 0.2185, step time: 0.2456\n",
      "153/281, train_loss: 0.0762, step time: 0.2419\n",
      "154/281, train_loss: 0.0524, step time: 0.2406\n",
      "155/281, train_loss: 0.0630, step time: 0.2458\n",
      "156/281, train_loss: 0.0652, step time: 0.2435\n",
      "157/281, train_loss: 0.0644, step time: 0.2481\n",
      "158/281, train_loss: 0.1158, step time: 0.2470\n",
      "159/281, train_loss: 0.0744, step time: 0.2486\n",
      "160/281, train_loss: 0.0767, step time: 0.2524\n",
      "161/281, train_loss: 0.2072, step time: 0.2455\n",
      "162/281, train_loss: 0.2238, step time: 0.2486\n",
      "163/281, train_loss: 0.0624, step time: 0.2510\n",
      "164/281, train_loss: 0.1021, step time: 0.2438\n",
      "165/281, train_loss: 0.0501, step time: 0.2445\n",
      "166/281, train_loss: 0.2311, step time: 0.2525\n",
      "167/281, train_loss: 0.2175, step time: 0.2534\n",
      "168/281, train_loss: 0.1186, step time: 0.2533\n",
      "169/281, train_loss: 0.0771, step time: 0.2441\n",
      "170/281, train_loss: 0.0486, step time: 0.2443\n",
      "171/281, train_loss: 0.0563, step time: 0.2508\n",
      "172/281, train_loss: 0.2011, step time: 0.2435\n",
      "173/281, train_loss: 0.0610, step time: 0.2455\n",
      "174/281, train_loss: 0.1104, step time: 0.2438\n",
      "175/281, train_loss: 0.2072, step time: 0.2497\n",
      "176/281, train_loss: 0.2378, step time: 0.2501\n",
      "177/281, train_loss: 0.0887, step time: 0.2487\n",
      "178/281, train_loss: 0.0679, step time: 0.2493\n",
      "179/281, train_loss: 0.0747, step time: 0.2476\n",
      "180/281, train_loss: 0.0903, step time: 0.2457\n",
      "181/281, train_loss: 0.2219, step time: 0.2506\n",
      "182/281, train_loss: 0.0706, step time: 0.2435\n",
      "183/281, train_loss: 0.2509, step time: 0.2465\n",
      "184/281, train_loss: 0.0634, step time: 0.2425\n",
      "185/281, train_loss: 0.1197, step time: 0.2408\n",
      "186/281, train_loss: 0.0966, step time: 0.2410\n",
      "187/281, train_loss: 0.0744, step time: 0.2452\n",
      "188/281, train_loss: 0.0936, step time: 0.2433\n",
      "189/281, train_loss: 0.0864, step time: 0.2456\n",
      "190/281, train_loss: 0.0527, step time: 0.2421\n",
      "191/281, train_loss: 0.0661, step time: 0.2426\n",
      "192/281, train_loss: 0.0981, step time: 0.2426\n",
      "193/281, train_loss: 0.0736, step time: 0.2381\n",
      "194/281, train_loss: 0.2447, step time: 0.2411\n",
      "195/281, train_loss: 0.0875, step time: 0.2428\n",
      "196/281, train_loss: 0.0608, step time: 0.2413\n",
      "197/281, train_loss: 0.0697, step time: 0.2403\n",
      "198/281, train_loss: 0.0657, step time: 0.2417\n",
      "199/281, train_loss: 0.1318, step time: 0.2455\n",
      "200/281, train_loss: 0.2261, step time: 0.2484\n",
      "201/281, train_loss: 0.1062, step time: 0.2470\n",
      "202/281, train_loss: 0.0451, step time: 0.2433\n",
      "203/281, train_loss: 0.0514, step time: 0.2433\n",
      "204/281, train_loss: 0.2063, step time: 0.2417\n",
      "205/281, train_loss: 0.2273, step time: 0.2451\n",
      "206/281, train_loss: 0.0906, step time: 0.2576\n",
      "207/281, train_loss: 0.0612, step time: 0.2435\n",
      "208/281, train_loss: 0.1228, step time: 0.2450\n",
      "209/281, train_loss: 0.0682, step time: 0.2409\n",
      "210/281, train_loss: 0.0733, step time: 0.2405\n",
      "211/281, train_loss: 0.0700, step time: 0.2394\n",
      "212/281, train_loss: 0.0739, step time: 0.2426\n",
      "213/281, train_loss: 0.0696, step time: 0.2398\n",
      "214/281, train_loss: 0.1448, step time: 0.2421\n",
      "215/281, train_loss: 0.0484, step time: 0.2524\n",
      "216/281, train_loss: 0.0535, step time: 0.2441\n",
      "217/281, train_loss: 0.0595, step time: 0.2408\n",
      "218/281, train_loss: 0.0634, step time: 0.2474\n",
      "219/281, train_loss: 0.2821, step time: 0.2450\n",
      "220/281, train_loss: 0.2124, step time: 0.2450\n",
      "221/281, train_loss: 0.1125, step time: 0.2481\n",
      "222/281, train_loss: 0.0873, step time: 0.2445\n",
      "223/281, train_loss: 0.0509, step time: 0.2396\n",
      "224/281, train_loss: 0.0701, step time: 0.2439\n",
      "225/281, train_loss: 0.0489, step time: 0.2439\n",
      "226/281, train_loss: 0.0761, step time: 0.2430\n",
      "227/281, train_loss: 0.0857, step time: 0.2459\n",
      "228/281, train_loss: 0.0546, step time: 0.2427\n",
      "229/281, train_loss: 0.0709, step time: 0.2405\n",
      "230/281, train_loss: 0.0697, step time: 0.2408\n",
      "231/281, train_loss: 0.1046, step time: 0.2463\n",
      "232/281, train_loss: 0.0731, step time: 0.2463\n",
      "233/281, train_loss: 0.1007, step time: 0.2422\n",
      "234/281, train_loss: 0.0556, step time: 0.2448\n",
      "235/281, train_loss: 0.0549, step time: 0.2453\n",
      "236/281, train_loss: 0.1426, step time: 0.2471\n",
      "237/281, train_loss: 0.0825, step time: 0.2501\n",
      "238/281, train_loss: 0.0663, step time: 0.2460\n",
      "239/281, train_loss: 0.0861, step time: 0.2421\n",
      "240/281, train_loss: 0.0575, step time: 0.2433\n",
      "241/281, train_loss: 0.0543, step time: 0.2429\n",
      "242/281, train_loss: 0.0893, step time: 0.2414\n",
      "243/281, train_loss: 0.0834, step time: 0.2506\n",
      "244/281, train_loss: 0.1090, step time: 0.2453\n",
      "245/281, train_loss: 0.0697, step time: 0.2477\n",
      "246/281, train_loss: 0.0503, step time: 0.2457\n",
      "247/281, train_loss: 0.0759, step time: 0.2385\n",
      "248/281, train_loss: 0.1075, step time: 0.2471\n",
      "249/281, train_loss: 0.0616, step time: 0.2523\n",
      "250/281, train_loss: 0.0617, step time: 0.2498\n",
      "251/281, train_loss: 0.0654, step time: 0.2453\n",
      "252/281, train_loss: 0.0735, step time: 0.2484\n",
      "253/281, train_loss: 0.0647, step time: 0.2504\n",
      "254/281, train_loss: 0.0611, step time: 0.2419\n",
      "255/281, train_loss: 0.0866, step time: 0.2422\n",
      "256/281, train_loss: 0.0839, step time: 0.2433\n",
      "257/281, train_loss: 0.2188, step time: 0.2492\n",
      "258/281, train_loss: 0.0806, step time: 0.2437\n",
      "259/281, train_loss: 0.0666, step time: 0.2454\n",
      "260/281, train_loss: 0.0890, step time: 0.2435\n",
      "261/281, train_loss: 0.0533, step time: 0.2432\n",
      "262/281, train_loss: 0.0805, step time: 0.2445\n",
      "263/281, train_loss: 0.0583, step time: 0.2442\n",
      "264/281, train_loss: 0.0654, step time: 0.2497\n",
      "265/281, train_loss: 0.0613, step time: 0.2501\n",
      "266/281, train_loss: 0.0920, step time: 0.2439\n",
      "267/281, train_loss: 0.0583, step time: 0.2535\n",
      "268/281, train_loss: 0.0722, step time: 0.2499\n",
      "269/281, train_loss: 0.0706, step time: 0.2486\n",
      "270/281, train_loss: 0.2303, step time: 0.2508\n",
      "271/281, train_loss: 0.0684, step time: 0.2474\n",
      "272/281, train_loss: 0.0642, step time: 0.2444\n",
      "273/281, train_loss: 0.0918, step time: 0.2409\n",
      "274/281, train_loss: 0.0721, step time: 0.2402\n",
      "275/281, train_loss: 0.0660, step time: 0.2416\n",
      "276/281, train_loss: 0.0835, step time: 0.2459\n",
      "277/281, train_loss: 0.0711, step time: 0.2434\n",
      "278/281, train_loss: 0.0876, step time: 0.2433\n",
      "279/281, train_loss: 0.0701, step time: 0.2504\n",
      "280/281, train_loss: 0.0817, step time: 0.2415\n",
      "281/281, train_loss: 0.1032, step time: 0.2389\n",
      "282/281, train_loss: 0.1089, step time: 0.1432\n",
      "epoch 163 average loss: 0.0986\n",
      "current epoch: 163 current mean dice: 0.8562 tc: 0.8511 wt: 0.8686 et: 0.8641\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 163 is: 399.6498\n",
      "----------\n",
      "epoch 164/200\n",
      "1/281, train_loss: 0.0689, step time: 0.2569\n",
      "2/281, train_loss: 0.0610, step time: 0.2491\n",
      "3/281, train_loss: 0.0805, step time: 0.2518\n",
      "4/281, train_loss: 0.1446, step time: 0.2459\n",
      "5/281, train_loss: 0.0772, step time: 0.2580\n",
      "6/281, train_loss: 0.0723, step time: 0.2497\n",
      "7/281, train_loss: 0.0572, step time: 0.2533\n",
      "8/281, train_loss: 0.0995, step time: 0.2723\n",
      "9/281, train_loss: 0.2449, step time: 0.2767\n",
      "10/281, train_loss: 0.1038, step time: 0.2584\n",
      "11/281, train_loss: 0.2180, step time: 0.2480\n",
      "12/281, train_loss: 0.1053, step time: 0.2481\n",
      "13/281, train_loss: 0.0695, step time: 0.2562\n",
      "14/281, train_loss: 0.0581, step time: 0.2575\n",
      "15/281, train_loss: 0.0811, step time: 0.2520\n",
      "16/281, train_loss: 0.2158, step time: 0.2509\n",
      "17/281, train_loss: 0.0829, step time: 0.2539\n",
      "18/281, train_loss: 0.0720, step time: 0.2917\n",
      "19/281, train_loss: 0.2091, step time: 0.2611\n",
      "20/281, train_loss: 0.0662, step time: 0.2569\n",
      "21/281, train_loss: 0.0404, step time: 0.2457\n",
      "22/281, train_loss: 0.2204, step time: 0.2452\n",
      "23/281, train_loss: 0.0987, step time: 0.2526\n",
      "24/281, train_loss: 0.0908, step time: 0.2547\n",
      "25/281, train_loss: 0.0738, step time: 0.2499\n",
      "26/281, train_loss: 0.0762, step time: 0.2547\n",
      "27/281, train_loss: 0.0923, step time: 0.2577\n",
      "28/281, train_loss: 0.0871, step time: 0.2570\n",
      "29/281, train_loss: 0.0959, step time: 0.2456\n",
      "30/281, train_loss: 0.0737, step time: 0.2538\n",
      "31/281, train_loss: 0.0486, step time: 0.2554\n",
      "32/281, train_loss: 0.0600, step time: 0.2482\n",
      "33/281, train_loss: 0.0510, step time: 0.2426\n",
      "34/281, train_loss: 0.0745, step time: 0.2511\n",
      "35/281, train_loss: 0.0926, step time: 0.2537\n",
      "36/281, train_loss: 0.2395, step time: 0.2601\n",
      "37/281, train_loss: 0.2285, step time: 0.2569\n",
      "38/281, train_loss: 0.2427, step time: 0.2560\n",
      "39/281, train_loss: 0.0759, step time: 0.2588\n",
      "40/281, train_loss: 0.2234, step time: 0.2604\n",
      "41/281, train_loss: 0.0701, step time: 0.2515\n",
      "42/281, train_loss: 0.1223, step time: 0.2450\n",
      "43/281, train_loss: 0.2185, step time: 0.2459\n",
      "44/281, train_loss: 0.0489, step time: 0.2440\n",
      "45/281, train_loss: 0.0556, step time: 0.2501\n",
      "46/281, train_loss: 0.2477, step time: 0.2592\n",
      "47/281, train_loss: 0.0634, step time: 0.2517\n",
      "48/281, train_loss: 0.1141, step time: 0.2547\n",
      "49/281, train_loss: 0.0952, step time: 0.2538\n",
      "50/281, train_loss: 0.1384, step time: 0.2434\n",
      "51/281, train_loss: 0.0859, step time: 0.2461\n",
      "52/281, train_loss: 0.0649, step time: 0.2513\n",
      "53/281, train_loss: 0.0806, step time: 0.2437\n",
      "54/281, train_loss: 0.0691, step time: 0.2467\n",
      "55/281, train_loss: 0.0738, step time: 0.2478\n",
      "56/281, train_loss: 0.2375, step time: 0.2443\n",
      "57/281, train_loss: 0.0591, step time: 0.2452\n",
      "58/281, train_loss: 0.0819, step time: 0.2498\n",
      "59/281, train_loss: 0.0849, step time: 0.2557\n",
      "60/281, train_loss: 0.2204, step time: 0.2576\n",
      "61/281, train_loss: 0.0807, step time: 0.2593\n",
      "62/281, train_loss: 0.1098, step time: 0.2555\n",
      "63/281, train_loss: 0.0778, step time: 0.2446\n",
      "64/281, train_loss: 0.0692, step time: 0.2438\n",
      "65/281, train_loss: 0.0796, step time: 0.2411\n",
      "66/281, train_loss: 0.0681, step time: 0.2515\n",
      "67/281, train_loss: 0.0518, step time: 0.2544\n",
      "68/281, train_loss: 0.0589, step time: 0.2467\n",
      "69/281, train_loss: 0.0664, step time: 0.2528\n",
      "70/281, train_loss: 0.2516, step time: 0.2487\n",
      "71/281, train_loss: 0.1064, step time: 0.2486\n",
      "72/281, train_loss: 0.0898, step time: 0.2461\n",
      "73/281, train_loss: 0.0503, step time: 0.2513\n",
      "74/281, train_loss: 0.1035, step time: 0.2526\n",
      "75/281, train_loss: 0.0728, step time: 0.2516\n",
      "76/281, train_loss: 0.0682, step time: 0.2504\n",
      "77/281, train_loss: 0.0762, step time: 0.2442\n",
      "78/281, train_loss: 0.0565, step time: 0.2738\n",
      "79/281, train_loss: 0.0727, step time: 0.2462\n",
      "80/281, train_loss: 0.0595, step time: 0.2444\n",
      "81/281, train_loss: 0.0822, step time: 0.2451\n",
      "82/281, train_loss: 0.0740, step time: 0.2518\n",
      "83/281, train_loss: 0.0908, step time: 0.2483\n",
      "84/281, train_loss: 0.0752, step time: 0.2537\n",
      "85/281, train_loss: 0.0592, step time: 0.2503\n",
      "86/281, train_loss: 0.0532, step time: 0.2461\n",
      "87/281, train_loss: 0.0914, step time: 0.2494\n",
      "88/281, train_loss: 0.3809, step time: 0.2459\n",
      "89/281, train_loss: 0.0560, step time: 0.2482\n",
      "90/281, train_loss: 0.4122, step time: 0.2454\n",
      "91/281, train_loss: 0.0716, step time: 0.2428\n",
      "92/281, train_loss: 0.0678, step time: 0.2498\n",
      "93/281, train_loss: 0.0494, step time: 0.2492\n",
      "94/281, train_loss: 0.0887, step time: 0.2485\n",
      "95/281, train_loss: 0.0858, step time: 0.2504\n",
      "96/281, train_loss: 0.2210, step time: 0.2457\n",
      "97/281, train_loss: 0.0640, step time: 0.2432\n",
      "98/281, train_loss: 0.2533, step time: 0.2674\n",
      "99/281, train_loss: 0.0588, step time: 0.2550\n",
      "100/281, train_loss: 0.0697, step time: 0.2419\n",
      "101/281, train_loss: 0.0805, step time: 0.2423\n",
      "102/281, train_loss: 0.2131, step time: 0.2506\n",
      "103/281, train_loss: 0.2418, step time: 0.2454\n",
      "104/281, train_loss: 0.1043, step time: 0.2532\n",
      "105/281, train_loss: 0.0598, step time: 0.2467\n",
      "106/281, train_loss: 0.0668, step time: 0.2490\n",
      "107/281, train_loss: 0.0958, step time: 0.2448\n",
      "108/281, train_loss: 0.0541, step time: 0.2414\n",
      "109/281, train_loss: 0.0508, step time: 0.2407\n",
      "110/281, train_loss: 0.1529, step time: 0.2499\n",
      "111/281, train_loss: 0.0474, step time: 0.2504\n",
      "112/281, train_loss: 0.0673, step time: 0.2518\n",
      "113/281, train_loss: 0.1074, step time: 0.2468\n",
      "114/281, train_loss: 0.0788, step time: 0.2523\n",
      "115/281, train_loss: 0.0567, step time: 0.2508\n",
      "116/281, train_loss: 0.0586, step time: 0.2490\n",
      "117/281, train_loss: 0.0793, step time: 0.2560\n",
      "118/281, train_loss: 0.2081, step time: 0.2495\n",
      "119/281, train_loss: 0.0673, step time: 0.2444\n",
      "120/281, train_loss: 0.0543, step time: 0.2460\n",
      "121/281, train_loss: 0.0629, step time: 0.2501\n",
      "122/281, train_loss: 0.0553, step time: 0.2462\n",
      "123/281, train_loss: 0.2521, step time: 0.2442\n",
      "124/281, train_loss: 0.0733, step time: 0.2428\n",
      "125/281, train_loss: 0.1160, step time: 0.2432\n",
      "126/281, train_loss: 0.1220, step time: 0.2508\n",
      "127/281, train_loss: 0.1221, step time: 0.2504\n",
      "128/281, train_loss: 0.0803, step time: 0.2469\n",
      "129/281, train_loss: 0.0487, step time: 0.2537\n",
      "130/281, train_loss: 0.0821, step time: 0.2449\n",
      "131/281, train_loss: 0.0910, step time: 0.2483\n",
      "132/281, train_loss: 0.0828, step time: 0.2492\n",
      "133/281, train_loss: 0.0830, step time: 0.2479\n",
      "134/281, train_loss: 0.0980, step time: 0.2446\n",
      "135/281, train_loss: 0.2306, step time: 0.2434\n",
      "136/281, train_loss: 0.0609, step time: 0.2485\n",
      "137/281, train_loss: 0.0670, step time: 0.2481\n",
      "138/281, train_loss: 0.0523, step time: 0.2537\n",
      "139/281, train_loss: 0.0755, step time: 0.2676\n",
      "140/281, train_loss: 0.0802, step time: 0.2557\n",
      "141/281, train_loss: 0.0582, step time: 0.2578\n",
      "142/281, train_loss: 0.0974, step time: 0.2537\n",
      "143/281, train_loss: 0.0965, step time: 0.2604\n",
      "144/281, train_loss: 0.0781, step time: 0.2525\n",
      "145/281, train_loss: 0.1033, step time: 0.2511\n",
      "146/281, train_loss: 0.0829, step time: 0.2518\n",
      "147/281, train_loss: 0.0879, step time: 0.2483\n",
      "148/281, train_loss: 0.2301, step time: 0.2484\n",
      "149/281, train_loss: 0.0635, step time: 0.2495\n",
      "150/281, train_loss: 0.0587, step time: 0.2534\n",
      "151/281, train_loss: 0.0920, step time: 0.2758\n",
      "152/281, train_loss: 0.0857, step time: 0.2508\n",
      "153/281, train_loss: 0.2358, step time: 0.2496\n",
      "154/281, train_loss: 0.2368, step time: 0.2531\n",
      "155/281, train_loss: 0.0504, step time: 0.2562\n",
      "156/281, train_loss: 0.0696, step time: 0.2486\n",
      "157/281, train_loss: 0.0724, step time: 0.2542\n",
      "158/281, train_loss: 0.2412, step time: 0.2457\n",
      "159/281, train_loss: 0.0887, step time: 0.2485\n",
      "160/281, train_loss: 0.0457, step time: 0.2490\n",
      "161/281, train_loss: 0.0951, step time: 0.2507\n",
      "162/281, train_loss: 0.0586, step time: 0.2531\n",
      "163/281, train_loss: 0.0581, step time: 0.2467\n",
      "164/281, train_loss: 0.3939, step time: 0.2453\n",
      "165/281, train_loss: 0.2182, step time: 0.2485\n",
      "166/281, train_loss: 0.0778, step time: 0.2530\n",
      "167/281, train_loss: 0.0635, step time: 0.2502\n",
      "168/281, train_loss: 0.0712, step time: 0.2520\n",
      "169/281, train_loss: 0.0478, step time: 0.2510\n",
      "170/281, train_loss: 0.0719, step time: 0.2488\n",
      "171/281, train_loss: 0.0775, step time: 0.2483\n",
      "172/281, train_loss: 0.0563, step time: 0.2517\n",
      "173/281, train_loss: 0.0717, step time: 0.2521\n",
      "174/281, train_loss: 0.0979, step time: 0.2512\n",
      "175/281, train_loss: 0.0573, step time: 0.2489\n",
      "176/281, train_loss: 0.0708, step time: 0.2473\n",
      "177/281, train_loss: 0.0853, step time: 0.2493\n",
      "178/281, train_loss: 0.1415, step time: 0.2461\n",
      "179/281, train_loss: 0.0448, step time: 0.2456\n",
      "180/281, train_loss: 0.1000, step time: 0.2511\n",
      "181/281, train_loss: 0.2450, step time: 0.2485\n",
      "182/281, train_loss: 0.0798, step time: 0.2528\n",
      "183/281, train_loss: 0.0669, step time: 0.2483\n",
      "184/281, train_loss: 0.0627, step time: 0.2457\n",
      "185/281, train_loss: 0.0860, step time: 0.2482\n",
      "186/281, train_loss: 0.0861, step time: 0.2480\n",
      "187/281, train_loss: 0.0728, step time: 0.2526\n",
      "188/281, train_loss: 0.0657, step time: 0.2523\n",
      "189/281, train_loss: 0.0866, step time: 0.2516\n",
      "190/281, train_loss: 0.0678, step time: 0.2540\n",
      "191/281, train_loss: 0.0939, step time: 0.2517\n",
      "192/281, train_loss: 0.0741, step time: 0.2554\n",
      "193/281, train_loss: 0.0626, step time: 0.2526\n",
      "194/281, train_loss: 0.1101, step time: 0.2621\n",
      "195/281, train_loss: 0.0708, step time: 0.2686\n",
      "196/281, train_loss: 0.1213, step time: 0.2691\n",
      "197/281, train_loss: 0.1175, step time: 0.2536\n",
      "198/281, train_loss: 0.0685, step time: 0.2637\n",
      "199/281, train_loss: 0.0673, step time: 0.2502\n",
      "200/281, train_loss: 0.0619, step time: 0.2547\n",
      "201/281, train_loss: 0.0642, step time: 0.2560\n",
      "202/281, train_loss: 0.2358, step time: 0.2610\n",
      "203/281, train_loss: 0.1019, step time: 0.2582\n",
      "204/281, train_loss: 0.1355, step time: 0.2540\n",
      "205/281, train_loss: 0.0652, step time: 0.2483\n",
      "206/281, train_loss: 0.0690, step time: 0.2548\n",
      "207/281, train_loss: 0.0771, step time: 0.2519\n",
      "208/281, train_loss: 0.0842, step time: 0.2465\n",
      "209/281, train_loss: 0.0631, step time: 0.2450\n",
      "210/281, train_loss: 0.0848, step time: 0.2507\n",
      "211/281, train_loss: 0.0712, step time: 0.2518\n",
      "212/281, train_loss: 0.0757, step time: 0.2515\n",
      "213/281, train_loss: 0.1121, step time: 0.2486\n",
      "214/281, train_loss: 0.1022, step time: 0.2523\n",
      "215/281, train_loss: 0.0710, step time: 0.2506\n",
      "216/281, train_loss: 0.0673, step time: 0.2531\n",
      "217/281, train_loss: 0.0641, step time: 0.2540\n",
      "218/281, train_loss: 0.1199, step time: 0.2496\n",
      "219/281, train_loss: 0.0766, step time: 0.2481\n",
      "220/281, train_loss: 0.0744, step time: 0.2497\n",
      "221/281, train_loss: 0.0640, step time: 0.2474\n",
      "222/281, train_loss: 0.2567, step time: 0.2562\n",
      "223/281, train_loss: 0.0812, step time: 0.2480\n",
      "224/281, train_loss: 0.0713, step time: 0.2515\n",
      "225/281, train_loss: 0.0283, step time: 0.2522\n",
      "226/281, train_loss: 0.0552, step time: 0.2520\n",
      "227/281, train_loss: 0.0592, step time: 0.2517\n",
      "228/281, train_loss: 0.0635, step time: 0.2530\n",
      "229/281, train_loss: 0.0934, step time: 0.2554\n",
      "230/281, train_loss: 0.0567, step time: 0.2483\n",
      "231/281, train_loss: 0.0939, step time: 0.2475\n",
      "232/281, train_loss: 0.2724, step time: 0.2467\n",
      "233/281, train_loss: 0.0743, step time: 0.2478\n",
      "234/281, train_loss: 0.0912, step time: 0.2539\n",
      "235/281, train_loss: 0.0779, step time: 0.2539\n",
      "236/281, train_loss: 0.0691, step time: 0.2456\n",
      "237/281, train_loss: 0.1208, step time: 0.2471\n",
      "238/281, train_loss: 0.0832, step time: 0.2508\n",
      "239/281, train_loss: 0.0464, step time: 0.2477\n",
      "240/281, train_loss: 0.0461, step time: 0.2478\n",
      "241/281, train_loss: 0.1316, step time: 0.2511\n",
      "242/281, train_loss: 0.0481, step time: 0.2671\n",
      "243/281, train_loss: 0.1042, step time: 0.2518\n",
      "244/281, train_loss: 0.0754, step time: 0.2482\n",
      "245/281, train_loss: 0.0490, step time: 0.2491\n",
      "246/281, train_loss: 0.0552, step time: 0.2447\n",
      "247/281, train_loss: 0.0945, step time: 0.2486\n",
      "248/281, train_loss: 0.0569, step time: 0.2457\n",
      "249/281, train_loss: 0.2364, step time: 0.2483\n",
      "250/281, train_loss: 0.0516, step time: 0.2452\n",
      "251/281, train_loss: 0.0837, step time: 0.2448\n",
      "252/281, train_loss: 0.2308, step time: 0.2472\n",
      "253/281, train_loss: 0.0908, step time: 0.2443\n",
      "254/281, train_loss: 0.1097, step time: 0.2439\n",
      "255/281, train_loss: 0.1012, step time: 0.2508\n",
      "256/281, train_loss: 0.0818, step time: 0.2486\n",
      "257/281, train_loss: 0.0709, step time: 0.2451\n",
      "258/281, train_loss: 0.0891, step time: 0.2494\n",
      "259/281, train_loss: 0.1031, step time: 0.2514\n",
      "260/281, train_loss: 0.0755, step time: 0.2431\n",
      "261/281, train_loss: 0.1268, step time: 0.2444\n",
      "262/281, train_loss: 0.0950, step time: 0.2503\n",
      "263/281, train_loss: 0.0632, step time: 0.2452\n",
      "264/281, train_loss: 0.2280, step time: 0.2500\n",
      "265/281, train_loss: 0.0444, step time: 0.2483\n",
      "266/281, train_loss: 0.0824, step time: 0.2455\n",
      "267/281, train_loss: 0.0466, step time: 0.2499\n",
      "268/281, train_loss: 0.0931, step time: 0.2482\n",
      "269/281, train_loss: 0.2536, step time: 0.2446\n",
      "270/281, train_loss: 0.0507, step time: 0.2466\n",
      "271/281, train_loss: 0.0774, step time: 0.2452\n",
      "272/281, train_loss: 0.1008, step time: 0.2527\n",
      "273/281, train_loss: 0.0866, step time: 0.2710\n",
      "274/281, train_loss: 0.2255, step time: 0.2531\n",
      "275/281, train_loss: 0.0916, step time: 0.2514\n",
      "276/281, train_loss: 0.0883, step time: 0.2477\n",
      "277/281, train_loss: 0.0765, step time: 0.2484\n",
      "278/281, train_loss: 0.0642, step time: 0.2503\n",
      "279/281, train_loss: 0.1110, step time: 0.2482\n",
      "280/281, train_loss: 0.1299, step time: 0.2490\n",
      "281/281, train_loss: 0.0843, step time: 0.2488\n",
      "282/281, train_loss: 0.0758, step time: 0.1520\n",
      "epoch 164 average loss: 0.1008\n",
      "current epoch: 164 current mean dice: 0.8762 tc: 0.8704 wt: 0.9030 et: 0.8704\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 164 is: 360.3797\n",
      "----------\n",
      "epoch 165/200\n",
      "1/281, train_loss: 0.0573, step time: 0.2575\n",
      "2/281, train_loss: 0.0485, step time: 0.2522\n",
      "3/281, train_loss: 0.0903, step time: 0.2548\n",
      "4/281, train_loss: 0.0590, step time: 0.2495\n",
      "5/281, train_loss: 0.0565, step time: 0.2500\n",
      "6/281, train_loss: 0.0565, step time: 0.2537\n",
      "7/281, train_loss: 0.0756, step time: 0.2639\n",
      "8/281, train_loss: 0.0878, step time: 0.2794\n",
      "9/281, train_loss: 0.0757, step time: 0.2538\n",
      "10/281, train_loss: 0.0866, step time: 0.2608\n",
      "11/281, train_loss: 0.0678, step time: 0.2586\n",
      "12/281, train_loss: 0.1002, step time: 0.2593\n",
      "13/281, train_loss: 0.2445, step time: 0.2522\n",
      "14/281, train_loss: 0.0810, step time: 0.2508\n",
      "15/281, train_loss: 0.0812, step time: 0.2571\n",
      "16/281, train_loss: 0.0823, step time: 0.2559\n",
      "17/281, train_loss: 0.0651, step time: 0.2441\n",
      "18/281, train_loss: 0.0760, step time: 0.2477\n",
      "19/281, train_loss: 0.0564, step time: 0.2479\n",
      "20/281, train_loss: 0.0808, step time: 0.2510\n",
      "21/281, train_loss: 0.0818, step time: 0.2537\n",
      "22/281, train_loss: 0.0878, step time: 0.2688\n",
      "23/281, train_loss: 0.0761, step time: 0.2464\n",
      "24/281, train_loss: 0.1057, step time: 0.2532\n",
      "25/281, train_loss: 0.2560, step time: 0.2517\n",
      "26/281, train_loss: 0.0776, step time: 0.2470\n",
      "27/281, train_loss: 0.0712, step time: 0.2456\n",
      "28/281, train_loss: 0.2143, step time: 0.2592\n",
      "29/281, train_loss: 0.0611, step time: 0.2527\n",
      "30/281, train_loss: 0.2428, step time: 0.2512\n",
      "31/281, train_loss: 0.1032, step time: 0.2478\n",
      "32/281, train_loss: 0.0996, step time: 0.2454\n",
      "33/281, train_loss: 0.0610, step time: 0.2506\n",
      "34/281, train_loss: 0.0819, step time: 0.2537\n",
      "35/281, train_loss: 0.1120, step time: 0.2525\n",
      "36/281, train_loss: 0.0703, step time: 0.2529\n",
      "37/281, train_loss: 0.2166, step time: 0.2504\n",
      "38/281, train_loss: 0.0740, step time: 0.2486\n",
      "39/281, train_loss: 0.0576, step time: 0.2469\n",
      "40/281, train_loss: 0.0797, step time: 0.2470\n",
      "41/281, train_loss: 0.0704, step time: 0.2643\n",
      "42/281, train_loss: 0.0608, step time: 0.2602\n",
      "43/281, train_loss: 0.0765, step time: 0.2516\n",
      "44/281, train_loss: 0.0650, step time: 0.2477\n",
      "45/281, train_loss: 0.0931, step time: 0.2536\n",
      "46/281, train_loss: 0.0587, step time: 0.2504\n",
      "47/281, train_loss: 0.0765, step time: 0.2516\n",
      "48/281, train_loss: 0.2367, step time: 0.2532\n",
      "49/281, train_loss: 0.0686, step time: 0.2564\n",
      "50/281, train_loss: 0.1164, step time: 0.2542\n",
      "51/281, train_loss: 0.0793, step time: 0.2524\n",
      "52/281, train_loss: 0.0478, step time: 0.2467\n",
      "53/281, train_loss: 0.0679, step time: 0.2531\n",
      "54/281, train_loss: 0.2248, step time: 0.2499\n",
      "55/281, train_loss: 0.1018, step time: 0.2454\n",
      "56/281, train_loss: 0.0805, step time: 0.2456\n",
      "57/281, train_loss: 0.0610, step time: 0.2481\n",
      "58/281, train_loss: 0.0864, step time: 0.2508\n",
      "59/281, train_loss: 0.0801, step time: 0.2561\n",
      "60/281, train_loss: 0.2270, step time: 0.2491\n",
      "61/281, train_loss: 0.0982, step time: 0.2528\n",
      "62/281, train_loss: 0.0712, step time: 0.2501\n",
      "63/281, train_loss: 0.0894, step time: 0.2468\n",
      "64/281, train_loss: 0.2287, step time: 0.2474\n",
      "65/281, train_loss: 0.1047, step time: 0.2503\n",
      "66/281, train_loss: 0.0580, step time: 0.2497\n",
      "67/281, train_loss: 0.2200, step time: 0.2491\n",
      "68/281, train_loss: 0.3889, step time: 0.2487\n",
      "69/281, train_loss: 0.1147, step time: 0.2545\n",
      "70/281, train_loss: 0.0646, step time: 0.2523\n",
      "71/281, train_loss: 0.0780, step time: 0.2490\n",
      "72/281, train_loss: 0.0532, step time: 0.2510\n",
      "73/281, train_loss: 0.0536, step time: 0.2571\n",
      "74/281, train_loss: 0.2418, step time: 0.2553\n",
      "75/281, train_loss: 0.0770, step time: 0.2548\n",
      "76/281, train_loss: 0.0690, step time: 0.2493\n",
      "77/281, train_loss: 0.0733, step time: 0.2544\n",
      "78/281, train_loss: 0.0453, step time: 0.2560\n",
      "79/281, train_loss: 0.1011, step time: 0.2593\n",
      "80/281, train_loss: 0.0944, step time: 0.2547\n",
      "81/281, train_loss: 0.0993, step time: 0.2581\n",
      "82/281, train_loss: 0.0847, step time: 0.2535\n",
      "83/281, train_loss: 0.0704, step time: 0.2539\n",
      "84/281, train_loss: 0.0892, step time: 0.2507\n",
      "85/281, train_loss: 0.0514, step time: 0.2519\n",
      "86/281, train_loss: 0.1023, step time: 0.2493\n",
      "87/281, train_loss: 0.0805, step time: 0.2504\n",
      "88/281, train_loss: 0.1102, step time: 0.2533\n",
      "89/281, train_loss: 0.0521, step time: 0.2536\n",
      "90/281, train_loss: 0.0587, step time: 0.2565\n",
      "91/281, train_loss: 0.2170, step time: 0.2576\n",
      "92/281, train_loss: 0.0817, step time: 0.2591\n",
      "93/281, train_loss: 0.2245, step time: 0.2559\n",
      "94/281, train_loss: 0.2289, step time: 0.2584\n",
      "95/281, train_loss: 0.0679, step time: 0.2498\n",
      "96/281, train_loss: 0.1070, step time: 0.2505\n",
      "97/281, train_loss: 0.0629, step time: 0.2499\n",
      "98/281, train_loss: 0.0840, step time: 0.2513\n",
      "99/281, train_loss: 0.0688, step time: 0.2517\n",
      "100/281, train_loss: 0.1365, step time: 0.2526\n",
      "101/281, train_loss: 0.0809, step time: 0.2535\n",
      "102/281, train_loss: 0.0782, step time: 0.2501\n",
      "103/281, train_loss: 0.2324, step time: 0.2488\n",
      "104/281, train_loss: 0.0626, step time: 0.2484\n",
      "105/281, train_loss: 0.0532, step time: 0.2553\n",
      "106/281, train_loss: 0.0706, step time: 0.2807\n",
      "107/281, train_loss: 0.0697, step time: 0.2512\n",
      "108/281, train_loss: 0.1036, step time: 0.2578\n",
      "109/281, train_loss: 0.0622, step time: 0.2548\n",
      "110/281, train_loss: 0.0653, step time: 0.2507\n",
      "111/281, train_loss: 0.0843, step time: 0.2495\n",
      "112/281, train_loss: 0.0520, step time: 0.2510\n",
      "113/281, train_loss: 0.1033, step time: 0.2564\n",
      "114/281, train_loss: 0.0679, step time: 0.2527\n",
      "115/281, train_loss: 0.0721, step time: 0.2481\n",
      "116/281, train_loss: 0.0768, step time: 0.2530\n",
      "117/281, train_loss: 0.0950, step time: 0.2522\n",
      "118/281, train_loss: 0.0992, step time: 0.2513\n",
      "119/281, train_loss: 0.0558, step time: 0.2531\n",
      "120/281, train_loss: 0.0594, step time: 0.2519\n",
      "121/281, train_loss: 0.0710, step time: 0.2529\n",
      "122/281, train_loss: 0.0718, step time: 0.2487\n",
      "123/281, train_loss: 0.2316, step time: 0.2532\n",
      "124/281, train_loss: 0.0828, step time: 0.2498\n",
      "125/281, train_loss: 0.0829, step time: 0.2548\n",
      "126/281, train_loss: 0.0741, step time: 0.2584\n",
      "127/281, train_loss: 0.1037, step time: 0.2570\n",
      "128/281, train_loss: 0.0552, step time: 0.2512\n",
      "129/281, train_loss: 0.2318, step time: 0.2555\n",
      "130/281, train_loss: 0.0947, step time: 0.2519\n",
      "131/281, train_loss: 0.0547, step time: 0.2531\n",
      "132/281, train_loss: 0.0865, step time: 0.2513\n",
      "133/281, train_loss: 0.0897, step time: 0.2506\n",
      "134/281, train_loss: 0.0584, step time: 0.2500\n",
      "135/281, train_loss: 0.2764, step time: 0.2520\n",
      "136/281, train_loss: 0.0754, step time: 0.2499\n",
      "137/281, train_loss: 0.0889, step time: 0.2527\n",
      "138/281, train_loss: 0.1115, step time: 0.2512\n",
      "139/281, train_loss: 0.2174, step time: 0.2495\n",
      "140/281, train_loss: 0.0682, step time: 0.2522\n",
      "141/281, train_loss: 0.0460, step time: 0.2571\n",
      "142/281, train_loss: 0.0614, step time: 0.2542\n",
      "143/281, train_loss: 0.0836, step time: 0.2593\n",
      "144/281, train_loss: 0.0988, step time: 0.2490\n",
      "145/281, train_loss: 0.0614, step time: 0.2547\n",
      "146/281, train_loss: 0.0827, step time: 0.2559\n",
      "147/281, train_loss: 0.0633, step time: 0.2518\n",
      "148/281, train_loss: 0.0668, step time: 0.2440\n",
      "149/281, train_loss: 0.1057, step time: 0.2517\n",
      "150/281, train_loss: 0.0549, step time: 0.2521\n",
      "151/281, train_loss: 0.0562, step time: 0.2531\n",
      "152/281, train_loss: 0.0567, step time: 0.2511\n",
      "153/281, train_loss: 0.1319, step time: 0.2566\n",
      "154/281, train_loss: 0.0807, step time: 0.2556\n",
      "155/281, train_loss: 0.0429, step time: 0.2496\n",
      "156/281, train_loss: 0.1384, step time: 0.2516\n",
      "157/281, train_loss: 0.0803, step time: 0.2544\n",
      "158/281, train_loss: 0.0840, step time: 0.2540\n",
      "159/281, train_loss: 0.0611, step time: 0.2562\n",
      "160/281, train_loss: 0.0597, step time: 0.2555\n",
      "161/281, train_loss: 0.0509, step time: 0.2507\n",
      "162/281, train_loss: 0.0863, step time: 0.2484\n",
      "163/281, train_loss: 0.0692, step time: 0.2534\n",
      "164/281, train_loss: 0.0912, step time: 0.2532\n",
      "165/281, train_loss: 0.0704, step time: 0.2497\n",
      "166/281, train_loss: 0.0914, step time: 0.2562\n",
      "167/281, train_loss: 0.0817, step time: 0.2544\n",
      "168/281, train_loss: 0.2295, step time: 0.2561\n",
      "169/281, train_loss: 0.0896, step time: 0.2514\n",
      "170/281, train_loss: 0.0988, step time: 0.2645\n",
      "171/281, train_loss: 0.0716, step time: 0.2559\n",
      "172/281, train_loss: 0.2366, step time: 0.2508\n",
      "173/281, train_loss: 0.1830, step time: 0.2556\n",
      "174/281, train_loss: 0.1079, step time: 0.2544\n",
      "175/281, train_loss: 0.1147, step time: 0.2545\n",
      "176/281, train_loss: 0.0862, step time: 0.2508\n",
      "177/281, train_loss: 0.0790, step time: 0.2490\n",
      "178/281, train_loss: 0.1392, step time: 0.2519\n",
      "179/281, train_loss: 0.2287, step time: 0.2466\n",
      "180/281, train_loss: 0.0897, step time: 0.2474\n",
      "181/281, train_loss: 0.1000, step time: 0.2498\n",
      "182/281, train_loss: 0.1217, step time: 0.2586\n",
      "183/281, train_loss: 0.0725, step time: 0.2661\n",
      "184/281, train_loss: 0.0631, step time: 0.2498\n",
      "185/281, train_loss: 0.2451, step time: 0.2513\n",
      "186/281, train_loss: 0.0670, step time: 0.2483\n",
      "187/281, train_loss: 0.0757, step time: 0.2512\n",
      "188/281, train_loss: 0.0637, step time: 0.2748\n",
      "189/281, train_loss: 0.2392, step time: 0.2506\n",
      "190/281, train_loss: 0.2702, step time: 0.2507\n",
      "191/281, train_loss: 0.0850, step time: 0.2573\n",
      "192/281, train_loss: 0.0453, step time: 0.2594\n",
      "193/281, train_loss: 0.2265, step time: 0.2519\n",
      "194/281, train_loss: 0.0728, step time: 0.2508\n",
      "195/281, train_loss: 0.1064, step time: 0.2512\n",
      "196/281, train_loss: 0.1304, step time: 0.2549\n",
      "197/281, train_loss: 0.0763, step time: 0.2446\n",
      "198/281, train_loss: 0.1013, step time: 0.2514\n",
      "199/281, train_loss: 0.0683, step time: 0.2547\n",
      "200/281, train_loss: 0.0664, step time: 0.2480\n",
      "201/281, train_loss: 0.0566, step time: 0.2489\n",
      "202/281, train_loss: 0.0809, step time: 0.2505\n",
      "203/281, train_loss: 0.0912, step time: 0.2524\n",
      "204/281, train_loss: 0.0623, step time: 0.2447\n",
      "205/281, train_loss: 0.0815, step time: 0.2491\n",
      "206/281, train_loss: 0.0688, step time: 0.2455\n",
      "207/281, train_loss: 0.0725, step time: 0.2497\n",
      "208/281, train_loss: 0.2357, step time: 0.2493\n",
      "209/281, train_loss: 0.0820, step time: 0.2500\n",
      "210/281, train_loss: 0.0717, step time: 0.2530\n",
      "211/281, train_loss: 0.0775, step time: 0.2494\n",
      "212/281, train_loss: 0.0715, step time: 0.2481\n",
      "213/281, train_loss: 0.0883, step time: 0.2478\n",
      "214/281, train_loss: 0.0970, step time: 0.2481\n",
      "215/281, train_loss: 0.0639, step time: 0.2489\n",
      "216/281, train_loss: 0.0673, step time: 0.2494\n",
      "217/281, train_loss: 0.0739, step time: 0.2504\n",
      "218/281, train_loss: 0.0537, step time: 0.2476\n",
      "219/281, train_loss: 0.0759, step time: 0.2514\n",
      "220/281, train_loss: 0.0625, step time: 0.2551\n",
      "221/281, train_loss: 0.0858, step time: 0.2480\n",
      "222/281, train_loss: 0.0654, step time: 0.2484\n",
      "223/281, train_loss: 0.1055, step time: 0.2506\n",
      "224/281, train_loss: 0.0918, step time: 0.2475\n",
      "225/281, train_loss: 0.2227, step time: 0.2515\n",
      "226/281, train_loss: 0.0568, step time: 0.2514\n",
      "227/281, train_loss: 0.2231, step time: 0.2539\n",
      "228/281, train_loss: 0.1116, step time: 0.2451\n",
      "229/281, train_loss: 0.0724, step time: 0.2443\n",
      "230/281, train_loss: 0.2413, step time: 0.2536\n",
      "231/281, train_loss: 0.1182, step time: 0.2451\n",
      "232/281, train_loss: 0.0854, step time: 0.2438\n",
      "233/281, train_loss: 0.0440, step time: 0.2490\n",
      "234/281, train_loss: 0.0674, step time: 0.2468\n",
      "235/281, train_loss: 0.0770, step time: 0.2501\n",
      "236/281, train_loss: 0.0754, step time: 0.2492\n",
      "237/281, train_loss: 0.0826, step time: 0.2456\n",
      "238/281, train_loss: 0.0741, step time: 0.2529\n",
      "239/281, train_loss: 0.0852, step time: 0.2458\n",
      "240/281, train_loss: 0.1356, step time: 0.2474\n",
      "241/281, train_loss: 0.2226, step time: 0.2478\n",
      "242/281, train_loss: 0.0976, step time: 0.2479\n",
      "243/281, train_loss: 0.0386, step time: 0.2452\n",
      "244/281, train_loss: 0.0815, step time: 0.2447\n",
      "245/281, train_loss: 0.2126, step time: 0.2471\n",
      "246/281, train_loss: 0.1089, step time: 0.2479\n",
      "247/281, train_loss: 0.0696, step time: 0.2502\n",
      "248/281, train_loss: 0.1043, step time: 0.2468\n",
      "249/281, train_loss: 0.0920, step time: 0.2479\n",
      "250/281, train_loss: 0.0617, step time: 0.2509\n",
      "251/281, train_loss: 0.0923, step time: 0.2547\n",
      "252/281, train_loss: 0.2235, step time: 0.2694\n",
      "253/281, train_loss: 0.0910, step time: 0.2479\n",
      "254/281, train_loss: 0.2267, step time: 0.2493\n",
      "255/281, train_loss: 0.0964, step time: 0.2466\n",
      "256/281, train_loss: 0.2577, step time: 0.2473\n",
      "257/281, train_loss: 0.0607, step time: 0.2509\n",
      "258/281, train_loss: 0.0861, step time: 0.2508\n",
      "259/281, train_loss: 0.0957, step time: 0.2500\n",
      "260/281, train_loss: 0.1255, step time: 0.2502\n",
      "261/281, train_loss: 0.0531, step time: 0.2527\n",
      "262/281, train_loss: 0.2409, step time: 0.2454\n",
      "263/281, train_loss: 0.2480, step time: 0.2465\n",
      "264/281, train_loss: 0.1232, step time: 0.2456\n",
      "265/281, train_loss: 0.0977, step time: 0.2465\n",
      "266/281, train_loss: 0.0823, step time: 0.2482\n",
      "267/281, train_loss: 0.0510, step time: 0.2476\n",
      "268/281, train_loss: 0.0528, step time: 0.2506\n",
      "269/281, train_loss: 0.0665, step time: 0.2497\n",
      "270/281, train_loss: 0.0780, step time: 0.2509\n",
      "271/281, train_loss: 0.2634, step time: 0.2481\n",
      "272/281, train_loss: 0.0927, step time: 0.2452\n",
      "273/281, train_loss: 0.0741, step time: 0.2481\n",
      "274/281, train_loss: 0.0744, step time: 0.2496\n",
      "275/281, train_loss: 0.0878, step time: 0.2473\n",
      "276/281, train_loss: 0.0743, step time: 0.2430\n",
      "277/281, train_loss: 0.0747, step time: 0.2426\n",
      "278/281, train_loss: 0.2331, step time: 0.2507\n",
      "279/281, train_loss: 0.0662, step time: 0.2470\n",
      "280/281, train_loss: 0.0713, step time: 0.2450\n",
      "281/281, train_loss: 0.0524, step time: 0.2450\n",
      "282/281, train_loss: 0.0387, step time: 0.1493\n",
      "epoch 165 average loss: 0.1018\n",
      "current epoch: 165 current mean dice: 0.8718 tc: 0.8704 wt: 0.8873 et: 0.8723\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 165 is: 405.0151\n",
      "----------\n",
      "epoch 166/200\n",
      "1/281, train_loss: 0.0829, step time: 0.2484\n",
      "2/281, train_loss: 0.2076, step time: 0.2459\n",
      "3/281, train_loss: 0.0594, step time: 0.2450\n",
      "4/281, train_loss: 0.0736, step time: 0.2460\n",
      "5/281, train_loss: 0.1299, step time: 0.2525\n",
      "6/281, train_loss: 0.0858, step time: 0.2487\n",
      "7/281, train_loss: 0.0741, step time: 0.2481\n",
      "8/281, train_loss: 0.2361, step time: 0.2478\n",
      "9/281, train_loss: 0.0971, step time: 0.2648\n",
      "10/281, train_loss: 0.0571, step time: 0.2600\n",
      "11/281, train_loss: 0.0718, step time: 0.2569\n",
      "12/281, train_loss: 0.0862, step time: 0.2569\n",
      "13/281, train_loss: 0.0658, step time: 0.2548\n",
      "14/281, train_loss: 0.0599, step time: 0.2553\n",
      "15/281, train_loss: 0.0915, step time: 0.2627\n",
      "16/281, train_loss: 0.0962, step time: 0.2562\n",
      "17/281, train_loss: 0.0590, step time: 0.2573\n",
      "18/281, train_loss: 0.0631, step time: 0.2530\n",
      "19/281, train_loss: 0.0746, step time: 0.2809\n",
      "20/281, train_loss: 0.0879, step time: 0.2522\n",
      "21/281, train_loss: 0.1045, step time: 0.2521\n",
      "22/281, train_loss: 0.0727, step time: 0.2516\n",
      "23/281, train_loss: 0.0967, step time: 0.2499\n",
      "24/281, train_loss: 0.0756, step time: 0.2538\n",
      "25/281, train_loss: 0.0628, step time: 0.2439\n",
      "26/281, train_loss: 0.0677, step time: 0.2440\n",
      "27/281, train_loss: 0.0936, step time: 0.2518\n",
      "28/281, train_loss: 0.1157, step time: 0.2472\n",
      "29/281, train_loss: 0.0486, step time: 0.2498\n",
      "30/281, train_loss: 0.0985, step time: 0.2512\n",
      "31/281, train_loss: 0.0732, step time: 0.2569\n",
      "32/281, train_loss: 0.0752, step time: 0.2516\n",
      "33/281, train_loss: 0.0472, step time: 0.2476\n",
      "34/281, train_loss: 0.0824, step time: 0.2503\n",
      "35/281, train_loss: 0.2391, step time: 0.2530\n",
      "36/281, train_loss: 0.0927, step time: 0.2427\n",
      "37/281, train_loss: 0.0574, step time: 0.2579\n",
      "38/281, train_loss: 0.0699, step time: 0.2558\n",
      "39/281, train_loss: 0.0959, step time: 0.2543\n",
      "40/281, train_loss: 0.1127, step time: 0.2437\n",
      "41/281, train_loss: 0.0655, step time: 0.2570\n",
      "42/281, train_loss: 0.0684, step time: 0.2489\n",
      "43/281, train_loss: 0.0499, step time: 0.2509\n",
      "44/281, train_loss: 0.0688, step time: 0.2479\n",
      "45/281, train_loss: 0.0690, step time: 0.2551\n",
      "46/281, train_loss: 0.2740, step time: 0.2538\n",
      "47/281, train_loss: 0.0613, step time: 0.2489\n",
      "48/281, train_loss: 0.0593, step time: 0.2467\n",
      "49/281, train_loss: 0.2325, step time: 0.2513\n",
      "50/281, train_loss: 0.0701, step time: 0.2654\n",
      "51/281, train_loss: 0.0869, step time: 0.2460\n",
      "52/281, train_loss: 0.1029, step time: 0.2435\n",
      "53/281, train_loss: 0.0712, step time: 0.2586\n",
      "54/281, train_loss: 0.2454, step time: 0.2417\n",
      "55/281, train_loss: 0.0774, step time: 0.2401\n",
      "56/281, train_loss: 0.1177, step time: 0.2439\n",
      "57/281, train_loss: 0.0479, step time: 0.2473\n",
      "58/281, train_loss: 0.0613, step time: 0.2463\n",
      "59/281, train_loss: 0.2194, step time: 0.2449\n",
      "60/281, train_loss: 0.2562, step time: 0.2469\n",
      "61/281, train_loss: 0.0755, step time: 0.2549\n",
      "62/281, train_loss: 0.0930, step time: 0.2594\n",
      "63/281, train_loss: 0.2166, step time: 0.2570\n",
      "64/281, train_loss: 0.0605, step time: 0.2439\n",
      "65/281, train_loss: 0.2456, step time: 0.2377\n",
      "66/281, train_loss: 0.0795, step time: 0.2522\n",
      "67/281, train_loss: 0.0795, step time: 0.2506\n",
      "68/281, train_loss: 0.0896, step time: 0.2485\n",
      "69/281, train_loss: 0.0851, step time: 0.2479\n",
      "70/281, train_loss: 0.0920, step time: 0.2464\n",
      "71/281, train_loss: 0.2463, step time: 0.2536\n",
      "72/281, train_loss: 0.0796, step time: 0.2484\n",
      "73/281, train_loss: 0.0633, step time: 0.2451\n",
      "74/281, train_loss: 0.1020, step time: 0.2474\n",
      "75/281, train_loss: 0.0584, step time: 0.2477\n",
      "76/281, train_loss: 0.0549, step time: 0.2482\n",
      "77/281, train_loss: 0.1097, step time: 0.2498\n",
      "78/281, train_loss: 0.1111, step time: 0.2497\n",
      "79/281, train_loss: 0.0795, step time: 0.2445\n",
      "80/281, train_loss: 0.0761, step time: 0.2464\n",
      "81/281, train_loss: 0.0861, step time: 0.2399\n",
      "82/281, train_loss: 0.0797, step time: 0.2472\n",
      "83/281, train_loss: 0.0691, step time: 0.2424\n",
      "84/281, train_loss: 0.0928, step time: 0.2418\n",
      "85/281, train_loss: 0.0653, step time: 0.2457\n",
      "86/281, train_loss: 0.0921, step time: 0.2456\n",
      "87/281, train_loss: 0.0752, step time: 0.2452\n",
      "88/281, train_loss: 0.0788, step time: 0.2423\n",
      "89/281, train_loss: 0.1012, step time: 0.2428\n",
      "90/281, train_loss: 0.0847, step time: 0.2480\n",
      "91/281, train_loss: 0.1035, step time: 0.2479\n",
      "92/281, train_loss: 0.0659, step time: 0.2488\n",
      "93/281, train_loss: 0.2444, step time: 0.2519\n",
      "94/281, train_loss: 0.2385, step time: 0.2475\n",
      "95/281, train_loss: 0.0594, step time: 0.2481\n",
      "96/281, train_loss: 0.0939, step time: 0.2475\n",
      "97/281, train_loss: 0.2331, step time: 0.2448\n",
      "98/281, train_loss: 0.0929, step time: 0.2436\n",
      "99/281, train_loss: 0.0916, step time: 0.2464\n",
      "100/281, train_loss: 0.1174, step time: 0.2465\n",
      "101/281, train_loss: 0.0863, step time: 0.2457\n",
      "102/281, train_loss: 0.1125, step time: 0.2396\n",
      "103/281, train_loss: 0.0634, step time: 0.2467\n",
      "104/281, train_loss: 0.1292, step time: 0.2458\n",
      "105/281, train_loss: 0.0850, step time: 0.2414\n",
      "106/281, train_loss: 0.0942, step time: 0.2458\n",
      "107/281, train_loss: 0.0815, step time: 0.2429\n",
      "108/281, train_loss: 0.0803, step time: 0.2388\n",
      "109/281, train_loss: 0.2417, step time: 0.2432\n",
      "110/281, train_loss: 0.0635, step time: 0.2445\n",
      "111/281, train_loss: 0.0719, step time: 0.2468\n",
      "112/281, train_loss: 0.0459, step time: 0.2427\n",
      "113/281, train_loss: 0.2545, step time: 0.2451\n",
      "114/281, train_loss: 0.2199, step time: 0.2475\n",
      "115/281, train_loss: 0.0468, step time: 0.2472\n",
      "116/281, train_loss: 0.0550, step time: 0.2424\n",
      "117/281, train_loss: 0.0635, step time: 0.2383\n",
      "118/281, train_loss: 0.0788, step time: 0.2480\n",
      "119/281, train_loss: 0.2385, step time: 0.2487\n",
      "120/281, train_loss: 0.1178, step time: 0.2556\n",
      "121/281, train_loss: 0.0653, step time: 0.2467\n",
      "122/281, train_loss: 0.2280, step time: 0.2473\n",
      "123/281, train_loss: 0.2209, step time: 0.2481\n",
      "124/281, train_loss: 0.0657, step time: 0.2470\n",
      "125/281, train_loss: 0.0845, step time: 0.2466\n",
      "126/281, train_loss: 0.0911, step time: 0.2543\n",
      "127/281, train_loss: 0.2250, step time: 0.2556\n",
      "128/281, train_loss: 0.0666, step time: 0.2527\n",
      "129/281, train_loss: 0.2183, step time: 0.2455\n",
      "130/281, train_loss: 0.0814, step time: 0.2473\n",
      "131/281, train_loss: 0.0715, step time: 0.2465\n",
      "132/281, train_loss: 0.0388, step time: 0.2380\n",
      "133/281, train_loss: 0.0724, step time: 0.2418\n",
      "134/281, train_loss: 0.0723, step time: 0.2484\n",
      "135/281, train_loss: 0.1138, step time: 0.2483\n",
      "136/281, train_loss: 0.0852, step time: 0.2462\n",
      "137/281, train_loss: 0.0440, step time: 0.2453\n",
      "138/281, train_loss: 0.2486, step time: 0.2479\n",
      "139/281, train_loss: 0.0718, step time: 0.2538\n",
      "140/281, train_loss: 0.3904, step time: 0.2497\n",
      "141/281, train_loss: 0.0940, step time: 0.2606\n",
      "142/281, train_loss: 0.0873, step time: 0.2452\n",
      "143/281, train_loss: 0.1275, step time: 0.2488\n",
      "144/281, train_loss: 0.0729, step time: 0.2513\n",
      "145/281, train_loss: 0.0434, step time: 0.2416\n",
      "146/281, train_loss: 0.1039, step time: 0.2489\n",
      "147/281, train_loss: 0.0703, step time: 0.2518\n",
      "148/281, train_loss: 0.0569, step time: 0.2476\n",
      "149/281, train_loss: 0.0615, step time: 0.2463\n",
      "150/281, train_loss: 0.0805, step time: 0.2568\n",
      "151/281, train_loss: 0.1036, step time: 0.2487\n",
      "152/281, train_loss: 0.0792, step time: 0.2433\n",
      "153/281, train_loss: 0.0688, step time: 0.2493\n",
      "154/281, train_loss: 0.0894, step time: 0.2474\n",
      "155/281, train_loss: 0.0857, step time: 0.2512\n",
      "156/281, train_loss: 0.0538, step time: 0.2490\n",
      "157/281, train_loss: 0.0688, step time: 0.2503\n",
      "158/281, train_loss: 0.0668, step time: 0.2507\n",
      "159/281, train_loss: 0.0725, step time: 0.2507\n",
      "160/281, train_loss: 0.2172, step time: 0.2508\n",
      "161/281, train_loss: 0.0812, step time: 0.2575\n",
      "162/281, train_loss: 0.0691, step time: 0.2489\n",
      "163/281, train_loss: 0.0941, step time: 0.2442\n",
      "164/281, train_loss: 0.1343, step time: 0.2452\n",
      "165/281, train_loss: 0.0553, step time: 0.2511\n",
      "166/281, train_loss: 0.1028, step time: 0.2576\n",
      "167/281, train_loss: 0.2106, step time: 0.2504\n",
      "168/281, train_loss: 0.0695, step time: 0.2453\n",
      "169/281, train_loss: 0.1324, step time: 0.2469\n",
      "170/281, train_loss: 0.2118, step time: 0.2515\n",
      "171/281, train_loss: 0.2085, step time: 0.2439\n",
      "172/281, train_loss: 0.2877, step time: 0.2446\n",
      "173/281, train_loss: 0.2239, step time: 0.2475\n",
      "174/281, train_loss: 0.0789, step time: 0.2457\n",
      "175/281, train_loss: 0.2373, step time: 0.2462\n",
      "176/281, train_loss: 0.0926, step time: 0.2444\n",
      "177/281, train_loss: 0.1033, step time: 0.2478\n",
      "178/281, train_loss: 0.0569, step time: 0.2475\n",
      "179/281, train_loss: 0.0757, step time: 0.2451\n",
      "180/281, train_loss: 0.0834, step time: 0.2494\n",
      "181/281, train_loss: 0.2267, step time: 0.2473\n",
      "182/281, train_loss: 0.0657, step time: 0.2434\n",
      "183/281, train_loss: 0.0440, step time: 0.2449\n",
      "184/281, train_loss: 0.0653, step time: 0.2473\n",
      "185/281, train_loss: 0.0550, step time: 0.2430\n",
      "186/281, train_loss: 0.0728, step time: 0.2445\n",
      "187/281, train_loss: 0.0491, step time: 0.2425\n",
      "188/281, train_loss: 0.0942, step time: 0.2423\n",
      "189/281, train_loss: 0.0698, step time: 0.2425\n",
      "190/281, train_loss: 0.1013, step time: 0.2456\n",
      "191/281, train_loss: 0.0604, step time: 0.2433\n",
      "192/281, train_loss: 0.0759, step time: 0.2467\n",
      "193/281, train_loss: 0.0552, step time: 0.2469\n",
      "194/281, train_loss: 0.0596, step time: 0.2506\n",
      "195/281, train_loss: 0.0892, step time: 0.2470\n",
      "196/281, train_loss: 0.0624, step time: 0.2495\n",
      "197/281, train_loss: 0.0914, step time: 0.2463\n",
      "198/281, train_loss: 0.0856, step time: 0.2446\n",
      "199/281, train_loss: 0.0973, step time: 0.2443\n",
      "200/281, train_loss: 0.0544, step time: 0.2401\n",
      "201/281, train_loss: 0.0859, step time: 0.2374\n",
      "202/281, train_loss: 0.0764, step time: 0.2471\n",
      "203/281, train_loss: 0.0812, step time: 0.2433\n",
      "204/281, train_loss: 0.1128, step time: 0.2446\n",
      "205/281, train_loss: 0.0547, step time: 0.2460\n",
      "206/281, train_loss: 0.0645, step time: 0.2491\n",
      "207/281, train_loss: 0.0721, step time: 0.2526\n",
      "208/281, train_loss: 0.1200, step time: 0.2451\n",
      "209/281, train_loss: 0.0935, step time: 0.2458\n",
      "210/281, train_loss: 0.0789, step time: 0.2531\n",
      "211/281, train_loss: 0.0591, step time: 0.2459\n",
      "212/281, train_loss: 0.0831, step time: 0.2390\n",
      "213/281, train_loss: 0.1118, step time: 0.2454\n",
      "214/281, train_loss: 0.2236, step time: 0.2469\n",
      "215/281, train_loss: 0.0802, step time: 0.2440\n",
      "216/281, train_loss: 0.0687, step time: 0.2427\n",
      "217/281, train_loss: 0.1440, step time: 0.2506\n",
      "218/281, train_loss: 0.1525, step time: 0.2481\n",
      "219/281, train_loss: 0.0785, step time: 0.2446\n",
      "220/281, train_loss: 0.0791, step time: 0.2469\n",
      "221/281, train_loss: 0.0721, step time: 0.2490\n",
      "222/281, train_loss: 0.1099, step time: 0.2459\n",
      "223/281, train_loss: 0.1032, step time: 0.2500\n",
      "224/281, train_loss: 0.1025, step time: 0.2439\n",
      "225/281, train_loss: 0.0668, step time: 0.2506\n",
      "226/281, train_loss: 0.0709, step time: 0.2461\n",
      "227/281, train_loss: 0.0844, step time: 0.2506\n",
      "228/281, train_loss: 0.0599, step time: 0.2486\n",
      "229/281, train_loss: 0.0559, step time: 0.2513\n",
      "230/281, train_loss: 0.0660, step time: 0.2489\n",
      "231/281, train_loss: 0.2119, step time: 0.2486\n",
      "232/281, train_loss: 0.0519, step time: 0.2498\n",
      "233/281, train_loss: 0.1178, step time: 0.2504\n",
      "234/281, train_loss: 0.0688, step time: 0.2450\n",
      "235/281, train_loss: 0.0735, step time: 0.2507\n",
      "236/281, train_loss: 0.0842, step time: 0.2464\n",
      "237/281, train_loss: 0.0655, step time: 0.2494\n",
      "238/281, train_loss: 0.2095, step time: 0.2492\n",
      "239/281, train_loss: 0.0584, step time: 0.2461\n",
      "240/281, train_loss: 0.1036, step time: 0.2449\n",
      "241/281, train_loss: 0.0945, step time: 0.2506\n",
      "242/281, train_loss: 0.0646, step time: 0.2482\n",
      "243/281, train_loss: 0.2621, step time: 0.2470\n",
      "244/281, train_loss: 0.0609, step time: 0.2490\n",
      "245/281, train_loss: 0.0726, step time: 0.2475\n",
      "246/281, train_loss: 0.0487, step time: 0.2491\n",
      "247/281, train_loss: 0.1166, step time: 0.2446\n",
      "248/281, train_loss: 0.0629, step time: 0.2455\n",
      "249/281, train_loss: 0.0858, step time: 0.2461\n",
      "250/281, train_loss: 0.0723, step time: 0.2465\n",
      "251/281, train_loss: 0.1126, step time: 0.2444\n",
      "252/281, train_loss: 0.0635, step time: 0.2454\n",
      "253/281, train_loss: 0.0855, step time: 0.2513\n",
      "254/281, train_loss: 0.0938, step time: 0.2627\n",
      "255/281, train_loss: 0.0631, step time: 0.2479\n",
      "256/281, train_loss: 0.0700, step time: 0.2456\n",
      "257/281, train_loss: 0.0793, step time: 0.2465\n",
      "258/281, train_loss: 0.0998, step time: 0.2437\n",
      "259/281, train_loss: 0.0554, step time: 0.2458\n",
      "260/281, train_loss: 0.1153, step time: 0.2441\n",
      "261/281, train_loss: 0.0741, step time: 0.2443\n",
      "262/281, train_loss: 0.0607, step time: 0.2486\n",
      "263/281, train_loss: 0.0970, step time: 0.2474\n",
      "264/281, train_loss: 0.2480, step time: 0.2448\n",
      "265/281, train_loss: 0.0731, step time: 0.2490\n",
      "266/281, train_loss: 0.0804, step time: 0.2448\n",
      "267/281, train_loss: 0.0415, step time: 0.2490\n",
      "268/281, train_loss: 0.0821, step time: 0.2462\n",
      "269/281, train_loss: 0.0688, step time: 0.2502\n",
      "270/281, train_loss: 0.0894, step time: 0.2524\n",
      "271/281, train_loss: 0.1093, step time: 0.2469\n",
      "272/281, train_loss: 0.0500, step time: 0.2452\n",
      "273/281, train_loss: 0.0861, step time: 0.2486\n",
      "274/281, train_loss: 0.2341, step time: 0.2536\n",
      "275/281, train_loss: 0.2437, step time: 0.2475\n",
      "276/281, train_loss: 0.0822, step time: 0.2477\n",
      "277/281, train_loss: 0.0674, step time: 0.2453\n",
      "278/281, train_loss: 0.2301, step time: 0.2512\n",
      "279/281, train_loss: 0.0523, step time: 0.2495\n",
      "280/281, train_loss: 0.1053, step time: 0.2508\n",
      "281/281, train_loss: 0.0791, step time: 0.2437\n",
      "282/281, train_loss: 0.0987, step time: 0.1478\n",
      "epoch 166 average loss: 0.1020\n",
      "current epoch: 166 current mean dice: 0.8739 tc: 0.8668 wt: 0.8964 et: 0.8742\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 166 is: 356.1169\n",
      "----------\n",
      "epoch 167/200\n",
      "1/281, train_loss: 0.0573, step time: 0.2525\n",
      "2/281, train_loss: 0.0681, step time: 0.2518\n",
      "3/281, train_loss: 0.0869, step time: 0.2488\n",
      "4/281, train_loss: 0.0622, step time: 0.2520\n",
      "5/281, train_loss: 0.1161, step time: 0.2469\n",
      "6/281, train_loss: 0.0910, step time: 0.2527\n",
      "7/281, train_loss: 0.0974, step time: 0.2528\n",
      "8/281, train_loss: 0.1181, step time: 0.2475\n",
      "9/281, train_loss: 0.1092, step time: 0.2502\n",
      "10/281, train_loss: 0.0833, step time: 0.2512\n",
      "11/281, train_loss: 0.0813, step time: 0.2493\n",
      "12/281, train_loss: 0.0510, step time: 0.2573\n",
      "13/281, train_loss: 0.1124, step time: 0.2532\n",
      "14/281, train_loss: 0.2388, step time: 0.2503\n",
      "15/281, train_loss: 0.0786, step time: 0.2503\n",
      "16/281, train_loss: 0.0714, step time: 0.2539\n",
      "17/281, train_loss: 0.0851, step time: 0.2465\n",
      "18/281, train_loss: 0.0739, step time: 0.2582\n",
      "19/281, train_loss: 0.0810, step time: 0.2593\n",
      "20/281, train_loss: 0.0587, step time: 0.2590\n",
      "21/281, train_loss: 0.2386, step time: 0.2543\n",
      "22/281, train_loss: 0.0697, step time: 0.2466\n",
      "23/281, train_loss: 0.0785, step time: 0.2504\n",
      "24/281, train_loss: 0.0513, step time: 0.2524\n",
      "25/281, train_loss: 0.1042, step time: 0.2455\n",
      "26/281, train_loss: 0.2477, step time: 0.2467\n",
      "27/281, train_loss: 0.2308, step time: 0.2536\n",
      "28/281, train_loss: 0.0793, step time: 0.2550\n",
      "29/281, train_loss: 0.0710, step time: 0.2563\n",
      "30/281, train_loss: 0.0552, step time: 0.2605\n",
      "31/281, train_loss: 0.0675, step time: 0.2747\n",
      "32/281, train_loss: 0.2459, step time: 0.2569\n",
      "33/281, train_loss: 0.0497, step time: 0.2581\n",
      "34/281, train_loss: 0.1309, step time: 0.2519\n",
      "35/281, train_loss: 0.2610, step time: 0.2524\n",
      "36/281, train_loss: 0.0616, step time: 0.2562\n",
      "37/281, train_loss: 0.0743, step time: 0.2463\n",
      "38/281, train_loss: 0.0721, step time: 0.2488\n",
      "39/281, train_loss: 0.0627, step time: 0.2517\n",
      "40/281, train_loss: 0.2366, step time: 0.2518\n",
      "41/281, train_loss: 0.0626, step time: 0.2598\n",
      "42/281, train_loss: 0.0661, step time: 0.2535\n",
      "43/281, train_loss: 0.2246, step time: 0.2515\n",
      "44/281, train_loss: 0.0839, step time: 0.2508\n",
      "45/281, train_loss: 0.0752, step time: 0.2479\n",
      "46/281, train_loss: 0.0579, step time: 0.2502\n",
      "47/281, train_loss: 0.2262, step time: 0.2502\n",
      "48/281, train_loss: 0.1132, step time: 0.2492\n",
      "49/281, train_loss: 0.0693, step time: 0.2491\n",
      "50/281, train_loss: 0.0933, step time: 0.2582\n",
      "51/281, train_loss: 0.0607, step time: 0.2458\n",
      "52/281, train_loss: 0.2189, step time: 0.2523\n",
      "53/281, train_loss: 0.0471, step time: 0.2547\n",
      "54/281, train_loss: 0.0595, step time: 0.2517\n",
      "55/281, train_loss: 0.0766, step time: 0.2512\n",
      "56/281, train_loss: 0.0836, step time: 0.2535\n",
      "57/281, train_loss: 0.0955, step time: 0.2504\n",
      "58/281, train_loss: 0.0818, step time: 0.2537\n",
      "59/281, train_loss: 0.0664, step time: 0.2514\n",
      "60/281, train_loss: 0.0875, step time: 0.2539\n",
      "61/281, train_loss: 0.0534, step time: 0.2492\n",
      "62/281, train_loss: 0.1293, step time: 0.2536\n",
      "63/281, train_loss: 0.0757, step time: 0.2563\n",
      "64/281, train_loss: 0.0713, step time: 0.2492\n",
      "65/281, train_loss: 0.0730, step time: 0.2480\n",
      "66/281, train_loss: 0.2510, step time: 0.2507\n",
      "67/281, train_loss: 0.0753, step time: 0.2486\n",
      "68/281, train_loss: 0.0629, step time: 0.2591\n",
      "69/281, train_loss: 0.0950, step time: 0.2512\n",
      "70/281, train_loss: 0.1028, step time: 0.2480\n",
      "71/281, train_loss: 0.1000, step time: 0.2516\n",
      "72/281, train_loss: 0.0995, step time: 0.2518\n",
      "73/281, train_loss: 0.0576, step time: 0.2542\n",
      "74/281, train_loss: 0.2172, step time: 0.2542\n",
      "75/281, train_loss: 0.1027, step time: 0.2605\n",
      "76/281, train_loss: 0.1057, step time: 0.2518\n",
      "77/281, train_loss: 0.0710, step time: 0.2526\n",
      "78/281, train_loss: 0.0865, step time: 0.2500\n",
      "79/281, train_loss: 0.0751, step time: 0.2517\n",
      "80/281, train_loss: 0.0876, step time: 0.2459\n",
      "81/281, train_loss: 0.0944, step time: 0.2452\n",
      "82/281, train_loss: 0.1031, step time: 0.2501\n",
      "83/281, train_loss: 0.2362, step time: 0.2477\n",
      "84/281, train_loss: 0.0929, step time: 0.2549\n",
      "85/281, train_loss: 0.0781, step time: 0.2560\n",
      "86/281, train_loss: 0.0732, step time: 0.2555\n",
      "87/281, train_loss: 0.1105, step time: 0.2528\n",
      "88/281, train_loss: 0.0805, step time: 0.2526\n",
      "89/281, train_loss: 0.0758, step time: 0.2542\n",
      "90/281, train_loss: 0.0637, step time: 0.2529\n",
      "91/281, train_loss: 0.0810, step time: 0.2534\n",
      "92/281, train_loss: 0.0713, step time: 0.2486\n",
      "93/281, train_loss: 0.0645, step time: 0.2480\n",
      "94/281, train_loss: 0.0893, step time: 0.2489\n",
      "95/281, train_loss: 0.0674, step time: 0.2522\n",
      "96/281, train_loss: 0.0510, step time: 0.2521\n",
      "97/281, train_loss: 0.0609, step time: 0.2517\n",
      "98/281, train_loss: 0.1014, step time: 0.2506\n",
      "99/281, train_loss: 0.1053, step time: 0.2549\n",
      "100/281, train_loss: 0.0740, step time: 0.2439\n",
      "101/281, train_loss: 0.2202, step time: 0.2437\n",
      "102/281, train_loss: 0.0794, step time: 0.2470\n",
      "103/281, train_loss: 0.2257, step time: 0.2504\n",
      "104/281, train_loss: 0.1273, step time: 0.2496\n",
      "105/281, train_loss: 0.0642, step time: 0.2483\n",
      "106/281, train_loss: 0.0468, step time: 0.2436\n",
      "107/281, train_loss: 0.2455, step time: 0.2483\n",
      "108/281, train_loss: 0.0787, step time: 0.2545\n",
      "109/281, train_loss: 0.0986, step time: 0.2533\n",
      "110/281, train_loss: 0.0573, step time: 0.2558\n",
      "111/281, train_loss: 0.1199, step time: 0.2591\n",
      "112/281, train_loss: 0.1313, step time: 0.2505\n",
      "113/281, train_loss: 0.0665, step time: 0.2446\n",
      "114/281, train_loss: 0.0928, step time: 0.2485\n",
      "115/281, train_loss: 0.1223, step time: 0.2532\n",
      "116/281, train_loss: 0.0852, step time: 0.2454\n",
      "117/281, train_loss: 0.0760, step time: 0.2463\n",
      "118/281, train_loss: 0.0935, step time: 0.2530\n",
      "119/281, train_loss: 0.0681, step time: 0.2566\n",
      "120/281, train_loss: 0.0513, step time: 0.2540\n",
      "121/281, train_loss: 0.2203, step time: 0.2558\n",
      "122/281, train_loss: 0.1289, step time: 0.2551\n",
      "123/281, train_loss: 0.0877, step time: 0.2541\n",
      "124/281, train_loss: 0.1034, step time: 0.2665\n",
      "125/281, train_loss: 0.2619, step time: 0.2479\n",
      "126/281, train_loss: 0.0751, step time: 0.2475\n",
      "127/281, train_loss: 0.1081, step time: 0.2499\n",
      "128/281, train_loss: 0.0794, step time: 0.2506\n",
      "129/281, train_loss: 0.0841, step time: 0.2534\n",
      "130/281, train_loss: 0.2432, step time: 0.2559\n",
      "131/281, train_loss: 0.1185, step time: 0.2516\n",
      "132/281, train_loss: 0.0825, step time: 0.2494\n",
      "133/281, train_loss: 0.0975, step time: 0.2479\n",
      "134/281, train_loss: 0.1060, step time: 0.2538\n",
      "135/281, train_loss: 0.0747, step time: 0.2510\n",
      "136/281, train_loss: 0.0908, step time: 0.2506\n",
      "137/281, train_loss: 0.1331, step time: 0.2527\n",
      "138/281, train_loss: 0.0652, step time: 0.2519\n",
      "139/281, train_loss: 0.0841, step time: 0.2488\n",
      "140/281, train_loss: 0.0716, step time: 0.2595\n",
      "141/281, train_loss: 0.1062, step time: 0.2572\n",
      "142/281, train_loss: 0.0905, step time: 0.2711\n",
      "143/281, train_loss: 0.0754, step time: 0.2555\n",
      "144/281, train_loss: 0.2398, step time: 0.2514\n",
      "145/281, train_loss: 0.0582, step time: 0.2550\n",
      "146/281, train_loss: 0.2323, step time: 0.2524\n",
      "147/281, train_loss: 0.2433, step time: 0.2543\n",
      "148/281, train_loss: 0.1086, step time: 0.2562\n",
      "149/281, train_loss: 0.2187, step time: 0.2587\n",
      "150/281, train_loss: 0.1432, step time: 0.2521\n",
      "151/281, train_loss: 0.1349, step time: 0.2507\n",
      "152/281, train_loss: 0.0586, step time: 0.2535\n",
      "153/281, train_loss: 0.0941, step time: 0.2529\n",
      "154/281, train_loss: 0.0954, step time: 0.2564\n",
      "155/281, train_loss: 0.0680, step time: 0.2520\n",
      "156/281, train_loss: 0.1023, step time: 0.2538\n",
      "157/281, train_loss: 0.0969, step time: 0.2495\n",
      "158/281, train_loss: 0.0596, step time: 0.2514\n",
      "159/281, train_loss: 0.0744, step time: 0.2540\n",
      "160/281, train_loss: 0.1131, step time: 0.2504\n",
      "161/281, train_loss: 0.2325, step time: 0.2493\n",
      "162/281, train_loss: 0.1208, step time: 0.2536\n",
      "163/281, train_loss: 0.0763, step time: 0.2707\n",
      "164/281, train_loss: 0.0981, step time: 0.2780\n",
      "165/281, train_loss: 0.0722, step time: 0.2503\n",
      "166/281, train_loss: 0.1146, step time: 0.2549\n",
      "167/281, train_loss: 0.2651, step time: 0.2589\n",
      "168/281, train_loss: 0.0678, step time: 0.2618\n",
      "169/281, train_loss: 0.0789, step time: 0.2548\n",
      "170/281, train_loss: 0.1027, step time: 0.2599\n",
      "171/281, train_loss: 0.2192, step time: 0.2485\n",
      "172/281, train_loss: 0.0663, step time: 0.2540\n",
      "173/281, train_loss: 0.0690, step time: 0.2517\n",
      "174/281, train_loss: 0.2317, step time: 0.2550\n",
      "175/281, train_loss: 0.1115, step time: 0.2512\n",
      "176/281, train_loss: 0.0705, step time: 0.2443\n",
      "177/281, train_loss: 0.0776, step time: 0.2498\n",
      "178/281, train_loss: 0.0714, step time: 0.2469\n",
      "179/281, train_loss: 0.0653, step time: 0.2589\n",
      "180/281, train_loss: 0.0706, step time: 0.2505\n",
      "181/281, train_loss: 0.1042, step time: 0.2483\n",
      "182/281, train_loss: 0.1149, step time: 0.2532\n",
      "183/281, train_loss: 0.0556, step time: 0.2535\n",
      "184/281, train_loss: 0.0882, step time: 0.2511\n",
      "185/281, train_loss: 0.1111, step time: 0.2487\n",
      "186/281, train_loss: 0.0701, step time: 0.2468\n",
      "187/281, train_loss: 0.2110, step time: 0.2491\n",
      "188/281, train_loss: 0.1180, step time: 0.2530\n",
      "189/281, train_loss: 0.0790, step time: 0.2543\n",
      "190/281, train_loss: 0.0854, step time: 0.2484\n",
      "191/281, train_loss: 0.1226, step time: 0.2531\n",
      "192/281, train_loss: 0.0576, step time: 0.2517\n",
      "193/281, train_loss: 0.0947, step time: 0.2568\n",
      "194/281, train_loss: 0.0595, step time: 0.2515\n",
      "195/281, train_loss: 0.2096, step time: 0.2544\n",
      "196/281, train_loss: 0.0665, step time: 0.2528\n",
      "197/281, train_loss: 0.0991, step time: 0.2499\n",
      "198/281, train_loss: 0.0913, step time: 0.2490\n",
      "199/281, train_loss: 0.0657, step time: 0.2543\n",
      "200/281, train_loss: 0.0881, step time: 0.2483\n",
      "201/281, train_loss: 0.0873, step time: 0.2484\n",
      "202/281, train_loss: 0.1016, step time: 0.2501\n",
      "203/281, train_loss: 0.1060, step time: 0.2508\n",
      "204/281, train_loss: 0.0959, step time: 0.2521\n",
      "205/281, train_loss: 0.0541, step time: 0.2518\n",
      "206/281, train_loss: 0.1484, step time: 0.2500\n",
      "207/281, train_loss: 0.2651, step time: 0.2490\n",
      "208/281, train_loss: 0.0628, step time: 0.2510\n",
      "209/281, train_loss: 0.0882, step time: 0.2528\n",
      "210/281, train_loss: 0.0739, step time: 0.2496\n",
      "211/281, train_loss: 0.0757, step time: 0.2481\n",
      "212/281, train_loss: 0.1238, step time: 0.2531\n",
      "213/281, train_loss: 0.0646, step time: 0.2539\n",
      "214/281, train_loss: 0.0975, step time: 0.2493\n",
      "215/281, train_loss: 0.2424, step time: 0.2509\n",
      "216/281, train_loss: 0.0844, step time: 0.2556\n",
      "217/281, train_loss: 0.1026, step time: 0.2680\n",
      "218/281, train_loss: 0.0778, step time: 0.2573\n",
      "219/281, train_loss: 0.0415, step time: 0.2520\n",
      "220/281, train_loss: 0.1124, step time: 0.2531\n",
      "221/281, train_loss: 0.0646, step time: 0.2424\n",
      "222/281, train_loss: 0.0683, step time: 0.2451\n",
      "223/281, train_loss: 0.0558, step time: 0.2512\n",
      "224/281, train_loss: 0.0761, step time: 0.2504\n",
      "225/281, train_loss: 0.0617, step time: 0.2532\n",
      "226/281, train_loss: 0.2303, step time: 0.2572\n",
      "227/281, train_loss: 0.3988, step time: 0.2511\n",
      "228/281, train_loss: 0.1047, step time: 0.2532\n",
      "229/281, train_loss: 0.0661, step time: 0.2506\n",
      "230/281, train_loss: 0.0976, step time: 0.2529\n",
      "231/281, train_loss: 0.0675, step time: 0.2472\n",
      "232/281, train_loss: 0.0546, step time: 0.2531\n",
      "233/281, train_loss: 0.0855, step time: 0.2516\n",
      "234/281, train_loss: 0.0769, step time: 0.2556\n",
      "235/281, train_loss: 0.2114, step time: 0.2532\n",
      "236/281, train_loss: 0.0800, step time: 0.2500\n",
      "237/281, train_loss: 0.0803, step time: 0.2527\n",
      "238/281, train_loss: 0.0842, step time: 0.2527\n",
      "239/281, train_loss: 0.0874, step time: 0.2538\n",
      "240/281, train_loss: 0.0777, step time: 0.2692\n",
      "241/281, train_loss: 0.0721, step time: 0.2497\n",
      "242/281, train_loss: 0.0875, step time: 0.2505\n",
      "243/281, train_loss: 0.0551, step time: 0.2500\n",
      "244/281, train_loss: 0.0600, step time: 0.2501\n",
      "245/281, train_loss: 0.0646, step time: 0.2507\n",
      "246/281, train_loss: 0.1420, step time: 0.2496\n",
      "247/281, train_loss: 0.2337, step time: 0.2491\n",
      "248/281, train_loss: 0.2185, step time: 0.2465\n",
      "249/281, train_loss: 0.0627, step time: 0.2467\n",
      "250/281, train_loss: 0.0401, step time: 0.2492\n",
      "251/281, train_loss: 0.0908, step time: 0.2516\n",
      "252/281, train_loss: 0.0703, step time: 0.2421\n",
      "253/281, train_loss: 0.2225, step time: 0.2495\n",
      "254/281, train_loss: 0.0673, step time: 0.2514\n",
      "255/281, train_loss: 0.2243, step time: 0.2559\n",
      "256/281, train_loss: 0.0596, step time: 0.2503\n",
      "257/281, train_loss: 0.1067, step time: 0.2490\n",
      "258/281, train_loss: 0.0731, step time: 0.2419\n",
      "259/281, train_loss: 0.0810, step time: 0.2453\n",
      "260/281, train_loss: 0.1117, step time: 0.2449\n",
      "261/281, train_loss: 0.0845, step time: 0.2414\n",
      "262/281, train_loss: 0.1149, step time: 0.2427\n",
      "263/281, train_loss: 0.0549, step time: 0.2457\n",
      "264/281, train_loss: 0.1054, step time: 0.2483\n",
      "265/281, train_loss: 0.0788, step time: 0.2561\n",
      "266/281, train_loss: 0.2757, step time: 0.2500\n",
      "267/281, train_loss: 0.0736, step time: 0.2441\n",
      "268/281, train_loss: 0.0665, step time: 0.2483\n",
      "269/281, train_loss: 0.2798, step time: 0.2437\n",
      "270/281, train_loss: 0.0995, step time: 0.2441\n",
      "271/281, train_loss: 0.2258, step time: 0.2490\n",
      "272/281, train_loss: 0.0921, step time: 0.2539\n",
      "273/281, train_loss: 0.0773, step time: 0.2499\n",
      "274/281, train_loss: 0.0767, step time: 0.2494\n",
      "275/281, train_loss: 0.0763, step time: 0.2508\n",
      "276/281, train_loss: 0.0775, step time: 0.2560\n",
      "277/281, train_loss: 0.0807, step time: 0.2511\n",
      "278/281, train_loss: 0.0855, step time: 0.2531\n",
      "279/281, train_loss: 0.2346, step time: 0.2518\n",
      "280/281, train_loss: 0.0819, step time: 0.2470\n",
      "281/281, train_loss: 0.0597, step time: 0.2536\n",
      "282/281, train_loss: 0.0643, step time: 0.1532\n",
      "epoch 167 average loss: 0.1065\n",
      "current epoch: 167 current mean dice: 0.8760 tc: 0.8700 wt: 0.8956 et: 0.8774\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 167 is: 387.9420\n",
      "----------\n",
      "epoch 168/200\n",
      "1/281, train_loss: 0.0920, step time: 0.2588\n",
      "2/281, train_loss: 0.0555, step time: 0.2473\n",
      "3/281, train_loss: 0.0659, step time: 0.2481\n",
      "4/281, train_loss: 0.0684, step time: 0.2501\n",
      "5/281, train_loss: 0.0650, step time: 0.2467\n",
      "6/281, train_loss: 0.0897, step time: 0.2475\n",
      "7/281, train_loss: 0.0749, step time: 0.2488\n",
      "8/281, train_loss: 0.2296, step time: 0.2461\n",
      "9/281, train_loss: 0.2549, step time: 0.2517\n",
      "10/281, train_loss: 0.0833, step time: 0.2493\n",
      "11/281, train_loss: 0.2656, step time: 0.2520\n",
      "12/281, train_loss: 0.1085, step time: 0.2463\n",
      "13/281, train_loss: 0.2410, step time: 0.2475\n",
      "14/281, train_loss: 0.1117, step time: 0.2485\n",
      "15/281, train_loss: 0.2503, step time: 0.2529\n",
      "16/281, train_loss: 0.2115, step time: 0.2567\n",
      "17/281, train_loss: 0.0692, step time: 0.2453\n",
      "18/281, train_loss: 0.0696, step time: 0.2461\n",
      "19/281, train_loss: 0.0831, step time: 0.2497\n",
      "20/281, train_loss: 0.1068, step time: 0.2547\n",
      "21/281, train_loss: 0.1344, step time: 0.2563\n",
      "22/281, train_loss: 0.0929, step time: 0.2610\n",
      "23/281, train_loss: 0.0610, step time: 0.2532\n",
      "24/281, train_loss: 0.0665, step time: 0.2524\n",
      "25/281, train_loss: 0.0920, step time: 0.2508\n",
      "26/281, train_loss: 0.2469, step time: 0.2509\n",
      "27/281, train_loss: 0.1115, step time: 0.2508\n",
      "28/281, train_loss: 0.0936, step time: 0.2489\n",
      "29/281, train_loss: 0.0727, step time: 0.2572\n",
      "30/281, train_loss: 0.1000, step time: 0.2545\n",
      "31/281, train_loss: 0.0806, step time: 0.2472\n",
      "32/281, train_loss: 0.0627, step time: 0.2548\n",
      "33/281, train_loss: 0.0967, step time: 0.2464\n",
      "34/281, train_loss: 0.2207, step time: 0.2487\n",
      "35/281, train_loss: 0.1086, step time: 0.2498\n",
      "36/281, train_loss: 0.1445, step time: 0.2476\n",
      "37/281, train_loss: 0.0713, step time: 0.2531\n",
      "38/281, train_loss: 0.1070, step time: 0.2482\n",
      "39/281, train_loss: 0.0812, step time: 0.2501\n",
      "40/281, train_loss: 0.0720, step time: 0.2489\n",
      "41/281, train_loss: 0.1124, step time: 0.2438\n",
      "42/281, train_loss: 0.0981, step time: 0.2532\n",
      "43/281, train_loss: 0.0700, step time: 0.2430\n",
      "44/281, train_loss: 0.0716, step time: 0.2498\n",
      "45/281, train_loss: 0.0623, step time: 0.2509\n",
      "46/281, train_loss: 0.0853, step time: 0.2518\n",
      "47/281, train_loss: 0.0876, step time: 0.2511\n",
      "48/281, train_loss: 0.0813, step time: 0.2496\n",
      "49/281, train_loss: 0.2866, step time: 0.2468\n",
      "50/281, train_loss: 0.1162, step time: 0.2602\n",
      "51/281, train_loss: 0.1097, step time: 0.2529\n",
      "52/281, train_loss: 0.0815, step time: 0.2492\n",
      "53/281, train_loss: 0.1024, step time: 0.2455\n",
      "54/281, train_loss: 0.2122, step time: 0.2453\n",
      "55/281, train_loss: 0.2380, step time: 0.2458\n",
      "56/281, train_loss: 0.0954, step time: 0.2503\n",
      "57/281, train_loss: 0.0748, step time: 0.2495\n",
      "58/281, train_loss: 0.1023, step time: 0.2470\n",
      "59/281, train_loss: 0.1201, step time: 0.2498\n",
      "60/281, train_loss: 0.0774, step time: 0.2481\n",
      "61/281, train_loss: 0.0696, step time: 0.2494\n",
      "62/281, train_loss: 0.0780, step time: 0.2543\n",
      "63/281, train_loss: 0.1454, step time: 0.2548\n",
      "64/281, train_loss: 0.0707, step time: 0.2434\n",
      "65/281, train_loss: 0.0964, step time: 0.2512\n",
      "66/281, train_loss: 0.0926, step time: 0.2500\n",
      "67/281, train_loss: 0.0918, step time: 0.2465\n",
      "68/281, train_loss: 0.1093, step time: 0.2441\n",
      "69/281, train_loss: 0.1351, step time: 0.2533\n",
      "70/281, train_loss: 0.0838, step time: 0.2471\n",
      "71/281, train_loss: 0.0866, step time: 0.2478\n",
      "72/281, train_loss: 0.1153, step time: 0.2540\n",
      "73/281, train_loss: 0.2369, step time: 0.2452\n",
      "74/281, train_loss: 0.2562, step time: 0.2498\n",
      "75/281, train_loss: 0.1164, step time: 0.2466\n",
      "76/281, train_loss: 0.3779, step time: 0.2422\n",
      "77/281, train_loss: 0.0779, step time: 0.2513\n",
      "78/281, train_loss: 0.0585, step time: 0.2518\n",
      "79/281, train_loss: 0.1064, step time: 0.2463\n",
      "80/281, train_loss: 0.0688, step time: 0.2481\n",
      "81/281, train_loss: 0.0650, step time: 0.2534\n",
      "82/281, train_loss: 0.2776, step time: 0.2478\n",
      "83/281, train_loss: 0.0770, step time: 0.2465\n",
      "84/281, train_loss: 0.1089, step time: 0.2498\n",
      "85/281, train_loss: 0.0768, step time: 0.2451\n",
      "86/281, train_loss: 0.0642, step time: 0.2492\n",
      "87/281, train_loss: 0.1058, step time: 0.2470\n",
      "88/281, train_loss: 0.2357, step time: 0.2439\n",
      "89/281, train_loss: 0.0658, step time: 0.2447\n",
      "90/281, train_loss: 0.2598, step time: 0.2474\n",
      "91/281, train_loss: 0.0577, step time: 0.2421\n",
      "92/281, train_loss: 0.0682, step time: 0.2463\n",
      "93/281, train_loss: 0.2193, step time: 0.2432\n",
      "94/281, train_loss: 0.0837, step time: 0.2472\n",
      "95/281, train_loss: 0.0804, step time: 0.2473\n",
      "96/281, train_loss: 0.1193, step time: 0.2464\n",
      "97/281, train_loss: 0.0602, step time: 0.2452\n",
      "98/281, train_loss: 0.0802, step time: 0.2489\n",
      "99/281, train_loss: 0.0608, step time: 0.2515\n",
      "100/281, train_loss: 0.0725, step time: 0.2465\n",
      "101/281, train_loss: 0.0866, step time: 0.2467\n",
      "102/281, train_loss: 0.1036, step time: 0.2516\n",
      "103/281, train_loss: 0.1074, step time: 0.2553\n",
      "104/281, train_loss: 0.0764, step time: 0.2701\n",
      "105/281, train_loss: 0.2201, step time: 0.2436\n",
      "106/281, train_loss: 0.0473, step time: 0.2408\n",
      "107/281, train_loss: 0.0402, step time: 0.2443\n",
      "108/281, train_loss: 0.0590, step time: 0.2499\n",
      "109/281, train_loss: 0.1487, step time: 0.2480\n",
      "110/281, train_loss: 0.0593, step time: 0.2504\n",
      "111/281, train_loss: 0.0593, step time: 0.2466\n",
      "112/281, train_loss: 0.1069, step time: 0.2436\n",
      "113/281, train_loss: 0.0668, step time: 0.2473\n",
      "114/281, train_loss: 0.0707, step time: 0.2444\n",
      "115/281, train_loss: 0.0666, step time: 0.2465\n",
      "116/281, train_loss: 0.0952, step time: 0.2480\n",
      "117/281, train_loss: 0.2518, step time: 0.2467\n",
      "118/281, train_loss: 0.0614, step time: 0.2464\n",
      "119/281, train_loss: 0.2519, step time: 0.2458\n",
      "120/281, train_loss: 0.0609, step time: 0.2442\n",
      "121/281, train_loss: 0.0899, step time: 0.2431\n",
      "122/281, train_loss: 0.0962, step time: 0.2416\n",
      "123/281, train_loss: 0.0624, step time: 0.2396\n",
      "124/281, train_loss: 0.0960, step time: 0.2426\n",
      "125/281, train_loss: 0.0760, step time: 0.2467\n",
      "126/281, train_loss: 0.2592, step time: 0.2402\n",
      "127/281, train_loss: 0.1481, step time: 0.2398\n",
      "128/281, train_loss: 0.0781, step time: 0.2397\n",
      "129/281, train_loss: 0.0649, step time: 0.2423\n",
      "130/281, train_loss: 0.0818, step time: 0.2436\n",
      "131/281, train_loss: 0.0906, step time: 0.2470\n",
      "132/281, train_loss: 0.1082, step time: 0.2494\n",
      "133/281, train_loss: 0.0589, step time: 0.2435\n",
      "134/281, train_loss: 0.0963, step time: 0.2473\n",
      "135/281, train_loss: 0.0560, step time: 0.2528\n",
      "136/281, train_loss: 0.0820, step time: 0.2434\n",
      "137/281, train_loss: 0.0824, step time: 0.2410\n",
      "138/281, train_loss: 0.0823, step time: 0.2462\n",
      "139/281, train_loss: 0.0588, step time: 0.2572\n",
      "140/281, train_loss: 0.0993, step time: 0.2522\n",
      "141/281, train_loss: 0.0849, step time: 0.2504\n",
      "142/281, train_loss: 0.0657, step time: 0.2482\n",
      "143/281, train_loss: 0.2411, step time: 0.2473\n",
      "144/281, train_loss: 0.1093, step time: 0.2480\n",
      "145/281, train_loss: 0.2408, step time: 0.2494\n",
      "146/281, train_loss: 0.0858, step time: 0.2488\n",
      "147/281, train_loss: 0.0620, step time: 0.2479\n",
      "148/281, train_loss: 0.0771, step time: 0.2481\n",
      "149/281, train_loss: 0.0644, step time: 0.2500\n",
      "150/281, train_loss: 0.0678, step time: 0.2498\n",
      "151/281, train_loss: 0.0798, step time: 0.2486\n",
      "152/281, train_loss: 0.2610, step time: 0.2495\n",
      "153/281, train_loss: 0.2511, step time: 0.2527\n",
      "154/281, train_loss: 0.0950, step time: 0.2529\n",
      "155/281, train_loss: 0.0865, step time: 0.2511\n",
      "156/281, train_loss: 0.1086, step time: 0.2502\n",
      "157/281, train_loss: 0.0740, step time: 0.2494\n",
      "158/281, train_loss: 0.0754, step time: 0.2473\n",
      "159/281, train_loss: 0.0546, step time: 0.2551\n",
      "160/281, train_loss: 0.2254, step time: 0.2472\n",
      "161/281, train_loss: 0.0619, step time: 0.2592\n",
      "162/281, train_loss: 0.0584, step time: 0.2526\n",
      "163/281, train_loss: 0.0951, step time: 0.2445\n",
      "164/281, train_loss: 0.0910, step time: 0.2415\n",
      "165/281, train_loss: 0.2234, step time: 0.2395\n",
      "166/281, train_loss: 0.0614, step time: 0.2436\n",
      "167/281, train_loss: 0.1114, step time: 0.2471\n",
      "168/281, train_loss: 0.0931, step time: 0.2451\n",
      "169/281, train_loss: 0.0802, step time: 0.2453\n",
      "170/281, train_loss: 0.0489, step time: 0.2486\n",
      "171/281, train_loss: 0.0955, step time: 0.2519\n",
      "172/281, train_loss: 0.0774, step time: 0.2497\n",
      "173/281, train_loss: 0.2534, step time: 0.2452\n",
      "174/281, train_loss: 0.0516, step time: 0.2511\n",
      "175/281, train_loss: 0.0880, step time: 0.2462\n",
      "176/281, train_loss: 0.0680, step time: 0.2476\n",
      "177/281, train_loss: 0.0689, step time: 0.2473\n",
      "178/281, train_loss: 0.1303, step time: 0.2427\n",
      "179/281, train_loss: 0.0965, step time: 0.2403\n",
      "180/281, train_loss: 0.1643, step time: 0.2394\n",
      "181/281, train_loss: 0.0507, step time: 0.2391\n",
      "182/281, train_loss: 0.0978, step time: 0.2452\n",
      "183/281, train_loss: 0.0834, step time: 0.2465\n",
      "184/281, train_loss: 0.0700, step time: 0.2457\n",
      "185/281, train_loss: 0.0711, step time: 0.2494\n",
      "186/281, train_loss: 0.0670, step time: 0.2441\n",
      "187/281, train_loss: 0.0897, step time: 0.2436\n",
      "188/281, train_loss: 0.0875, step time: 0.2465\n",
      "189/281, train_loss: 0.0605, step time: 0.2479\n",
      "190/281, train_loss: 0.0642, step time: 0.2510\n",
      "191/281, train_loss: 0.0876, step time: 0.2457\n",
      "192/281, train_loss: 0.0698, step time: 0.2508\n",
      "193/281, train_loss: 0.1128, step time: 0.2458\n",
      "194/281, train_loss: 0.2315, step time: 0.2426\n",
      "195/281, train_loss: 0.0686, step time: 0.2490\n",
      "196/281, train_loss: 0.0763, step time: 0.2405\n",
      "197/281, train_loss: 0.0613, step time: 0.2429\n",
      "198/281, train_loss: 0.1065, step time: 0.2430\n",
      "199/281, train_loss: 0.0670, step time: 0.2479\n",
      "200/281, train_loss: 0.2516, step time: 0.2424\n",
      "201/281, train_loss: 0.0809, step time: 0.2462\n",
      "202/281, train_loss: 0.2378, step time: 0.2475\n",
      "203/281, train_loss: 0.1096, step time: 0.2468\n",
      "204/281, train_loss: 0.0886, step time: 0.2478\n",
      "205/281, train_loss: 0.0798, step time: 0.2455\n",
      "206/281, train_loss: 0.2245, step time: 0.2434\n",
      "207/281, train_loss: 0.1033, step time: 0.2471\n",
      "208/281, train_loss: 0.0869, step time: 0.2525\n",
      "209/281, train_loss: 0.4013, step time: 0.2533\n",
      "210/281, train_loss: 0.0781, step time: 0.2482\n",
      "211/281, train_loss: 0.0618, step time: 0.2468\n",
      "212/281, train_loss: 0.0828, step time: 0.2491\n",
      "213/281, train_loss: 0.0655, step time: 0.2592\n",
      "214/281, train_loss: 0.0927, step time: 0.2573\n",
      "215/281, train_loss: 0.2423, step time: 0.2509\n",
      "216/281, train_loss: 0.0817, step time: 0.2434\n",
      "217/281, train_loss: 0.2480, step time: 0.2499\n",
      "218/281, train_loss: 0.0675, step time: 0.2521\n",
      "219/281, train_loss: 0.1392, step time: 0.2452\n",
      "220/281, train_loss: 0.0820, step time: 0.2440\n",
      "221/281, train_loss: 0.0724, step time: 0.2436\n",
      "222/281, train_loss: 0.0640, step time: 0.2475\n",
      "223/281, train_loss: 0.0754, step time: 0.2426\n",
      "224/281, train_loss: 0.0522, step time: 0.2516\n",
      "225/281, train_loss: 0.0726, step time: 0.2450\n",
      "226/281, train_loss: 0.1052, step time: 0.2473\n",
      "227/281, train_loss: 0.0779, step time: 0.2476\n",
      "228/281, train_loss: 0.0781, step time: 0.2443\n",
      "229/281, train_loss: 0.0670, step time: 0.2461\n",
      "230/281, train_loss: 0.0646, step time: 0.2491\n",
      "231/281, train_loss: 0.1195, step time: 0.2465\n",
      "232/281, train_loss: 0.1074, step time: 0.2454\n",
      "233/281, train_loss: 0.0459, step time: 0.2434\n",
      "234/281, train_loss: 0.1162, step time: 0.2449\n",
      "235/281, train_loss: 0.0638, step time: 0.2531\n",
      "236/281, train_loss: 0.1076, step time: 0.2514\n",
      "237/281, train_loss: 0.0798, step time: 0.2455\n",
      "238/281, train_loss: 0.0973, step time: 0.2505\n",
      "239/281, train_loss: 0.0803, step time: 0.2500\n",
      "240/281, train_loss: 0.1026, step time: 0.2467\n",
      "241/281, train_loss: 0.1298, step time: 0.2453\n",
      "242/281, train_loss: 0.0685, step time: 0.2474\n",
      "243/281, train_loss: 0.0647, step time: 0.2443\n",
      "244/281, train_loss: 0.0591, step time: 0.2514\n",
      "245/281, train_loss: 0.0936, step time: 0.2451\n",
      "246/281, train_loss: 0.0767, step time: 0.2489\n",
      "247/281, train_loss: 0.0646, step time: 0.2531\n",
      "248/281, train_loss: 0.1123, step time: 0.2461\n",
      "249/281, train_loss: 0.2301, step time: 0.2432\n",
      "250/281, train_loss: 0.2517, step time: 0.2446\n",
      "251/281, train_loss: 0.0516, step time: 0.2514\n",
      "252/281, train_loss: 0.0785, step time: 0.2520\n",
      "253/281, train_loss: 0.0651, step time: 0.2471\n",
      "254/281, train_loss: 0.1444, step time: 0.2442\n",
      "255/281, train_loss: 0.0432, step time: 0.2521\n",
      "256/281, train_loss: 0.0745, step time: 0.2505\n",
      "257/281, train_loss: 0.0976, step time: 0.2482\n",
      "258/281, train_loss: 0.2123, step time: 0.2451\n",
      "259/281, train_loss: 0.0877, step time: 0.2534\n",
      "260/281, train_loss: 0.0655, step time: 0.2456\n",
      "261/281, train_loss: 0.2362, step time: 0.2447\n",
      "262/281, train_loss: 0.1043, step time: 0.2513\n",
      "263/281, train_loss: 0.0771, step time: 0.2502\n",
      "264/281, train_loss: 0.1018, step time: 0.2509\n",
      "265/281, train_loss: 0.0713, step time: 0.2443\n",
      "266/281, train_loss: 0.0597, step time: 0.2427\n",
      "267/281, train_loss: 0.0740, step time: 0.2485\n",
      "268/281, train_loss: 0.1035, step time: 0.2467\n",
      "269/281, train_loss: 0.0665, step time: 0.2450\n",
      "270/281, train_loss: 0.0852, step time: 0.2495\n",
      "271/281, train_loss: 0.0459, step time: 0.2525\n",
      "272/281, train_loss: 0.0969, step time: 0.2448\n",
      "273/281, train_loss: 0.2416, step time: 0.2462\n",
      "274/281, train_loss: 0.0923, step time: 0.2435\n",
      "275/281, train_loss: 0.0822, step time: 0.2400\n",
      "276/281, train_loss: 0.0600, step time: 0.2443\n",
      "277/281, train_loss: 0.0705, step time: 0.2479\n",
      "278/281, train_loss: 0.0924, step time: 0.2457\n",
      "279/281, train_loss: 0.0906, step time: 0.2439\n",
      "280/281, train_loss: 0.0561, step time: 0.2430\n",
      "281/281, train_loss: 0.0448, step time: 0.2435\n",
      "282/281, train_loss: 0.1987, step time: 0.1480\n",
      "epoch 168 average loss: 0.1080\n",
      "current epoch: 168 current mean dice: 0.8759 tc: 0.8683 wt: 0.9001 et: 0.8741\n",
      "best mean dice: 0.9048 at epoch: 118\n",
      "time consuming of epoch 168 is: 425.1902\n",
      "Training process is stopped early....\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import os\n",
    "training = True\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "not_improved_epoch = 0\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tc = []\n",
    "metric_values_wt = []\n",
    "metric_values_et = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(cfg.epoch):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{cfg.epoch}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_outputs = inference(val_inputs)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_tc = metric_batch[0].item()\n",
    "            metric_values_tc.append(metric_tc)\n",
    "            metric_wt = metric_batch[1].item()\n",
    "            metric_values_wt.append(metric_wt)\n",
    "            metric_et = metric_batch[2].item()\n",
    "            metric_values_et.append(metric_et)\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                not_improved_epoch = 0\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(output_dir, \"best_metric_model.pth\"),\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "            else:\n",
    "                not_improved_epoch += 1\n",
    "                if not_improved_epoch >= cfg.patience:\n",
    "                    training = False\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "    \n",
    "    if not training:\n",
    "        print(\"Training process is stopped early....\")\n",
    "        break\n",
    "total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train completed, best_metric: 0.9048 at epoch: 118, total time: 64841.09350204468.\n"
     ]
    }
   ],
   "source": [
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(\"train\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Val Mean Dice TC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\n",
    "y = metric_values_tc\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice WT\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\n",
    "y = metric_values_wt\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"brown\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice ET\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_et))]\n",
    "y = metric_values_et\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABaIAAAHcCAYAAAAk+t1cAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzH0lEQVR4nO3de5RWdb0/8M8wwIDAzCAyAxy5DB6XqNhSQQnFKyQZJRpqnkOFlyITTNSTSYWXUhEr46CJ6Um0jmbZqrxkmQsRD4WAeClMRxJSTjojdpoZFAVk9u8Pfzw6DF5meL7M7fVa61nL2Xs/+/nOF5w3z3v2890FWZZlAQAAAAAAiXRq6QEAAAAAANC+KaIBAAAAAEhKEQ0AAAAAQFKKaAAAAAAAklJEAwAAAACQlCIaAAAAAICkFNEAAAAAACSliAYAAAAAIClFNAAAAAAASSmioR247LLLoqCgIF599dWWHkpebPt+AKA9kdcA0DbIbEhDEQ0RccMNN8Stt966S17rL3/5S1x22WXxt7/9bZe8Xnv397//PU499dQoLS2N4uLimDhxYqxZs6alhwVAAvK6baqsrIzzzz8/DjvssOjWrVsUFBSYV4B2Tma3Tb/85S/jM5/5TAwdOjR222232GeffeLCCy+Mmpqalh4a7YQiGmLXh+Tll18uJPPgtddei2OOOSYWL14cX//61+Pyyy+PJ554Io466qj4xz/+0dLDAyDP5HXbtHTp0pg3b15s2LAh9t1335YeDgC7gMxum6ZOnRrPPPNMfPazn4158+bFxz/+8bj++utj9OjR8cYbb7T08GgHOrf0AACa64YbbojVq1fH8uXL45BDDomIiOOPPz6GDx8e3/ve9+Kqq65q4RECACeccELU1NREr1694rvf/W48+eSTLT0kAGAHfvGLX8TRRx/dYNuIESNiypQpcfvtt8cXvvCFlhkY7YYrommTnnjiiTj++OOjuLg4evbsGWPHjo1HH320wTHvtQbSrbfe2uAjoUOGDImnn346Fi9eHAUFBVFQUJD7wbvt2EceeSS+9KUvRZ8+faK4uDg+//nPxz//+c8G5y0oKIjLLrus0esNGTIkTj/99Nz5TjnllIiIOOaYY3Kv9/DDD7/v9/vss8/GqaeeGn379o3u3bvHPvvsE9/4xjcaHVdTUxOnn356lJaWRklJSZxxxhmxcePGBscsWLAgjj322CgrK4uioqLYb7/9Yv78+Tsc9yc/+clYsmRJHHroodGtW7cYOnRo/PjHP97hfP7hD3+ICy64IPr27Rs9evSIk046KdavX9/ovL/97W/jiCOOiB49ekSvXr1iwoQJ8fTTT7/v9/9efvGLX8QhhxySK6EjIoYNGxZjx46Nn//85806JwD5I6/ldUTE7rvvHr169WrWcwHYNWS2zI6IRiV0RMRJJ50UERHPPPNMs84J7+aKaNqcp59+Oo444ogoLi6Oiy66KLp06RI//OEP4+ijj47FixfHqFGjmnS+uXPnxrnnnhs9e/bMBU95eXmDY6ZPnx6lpaVx2WWXRWVlZcyfPz9eeOGFePjhh5u04P+RRx4ZX/nKV2LevHnx9a9/Pffx1Pf7mOqf/vSnOOKII6JLly4xderUGDJkSDz//PNx7733xpVXXtng2FNPPTUqKipi9uzZ8fjjj8d//dd/RVlZWcyZMyd3zPz582P//fePE044ITp37hz33ntvnHPOOVFfXx/Tpk1rcL6//vWvcfLJJ8dZZ50VU6ZMiVtuuSVOP/30GDFiROy///4Njj333HOjd+/ecemll8bf/va3mDt3bkyfPj1+9rOf5Y75yU9+ElOmTInx48fHnDlzYuPGjTF//vwYM2ZMPPHEEzFkyJAPPZf19fXxpz/9Kc4888xG+w499ND4/e9/Hxs2bPDGF6CFyGt5DUDbILNl9vupqqqKiIg99thjp88FkUEbc+KJJ2Zdu3bNnn/++dy2l156KevVq1d25JFH5rZdeuml2Y7+ii9YsCCLiGzt2rW5bfvvv3921FFHveexI0aMyDZv3pzbfs0112QRkd199925bRGRXXrppY3OMXjw4GzKlCm5r++6664sIrJFixZ9qO/3yCOPzHr16pW98MILDbbX19fn/nvb93rmmWc2OOakk07K+vTp02Dbxo0bG73G+PHjs6FDhzYad0RkjzzySG7bK6+8khUVFWUXXnhhbtu2ORo3blyDMZ1//vlZYWFhVlNTk2VZlm3YsCErLS3NvvjFLzZ4naqqqqykpKTB9vf6s3u39evXZxGRfetb32q07wc/+EEWEdmzzz77vucAIB15/baOntfb+853vtPozxWAliWz3yazd+yss87KCgsLs+eee65Zz4d3szQHbcrWrVvj97//fZx44okxdOjQ3Pb+/fvHv//7v8eSJUuirq4u7687derU6NKlS+7rL3/5y9G5c+e4//778/5a77Z+/fp45JFH4swzz4xBgwY12Lej3xKfffbZDb4+4ogj4h//+EeDOenevXvuv2tra+PVV1+No446KtasWRO1tbUNnr/ffvvFEUcckfu6b9++sc8++8SaNWsavfbUqVMbjOmII46IrVu3xgsvvBAREQ8++GDU1NTEv/3bv8Wrr76aexQWFsaoUaNi0aJFH2ZKcrbdKKGoqKjRvm7dujU4BoBdS16/o6PnNQCtm8x+h8xu7I477ogf/ehHceGFF8bee++90+cDS3PQpqxfvz42btwY++yzT6N9++67b9TX18e6desafaRlZ23/A7dnz57Rv3//5Hfl3RZGw4cP/1DHbx+kvXv3joiIf/7zn1FcXBwREX/4wx/i0ksvjaVLlzZa26q2tjZKSkre83zbzrn92l0f9NoREatXr46IiGOPPXaHY982vg9rW9hv2rSp0b4333yzwTEA7Fry+v11pLwGoHWT2e+vI2f2//zP/8RZZ50V48ePb7RkCTSXIpp2673Wldq6desuHceufL3CwsIdbs+yLCIinn/++Rg7dmwMGzYsrr322hg4cGB07do17r///vj+978f9fX1TTpfU47ddu6f/OQn0a9fv0bHde7ctB9Hu+++exQVFcXLL7/caN+2bQMGDGjSOQHY9eT1O9pjXgPQfsjsd7T3zH7qqafihBNOiOHDh8cvfvEL+U/e+JtEm9K3b9/YbbfdorKystG+Z599Njp16hQDBw6MiHd+W1hTUxOlpaW547Z9jOXdPuhmCKtXr45jjjkm9/Vrr70WL7/8cnziE5/Ibevdu3fU1NQ0eN7mzZsbFaVNufHCto9GrVq16kM/5/3ce++9sWnTprjnnnsa/HZ1V3zMdq+99oqIiLKyshg3btxOn69Tp05xwAEHxGOPPdZo37Jly2Lo0KFuVAjQQuT1zmlPeQ1A6yazd057zOznn38+Pv7xj0dZWVncf//90bNnz7ydG6wRTZtSWFgYxx13XNx9990NPrJTXV0dd9xxR4wZMyb38ZNtP5QfeeSR3HGvv/563HbbbY3O26NHj0YB92433XRTbNmyJff1/Pnz46233orjjz8+t22vvfZq8Frbnrf9b2t79OgREfG+r7dN375948gjj4xbbrklXnzxxQb7dvQb0w+y7Teq735ubW1tLFiwoMnnaqrx48dHcXFxXHXVVQ3mcpv169c3+Zwnn3xyrFixokEZXVlZGQ899FCccsopOzVeAJpPXr9DXgPQmsnsd8jsiKqqqjjuuOOiU6dO8cADD0Tfvn3zMVTIcUU0bc4VV1wRDz74YIwZMybOOeec6Ny5c/zwhz+MTZs2xTXXXJM77rjjjotBgwbFWWedFV/96lejsLAwbrnllujbt2+jwBkxYkTMnz8/rrjiivjXf/3XKCsra7DO0ubNm2Ps2LFx6qmnRmVlZdxwww0xZsyYOOGEE3LHfOELX4izzz47Jk2aFB/72MfiqaeeigceeCD22GOPBq914IEHRmFhYcyZMydqa2ujqKgojj322CgrK9vh9ztv3rwYM2ZMHHzwwTF16tSoqKiIv/3tb/Gb3/wmnnzyySbN3XHHHRddu3aNT33qU/GlL30pXnvttbj55pujrKxsh0tc5FNxcXHMnz8/Pve5z8XBBx8cp512Wu7P4je/+U0cfvjhcf311zfpnOecc07cfPPNMWHChPiP//iP6NKlS1x77bVRXl4eF154YaLvBIAPQ17L621qa2vjuuuui4i319GMiLj++uujtLQ0SktLY/r06Xn/PgD48GS2zN7m4x//eKxZsyYuuuiiWLJkSSxZsiS3r7y8PD72sY/l+9ugo8mgDXr88cez8ePHZz179sx222237Jhjjsn++Mc/Njpu5cqV2ahRo7KuXbtmgwYNyq699tpswYIFWURka9euzR1XVVWVTZgwIevVq1cWEdlRRx2VZVmWO3bx4sXZ1KlTs969e2c9e/bMJk+enP3jH/9o8Fpbt27Nvva1r2V77LFHtttuu2Xjx4/P/vrXv2aDBw/OpkyZ0uDYm2++ORs6dGhWWFiYRUS2aNGi9/1+V61alZ100klZaWlp1q1bt2yfffbJZs2aldt/6aWXZhGRrV+/vsHzdvS93nPPPdlHPvKRrFu3btmQIUOyOXPmZLfcckuj4wYPHpxNmDCh0ViOOuqo3Py8+zVWrFjR4LhFixbt8HtbtGhRNn78+KykpCTr1q1bttdee2Wnn3569thjjzX6fj6MdevWZSeffHJWXFyc9ezZM/vkJz+ZrV69+kM9F4C05LW8zrIsW7t2bRYRO3wMHjz4A58PQHoyW2ZnWfaeef3uP0PYGQVZ1ozPHkAHceutt8YZZ5wRK1asiJEjR7b0cACAHZDXANA2yGzo2KwRDQAAAABAUopoAAAAAACSUkQDAAAAAJCUNaIBAAAAAEjKFdEAAAAAACSVrIj+wQ9+EEOGDIlu3brFqFGjYvny5aleCgBoJnkNAK2fvAagPUiyNMfPfvaz+PznPx833nhjjBo1KubOnRt33XVXVFZWRllZ2fs+t76+Pl566aXo1atXFBQU5HtoABBZlsWGDRtiwIAB0alTx/1w0M7kdYTMBiAtef02eQ1Aa9aUvE5SRI8aNSoOOeSQuP766yPi7eAbOHBgnHvuuXHxxRe/73P/93//NwYOHJjvIQFAI+vWrYs999yzpYfRYnYmryNkNgC7hryW1wC0fh8mrzvn+0U3b94cK1eujJkzZ+a2derUKcaNGxdLly5tdPymTZti06ZNua+39eJj4hPRObrke3gAEG/FllgS90evXr1aeigtpql5HSGzAdi15LW8BqD1a0pe572IfvXVV2Pr1q1RXl7eYHt5eXk8++yzjY6fPXt2XH755TsYWJfoXCAkAUjg/38WqCN/PLWpeR0hswHYxeS1vAag9WtCXrf4QlszZ86M2tra3GPdunUtPSQAYAdkNgC0fvIagNYq71dE77HHHlFYWBjV1dUNtldXV0e/fv0aHV9UVBRFRUX5HgYA8D6amtcRMhsAdjV5DUB7kvcrort27RojRoyIhQsX5rbV19fHwoULY/To0fl+OQCgGeQ1ALR+8hqA9iTvV0RHRFxwwQUxZcqUGDlyZBx66KExd+7ceP311+OMM85I8XIAQDPIawBo/eQ1AO1FkiL6M5/5TKxfvz4uueSSqKqqigMPPDB+97vfNbrBAgDQcuQ1ALR+8hqA9qIgy7KspQfxbnV1dVFSUhJHx0R39AUgibeyLfFw3B21tbVRXFzc0sNps2Q2ACnJ6/yQ1wCk1JS8zvsa0QAAAAAA8G6KaAAAAAAAklJEAwAAAACQlCIaAAAAAICkFNEAAAAAACSliAYAAAAAIClFNAAAAAAASSmiAQAAAABIShENAAAAAEBSimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApBTRAAAAAAAkpYgGAAAAACApRTQAAAAAAEkpogEAAAAASEoRDQAAAABAUopoAAAAAACSUkQDAAAAAJCUIhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKUU0AAAAAABJKaIBAAAAAEhKEQ0AAAAAQFKKaAAAAAAAklJEAwAAAACQlCIaAAAAAICkFNEAAAAAACSliAYAAAAAIClFNAAAAAAASSmiAQAAAABIShENAAAAAEBSimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApBTRAAAAAAAkpYgGAAAAACApRTQAAAAAAEkpogEAAAAASEoRDQAAAABAUopoAAAAAACSUkQDAAAAAJCUIhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKUU0AAAAAABJKaIBAAAAAEhKEQ0AAAAAQFKKaAAAAAAAklJEAwAAAACQlCIaAAAAAICkFNEAAAAAACSliAYAAAAAIClFNAAAAAAASSmiAQAAAABIShENAAAAAEBSimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApBTRAAAAAAAkpYgGAAAAACApRTQAAAAAAEkpogEAAAAASEoRDQAAAABAUopoAAAAAACSUkQDAAAAAJCUIhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKUU0AAAAAABJKaIBAAAAAEhKEQ0AAAAAQFKKaAAAAAAAkmpSET179uw45JBDolevXlFWVhYnnnhiVFZWNjjmzTffjGnTpkWfPn2iZ8+eMWnSpKiurs7roAGA9yavAaBtkNkAdCRNKqIXL14c06ZNi0cffTQefPDB2LJlSxx33HHx+uuv5445//zz495774277rorFi9eHC+99FJ8+tOfzvvAAYAdk9cA0DbIbAA6koIsy7LmPnn9+vVRVlYWixcvjiOPPDJqa2ujb9++cccdd8TJJ58cERHPPvts7LvvvrF06dL46Ec/+oHnrKuri5KSkjg6Jkbngi7NHRoAvKe3si3xcNwdtbW1UVxc3NLDSS5FXkfIbADS6mh5HeE9NgBtT1PyeqfWiK6trY2IiN133z0iIlauXBlbtmyJcePG5Y4ZNmxYDBo0KJYuXbozLwUANJO8BoC2QWYD0J51bu4T6+vrY8aMGXH44YfH8OHDIyKiqqoqunbtGqWlpQ2OLS8vj6qqqh2eZ9OmTbFp06bc13V1dc0dEgCwnXzldYTMBoCUvMcGoL1r9hXR06ZNi1WrVsWdd965UwOYPXt2lJSU5B4DBw7cqfMBAO/IV15HyGwASMl7bADau2YV0dOnT4/77rsvFi1aFHvuuWdue79+/WLz5s1RU1PT4Pjq6uro16/fDs81c+bMqK2tzT3WrVvXnCEBANvJZ15HyGwASMV7bAA6giYV0VmWxfTp0+NXv/pVPPTQQ1FRUdFg/4gRI6JLly6xcOHC3LbKysp48cUXY/To0Ts8Z1FRURQXFzd4AADNlyKvI2Q2AOSb99gAdCRNWiN62rRpcccdd8Tdd98dvXr1yq1JVVJSEt27d4+SkpI466yz4oILLojdd989iouL49xzz43Ro0d/qLv5AgA7T14DQNsgswHoSJpURM+fPz8iIo4++ugG2xcsWBCnn356RER8//vfj06dOsWkSZNi06ZNMX78+LjhhhvyMlgA4IPJawBoG2Q2AB1JQZZlWUsP4t3q6uqipKQkjo6J0bmgS0sPB4B26K1sSzwcd0dtba2Pq+4EmQ1ASvI6P+Q1ACk1Ja+bdbNCAAAAAAD4sBTRAAAAAAAkpYgGAAAAACApRTQAAAAAAEkpogEAAAAASEoRDQAAAABAUopoAAAAAACSUkQDAAAAAJCUIhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKUU0AAAAAABJdW7pAQAt44GXnmzyc8YPODDv4wCA1qI52ZjC9nkrswGg9ZPX8MFcEQ0AAAAAQFKKaAAAAAAAklJEAwAAAACQlDWioYPIx7qX1rwCgPRkNgC0fh8maz8oW+U1HY0rogEAAAAASEoRDQAAAABAUopoAAAAAACSUkQDAAAAAJCUmxUCSeXjBg4AkEI+bgrYnshsAGi+XZWR8pq2zBXRAAAAAAAkpYgGAAAAACApRTQAAAAAAElZIxraqba07uX2Y7WeFQD51pZysTWT2QDQ+slrWitXRAMAAAAAkJQiGgAAAACApBTRAAAAAAAkZY1oaAesewkAAADppHjfvf3azd7b0965IhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKWtEQxvU3taN2n5dLADYWe0tK1urD5pnGQ9AW9BS/25I9bryl9bKFdEAAAAAACSliAYAAAAAIClFNAAAAAAASSmiAQAAAABIys0KoQ1oTzdcctMEAGgbdpTZ7enfJADQHniPTVviimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApKwRDQAANGI9aAB4b81Zm1m20tG5IhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKWtEAwAASexoLczmrKkJAPnUUms1b5+B1oymo3FFNAAAAAAASSmiAQAAAABIShENAAAAAEBSimgAAAAAAJJys0JoA9zQAACaRna2Dm5MCEBrlI9/J3yY5/j3CDTkimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApKwRDQAA5IU1oQHgHdaEhoZcEQ0AAAAAQFKKaAAAAAAAklJEAwAAAACQlDWioQ3afv3F1rTulLUhAaDjkPsAtAe76j12c15H1tKeuCIaAAAAAICkFNEAAAAAACSliAYAAAAAIClrREM7sKM1o3bFutHWqgKgrWiprAQA2p4P8143H+s7e09NR+OKaAAAAAAAklJEAwAAAACQlCIaAAAAAICkFNEAAAAAACTlZoXQTrnpAQC8v+2z0s0LG/JvCQB4b3ISms4V0QAAAAAAJKWIBgAAAAAgKUU0AAAAAABJWSMaAADig9d6tIY0AAA0nyuiAQAAAABIShENAAAAAEBSO1VEX3311VFQUBAzZszIbXvzzTdj2rRp0adPn+jZs2dMmjQpqqurd3acAEAzyWsAaBtkNgDtWbPXiF6xYkX88Ic/jI985CMNtp9//vnxm9/8Ju66664oKSmJ6dOnx6c//en4wx/+sNODBQCaRl5D/nzQGtIfRltaZ3pHY83HHAA7JrMBaO+adUX0a6+9FpMnT46bb745evfundteW1sbP/rRj+Laa6+NY489NkaMGBELFiyIP/7xj/Hoo4/mbdAAwAeT1wDQNshsADqCZhXR06ZNiwkTJsS4ceMabF+5cmVs2bKlwfZhw4bFoEGDYunSpTs816ZNm6Kurq7BAwDYefnM6wiZDQCpeI8NQEfQ5KU57rzzznj88cdjxYoVjfZVVVVF165do7S0tMH28vLyqKqq2uH5Zs+eHZdffnlThwEAvI9853WEzAaAFLzHBqCjaNIV0evWrYvzzjsvbr/99ujWrVteBjBz5syora3NPdatW5eX8wJAR5UiryNkNgDkm/fYAHQkTboieuXKlfHKK6/EwQcfnNu2devWeOSRR+L666+PBx54IDZv3hw1NTUNfmNbXV0d/fr12+E5i4qKoqioqHmjBwAaSZHXETIb8mFX3ewvHzdFdGNCSM97bAA6kiYV0WPHjo0///nPDbadccYZMWzYsPja174WAwcOjC5dusTChQtj0qRJERFRWVkZL774YowePTp/owYA3pO8BoC2QWYD0JE0qYju1atXDB8+vMG2Hj16RJ8+fXLbzzrrrLjgggti9913j+Li4jj33HNj9OjR8dGPfjR/owYA3pO8BoC2QWYD0JE0+WaFH+T73/9+dOrUKSZNmhSbNm2K8ePHxw033JDvlwEAdoK8BoC2QWYD0F4UZFmWtfQg3q2uri5KSkri6JgYnQu6tPRwAGiH3sq2xMNxd9TW1kZxcXFLD6fNktkApCSv80NeA5BSU/K60y4aEwAAAAAAHZQiGgAAAACApBTRAAAAAAAkpYgGAAAAACApRTQAAAAAAEkpogEAAAAASEoRDQAAAABAUopoAAAAAACSUkQDAAAAAJCUIhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKUU0AAAAAABJKaIBAAAAAEhKEQ0AAAAAQFKKaAAAAAAAklJEAwAAAACQlCIaAAAAAICkFNEAAAAAACSliAYAAAAAIClFNAAAAAAASSmiAQAAAABIShENAAAAAEBSimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApBTRAAAAAAAkpYgGAAAAACApRTQAAAAAAEkpogEAAAAASEoRDQAAAABAUopoAAAAAACSUkQDAAAAAJCUIhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKUU0AAAAAABJKaIBAAAAAEhKEQ0AAAAAQFKKaAAAAAAAklJEAwAAAACQlCIaAAAAAICkFNEAAAAAACSliAYAAAAAIClFNAAAAAAASSmiAQAAAABIShENAAAAAEBSimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApBTRAAAAAAAkpYgGAAAAACApRTQAAAAAAEkpogEAAAAASEoRDQAAAABAUopoAAAAAACSUkQDAAAAAJCUIhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKUU0AAAAAABJKaIBAAAAAEhKEQ0AAAAAQFKKaAAAAAAAklJEAwAAAACQlCIaAAAAAICkFNEAAAAAACSliAYAAAAAIClFNAAAAAAASSmiAQAAAABIShENAAAAAEBSimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApJpcRP/973+Pz372s9GnT5/o3r17HHDAAfHYY4/l9mdZFpdcckn0798/unfvHuPGjYvVq1fnddAAwPuT1wDQNshsADqKJhXR//znP+Pwww+PLl26xG9/+9v4y1/+Et/73veid+/euWOuueaamDdvXtx4442xbNmy6NGjR4wfPz7efPPNvA8eAGhMXgNA2yCzAehIOjfl4Dlz5sTAgQNjwYIFuW0VFRW5/86yLObOnRvf/OY3Y+LEiRER8eMf/zjKy8vj17/+dZx22ml5GjYA8F7kNQC0DTIbgI6kSVdE33PPPTFy5Mg45ZRToqysLA466KC4+eabc/vXrl0bVVVVMW7cuNy2kpKSGDVqVCxdujR/owYA3pO8BoC2QWYD0JE0qYhes2ZNzJ8/P/bee+944IEH4stf/nJ85Stfidtuuy0iIqqqqiIiory8vMHzysvLc/u2t2nTpqirq2vwAACaL0VeR8hsAMg377EB6EiatDRHfX19jBw5Mq666qqIiDjooINi1apVceONN8aUKVOaNYDZs2fH5Zdf3qznAgCNpcjrCJkNAPnmPTYAHUmTroju379/7Lfffg227bvvvvHiiy9GRES/fv0iIqK6urrBMdXV1bl925s5c2bU1tbmHuvWrWvKkACA7aTI6wiZDQD55j02AB1Jk4roww8/PCorKxtse+6552Lw4MER8fZNFfr16xcLFy7M7a+rq4tly5bF6NGjd3jOoqKiKC4ubvAAAJovRV5HyGwAyDfvsQHoSJq0NMf5558fhx12WFx11VVx6qmnxvLly+Omm26Km266KSIiCgoKYsaMGXHFFVfE3nvvHRUVFTFr1qwYMGBAnHjiiSnGDwBsR14DQNsgswHoSJpURB9yyCHxq1/9KmbOnBnf+ta3oqKiIubOnRuTJ0/OHXPRRRfF66+/HlOnTo2ampoYM2ZM/O53v4tu3brlffAAQGPyGgDaBpkNQEdSkGVZ1tKDeLe6urooKSmJo2NidC7o0tLDAaAdeivbEg/H3VFbW+vjqjtBZgOQkrzOD3kNQEpNyesmrRENAAAAAABNpYgGAAAAACApRTQAAAAAAEkpogEAAAAASEoRDQAAAABAUopoAAAAAACSUkQDAAAAAJCUIhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKUU0AAAAAABJKaIBAAAAAEhKEQ0AAAAAQFKKaAAAAAAAklJEAwAAAACQlCIaAAAAAICkFNEAAAAAACSliAYAAAAAIClFNAAAAAAASSmiAQAAAABIShENAAAAAEBSimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApBTRAAAAAAAkpYgGAAAAACApRTQAAAAAAEkpogEAAAAASEoRDQAAAABAUopoAAAAAACSUkQDAAAAAJCUIhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKUU0AAAAAABJKaIBAAAAAEhKEQ0AAAAAQFKKaAAAAAAAklJEAwAAAACQlCIaAAAAAICkFNEAAAAAACSliAYAAAAAIClFNAAAAAAASSmiAQAAAABIShENAAAAAEBSimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApBTRAAAAAAAkpYgGAAAAACApRTQAAAAAAEkpogEAAAAASEoRDQAAAABAUopoAAAAAACSUkQDAAAAAJCUIhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKUU0AAAAAABJKaIBAAAAAEhKEQ0AAAAAQFKKaAAAAAAAklJEAwAAAACQlCIaAAAAAICkFNEAAAAAACSliAYAAAAAIClFNAAAAAAASSmiAQAAAABIShENAAAAAEBSimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApBTRAAAAAAAkpYgGAAAAACApRTQAAAAAAEk1qYjeunVrzJo1KyoqKqJ79+6x1157xbe//e3Isix3TJZlcckll0T//v2je/fuMW7cuFi9enXeBw4A7Ji8BoC2QWYD0JE0qYieM2dOzJ8/P66//vp45plnYs6cOXHNNdfEddddlzvmmmuuiXnz5sWNN94Yy5Ytix49esT48ePjzTffzPvgAYDG5DUAtA0yG4COpHNTDv7jH/8YEydOjAkTJkRExJAhQ+KnP/1pLF++PCLe/k3t3Llz45vf/GZMnDgxIiJ+/OMfR3l5efz617+O0047Lc/DBwC2J68BoG2Q2QB0JE26Ivqwww6LhQsXxnPPPRcREU899VQsWbIkjj/++IiIWLt2bVRVVcW4ceNyzykpKYlRo0bF0qVLd3jOTZs2RV1dXYMHANB8KfI6QmYDQL55jw1AR9KkK6IvvvjiqKuri2HDhkVhYWFs3bo1rrzyypg8eXJERFRVVUVERHl5eYPnlZeX5/Ztb/bs2XH55Zc3Z+wAwA6kyOsImQ0A+eY9NgAdSZOuiP75z38et99+e9xxxx3x+OOPx2233Rbf/e5347bbbmv2AGbOnBm1tbW5x7p165p9LgAgTV5HyGwAyDfvsQHoSJp0RfRXv/rVuPjii3PrUB1wwAHxwgsvxOzZs2PKlCnRr1+/iIiorq6O/v37555XXV0dBx544A7PWVRUFEVFRc0cPgCwvRR5HSGzASDfvMcGoCNp0hXRGzdujE6dGj6lsLAw6uvrIyKioqIi+vXrFwsXLsztr6uri2XLlsXo0aPzMFwA4IPIawBoG2Q2AB1Jk66I/tSnPhVXXnllDBo0KPbff/944okn4tprr40zzzwzIiIKCgpixowZccUVV8Tee+8dFRUVMWvWrBgwYECceOKJKcYPAGxHXgNA2yCzAehImlREX3fddTFr1qw455xz4pVXXokBAwbEl770pbjkkktyx1x00UXx+uuvx9SpU6OmpibGjBkTv/vd76Jbt255HzwA0Ji8BoC2QWYD0JEUZFmWtfQg3q2uri5KSkri6JgYnQu6tPRwAGiH3sq2xMNxd9TW1kZxcXFLD6fNktkApCSv80NeA5BSU/K6SWtEAwAAAABAUymiAQAAAABIShENAAAAAEBSimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApBTRAAAAAAAkpYgGAAAAACApRTQAAAAAAEkpogEAAAAASEoRDQAAAABAUopoAAAAAACSUkQDAAAAAJCUIhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKUU0AAAAAABJKaIBAAAAAEhKEQ0AAAAAQFKKaAAAAAAAklJEAwAAAACQlCIaAAAAAICkFNEAAAAAACSliAYAAAAAIClFNAAAAAAASSmiAQAAAABIShENAAAAAEBSimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApBTRAAAAAAAkpYgGAAAAACApRTQAAAAAAEkpogEAAAAASEoRDQAAAABAUopoAAAAAACSUkQDAAAAAJCUIhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKUU0AAAAAABJKaIBAAAAAEhKEQ0AAAAAQFKKaAAAAAAAklJEAwAAAACQlCIaAAAAAICkFNEAAAAAACSliAYAAAAAIClFNAAAAAAASSmiAQAAAABIShENAAAAAEBSimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApBTRAAAAAAAkpYgGAAAAACApRTQAAAAAAEkpogEAAAAASKpzSw9ge1mWRUTEW7ElImvhwQDQLr0VWyLincyheWQ2ACnJ6/yQ1wCk1JS8bnVF9IYNGyIiYknc38IjAaC927BhQ5SUlLT0MNosmQ3AriCvd468BmBX+DB5XZC1sl8v19fXx0svvRRZlsWgQYNi3bp1UVxc3NLDajfq6upi4MCB5jWPzGka5jX/zOk7siyLDRs2xIABA6JTJ6tUNZfMTsf/r2mY1/wzp2mY17fJ6/yQ12n5/zX/zGka5jX/zOnbmpLXre6K6E6dOsWee+4ZdXV1ERFRXFzcof8wUzGv+WdO0zCv+WdO3+bKqp0ns9Mzp2mY1/wzp2mYV3mdD/J61zCv+WdO0zCv+WdOP3xe+7UyAAAAAABJKaIBAAAAAEiq1RbRRUVFcemll0ZRUVFLD6VdMa/5Z07TMK/5Z05Jxd+t/DOnaZjX/DOnaZhXUvD3Kg3zmn/mNA3zmn/mtOla3c0KAQAAAABoX1rtFdEAAAAAALQPimgAAAAAAJJSRAMAAAAAkJQiGgAAAACApFptEf2DH/wghgwZEt26dYtRo0bF8uXLW3pIbcbs2bPjkEMOiV69ekVZWVmceOKJUVlZ2eCYN998M6ZNmxZ9+vSJnj17xqRJk6K6urqFRtz2XH311VFQUBAzZszIbTOnzfP3v/89PvvZz0afPn2ie/fuccABB8Rjjz2W259lWVxyySXRv3//6N69e4wbNy5Wr17dgiNu/bZu3RqzZs2KioqK6N69e+y1117x7W9/O959b1rzSr7I650js9OT2fkhr/NPXrOryezmk9fpyev8kdn5Ja/zLGuF7rzzzqxr167ZLbfckj399NPZF7/4xay0tDSrrq5u6aG1CePHj88WLFiQrVq1KnvyySezT3ziE9mgQYOy1157LXfM2WefnQ0cODBbuHBh9thjj2Uf/ehHs8MOO6wFR912LF++PBsyZEj2kY98JDvvvPNy281p0/3f//1fNnjw4Oz000/Pli1blq1ZsyZ74IEHsr/+9a+5Y66++uqspKQk+/Wvf5099dRT2QknnJBVVFRkb7zxRguOvHW78sorsz59+mT33Xdftnbt2uyuu+7Kevbsmf3nf/5n7hjzSj7I650ns9OS2fkhr9OQ1+xKMnvnyOu05HX+yOz8k9f51SqL6EMPPTSbNm1a7uutW7dmAwYMyGbPnt2Co2q7XnnllSwissWLF2dZlmU1NTVZly5dsrvuuit3zDPPPJNFRLZ06dKWGmabsGHDhmzvvffOHnzwweyoo47KhaQ5bZ6vfe1r2ZgxY95zf319fdavX7/sO9/5Tm5bTU1NVlRUlP30pz/dFUNskyZMmJCdeeaZDbZ9+tOfziZPnpxlmXklf+R1/sns/JHZ+SOv05DX7EoyO7/kdf7I6/yS2fknr/Or1S3NsXnz5li5cmWMGzcut61Tp04xbty4WLp0aQuOrO2qra2NiIjdd989IiJWrlwZW7ZsaTDHw4YNi0GDBpnjDzBt2rSYMGFCg7mLMKfNdc8998TIkSPjlFNOibKysjjooIPi5ptvzu1fu3ZtVFVVNZjXkpKSGDVqlHl9H4cddlgsXLgwnnvuuYiIeOqpp2LJkiVx/PHHR4R5JT/kdRoyO39kdv7I6zTkNbuKzM4/eZ0/8jq/ZHb+yev86tzSA9jeq6++Glu3bo3y8vIG28vLy+PZZ59toVG1XfX19TFjxow4/PDDY/jw4RERUVVVFV27do3S0tIGx5aXl0dVVVULjLJtuPPOO+Pxxx+PFStWNNpnTptnzZo1MX/+/Ljgggvi61//eqxYsSK+8pWvRNeuXWPKlCm5udvRzwPz+t4uvvjiqKuri2HDhkVhYWFs3bo1rrzyypg8eXJEhHklL+R1/sns/JHZ+SWv05DX7CoyO7/kdf7I6/yT2fknr/Or1RXR5Ne0adNi1apVsWTJkpYeSpu2bt26OO+88+LBBx+Mbt26tfRw2o36+voYOXJkXHXVVRERcdBBB8WqVavixhtvjClTprTw6Nqun//853H77bfHHXfcEfvvv388+eSTMWPGjBgwYIB5hVZMZueHzM4/eZ2GvIa2SV7nh7xOQ2bnn7zOr1a3NMcee+wRhYWFje6EWl1dHf369WuhUbVN06dPj/vuuy8WLVoUe+65Z257v379YvPmzVFTU9PgeHP83lauXBmvvPJKHHzwwdG5c+fo3LlzLF68OObNmxedO3eO8vJyc9oM/fv3j/3226/Btn333TdefPHFiIjc3Pl50DRf/epX4+KLL47TTjstDjjggPjc5z4X559/fsyePTsizCv5Ia/zS2bnj8zOP3mdhrxmV5HZ+SOv80depyGz809e51erK6K7du0aI0aMiIULF+a21dfXx8KFC2P06NEtOLK2I8uymD59evzqV7+Khx56KCoqKhrsHzFiRHTp0qXBHFdWVsaLL75ojt/D2LFj489//nM8+eSTucfIkSNj8uTJuf82p013+OGHR2VlZYNtzz33XAwePDgiIioqKqJfv34N5rWuri6WLVtmXt/Hxo0bo1Onhj/eCwsLo76+PiLMK/khr/NDZuefzM4/eZ2GvGZXkdk7T17nn7xOQ2bnn7zOsxa+WeIO3XnnnVlRUVF26623Zn/5y1+yqVOnZqWlpVlVVVVLD61N+PKXv5yVlJRkDz/8cPbyyy/nHhs3bswdc/bZZ2eDBg3KHnrooeyxxx7LRo8enY0ePboFR932vPuOvllmTptj+fLlWefOnbMrr7wyW716dXb77bdnu+22W/bf//3fuWOuvvrqrLS0NLv77ruzP/3pT9nEiROzioqK7I033mjBkbduU6ZMyf7lX/4lu++++7K1a9dmv/zlL7M99tgju+iii3LHmFfyQV7vPJm9a8jsnSOv05DX7Eoye+fI611DXu88mZ1/8jq/WmURnWVZdt1112WDBg3Kunbtmh166KHZo48+2tJDajMiYoePBQsW5I554403snPOOSfr3bt3tttuu2UnnXRS9vLLL7fcoNug7UPSnDbPvffemw0fPjwrKirKhg0blt10000N9tfX12ezZs3KysvLs6Kiomzs2LFZZWVlC422bairq8vOO++8bNCgQVm3bt2yoUOHZt/4xjeyTZs25Y4xr+SLvN45MnvXkNk7T17nn7xmV5PZzSevdw15nR8yO7/kdX4VZFmW7eqrsAEAAAAA6Dha3RrRAAAAAAC0L4poAAAAAACSUkQDAAAAAJCUIhoAAAAAgKQU0QAAAAAAJKWIBgAAAAAgKUU0AAAAAABJKaIBAAAAAEhKEQ0AAAAAQFKKaAAAAAAAklJEAwAAAACQlCIaAAAAAICk/h+gM+7HhhqF2gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(output_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "                     \n",
    "with torch.no_grad():\n",
    "    # select one image to evaluate and visualize the model output\n",
    "    val_data = val_ds[1]\n",
    "    val_input = val_data[\"image\"].unsqueeze(0).to(device)\n",
    "    val_output = inference(val_input)\n",
    "    val_output = post_trans(val_output[0])\n",
    "    plt.figure(\"image\", (24, 6))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.title(f\"image channel {i}\")\n",
    "        plt.imshow(val_data[\"image\"][i, :, :, val_input.shape[-1] // 2].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "    # visualize the 3 channels label corresponding to this image\n",
    "    plt.figure(\"label\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(val_data[\"label\"][i, :, :, val_input.shape[-1] // 2].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"output channel {i}\")\n",
    "        plt.imshow(val_output[i, :, :, val_input.shape[-1] // 2].detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric on test image:  0.9408921003341675\n",
      "metric_tc: 0.9583\n",
      "metric_wt: 0.9520\n",
      "metric_et: 0.9049\n"
     ]
    }
   ],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=[\"bilinear\", \"nearest\"]\n",
    "        ),\n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=cfg.unetr.img_shape, mode=\"nearest\"),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_ds = BratsDataset(\n",
    "    root_dir=root_dir,\n",
    "    section=\"test\",\n",
    "    transform=transform,\n",
    "    cache_num=0\n",
    ")\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "##### model.load_state_dict(torch.load(os.path.join(output_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_inputs, test_labels = (\n",
    "            test_data[\"image\"].to(device),\n",
    "            test_data[\"label\"].to(device),\n",
    "        )\n",
    "        test_outputs = inference(test_inputs)\n",
    "        test_outputs = [post_trans(i) for i in decollate_batch(test_outputs)]\n",
    "        dice_metric(y_pred=test_outputs, y=test_labels)\n",
    "        dice_metric_batch(y_pred=test_outputs, y=test_labels)\n",
    "\n",
    "    metric = dice_metric.aggregate().item()\n",
    "    metric_batch = dice_metric_batch.aggregate()\n",
    "    dice_metric.reset()\n",
    "    dice_metric_batch.reset()\n",
    "\n",
    "metric_tc, metric_wt, metric_et = metric_batch[0].item(), metric_batch[1].item(), metric_batch[2].item()\n",
    "\n",
    "print(\"Metric on test image: \", metric)\n",
    "print(f\"metric_tc: {metric_tc:.4f}\")\n",
    "print(f\"metric_wt: {metric_wt:.4f}\")\n",
    "print(f\"metric_et: {metric_et:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
